Model Name,Estimator Class,Dataset Size,Full Class Path,Estimator Type,Part_1,Part_2,Part_3,Part_4,Part_5,Sklearn Desc
ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>,"Small, Medium",sklearn.linear_model._bayes.ARDRegression,regressor,sklearn,linear_model,_bayes,ARDRegression,,"Bayesian ARD regression.  Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures (Evidence Maximization)  Read more in the :ref:`User Guide <bayesian_regression>`.  Parameters ---------- max_iter : int, default=300 Maximum number of iterations."
AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>,"Small, Medium",sklearn.ensemble._weight_boosting.AdaBoostClassifier,classifier,sklearn,ensemble,_weight_boosting,AdaBoostClassifier,,An AdaBoost classifier.  An AdaBoost [1]_ classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.  This class implements the algorithm based on [2]_.  Read more in the :ref:`User Guide <adaboost>`.
AdaBoostRegressor,<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>,"Small, Medium",sklearn.ensemble._weight_boosting.AdaBoostRegressor,regressor,sklearn,ensemble,_weight_boosting,AdaBoostRegressor,,"An AdaBoost regressor.  An AdaBoost [1] regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.  This class implements the algorithm known as AdaBoost.R2 [2].  Read more in the :ref:`User Guide <adaboost>`."
AdditiveChi2Sampler,<class 'sklearn.kernel_approximation.AdditiveChi2Sampler'>,Large,sklearn.kernel_approximation.AdditiveChi2Sampler,transformer,sklearn,kernel_approximation,AdditiveChi2Sampler,,,"Approximate feature map for additive chi2 kernel.  Uses sampling the fourier transform of the kernel characteristic at regular intervals.  Since the kernel that is to be approximated is additive, the components of the input vectors can be treated separately.  Each entry in the original space is transformed into 2*sample_steps-1 features, where sample_steps is a parameter of the method. Typical values of sample_steps include 1, 2 and 3.  Optimal choices for the sampling interval for certain data ranges can be computed (see the reference). The default values should be reasonable.  Read more in the :ref:`User Guide <additive_chi_kernel_approx>`.  Parameters ---------- sample_steps : int, default=2 Gives the number of (complex) sampling points.  sample_interval : float, default=None Sampling interval. Must be specified when sample_steps not in {1,2,3}.  Attributes ---------- n_features_in_ : int Number of features seen during :term:`fit`."
AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>,Small,sklearn.cluster._affinity_propagation.AffinityPropagation,cluster,sklearn,cluster,_affinity_propagation,AffinityPropagation,,"Perform Affinity Propagation Clustering of data.  Read more in the :ref:`User Guide <affinity_propagation>`.  Parameters ---------- damping : float, default=0.5 Damping factor in the range `[0.5, 1.0)` is the extent to which the current value is maintained relative to incoming values (weighted 1 - damping). This in order to avoid numerical oscillations when updating these values (messages).  max_iter : int, default=200 Maximum number of iterations.  convergence_iter : int, default=15 Number of iterations with no change in the number of estimated clusters that stops the convergence.  copy : bool, default=True Make a copy of input data.  preference : array-like of shape (n_samples,) or float, default=None Preferences for each point - points with larger values of preferences are more likely to be chosen as exemplars. The number of exemplars, ie of clusters, is influenced by the input preferences value. If the preferences are not passed as arguments, they will be set to the median of the input similarities.  affinity : {'euclidean', 'precomputed'}, default='euclidean' Which affinity to use. At the moment 'precomputed' and ``euclidean`` are supported. 'euclidean' uses the negative squared euclidean distance between points.  verbose : bool, default=False Whether to be verbose.  random_state : int, RandomState instance or None, default=None Pseudo-random number generator to control the starting state. Use an int for reproducible results across function calls. See the :term:`Glossary <random_state>`."
AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>,Small,sklearn.cluster._agglomerative.AgglomerativeClustering,cluster,sklearn,cluster,_agglomerative,AgglomerativeClustering,,"Agglomerative Clustering.  Recursively merges pair of clusters of sample data; uses linkage distance.  Read more in the :ref:`User Guide <hierarchical_clustering>`.  Parameters ---------- n_clusters : int or None, default=2 The number of clusters to find. It must be ``None`` if ``distance_threshold`` is not ``None``.  metric : str or callable, default=""euclidean"" Metric used to compute the linkage. Can be ""euclidean"", ""l1"", ""l2"", ""manhattan"", ""cosine"", or ""precomputed"". If linkage is ""ward"", only ""euclidean"" is accepted. If ""precomputed"", a distance matrix is needed as input for the fit method. If connectivity is None, linkage is ""single"" and affinity is not ""precomputed"" any valid pairwise distance metric can be assigned."
AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>,Small,sklearn.cluster._agglomerative.AgglomerativeClustering,cluster,sklearn,cluster,_agglomerative,AgglomerativeClustering,,"Agglomerative Clustering.  Recursively merges pair of clusters of sample data; uses linkage distance.  Read more in the :ref:`User Guide <hierarchical_clustering>`.  Parameters ---------- n_clusters : int or None, default=2 The number of clusters to find. It must be ``None`` if ``distance_threshold`` is not ``None``.  metric : str or callable, default=""euclidean"" Metric used to compute the linkage. Can be ""euclidean"", ""l1"", ""l2"", ""manhattan"", ""cosine"", or ""precomputed"". If linkage is ""ward"", only ""euclidean"" is accepted. If ""precomputed"", a distance matrix is needed as input for the fit method. If connectivity is None, linkage is ""single"" and affinity is not ""precomputed"" any valid pairwise distance metric can be assigned."
AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>,Small,sklearn.cluster._agglomerative.AgglomerativeClustering,cluster,sklearn,cluster,_agglomerative,AgglomerativeClustering,,"Agglomerative Clustering.  Recursively merges pair of clusters of sample data; uses linkage distance.  Read more in the :ref:`User Guide <hierarchical_clustering>`.  Parameters ---------- n_clusters : int or None, default=2 The number of clusters to find. It must be ``None`` if ``distance_threshold`` is not ``None``.  metric : str or callable, default=""euclidean"" Metric used to compute the linkage. Can be ""euclidean"", ""l1"", ""l2"", ""manhattan"", ""cosine"", or ""precomputed"". If linkage is ""ward"", only ""euclidean"" is accepted. If ""precomputed"", a distance matrix is needed as input for the fit method. If connectivity is None, linkage is ""single"" and affinity is not ""precomputed"" any valid pairwise distance metric can be assigned."
AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>,Small,sklearn.cluster._agglomerative.AgglomerativeClustering,cluster,sklearn,cluster,_agglomerative,AgglomerativeClustering,,"Agglomerative Clustering.  Recursively merges pair of clusters of sample data; uses linkage distance.  Read more in the :ref:`User Guide <hierarchical_clustering>`.  Parameters ---------- n_clusters : int or None, default=2 The number of clusters to find. It must be ``None`` if ``distance_threshold`` is not ``None``.  metric : str or callable, default=""euclidean"" Metric used to compute the linkage. Can be ""euclidean"", ""l1"", ""l2"", ""manhattan"", ""cosine"", or ""precomputed"". If linkage is ""ward"", only ""euclidean"" is accepted. If ""precomputed"", a distance matrix is needed as input for the fit method. If connectivity is None, linkage is ""single"" and affinity is not ""precomputed"" any valid pairwise distance metric can be assigned."
BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>,"Medium, Large",sklearn.ensemble._bagging.BaggingClassifier,classifier,sklearn,ensemble,_bagging,BaggingClassifier,,"A Bagging classifier.  A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.  This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting [1]_. If samples are drawn with replacement, then the method is known as Bagging [2]_. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces [3]_. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches [4]_.  Read more in the :ref:`User Guide <bagging>`."
BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>,"Medium, Large",sklearn.ensemble._bagging.BaggingRegressor,regressor,sklearn,ensemble,_bagging,BaggingRegressor,,"A Bagging regressor.  A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.  This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting [1]_. If samples are drawn with replacement, then the method is known as Bagging [2]_. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces [3]_. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches [4]_.  Read more in the :ref:`User Guide <bagging>`."
BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>,Small,sklearn.mixture._bayesian_mixture.BayesianGaussianMixture,unknown,sklearn,mixture,_bayesian_mixture,BayesianGaussianMixture,,Variational Bayesian estimation of a Gaussian mixture.  This class allows to infer an approximate posterior distribution over the parameters of a Gaussian mixture distribution. The effective number of components can be inferred from the data.  This class implements two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.
BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>,"Small, Medium",sklearn.linear_model._bayes.BayesianRidge,regressor,sklearn,linear_model,_bayes,BayesianRidge,,"Bayesian ridge regression.  Fit a Bayesian ridge model. See the Notes section for details on this implementation and the optimization of the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).  Read more in the :ref:`User Guide <bayesian_regression>`. For an intuitive visualization of how the sinusoid is approximated by a polynomial using different pairs of initial values, see :ref:`sphx_glr_auto_examples_linear_model_plot_bayesian_ridge_curvefit.py`.  Parameters ---------- max_iter : int, default=300 Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion."
BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>,"Small, Medium",sklearn.linear_model._bayes.BayesianRidge,regressor,sklearn,linear_model,_bayes,BayesianRidge,,"Bayesian ridge regression.  Fit a Bayesian ridge model. See the Notes section for details on this implementation and the optimization of the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).  Read more in the :ref:`User Guide <bayesian_regression>`. For an intuitive visualization of how the sinusoid is approximated by a polynomial using different pairs of initial values, see :ref:`sphx_glr_auto_examples_linear_model_plot_bayesian_ridge_curvefit.py`.  Parameters ---------- max_iter : int, default=300 Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion."
BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>,"Small, Medium",sklearn.linear_model._bayes.BayesianRidge,regressor,sklearn,linear_model,_bayes,BayesianRidge,,"Bayesian ridge regression.  Fit a Bayesian ridge model. See the Notes section for details on this implementation and the optimization of the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).  Read more in the :ref:`User Guide <bayesian_regression>`. For an intuitive visualization of how the sinusoid is approximated by a polynomial using different pairs of initial values, see :ref:`sphx_glr_auto_examples_linear_model_plot_bayesian_ridge_curvefit.py`.  Parameters ---------- max_iter : int, default=300 Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion."
BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>,"Small, Medium",sklearn.linear_model._bayes.BayesianRidge,regressor,sklearn,linear_model,_bayes,BayesianRidge,,"Bayesian ridge regression.  Fit a Bayesian ridge model. See the Notes section for details on this implementation and the optimization of the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).  Read more in the :ref:`User Guide <bayesian_regression>`. For an intuitive visualization of how the sinusoid is approximated by a polynomial using different pairs of initial values, see :ref:`sphx_glr_auto_examples_linear_model_plot_bayesian_ridge_curvefit.py`.  Parameters ---------- max_iter : int, default=300 Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion."
BernoulliNB,<class 'sklearn.naive_bayes.BernoulliNB'>,"Medium, Large",sklearn.naive_bayes.BernoulliNB,classifier,sklearn,naive_bayes,BernoulliNB,,,"Naive Bayes classifier for multivariate Bernoulli models.  Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.  Read more in the :ref:`User Guide <bernoulli_naive_bayes>`.  Parameters ---------- alpha : float or array-like of shape (n_features,), default=1.0 Additive (Laplace/Lidstone) smoothing parameter (set alpha=0 and force_alpha=True, for no smoothing).  force_alpha : bool, default=True If False and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged. This may cause numerical errors if alpha is too close to 0."
BernoulliRBM,<class 'sklearn.neural_network._rbm.BernoulliRBM'>,Small,sklearn.neural_network._rbm.BernoulliRBM,transformer,sklearn,neural_network,_rbm,BernoulliRBM,,"Bernoulli Restricted Boltzmann Machine (RBM).  A Restricted Boltzmann Machine with binary visible units and binary hidden units. Parameters are estimated using Stochastic Maximum Likelihood (SML), also known as Persistent Contrastive Divergence (PCD) [2].  The time complexity of this implementation is ``O(d ** 2)`` assuming d ~ n_features ~ n_components.  Read more in the :ref:`User Guide <rbm>`.  Parameters ---------- n_components : int, default=256 Number of binary hidden units.  learning_rate : float, default=0.1 The learning rate for weight updates. It is *highly* recommended to tune this hyper-parameter. Reasonable values are in the 10**[0., -3.] range.  batch_size : int, default=10 Number of examples per minibatch.  n_iter : int, default=10 Number of iterations/sweeps over the training dataset to perform during training.  verbose : int, default=0 The verbosity level. The default, zero, means silent mode. Range of values is [0, inf].  random_state : int, RandomState instance or None, default=None Determines random number generation for:  - Gibbs sampling from visible and hidden layers.  - Initializing components, sampling from layers during fit.  - Corrupting the data when scoring samples.  Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.  Attributes ---------- intercept_hidden_ : array-like of shape (n_components,) Biases of the hidden units.  intercept_visible_ : array-like of shape (n_features,) Biases of the visible units.  components_ : array-like of shape (n_components, n_features) Weight matrix, where `n_features` is the number of visible units and `n_components` is the number of hidden units.  h_samples_ : array-like of shape (batch_size, n_components) Hidden Activation sampled from the model distribution, where `batch_size` is the number of examples per minibatch and `n_components` is the number of hidden units.  n_features_in_ : int Number of features seen during :term:`fit`."
Binarizer,<class 'sklearn.preprocessing._data.Binarizer'>,Large,sklearn.preprocessing._data.Binarizer,transformer,sklearn,preprocessing,_data,Binarizer,,"Binarize data (set feature values to 0 or 1) according to a threshold.  Values greater than the threshold map to 1, while values less than or equal to the threshold map to 0. With the default threshold of 0, only positive values map to 1.  Binarization is a common operation on text count data where the analyst can decide to only consider the presence or absence of a feature rather than a quantified number of occurrences for instance.  It can also be used as a pre-processing step for estimators that consider boolean random variables (e.g. modelled using the Bernoulli distribution in a Bayesian setting).  Read more in the :ref:`User Guide <preprocessing_binarization>`.  Parameters ---------- threshold : float, default=0.0 Feature values below or equal to this are replaced by 0, above it by 1. Threshold may not be less than 0 for operations on sparse matrices.  copy : bool, default=True Set to False to perform inplace binarization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix).  Attributes ---------- n_features_in_ : int Number of features seen during :term:`fit`."
Birch,<class 'sklearn.cluster._birch.Birch'>,"Medium, Large",sklearn.cluster._birch.Birch,"cluster, transformer",sklearn,cluster,_birch,Birch,,"Implements the BIRCH clustering algorithm.  It is a memory-efficient, online-learning algorithm provided as an alternative to :class:`MiniBatchKMeans`. It constructs a tree data structure with the cluster centroids being read off the leaf. These can be either the final cluster centroids or can be provided as input to another clustering algorithm such as :class:`AgglomerativeClustering`.  Read more in the :ref:`User Guide <birch>`."
BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>,"Medium, Large",sklearn.cluster._bisect_k_means.BisectingKMeans,"cluster, transformer",sklearn,cluster,_bisect_k_means,BisectingKMeans,,Bisecting K-Means clustering.  Read more in the :ref:`User Guide <bisect_k_means>`.
CCA,<class 'sklearn.cross_decomposition._pls.CCA'>,Small,sklearn.cross_decomposition._pls.CCA,"regressor, transformer",sklearn,cross_decomposition,_pls,CCA,,"Canonical Correlation Analysis, also known as ""Mode B"" PLS.  For a comparison between other cross decomposition algorithms, see :ref:`sphx_glr_auto_examples_cross_decomposition_plot_compare_cross_decomposition.py`.  Read more in the :ref:`User Guide <cross_decomposition>`.  Parameters ---------- n_components : int, default=2 Number of components to keep. Should be in `[1, min(n_samples, n_features, n_targets)]`.  scale : bool, default=True Whether to scale `X` and `Y`.  max_iter : int, default=500 The maximum number of iterations of the power method.  tol : float, default=1e-06 The tolerance used as convergence criteria in the power method: the algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less than `tol`, where `u` corresponds to the left singular vector.  copy : bool, default=True Whether to copy `X` and `Y` in fit before applying centering, and potentially scaling. If False, these operations will be done inplace, modifying both arrays.  Attributes ---------- x_weights_ : ndarray of shape (n_features, n_components) The left singular vectors of the cross-covariance matrices of each iteration.  y_weights_ : ndarray of shape (n_targets, n_components) The right singular vectors of the cross-covariance matrices of each iteration.  x_loadings_ : ndarray of shape (n_features, n_components) The loadings of `X`.  y_loadings_ : ndarray of shape (n_targets, n_components) The loadings of `Y`.  x_rotations_ : ndarray of shape (n_features, n_components) The projection matrix used to transform `X`.  y_rotations_ : ndarray of shape (n_targets, n_components) The projection matrix used to transform `Y`.  coef_ : ndarray of shape (n_targets, n_features) The coefficients of the linear model such that `Y` is approximated as `Y = X @ coef_.T + intercept_`.  intercept_ : ndarray of shape (n_targets,) The intercepts of the linear model such that `Y` is approximated as `Y = X @ coef_.T + intercept_`."
CalibratedClassifierCV,<class 'sklearn.calibration.CalibratedClassifierCV'>,"Small, Medium",sklearn.calibration.CalibratedClassifierCV,classifier,sklearn,calibration,CalibratedClassifierCV,,,"Probability calibration with isotonic regression or logistic regression.  This class uses cross-validation to both estimate the parameters of a classifier and subsequently calibrate a classifier. With default `ensemble=True`, for each cv split it fits a copy of the base estimator to the training subset, and calibrates it using the testing subset. For prediction, predicted probabilities are averaged across these individual calibrated classifiers. When `ensemble=False`, cross-validation is used to obtain unbiased predictions, via :func:`~sklearn.model_selection.cross_val_predict`, which are then used for calibration. For prediction, the base estimator, trained using all the data, is used. This is the prediction method implemented when `probabilities=True` for :class:`~sklearn.svm.SVC` and :class:`~sklearn.svm.NuSVC` estimators (see :ref:`User Guide <scores_probabilities>` for details).  Already fitted classifiers can be calibrated by wrapping the model in a :class:`~sklearn.frozen.FrozenEstimator`. In this case all provided data is used for calibration. The user has to take care manually that data for model fitting and calibration are disjoint.  The calibration is based on the :term:`decision_function` method of the `estimator` if it exists, else on :term:`predict_proba`.  Read more in the :ref:`User Guide <calibration>`. In order to learn more on the CalibratedClassifierCV class, see the following calibration examples: :ref:`sphx_glr_auto_examples_calibration_plot_calibration.py`, :ref:`sphx_glr_auto_examples_calibration_plot_calibration_curve.py`, and :ref:`sphx_glr_auto_examples_calibration_plot_calibration_multiclass.py`.  Parameters ---------- estimator : estimator instance, default=None The classifier whose output need to be calibrated to provide more accurate `predict_proba` outputs. The default classifier is a :class:`~sklearn.svm.LinearSVC`."
CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>,"Medium, Large",sklearn.naive_bayes.CategoricalNB,classifier,sklearn,naive_bayes,CategoricalNB,,,"Naive Bayes classifier for categorical features.  The categorical Naive Bayes classifier is suitable for classification with discrete features that are categorically distributed. The categories of each feature are drawn from a categorical distribution.  Read more in the :ref:`User Guide <categorical_naive_bayes>`.  Parameters ---------- alpha : float, default=1.0 Additive (Laplace/Lidstone) smoothing parameter (set alpha=0 and force_alpha=True, for no smoothing).  force_alpha : bool, default=True If False and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged. This may cause numerical errors if alpha is too close to 0."
ClassifierChain,<class 'sklearn.multioutput.ClassifierChain'>,"Small, Medium",sklearn.multioutput.ClassifierChain,classifier,sklearn,multioutput,ClassifierChain,,,"A multi-label model that arranges binary classifiers into a chain.  Each model makes a prediction in the order specified by the chain using all of the available features provided to the model plus the predictions of models that are earlier in the chain.  For an example of how to use ``ClassifierChain`` and benefit from its ensemble, see :ref:`ClassifierChain on a yeast dataset <sphx_glr_auto_examples_multioutput_plot_classifier_chain_yeast.py>` example.  Read more in the :ref:`User Guide <classifierchain>`."
ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>,Large,sklearn.compose._column_transformer.ColumnTransformer,transformer,sklearn,compose,_column_transformer,ColumnTransformer,,"Applies transformers to columns of an array or pandas DataFrame.  This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.  Read more in the :ref:`User Guide <column_transformer>`."
ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>,"Medium, Large",sklearn.naive_bayes.ComplementNB,classifier,sklearn,naive_bayes,ComplementNB,,,"The Complement Naive Bayes classifier described in Rennie et al. (2003).  The Complement Naive Bayes classifier was designed to correct the ""severe assumptions"" made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.  Read more in the :ref:`User Guide <complement_naive_bayes>`."
CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>,Large,sklearn.feature_extraction.text.CountVectorizer,unknown,sklearn,feature_extraction,text,CountVectorizer,,"Convert a collection of text documents to a matrix of token counts.  This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.  If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.  For an efficiency comparison of the different feature extractors, see :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.  Read more in the :ref:`User Guide <text_feature_extraction>`.  Parameters ---------- input : {'filename', 'file', 'content'}, default='content' - If `'filename'`, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.  - If `'file'`, the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory.  - If `'content'`, the input is expected to be a sequence of items that can be of type string or byte.  encoding : str, default='utf-8' If bytes or files are given to analyze, this encoding is used to decode.  decode_error : {'strict', 'ignore', 'replace'}, default='strict' Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'.  strip_accents : {'ascii', 'unicode'} or callable, default=None Remove accents and perform other character normalization during the preprocessing step. 'ascii' is a fast method that only works on characters that have a direct ASCII mapping. 'unicode' is a slightly slower method that works on any characters. None (default) means no character normalization is performed.  Both 'ascii' and 'unicode' use NFKD normalization from :func:`unicodedata.normalize`.  lowercase : bool, default=True Convert all characters to lowercase before tokenizing.  preprocessor : callable, default=None Override the preprocessing (strip_accents and lowercase) stage while preserving the tokenizing and n-grams generation steps. Only applies if ``analyzer`` is not callable.  tokenizer : callable, default=None Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if ``analyzer == 'word'``.  stop_words : {'english'}, list, default=None If 'english', a built-in stop word list for English is used. There are several known issues with 'english' and you should consider an alternative (see :ref:`stop_words`).  If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if ``analyzer == 'word'``.  If None, no stop words will be used. In this case, setting `max_df` to a higher value, such as in the range (0.7, 1.0), can automatically detect and filter stop words based on intra corpus document frequency of terms.  token_pattern : str or None, default=r""(?u)\\b\\w\\w+\\b"" Regular expression denoting what constitutes a ""token"", only used if ``analyzer == 'word'``. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).  If there is a capturing group in token_pattern then the captured group content, not the entire match, becomes the token. At most one capturing group is permitted.  ngram_range : tuple (min_n, max_n), default=(1, 1) The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means only bigrams. Only applies if ``analyzer`` is not callable.  analyzer : {'word', 'char', 'char_wb'} or callable, default='word' Whether the feature should be made of word n-gram or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.  If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input."
DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>,"Small, Medium",sklearn.cluster._dbscan.DBSCAN,cluster,sklearn,cluster,_dbscan,DBSCAN,,"Perform DBSCAN clustering from vector array or distance matrix.  DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.  This implementation has a worst case memory complexity of :math:`O({n}^2)`, which can occur when the `eps` param is large and `min_samples` is low, while the original DBSCAN only uses linear memory. For further details, see the Notes below.  Read more in the :ref:`User Guide <dbscan>`.  Parameters ---------- eps : float, default=0.5 The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.  min_samples : int, default=5 The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself. If `min_samples` is set to a higher value, DBSCAN will find denser clusters, whereas if it is set to a lower value, the found clusters will be more sparse.  metric : str, or callable, default='euclidean' The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by :func:`sklearn.metrics.pairwise_distances` for its metric parameter. If metric is ""precomputed"", X is assumed to be a distance matrix and must be square. X may be a :term:`sparse graph`, in which case only ""nonzero"" elements may be considered neighbors for DBSCAN."
DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>,Medium,sklearn.tree._classes.DecisionTreeClassifier,classifier,sklearn,tree,_classes,DecisionTreeClassifier,,"A decision tree classifier.  Read more in the :ref:`User Guide <tree>`.  Parameters ---------- criterion : {""gini"", ""entropy"", ""log_loss""}, default=""gini"" The function to measure the quality of a split. Supported criteria are ""gini"" for the Gini impurity and ""log_loss"" and ""entropy"" both for the Shannon information gain, see :ref:`tree_mathematical_formulation`.  splitter : {""best"", ""random""}, default=""best"" The strategy used to choose the split at each node. Supported strategies are ""best"" to choose the best split and ""random"" to choose the best random split.  max_depth : int, default=None The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.  min_samples_split : int or float, default=2 The minimum number of samples required to split an internal node:  - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split."
DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>,Medium,sklearn.tree._classes.DecisionTreeRegressor,regressor,sklearn,tree,_classes,DecisionTreeRegressor,,"A decision tree regressor.  Read more in the :ref:`User Guide <tree>`.  Parameters ---------- criterion : {""squared_error"", ""friedman_mse"", ""absolute_error"",             ""poisson""}, default=""squared_error"" The function to measure the quality of a split. Supported criteria are ""squared_error"" for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, ""friedman_mse"", which uses mean squared error with Friedman's improvement score for potential splits, ""absolute_error"" for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and ""poisson"" which uses reduction in the half mean Poisson deviance to find splits."
DictVectorizer,<class 'sklearn.feature_extraction._dict_vectorizer.DictVectorizer'>,Large,sklearn.feature_extraction._dict_vectorizer.DictVectorizer,transformer,sklearn,feature_extraction,_dict_vectorizer,DictVectorizer,,"Transforms lists of feature-value mappings to vectors.  This transformer turns lists of mappings (dict-like objects) of feature names to feature values into Numpy arrays or scipy.sparse matrices for use with scikit-learn estimators.  When feature values are strings, this transformer will do a binary one-hot (aka one-of-K) coding: one boolean-valued feature is constructed for each of the possible string values that the feature can take on. For instance, a feature ""f"" that can take on the values ""ham"" and ""spam"" will become two features in the output, one signifying ""f=ham"", the other ""f=spam"".  If a feature value is a sequence or set of strings, this transformer will iterate over the values and will count the occurrences of each string value.  However, note that this transformer will only do a binary one-hot encoding when feature values are of type string. If categorical features are represented as numeric values such as int or iterables of strings, the DictVectorizer can be followed by :class:`~sklearn.preprocessing.OneHotEncoder` to complete binary one-hot encoding.  Features that do not occur in a sample (mapping) will have a zero value in the resulting array/matrix.  For an efficiency comparison of the different feature extractors, see :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.  Read more in the :ref:`User Guide <dict_feature_extraction>`.  Parameters ---------- dtype : dtype, default=np.float64 The type of feature values. Passed to Numpy array/scipy.sparse matrix constructors as the dtype argument. separator : str, default=""="" Separator string used when constructing new features for one-hot coding. sparse : bool, default=True Whether transform should produce scipy.sparse matrices. sort : bool, default=True Whether ``feature_names_`` and ``vocabulary_`` should be sorted when fitting.  Attributes ---------- vocabulary_ : dict A dictionary mapping feature names to feature indices.  feature_names_ : list A list of length n_features containing the feature names (e.g., ""f=ham"" and ""f=spam"").  See Also -------- FeatureHasher : Performs vectorization using only a hash function. sklearn.preprocessing.OrdinalEncoder : Handles nominal/categorical features encoded as columns of arbitrary data types.  Examples -------- >>> from sklearn.feature_extraction import DictVectorizer >>> v = DictVectorizer(sparse=False) >>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}] >>> X = v.fit_transform(D) >>> X array([[2., 0., 1.], [0., 1., 3.]]) >>> v.inverse_transform(X) == [{'bar': 2.0, 'foo': 1.0},"
DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>,Small,sklearn.decomposition._dict_learning.DictionaryLearning,transformer,sklearn,decomposition,_dict_learning,DictionaryLearning,,"Dictionary learning.  Finds a dictionary (a set of atoms) that performs well at sparsely encoding the fitted data.  Solves the optimization problem::  (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1 (U,V) with || V_k ||_2 <= 1 for all  0 <= k < n_components  ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm which is the sum of the absolute values of all the entries in the matrix.  Read more in the :ref:`User Guide <DictionaryLearning>`.  Parameters ---------- n_components : int, default=None Number of dictionary elements to extract. If None, then ``n_components`` is set to ``n_features``.  alpha : float, default=1.0 Sparsity controlling parameter.  max_iter : int, default=1000 Maximum number of iterations to perform.  tol : float, default=1e-8 Tolerance for numerical error.  fit_algorithm : {'lars', 'cd'}, default='lars' * `'lars'`: uses the least angle regression method to solve the lasso problem (:func:`~sklearn.linear_model.lars_path`); * `'cd'`: uses the coordinate descent method to compute the Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be faster if the estimated components are sparse."
DummyClassifier,<class 'sklearn.dummy.DummyClassifier'>,"Small, Medium, Large",sklearn.dummy.DummyClassifier,classifier,sklearn,dummy,DummyClassifier,,,"DummyClassifier makes predictions that ignore the input features.  This classifier serves as a simple baseline to compare against other more complex classifiers.  The specific behavior of the baseline is selected with the `strategy` parameter.  All strategies make predictions that ignore the input feature values passed as the `X` argument to `fit` and `predict`. The predictions, however, typically depend on values observed in the `y` parameter passed to `fit`.  Note that the ""stratified"" and ""uniform"" strategies lead to non-deterministic predictions that can be rendered deterministic by setting the `random_state` parameter if needed. The other strategies are naturally deterministic and, once fit, always return the same constant prediction for any value of `X`.  Read more in the :ref:`User Guide <dummy_estimators>`."
DummyRegressor,<class 'sklearn.dummy.DummyRegressor'>,"Small, Medium, Large",sklearn.dummy.DummyRegressor,regressor,sklearn,dummy,DummyRegressor,,,Regressor that makes predictions using simple rules.  This regressor is useful as a simple baseline to compare with other (real) regressors. Do not use it for real problems.  Read more in the :ref:`User Guide <dummy_estimators>`.
ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>,"Small, Medium",sklearn.linear_model._coordinate_descent.ElasticNet,regressor,sklearn,linear_model,_coordinate_descent,ElasticNet,,"Linear regression with combined L1 and L2 priors as regularizer.  Minimizes the objective function::  1 / (2 * n_samples) * ||y - Xw||^2_2 + alpha * l1_ratio * ||w||_1 + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2  If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to::  a * ||w||_1 + 0.5 * b * ||w||_2^2  where::  alpha = a + b and l1_ratio = a / (a + b)  The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable, unless you supply your own sequence of alpha.  Read more in the :ref:`User Guide <elastic_net>`.  Parameters ---------- alpha : float, default=1.0 Constant that multiplies the penalty terms. Defaults to 1.0. See the notes for the exact mathematical meaning of this parameter. ``alpha = 0`` is equivalent to an ordinary least square, solved by the :class:`LinearRegression` object. For numerical reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised. Given this, you should use the :class:`LinearRegression` object.  l1_ratio : float, default=0.5 The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2.  fit_intercept : bool, default=True Whether the intercept should be estimated or not. If ``False``, the data is assumed to be already centered.  precompute : bool or array-like of shape (n_features, n_features),                 default=False Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix can also be passed as argument. For sparse input this option is always ``False`` to preserve sparsity. Check :ref:`an example on how to use a precomputed Gram Matrix in ElasticNet <sphx_glr_auto_examples_linear_model_plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py>` for details.  max_iter : int, default=1000 The maximum number of iterations.  copy_X : bool, default=True If ``True``, X will be copied; else, it may be overwritten.  tol : float, default=1e-4 The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``, see Notes below.  warm_start : bool, default=False When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.  positive : bool, default=False When set to ``True``, forces the coefficients to be positive.  random_state : int, RandomState instance, default=None The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.  selection : {'cyclic', 'random'}, default='cyclic' If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.  Attributes ---------- coef_ : ndarray of shape (n_features,) or (n_targets, n_features) Parameter vector (w in the cost function formula).  sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features) Sparse representation of the `coef_`.  intercept_ : float or ndarray of shape (n_targets,) Independent term in decision function.  n_iter_ : list of int Number of iterations run by the coordinate descent solver to reach the specified tolerance.  dual_gap_ : float or ndarray of shape (n_targets,) Given param alpha, the dual gaps at the end of the optimization, same shape as each observation of y.  n_features_in_ : int Number of features seen during :term:`fit`."
ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>,"Small, Medium",sklearn.linear_model._coordinate_descent.ElasticNetCV,regressor,sklearn,linear_model,_coordinate_descent,ElasticNetCV,,"Elastic Net model with iterative fitting along a regularization path.  See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <elastic_net>`.  Parameters ---------- l1_ratio : float or list of float, default=0.5 Float between 0 and 1 passed to ElasticNet (scaling between l1 and l2 penalties). For ``l1_ratio = 0`` the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2 This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7, .9, .95, .99, 1]``.  eps : float, default=1e-3 Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``.  n_alphas : int, default=100 Number of alphas along the regularization path, used for each l1_ratio.  alphas : array-like, default=None List of alphas where to compute the models. If None alphas are set automatically.  fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).  precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto' Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.  max_iter : int, default=1000 The maximum number of iterations.  tol : float, default=1e-4 The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``.  cv : int, cross-validation generator or iterable, default=None Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - int, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here."
EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>,Small,sklearn.covariance._elliptic_envelope.EllipticEnvelope,unknown,sklearn,covariance,_elliptic_envelope,EllipticEnvelope,,"An object for detecting outliers in a Gaussian distributed dataset.  Read more in the :ref:`User Guide <outlier_detection>`.  Parameters ---------- store_precision : bool, default=True Specify if the estimated precision is stored.  assume_centered : bool, default=False If True, the support of robust location and covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.  support_fraction : float, default=None The proportion of points to be included in the support of the raw MCD estimate. If None, the minimum value of support_fraction will be used within the algorithm: `(n_samples + n_features + 1) / 2 * n_samples`. Range is (0, 1).  contamination : float, default=0.1 The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Range is (0, 0.5].  random_state : int, RandomState instance or None, default=None Determines the pseudo random number generator for shuffling the data. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.  Attributes ---------- location_ : ndarray of shape (n_features,) Estimated robust location.  covariance_ : ndarray of shape (n_features, n_features) Estimated robust covariance matrix.  precision_ : ndarray of shape (n_features, n_features) Estimated pseudo inverse matrix. (stored only if store_precision is True)  support_ : ndarray of shape (n_samples,) A mask of the observations that have been used to compute the robust estimates of location and shape.  offset_ : float Offset used to define the decision function from the raw scores. We have the relation: ``decision_function = score_samples - offset_``. The offset depends on the contamination parameter and is defined in such a way we obtain the expected number of outliers (samples with decision function < 0) in training."
EmpiricalCovariance,<class 'sklearn.covariance._empirical_covariance.EmpiricalCovariance'>,Small,sklearn.covariance._empirical_covariance.EmpiricalCovariance,unknown,sklearn,covariance,_empirical_covariance,EmpiricalCovariance,,"Maximum likelihood covariance estimator.  Read more in the :ref:`User Guide <covariance>`.  Parameters ---------- store_precision : bool, default=True Specifies if the estimated precision is stored.  assume_centered : bool, default=False If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data are centered before computation.  Attributes ---------- location_ : ndarray of shape (n_features,) Estimated location, i.e. the estimated mean.  covariance_ : ndarray of shape (n_features, n_features) Estimated covariance matrix  precision_ : ndarray of shape (n_features, n_features) Estimated pseudo-inverse matrix. (stored only if store_precision is True)  n_features_in_ : int Number of features seen during :term:`fit`."
ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>,Medium,sklearn.tree._classes.ExtraTreeClassifier,classifier,sklearn,tree,_classes,ExtraTreeClassifier,,"An extremely randomized tree classifier.  Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the `max_features` randomly selected features and the best split among those is chosen. When `max_features` is set 1, this amounts to building a totally random decision tree.  Warning: Extra-trees should only be used within ensemble methods.  Read more in the :ref:`User Guide <tree>`.  Parameters ---------- criterion : {""gini"", ""entropy"", ""log_loss""}, default=""gini"" The function to measure the quality of a split. Supported criteria are ""gini"" for the Gini impurity and ""log_loss"" and ""entropy"" both for the Shannon information gain, see :ref:`tree_mathematical_formulation`.  splitter : {""random"", ""best""}, default=""random"" The strategy used to choose the split at each node. Supported strategies are ""best"" to choose the best split and ""random"" to choose the best random split.  max_depth : int, default=None The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.  min_samples_split : int or float, default=2 The minimum number of samples required to split an internal node:  - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split."
ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>,Medium,sklearn.tree._classes.ExtraTreeRegressor,regressor,sklearn,tree,_classes,ExtraTreeRegressor,,"An extremely randomized tree regressor.  Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the `max_features` randomly selected features and the best split among those is chosen. When `max_features` is set 1, this amounts to building a totally random decision tree.  Warning: Extra-trees should only be used within ensemble methods.  Read more in the :ref:`User Guide <tree>`.  Parameters ---------- criterion : {""squared_error"", ""friedman_mse"", ""absolute_error"", ""poisson""},             default=""squared_error"" The function to measure the quality of a split. Supported criteria are ""squared_error"" for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, ""friedman_mse"", which uses mean squared error with Friedman's improvement score for potential splits, ""absolute_error"" for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and ""poisson"" which uses reduction in Poisson deviance to find splits."
ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>,1,sklearn.ensemble._forest.ExtraTreesClassifier,classifier,sklearn,ensemble,_forest,ExtraTreesClassifier,,"An extra-trees classifier.  This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : int, default=100 The number of trees in the forest."
ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>,1,sklearn.ensemble._forest.ExtraTreesClassifier,classifier,sklearn,ensemble,_forest,ExtraTreesClassifier,,"An extra-trees classifier.  This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : int, default=100 The number of trees in the forest."
ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>,"Medium, Large",sklearn.ensemble._forest.ExtraTreesClassifier,classifier,sklearn,ensemble,_forest,ExtraTreesClassifier,,"An extra-trees classifier.  This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : int, default=100 The number of trees in the forest."
ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>,"Medium, Large",sklearn.ensemble._forest.ExtraTreesClassifier,classifier,sklearn,ensemble,_forest,ExtraTreesClassifier,,"An extra-trees classifier.  This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : int, default=100 The number of trees in the forest."
ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>,"Medium, Large",sklearn.ensemble._forest.ExtraTreesRegressor,regressor,sklearn,ensemble,_forest,ExtraTreesRegressor,,"An extra-trees regressor.  This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : int, default=100 The number of trees in the forest."
FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>,"Small, Medium",sklearn.decomposition._factor_analysis.FactorAnalysis,transformer,sklearn,decomposition,_factor_analysis,FactorAnalysis,,"Factor Analysis (FA).  A simple linear generative model with Gaussian latent variables.  The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. Without loss of generality the factors are distributed according to a Gaussian with zero mean and unit covariance. The noise is also zero mean and has an arbitrary diagonal covariance matrix.  If we would restrict the model further, by assuming that the Gaussian noise is even isotropic (all diagonal entries are the same) we would obtain :class:`PCA`.  FactorAnalysis performs a maximum likelihood estimate of the so-called `loading` matrix, the transformation of the latent variables to the observed ones, using SVD based approach.  Read more in the :ref:`User Guide <FA>`."
FastICA,<class 'sklearn.decomposition._fastica.FastICA'>,Small,sklearn.decomposition._fastica.FastICA,transformer,sklearn,decomposition,_fastica,FastICA,,"FastICA: a fast algorithm for Independent Component Analysis.  The implementation is based on [1]_.  Read more in the :ref:`User Guide <ICA>`.  Parameters ---------- n_components : int, default=None Number of components to use. If None is passed, all are used.  algorithm : {'parallel', 'deflation'}, default='parallel' Specify which algorithm to use for FastICA.  whiten : str or bool, default='unit-variance' Specify the whitening strategy to use.  - If 'arbitrary-variance', a whitening with variance arbitrary is used. - If 'unit-variance', the whitening matrix is rescaled to ensure that each recovered source has unit variance. - If False, the data is already considered to be whitened, and no whitening is performed."
FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>,Small,sklearn.cluster._agglomerative.FeatureAgglomeration,"cluster, transformer",sklearn,cluster,_agglomerative,FeatureAgglomeration,,"Agglomerate features.  Recursively merges pair of clusters of features.  Refer to :ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py` for an example comparison of :class:`FeatureAgglomeration` strategy with a univariate feature selection strategy (based on ANOVA).  Read more in the :ref:`User Guide <hierarchical_clustering>`.  Parameters ---------- n_clusters : int or None, default=2 The number of clusters to find. It must be ``None`` if ``distance_threshold`` is not ``None``.  metric : str or callable, default=""euclidean"" Metric used to compute the linkage. Can be ""euclidean"", ""l1"", ""l2"", ""manhattan"", ""cosine"", or ""precomputed"". If linkage is ""ward"", only ""euclidean"" is accepted. If ""precomputed"", a distance matrix is needed as input for the fit method."
FeatureHasher,<class 'sklearn.feature_extraction._hash.FeatureHasher'>,Large,sklearn.feature_extraction._hash.FeatureHasher,transformer,sklearn,feature_extraction,_hash,FeatureHasher,,"Implements feature hashing, aka the hashing trick.  This class turns sequences of symbolic feature names (strings) into scipy.sparse matrices, using a hash function to compute the matrix column corresponding to a name. The hash function employed is the signed 32-bit version of Murmurhash3.  Feature names of type byte string are used as-is. Unicode strings are converted to UTF-8 first, but no Unicode normalization is done. Feature values must be (finite) numbers.  This class is a low-memory alternative to DictVectorizer and CountVectorizer, intended for large-scale (online) learning and situations where memory is tight, e.g. when running prediction code on embedded devices.  For an efficiency comparison of the different feature extractors, see :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.  Read more in the :ref:`User Guide <feature_hashing>`."
FeatureUnion,<class 'sklearn.pipeline.FeatureUnion'>,Large,sklearn.pipeline.FeatureUnion,transformer,sklearn,pipeline,FeatureUnion,,,"Concatenates results of multiple transformer objects.  This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer.  Parameters of the transformers may be set using its name and the parameter name separated by a '__'. A transformer may be replaced entirely by setting the parameter with its name to another transformer, removed by setting to 'drop' or disabled by setting to 'passthrough' (features are passed without transformation).  Read more in the :ref:`User Guide <feature_union>`."
FixedThresholdClassifier,<class 'sklearn.model_selection._classification_threshold.FixedThresholdClassifier'>,Unknown,sklearn.model_selection._classification_threshold.FixedThresholdClassifier,classifier,sklearn,model_selection,_classification_threshold,FixedThresholdClassifier,,"Binary classifier that manually sets the decision threshold.  This classifier allows to change the default decision threshold used for converting posterior probability estimates (i.e. output of `predict_proba`) or decision scores (i.e. output of `decision_function`) into a class label.  Here, the threshold is not optimized and is set to a constant value.  Read more in the :ref:`User Guide <FixedThresholdClassifier>`."
FrozenEstimator,<class 'sklearn.frozen._frozen.FrozenEstimator'>,Unknown,sklearn.frozen._frozen.FrozenEstimator,unknown,sklearn,frozen,_frozen,FrozenEstimator,,"Estimator that wraps a fitted estimator to prevent re-fitting.  This meta-estimator takes an estimator and freezes it, in the sense that calling `fit` on it has no effect. `fit_predict` and `fit_transform` are also disabled. All other methods are delegated to the original estimator and original estimator's attributes are accessible as well.  This is particularly useful when you have a fitted or a pre-trained model as a transformer in a pipeline, and you'd like `pipeline.fit` to have no effect on this step.  Parameters ---------- estimator : estimator The estimator which is to be kept frozen.  See Also -------- None: No similar entry in the scikit-learn documentation.  Examples -------- >>> from sklearn.datasets import make_classification >>> from sklearn.frozen import FrozenEstimator >>> from sklearn.linear_model import LogisticRegression >>> X, y = make_classification(random_state=0) >>> clf = LogisticRegression(random_state=0).fit(X, y) >>> frozen_clf = FrozenEstimator(clf) >>> frozen_clf.fit(X, y)  # No-op FrozenEstimator(estimator=LogisticRegression(random_state=0)) >>> frozen_clf.predict(X)  # Predictions from `clf.predict`"
FunctionTransformer,<class 'sklearn.preprocessing._function_transformer.FunctionTransformer'>,"Small, Medium, Large",sklearn.preprocessing._function_transformer.FunctionTransformer,transformer,sklearn,preprocessing,_function_transformer,FunctionTransformer,,"Constructs a transformer from an arbitrary callable.  A FunctionTransformer forwards its X (and optionally y) arguments to a user-defined function or function object and returns the result of this function. This is useful for stateless transformations such as taking the log of frequencies, doing custom scaling, etc.  Note: If a lambda is used as the function, then the resulting transformer will not be pickleable."
GammaRegressor,<class 'sklearn.linear_model._glm.glm.GammaRegressor'>,"Small, Medium",sklearn.linear_model._glm.glm.GammaRegressor,regressor,sklearn,linear_model,_glm,glm,GammaRegressor,Generalized Linear Model with a Gamma distribution.  This regressor uses the 'log' link function.  Read more in the :ref:`User Guide <Generalized_linear_models>`.
GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>,Small,sklearn.mixture._gaussian_mixture.GaussianMixture,unknown,sklearn,mixture,_gaussian_mixture,GaussianMixture,,Gaussian Mixture.  Representation of a Gaussian mixture model probability distribution. This class allows to estimate the parameters of a Gaussian mixture distribution.  Read more in the :ref:`User Guide <gmm>`.
GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>,1,sklearn.naive_bayes.GaussianNB,classifier,sklearn,naive_bayes,GaussianNB,,,"Gaussian Naive Bayes (GaussianNB).  Can perform online updates to model parameters via :meth:`partial_fit`. For details on algorithm used to update feature means and variance online, see `Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque <http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf>`_.  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.  Parameters ---------- priors : array-like of shape (n_classes,), default=None Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.  var_smoothing : float, default=1e-9 Portion of the largest variance of all features that is added to variances for calculation stability."
GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>,1,sklearn.naive_bayes.GaussianNB,classifier,sklearn,naive_bayes,GaussianNB,,,"Gaussian Naive Bayes (GaussianNB).  Can perform online updates to model parameters via :meth:`partial_fit`. For details on algorithm used to update feature means and variance online, see `Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque <http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf>`_.  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.  Parameters ---------- priors : array-like of shape (n_classes,), default=None Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.  var_smoothing : float, default=1e-9 Portion of the largest variance of all features that is added to variances for calculation stability."
GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>,"Medium, Large",sklearn.naive_bayes.GaussianNB,classifier,sklearn,naive_bayes,GaussianNB,,,"Gaussian Naive Bayes (GaussianNB).  Can perform online updates to model parameters via :meth:`partial_fit`. For details on algorithm used to update feature means and variance online, see `Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque <http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf>`_.  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.  Parameters ---------- priors : array-like of shape (n_classes,), default=None Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.  var_smoothing : float, default=1e-9 Portion of the largest variance of all features that is added to variances for calculation stability."
GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>,"Medium, Large",sklearn.naive_bayes.GaussianNB,classifier,sklearn,naive_bayes,GaussianNB,,,"Gaussian Naive Bayes (GaussianNB).  Can perform online updates to model parameters via :meth:`partial_fit`. For details on algorithm used to update feature means and variance online, see `Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque <http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf>`_.  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.  Parameters ---------- priors : array-like of shape (n_classes,), default=None Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.  var_smoothing : float, default=1e-9 Portion of the largest variance of all features that is added to variances for calculation stability."
GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>,Small,sklearn.gaussian_process._gpc.GaussianProcessClassifier,classifier,sklearn,gaussian_process,_gpc,GaussianProcessClassifier,,"Gaussian process classification (GPC) based on Laplace approximation.  The implementation is based on Algorithm 3.1, 3.2, and 5.1 from [RW2006]_.  Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian.  Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.  Read more in the :ref:`User Guide <gaussian_process>`."
GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>,Small,sklearn.gaussian_process._gpr.GaussianProcessRegressor,regressor,sklearn,gaussian_process,_gpr,GaussianProcessRegressor,,"Gaussian process regression (GPR).  The implementation is based on Algorithm 2.1 of [RW2006]_.  In addition to standard scikit-learn estimator API, :class:`GaussianProcessRegressor`:  * allows prediction without prior fitting (based on the GP prior) * provides an additional method `sample_y(X)`, which evaluates samples drawn from the GPR (prior or posterior) at given inputs * exposes a method `log_marginal_likelihood(theta)`, which can be used externally for other ways of selecting hyperparameters, e.g., via Markov chain Monte Carlo.  To learn the difference between a point-estimate approach vs. a more Bayesian modelling approach, refer to the example entitled :ref:`sphx_glr_auto_examples_gaussian_process_plot_compare_gpr_krr.py`.  Read more in the :ref:`User Guide <gaussian_process>`."
GaussianRandomProjection,<class 'sklearn.random_projection.GaussianRandomProjection'>,Large,sklearn.random_projection.GaussianRandomProjection,transformer,sklearn,random_projection,GaussianRandomProjection,,,"Reduce dimensionality through Gaussian random projection.  The components of the random matrix are drawn from N(0, 1 / n_components).  Read more in the :ref:`User Guide <gaussian_random_matrix>`."
GenericUnivariateSelect,<class 'sklearn.feature_selection._univariate_selection.GenericUnivariateSelect'>,Large,sklearn.feature_selection._univariate_selection.GenericUnivariateSelect,transformer,sklearn,feature_selection,_univariate_selection,GenericUnivariateSelect,,"Univariate feature selector with configurable strategy.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable, default=f_classif Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). For modes 'percentile' or 'kbest' it can return a single array scores.  mode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}, default='percentile' Feature selection mode. Note that the `'percentile'` and `'kbest'` modes are supporting unsupervised feature selection (when `y` is `None`).  param : ""all"", float or int, default=1e-5 Parameter of the corresponding mode.  Attributes ---------- scores_ : array-like of shape (n_features,) Scores of features.  pvalues_ : array-like of shape (n_features,) p-values of feature scores, None if `score_func` returned scores only.  n_features_in_ : int Number of features seen during :term:`fit`."
GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>,1,sklearn.ensemble._gb.GradientBoostingClassifier,classifier,sklearn,ensemble,_gb,GradientBoostingClassifier,,"Gradient Boosting for classification.  This algorithm builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage ``n_classes_`` regression trees are fit on the negative gradient of the loss function, e.g. binary or multiclass log loss. Binary classification is a special case where only a single regression tree is induced.  :class:`~sklearn.ensemble.HistGradientBoostingClassifier` is a much faster variant of this algorithm for intermediate and large datasets (`n_samples >= 10_000`) and supports monotonic constraints.  Read more in the :ref:`User Guide <gradient_boosting>`.  Parameters ---------- loss : {'log_loss', 'exponential'}, default='log_loss' The loss function to be optimized. 'log_loss' refers to binomial and multinomial deviance, the same as used in logistic regression. It is a good choice for classification with probabilistic outputs. For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.  learning_rate : float, default=0.1 Learning rate shrinks the contribution of each tree by `learning_rate`. There is a trade-off between learning_rate and n_estimators. Values must be in the range `[0.0, inf)`.  n_estimators : int, default=100 The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Values must be in the range `[1, inf)`.  subsample : float, default=1.0 The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias. Values must be in the range `(0.0, 1.0]`.  criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse' The function to measure the quality of a split. Supported criteria are 'friedman_mse' for the mean squared error with improvement score by Friedman, 'squared_error' for mean squared error. The default value of 'friedman_mse' is generally the best as it can provide a better approximation in some cases."
GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>,1,sklearn.ensemble._gb.GradientBoostingClassifier,classifier,sklearn,ensemble,_gb,GradientBoostingClassifier,,"Gradient Boosting for classification.  This algorithm builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage ``n_classes_`` regression trees are fit on the negative gradient of the loss function, e.g. binary or multiclass log loss. Binary classification is a special case where only a single regression tree is induced.  :class:`~sklearn.ensemble.HistGradientBoostingClassifier` is a much faster variant of this algorithm for intermediate and large datasets (`n_samples >= 10_000`) and supports monotonic constraints.  Read more in the :ref:`User Guide <gradient_boosting>`.  Parameters ---------- loss : {'log_loss', 'exponential'}, default='log_loss' The loss function to be optimized. 'log_loss' refers to binomial and multinomial deviance, the same as used in logistic regression. It is a good choice for classification with probabilistic outputs. For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.  learning_rate : float, default=0.1 Learning rate shrinks the contribution of each tree by `learning_rate`. There is a trade-off between learning_rate and n_estimators. Values must be in the range `[0.0, inf)`.  n_estimators : int, default=100 The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Values must be in the range `[1, inf)`.  subsample : float, default=1.0 The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias. Values must be in the range `(0.0, 1.0]`.  criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse' The function to measure the quality of a split. Supported criteria are 'friedman_mse' for the mean squared error with improvement score by Friedman, 'squared_error' for mean squared error. The default value of 'friedman_mse' is generally the best as it can provide a better approximation in some cases."
GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>,Medium,sklearn.ensemble._gb.GradientBoostingClassifier,classifier,sklearn,ensemble,_gb,GradientBoostingClassifier,,"Gradient Boosting for classification.  This algorithm builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage ``n_classes_`` regression trees are fit on the negative gradient of the loss function, e.g. binary or multiclass log loss. Binary classification is a special case where only a single regression tree is induced.  :class:`~sklearn.ensemble.HistGradientBoostingClassifier` is a much faster variant of this algorithm for intermediate and large datasets (`n_samples >= 10_000`) and supports monotonic constraints.  Read more in the :ref:`User Guide <gradient_boosting>`.  Parameters ---------- loss : {'log_loss', 'exponential'}, default='log_loss' The loss function to be optimized. 'log_loss' refers to binomial and multinomial deviance, the same as used in logistic regression. It is a good choice for classification with probabilistic outputs. For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.  learning_rate : float, default=0.1 Learning rate shrinks the contribution of each tree by `learning_rate`. There is a trade-off between learning_rate and n_estimators. Values must be in the range `[0.0, inf)`.  n_estimators : int, default=100 The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Values must be in the range `[1, inf)`.  subsample : float, default=1.0 The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias. Values must be in the range `(0.0, 1.0]`.  criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse' The function to measure the quality of a split. Supported criteria are 'friedman_mse' for the mean squared error with improvement score by Friedman, 'squared_error' for mean squared error. The default value of 'friedman_mse' is generally the best as it can provide a better approximation in some cases."
GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>,Medium,sklearn.ensemble._gb.GradientBoostingClassifier,classifier,sklearn,ensemble,_gb,GradientBoostingClassifier,,"Gradient Boosting for classification.  This algorithm builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage ``n_classes_`` regression trees are fit on the negative gradient of the loss function, e.g. binary or multiclass log loss. Binary classification is a special case where only a single regression tree is induced.  :class:`~sklearn.ensemble.HistGradientBoostingClassifier` is a much faster variant of this algorithm for intermediate and large datasets (`n_samples >= 10_000`) and supports monotonic constraints.  Read more in the :ref:`User Guide <gradient_boosting>`.  Parameters ---------- loss : {'log_loss', 'exponential'}, default='log_loss' The loss function to be optimized. 'log_loss' refers to binomial and multinomial deviance, the same as used in logistic regression. It is a good choice for classification with probabilistic outputs. For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.  learning_rate : float, default=0.1 Learning rate shrinks the contribution of each tree by `learning_rate`. There is a trade-off between learning_rate and n_estimators. Values must be in the range `[0.0, inf)`.  n_estimators : int, default=100 The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Values must be in the range `[1, inf)`.  subsample : float, default=1.0 The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias. Values must be in the range `(0.0, 1.0]`.  criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse' The function to measure the quality of a split. Supported criteria are 'friedman_mse' for the mean squared error with improvement score by Friedman, 'squared_error' for mean squared error. The default value of 'friedman_mse' is generally the best as it can provide a better approximation in some cases."
GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>,Medium,sklearn.ensemble._gb.GradientBoostingRegressor,regressor,sklearn,ensemble,_gb,GradientBoostingRegressor,,"Gradient Boosting for regression.  This estimator builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.  :class:`~sklearn.ensemble.HistGradientBoostingRegressor` is a much faster variant of this algorithm for intermediate and large datasets (`n_samples >= 10_000`) and supports monotonic constraints.  Read more in the :ref:`User Guide <gradient_boosting>`.  Parameters ---------- loss : {'squared_error', 'absolute_error', 'huber', 'quantile'},             default='squared_error' Loss function to be optimized. 'squared_error' refers to the squared error for regression. 'absolute_error' refers to the absolute error of regression and is a robust loss function. 'huber' is a combination of the two. 'quantile' allows quantile regression (use `alpha` to specify the quantile). See :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py` for an example that demonstrates quantile regression for creating prediction intervals with `loss='quantile'`.  learning_rate : float, default=0.1 Learning rate shrinks the contribution of each tree by `learning_rate`. There is a trade-off between learning_rate and n_estimators. Values must be in the range `[0.0, inf)`.  n_estimators : int, default=100 The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Values must be in the range `[1, inf)`.  subsample : float, default=1.0 The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias. Values must be in the range `(0.0, 1.0]`.  criterion : {'friedman_mse', 'squared_error'}, default='friedman_mse' The function to measure the quality of a split. Supported criteria are ""friedman_mse"" for the mean squared error with improvement score by Friedman, ""squared_error"" for mean squared error. The default value of ""friedman_mse"" is generally the best as it can provide a better approximation in some cases."
GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>,Small,sklearn.covariance._graph_lasso.GraphicalLasso,unknown,sklearn,covariance,_graph_lasso,GraphicalLasso,,Sparse inverse covariance estimation with an l1-penalized estimator.  For a usage example see :ref:`sphx_glr_auto_examples_applications_plot_stock_market.py`.  Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>,Small,sklearn.covariance._graph_lasso.GraphicalLassoCV,unknown,sklearn,covariance,_graph_lasso,GraphicalLassoCV,,Sparse inverse covariance w/ cross-validated choice of the l1 penalty.  See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <sparse_inverse_covariance>`.
GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>,"Small, Medium",sklearn.model_selection._search.GridSearchCV,unknown,sklearn,model_selection,_search,GridSearchCV,,"Exhaustive search over specified parameter values for an estimator.  Important members are fit, predict.  GridSearchCV implements a ""fit"" and a ""score"" method. It also implements ""score_samples"", ""predict"", ""predict_proba"", ""decision_function"", ""transform"" and ""inverse_transform"" if they are implemented in the estimator used.  The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.  Read more in the :ref:`User Guide <grid_search>`.  Parameters ---------- estimator : estimator object This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a ``score`` function, or ``scoring`` must be passed.  param_grid : dict or list of dictionaries Dictionary with parameters names (`str`) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.  scoring : str, callable, list, tuple or dict, default=None Strategy to evaluate the performance of the cross-validated model on the test set.  If `scoring` represents a single score, one can use:  - a single string (see :ref:`scoring_parameter`); - a callable (see :ref:`scoring_callable`) that returns a single value.  If `scoring` represents multiple scores, one can use:  - a list or tuple of unique strings; - a callable returning a dictionary where the keys are the metric names and the values are the metric scores; - a dictionary with metric names as keys and callables as values.  See :ref:`multimetric_grid_search` for an example.  n_jobs : int, default=None Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>,"Small, Medium",sklearn.cluster._hdbscan.hdbscan.HDBSCAN,cluster,sklearn,cluster,_hdbscan,hdbscan,HDBSCAN,"Cluster data using hierarchical density-based clustering.  HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications with Noise. Performs :class:`~sklearn.cluster.DBSCAN` over varying epsilon values and integrates the result to find a clustering that gives the best stability over epsilon. This allows HDBSCAN to find clusters of varying densities (unlike :class:`~sklearn.cluster.DBSCAN`), and be more robust to parameter selection. Read more in the :ref:`User Guide <hdbscan>`.  For an example of how to use HDBSCAN, as well as a comparison to :class:`~sklearn.cluster.DBSCAN`, please see the :ref:`plotting demo <sphx_glr_auto_examples_cluster_plot_hdbscan.py>`."
HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>,Large,sklearn.feature_extraction.text.HashingVectorizer,transformer,sklearn,feature_extraction,text,HashingVectorizer,,"Convert a collection of text documents to a matrix of token occurrences.  It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm='l1' or projected on the euclidean unit sphere if norm='l2'.  This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.  This strategy has several advantages:  - it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory.  - it is fast to pickle and un-pickle as it holds no state besides the constructor parameters.  - it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit.  There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):  - there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model.  - there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems).  - no IDF weighting as this would render the transformer stateful.  The hash function employed is the signed 32-bit version of Murmurhash3.  For an efficiency comparison of the different feature extractors, see :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.  For an example of document clustering and comparison with :class:`~sklearn.feature_extraction.text.TfidfVectorizer`, see :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.  Read more in the :ref:`User Guide <text_feature_extraction>`.  Parameters ---------- input : {'filename', 'file', 'content'}, default='content' - If `'filename'`, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.  - If `'file'`, the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory.  - If `'content'`, the input is expected to be a sequence of items that can be of type string or byte.  encoding : str, default='utf-8' If bytes or files are given to analyze, this encoding is used to decode.  decode_error : {'strict', 'ignore', 'replace'}, default='strict' Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'.  strip_accents : {'ascii', 'unicode'} or callable, default=None Remove accents and perform other character normalization during the preprocessing step. 'ascii' is a fast method that only works on characters that have a direct ASCII mapping. 'unicode' is a slightly slower method that works on any character. None (default) means no character normalization is performed.  Both 'ascii' and 'unicode' use NFKD normalization from :func:`unicodedata.normalize`.  lowercase : bool, default=True Convert all characters to lowercase before tokenizing.  preprocessor : callable, default=None Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if ``analyzer`` is not callable.  tokenizer : callable, default=None Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if ``analyzer == 'word'``.  stop_words : {'english'}, list, default=None If 'english', a built-in stop word list for English is used. There are several known issues with 'english' and you should consider an alternative (see :ref:`stop_words`).  If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if ``analyzer == 'word'``.  token_pattern : str or None, default=r""(?u)\\b\\w\\w+\\b"" Regular expression denoting what constitutes a ""token"", only used if ``analyzer == 'word'``. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).  If there is a capturing group in token_pattern then the captured group content, not the entire match, becomes the token. At most one capturing group is permitted.  ngram_range : tuple (min_n, max_n), default=(1, 1) The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means only bigrams. Only applies if ``analyzer`` is not callable.  analyzer : {'word', 'char', 'char_wb'} or callable, default='word' Whether the feature should be made of word or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.  If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input."
HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>,Large,sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier,classifier,sklearn,ensemble,_hist_gradient_boosting,gradient_boosting,HistGradientBoostingClassifier,"Histogram-based Gradient Boosting Classification Tree.  This estimator is much faster than :class:`GradientBoostingClassifier<sklearn.ensemble.GradientBoostingClassifier>` for big datasets (n_samples >= 10 000).  This estimator has native support for missing values (NaNs). During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently. If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples.  This implementation is inspired by `LightGBM <https://github.com/Microsoft/LightGBM>`_.  Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`."
HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>,Large,sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor,regressor,sklearn,ensemble,_hist_gradient_boosting,gradient_boosting,HistGradientBoostingRegressor,"Histogram-based Gradient Boosting Regression Tree.  This estimator is much faster than :class:`GradientBoostingRegressor<sklearn.ensemble.GradientBoostingRegressor>` for big datasets (n_samples >= 10 000).  This estimator has native support for missing values (NaNs). During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently. If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples. See :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py` for a usecase example of this feature.  This implementation is inspired by `LightGBM <https://github.com/Microsoft/LightGBM>`_.  Read more in the :ref:`User Guide <histogram_based_gradient_boosting>`."
HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>,Small,sklearn.linear_model._huber.HuberRegressor,regressor,sklearn,linear_model,_huber,HuberRegressor,,"L2-regularized linear regression model that is robust to outliers.  The Huber Regressor optimizes the squared loss for the samples where ``|(y - Xw - c) / sigma| < epsilon`` and the absolute loss for the samples where ``|(y - Xw - c) / sigma| > epsilon``, where the model coefficients ``w``, the intercept ``c`` and the scale ``sigma`` are parameters to be optimized. The parameter `sigma` makes sure that if `y` is scaled up or down by a certain factor, one does not need to rescale `epsilon` to achieve the same robustness. Note that this does not take into account the fact that the different features of `X` may be of different scales.  The Huber loss function has the advantage of not being heavily influenced by the outliers while not completely ignoring their effect.  Read more in the :ref:`User Guide <huber_regression>`"
IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>,Large,sklearn.decomposition._incremental_pca.IncrementalPCA,transformer,sklearn,decomposition,_incremental_pca,IncrementalPCA,,"Incremental principal components analysis (IPCA).  Linear dimensionality reduction using Singular Value Decomposition of the data, keeping only the most significant singular vectors to project the data to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.  Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA, and allows sparse input.  This algorithm has constant memory complexity, on the order of ``batch_size * n_features``, enabling use of np.memmap files without loading the entire file into memory. For sparse matrices, the input is converted to dense in batches (in order to be able to subtract the mean) which avoids storing the entire dense matrix at any one time.  The computational overhead of each SVD is ``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples remain in memory at a time. There will be ``n_samples / batch_size`` SVD computations to get the principal components, versus 1 large SVD of complexity ``O(n_samples * n_features ** 2)`` for PCA.  For a usage example, see :ref:`sphx_glr_auto_examples_decomposition_plot_incremental_pca.py`.  Read more in the :ref:`User Guide <IncrementalPCA>`."
IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>,"Medium, Large",sklearn.ensemble._iforest.IsolationForest,unknown,sklearn,ensemble,_iforest,IsolationForest,,"Isolation Forest Algorithm.  Return the anomaly score of each sample using the IsolationForest algorithm  The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.  Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.  This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.  Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.  Read more in the :ref:`User Guide <isolation_forest>`."
Isomap,<class 'sklearn.manifold._isomap.Isomap'>,Small,sklearn.manifold._isomap.Isomap,transformer,sklearn,manifold,_isomap,Isomap,,"Isomap Embedding.  Non-linear dimensionality reduction through Isometric Mapping  Read more in the :ref:`User Guide <isomap>`.  Parameters ---------- n_neighbors : int or None, default=5 Number of neighbors to consider for each point. If `n_neighbors` is an int, then `radius` must be `None`.  radius : float or None, default=None Limiting distance of neighbors to return. If `radius` is a float, then `n_neighbors` must be set to `None`."
IsotonicRegression,<class 'sklearn.isotonic.IsotonicRegression'>,Small,sklearn.isotonic.IsotonicRegression,"regressor, transformer",sklearn,isotonic,IsotonicRegression,,,Isotonic regression model.  Read more in the :ref:`User Guide <isotonic>`.
KBinsDiscretizer,<class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>,Large,sklearn.preprocessing._discretization.KBinsDiscretizer,transformer,sklearn,preprocessing,_discretization,KBinsDiscretizer,,Bin continuous data into intervals.  Read more in the :ref:`User Guide <preprocessing_discretization>`.
KMeans,<class 'sklearn.cluster._kmeans.KMeans'>,Medium,sklearn.cluster._kmeans.KMeans,"cluster, transformer",sklearn,cluster,_kmeans,KMeans,,"K-Means clustering.  Read more in the :ref:`User Guide <k_means>`.  Parameters ----------  n_clusters : int, default=8 The number of clusters to form as well as the number of centroids to generate.  For an example of how to choose an optimal value for `n_clusters` refer to :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.  init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++' Method for initialization:  * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is ""greedy k-means++"". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them.  * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids.  * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers.  * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization.  For an example of how to use the different `init` strategies, see :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.  For an evaluation of the impact of initialization, see the example :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_stability_low_dim_dense.py`.  n_init : 'auto' or int, default='auto' Number of times the k-means algorithm is run with different centroid seeds. The final results is the best output of `n_init` consecutive runs in terms of inertia. Several runs are recommended for sparse high-dimensional problems (see :ref:`kmeans_sparse_high_dim`).  When `n_init='auto'`, the number of runs depends on the value of init: 10 if using `init='random'` or `init` is a callable; 1 if using `init='k-means++'` or `init` is an array-like."
KNNImputer,<class 'sklearn.impute._knn.KNNImputer'>,Medium,sklearn.impute._knn.KNNImputer,transformer,sklearn,impute,_knn,KNNImputer,,Imputation for completing missing values using k-Nearest Neighbors.  Each sample's missing values are imputed using the mean value from `n_neighbors` nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.  Read more in the :ref:`User Guide <knnimpute>`.
KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>,Small,sklearn.neighbors._classification.KNeighborsClassifier,classifier,sklearn,neighbors,_classification,KNeighborsClassifier,,"Classifier implementing the k-nearest neighbors vote.  Read more in the :ref:`User Guide <classification>`.  Parameters ---------- n_neighbors : int, default=5 Number of neighbors to use by default for :meth:`kneighbors` queries.  weights : {'uniform', 'distance'}, callable or None, default='uniform' Weight function used in prediction.  Possible values:  - 'uniform' : uniform weights.  All points in each neighborhood are weighted equally. - 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. - [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.  Refer to the example entitled :ref:`sphx_glr_auto_examples_neighbors_plot_classification.py` showing the impact of the `weights` parameter on the decision boundary.  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto' Algorithm used to compute the nearest neighbors:  - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDTree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method.  Note: fitting on sparse input will override the setting of this parameter, using brute force.  leaf_size : int, default=30 Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem.  p : float, default=2 Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected to be positive.  metric : str or callable, default='minkowski' Metric to use for distance computation. Default is ""minkowski"", which results in the standard Euclidean distance when p = 2. See the documentation of `scipy.spatial.distance <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric values.  If metric is ""precomputed"", X is assumed to be a distance matrix and must be square during fit. X may be a :term:`sparse graph`, in which case only ""nonzero"" elements may be considered neighbors.  If metric is a callable function, it takes two arrays representing 1D vectors as inputs and must return one value indicating the distance between those vectors. This works for Scipy's metrics, but is less efficient than passing the metric name as a string.  metric_params : dict, default=None Additional keyword arguments for the metric function.  n_jobs : int, default=None The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details. Doesn't affect :meth:`fit` method.  Attributes ---------- classes_ : array of shape (n_classes,) Class labels known to the classifier  effective_metric_ : str or callble The distance metric used. It will be same as the `metric` parameter or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to 'minkowski' and `p` parameter set to 2.  effective_metric_params_ : dict Additional keyword arguments for the metric function. For most metrics will be same with `metric_params` parameter, but may also contain the `p` parameter value if the `effective_metric_` attribute is set to 'minkowski'.  n_features_in_ : int Number of features seen during :term:`fit`."
KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>,Small,sklearn.neighbors._regression.KNeighborsRegressor,regressor,sklearn,neighbors,_regression,KNeighborsRegressor,,Regression based on k-nearest neighbors.  The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.  Read more in the :ref:`User Guide <regression>`.
KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>,Small,sklearn.neighbors._graph.KNeighborsTransformer,transformer,sklearn,neighbors,_graph,KNeighborsTransformer,,Transform X into a (weighted) graph of k nearest neighbors.  The transformed data is a sparse graph as returned by kneighbors_graph.  Read more in the :ref:`User Guide <neighbors_transformer>`.
KernelCenterer,<class 'sklearn.preprocessing._data.KernelCenterer'>,"Small, Medium, Large",sklearn.preprocessing._data.KernelCenterer,transformer,sklearn,preprocessing,_data,KernelCenterer,,Center an arbitrary kernel matrix :math:`K`.  Let define a kernel :math:`K` such that:
KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>,Small,sklearn.neighbors._kde.KernelDensity,unknown,sklearn,neighbors,_kde,KernelDensity,,"Kernel Density Estimation.  Read more in the :ref:`User Guide <kernel_density>`.  Parameters ---------- bandwidth : float or {""scott"", ""silverman""}, default=1.0 The bandwidth of the kernel. If bandwidth is a float, it defines the bandwidth of the kernel. If bandwidth is a string, one of the estimation methods is implemented.  algorithm : {'kd_tree', 'ball_tree', 'auto'}, default='auto' The tree algorithm to use.  kernel : {'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear',                  'cosine'}, default='gaussian' The kernel to use.  metric : str, default='euclidean' Metric to use for distance computation. See the documentation of `scipy.spatial.distance <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric values.  Not all metrics are valid with all algorithms: refer to the documentation of :class:`BallTree` and :class:`KDTree`. Note that the normalization of the density output is correct only for the Euclidean distance metric.  atol : float, default=0 The desired absolute tolerance of the result.  A larger tolerance will generally lead to faster execution.  rtol : float, default=0 The desired relative tolerance of the result.  A larger tolerance will generally lead to faster execution.  breadth_first : bool, default=True If true (default), use a breadth-first approach to the problem. Otherwise use a depth-first approach.  leaf_size : int, default=40 Specify the leaf size of the underlying tree.  See :class:`BallTree` or :class:`KDTree` for details.  metric_params : dict, default=None Additional parameters to be passed to the tree for use with the metric.  For more information, see the documentation of :class:`BallTree` or :class:`KDTree`.  Attributes ---------- n_features_in_ : int Number of features seen during :term:`fit`."
KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>,Small,sklearn.decomposition._kernel_pca.KernelPCA,transformer,sklearn,decomposition,_kernel_pca,KernelPCA,,"Kernel Principal component analysis (KPCA).  Non-linear dimensionality reduction through the use of kernels [1]_, see also :ref:`metrics`.  It uses the :func:`scipy.linalg.eigh` LAPACK implementation of the full SVD or the :func:`scipy.sparse.linalg.eigsh` ARPACK implementation of the truncated SVD, depending on the shape of the input data and the number of components to extract. It can also use a randomized truncated SVD by the method proposed in [3]_, see `eigen_solver`.  For a usage example and comparison between Principal Components Analysis (PCA) and its kernelized version (KPCA), see :ref:`sphx_glr_auto_examples_decomposition_plot_kernel_pca.py`.  For a usage example in denoising images using KPCA, see :ref:`sphx_glr_auto_examples_applications_plot_digits_denoising.py`.  Read more in the :ref:`User Guide <kernel_PCA>`.  Parameters ---------- n_components : int, default=None Number of components. If None, all non-zero components are kept.  kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'cosine', 'precomputed'}             or callable, default='linear' Kernel used for PCA.  gamma : float, default=None Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``.  degree : float, default=3 Degree for poly kernels. Ignored by other kernels.  coef0 : float, default=1 Independent term in poly and sigmoid kernels. Ignored by other kernels.  kernel_params : dict, default=None Parameters (keyword arguments) and values for kernel passed as callable object. Ignored by other kernels.  alpha : float, default=1.0 Hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True).  fit_inverse_transform : bool, default=False Learn the inverse transform for non-precomputed kernels (i.e. learn to find the pre-image of a point). This method is based on [2]_.  eigen_solver : {'auto', 'dense', 'arpack', 'randomized'},             default='auto' Select eigensolver to use. If `n_components` is much less than the number of training samples, randomized (or arpack to a smaller extent) may be more efficient than the dense eigensolver. Randomized SVD is performed according to the method of Halko et al [3]_.  auto : the solver is selected by a default policy based on n_samples (the number of training samples) and `n_components`: if the number of components to extract is less than 10 (strict) and the number of samples is more than 200 (strict), the 'arpack' method is enabled. Otherwise the exact full eigenvalue decomposition is computed and optionally truncated afterwards ('dense' method). dense : run exact full eigenvalue decomposition calling the standard LAPACK solver via `scipy.linalg.eigh`, and select the components by postprocessing arpack : run SVD truncated to n_components calling ARPACK solver using `scipy.sparse.linalg.eigsh`. It requires strictly 0 < n_components < n_samples randomized : run randomized SVD by the method of Halko et al. [3]_. The current implementation selects eigenvalues based on their module; therefore using this method can lead to unexpected results if the kernel is not positive semi-definite. See also [4]_."
KernelRidge,<class 'sklearn.kernel_ridge.KernelRidge'>,Small,sklearn.kernel_ridge.KernelRidge,regressor,sklearn,kernel_ridge,KernelRidge,,,"Kernel ridge regression.  Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.  The form of the model learned by KRR is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses epsilon-insensitive loss, both combined with l2 regularization. In contrast to SVR, fitting a KRR model can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for epsilon > 0, at prediction-time.  This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).  Read more in the :ref:`User Guide <kernel_ridge>`.  Parameters ---------- alpha : float or array-like of shape (n_targets,), default=1.0 Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear models such as :class:`~sklearn.linear_model.LogisticRegression` or :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number. See :ref:`ridge_regression` for formula.  kernel : str or callable, default=""linear"" Kernel mapping used internally. This parameter is directly passed to :class:`~sklearn.metrics.pairwise.pairwise_kernels`. If `kernel` is a string, it must be one of the metrics in `pairwise.PAIRWISE_KERNEL_FUNCTIONS` or ""precomputed"". If `kernel` is ""precomputed"", X is assumed to be a kernel matrix. Alternatively, if `kernel` is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two rows from X as input and return the corresponding kernel value as a single number. This means that callables from :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on matrices, not single samples. Use the string identifying the kernel instead.  gamma : float, default=None Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels.  degree : float, default=3 Degree of the polynomial kernel. Ignored by other kernels.  coef0 : float, default=1 Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.  kernel_params : dict, default=None Additional parameters (keyword arguments) for kernel function passed as callable object.  Attributes ---------- dual_coef_ : ndarray of shape (n_samples,) or (n_samples, n_targets) Representation of weight vector(s) in kernel space  X_fit_ : {ndarray, sparse matrix} of shape (n_samples, n_features) Training data, which is also required for prediction. If kernel == ""precomputed"" this is instead the precomputed training matrix, of shape (n_samples, n_samples).  n_features_in_ : int Number of features seen during :term:`fit`."
LabelBinarizer,<class 'sklearn.preprocessing._label.LabelBinarizer'>,Large,sklearn.preprocessing._label.LabelBinarizer,transformer,sklearn,preprocessing,_label,LabelBinarizer,,"Binarize labels in a one-vs-all fashion.  Several regression and binary classification algorithms are available in scikit-learn. A simple way to extend these algorithms to the multi-class classification case is to use the so-called one-vs-all scheme.  At learning time, this simply consists in learning one regressor or binary classifier per class. In doing so, one needs to convert multi-class labels to binary labels (belong or does not belong to the class). `LabelBinarizer` makes this process easy with the transform method.  At prediction time, one assigns the class for which the corresponding model gave the greatest confidence. `LabelBinarizer` makes this easy with the :meth:`inverse_transform` method.  Read more in the :ref:`User Guide <preprocessing_targets>`.  Parameters ---------- neg_label : int, default=0 Value with which negative labels must be encoded.  pos_label : int, default=1 Value with which positive labels must be encoded.  sparse_output : bool, default=False True if the returned array from transform is desired to be in sparse CSR format.  Attributes ---------- classes_ : ndarray of shape (n_classes,) Holds the label for each class.  y_type_ : str Represents the type of the target data as evaluated by :func:`~sklearn.utils.multiclass.type_of_target`. Possible type are 'continuous', 'continuous-multioutput', 'binary', 'multiclass', 'multiclass-multioutput', 'multilabel-indicator', and 'unknown'.  sparse_input_ : bool `True` if the input data to transform is given as a sparse matrix, `False` otherwise.  See Also -------- label_binarize : Function to perform the transform operation of LabelBinarizer with fixed classes. OneHotEncoder : Encode categorical features using a one-hot aka one-of-K scheme.  Examples -------- >>> from sklearn.preprocessing import LabelBinarizer >>> lb = LabelBinarizer() >>> lb.fit([1, 2, 6, 4, 2]) LabelBinarizer() >>> lb.classes_ array([1, 2, 4, 6]) >>> lb.transform([1, 6]) array([[1, 0, 0, 0], [0, 0, 0, 1]])  Binary targets transform to a column vector  >>> lb = LabelBinarizer() >>> lb.fit_transform(['yes', 'no', 'no', 'yes']) array([[1], [0], [0], [1]])  Passing a 2D matrix for multilabel classification  >>> import numpy as np >>> lb.fit(np.array([[0, 1, 1], [1, 0, 0]])) LabelBinarizer() >>> lb.classes_ array([0, 1, 2]) >>> lb.transform([0, 1, 2, 1]) array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0]])"
LabelEncoder,<class 'sklearn.preprocessing._label.LabelEncoder'>,Large,sklearn.preprocessing._label.LabelEncoder,transformer,sklearn,preprocessing,_label,LabelEncoder,,"Encode target labels with value between 0 and n_classes-1.  This transformer should be used to encode target values, *i.e.* `y`, and not the input `X`.  Read more in the :ref:`User Guide <preprocessing_targets>`."
LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>,"Small, Medium",sklearn.semi_supervised._label_propagation.LabelPropagation,classifier,sklearn,semi_supervised,_label_propagation,LabelPropagation,,"Label Propagation classifier.  Read more in the :ref:`User Guide <label_propagation>`.  Parameters ---------- kernel : {'knn', 'rbf'} or callable, default='rbf' String identifier for kernel function to use or the kernel function itself. Only 'rbf' and 'knn' strings are valid inputs. The function passed should take two inputs, each of shape (n_samples, n_features), and return a (n_samples, n_samples) shaped weight matrix.  gamma : float, default=20 Parameter for rbf kernel.  n_neighbors : int, default=7 Parameter for knn kernel which need to be strictly positive.  max_iter : int, default=1000 Change maximum number of iterations allowed.  tol : float, default=1e-3 Convergence tolerance: threshold to consider the system at steady state.  n_jobs : int, default=None The number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.  Attributes ---------- X_ : {array-like, sparse matrix} of shape (n_samples, n_features) Input array.  classes_ : ndarray of shape (n_classes,) The distinct labels used in classifying instances.  label_distributions_ : ndarray of shape (n_samples, n_classes) Categorical distribution for each item.  transduction_ : ndarray of shape (n_samples) Label assigned to each item during :term:`fit`.  n_features_in_ : int Number of features seen during :term:`fit`."
LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>,"Small, Medium",sklearn.semi_supervised._label_propagation.LabelSpreading,classifier,sklearn,semi_supervised,_label_propagation,LabelSpreading,,"LabelSpreading model for semi-supervised learning.  This model is similar to the basic Label Propagation algorithm, but uses affinity matrix based on the normalized graph Laplacian and soft clamping across the labels.  Read more in the :ref:`User Guide <label_propagation>`.  Parameters ---------- kernel : {'knn', 'rbf'} or callable, default='rbf' String identifier for kernel function to use or the kernel function itself. Only 'rbf' and 'knn' strings are valid inputs. The function passed should take two inputs, each of shape (n_samples, n_features), and return a (n_samples, n_samples) shaped weight matrix.  gamma : float, default=20 Parameter for rbf kernel.  n_neighbors : int, default=7 Parameter for knn kernel which is a strictly positive integer.  alpha : float, default=0.2 Clamping factor. A value in (0, 1) that specifies the relative amount that an instance should adopt the information from its neighbors as opposed to its initial label. alpha=0 means keeping the initial label information; alpha=1 means replacing all initial information.  max_iter : int, default=30 Maximum number of iterations allowed.  tol : float, default=1e-3 Convergence tolerance: threshold to consider the system at steady state.  n_jobs : int, default=None The number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.  Attributes ---------- X_ : ndarray of shape (n_samples, n_features) Input array.  classes_ : ndarray of shape (n_classes,) The distinct labels used in classifying instances.  label_distributions_ : ndarray of shape (n_samples, n_classes) Categorical distribution for each item.  transduction_ : ndarray of shape (n_samples,) Label assigned to each item during :term:`fit`.  n_features_in_ : int Number of features seen during :term:`fit`."
Lars,<class 'sklearn.linear_model._least_angle.Lars'>,"Small, Medium",sklearn.linear_model._least_angle.Lars,regressor,sklearn,linear_model,_least_angle,Lars,,"Least Angle Regression model a.k.a. LAR.  Read more in the :ref:`User Guide <least_angle_regression>`.  Parameters ---------- fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).  verbose : bool or int, default=False Sets the verbosity amount.  precompute : bool, 'auto' or array-like , default='auto' Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.  n_nonzero_coefs : int, default=500 Target number of non-zero coefficients. Use ``np.inf`` for no limit.  eps : float, default=np.finfo(float).eps The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.  copy_X : bool, default=True If ``True``, X will be copied; else, it may be overwritten.  fit_path : bool, default=True If True the full path is stored in the ``coef_path_`` attribute. If you compute the solution for a large problem or many targets, setting ``fit_path`` to ``False`` will lead to a speedup, especially with a small alpha.  jitter : float, default=None Upper bound on a uniform noise parameter to be added to the `y` values, to satisfy the model's assumption of one-at-a-time computations. Might help with stability."
LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>,"Small, Medium",sklearn.linear_model._least_angle.LarsCV,regressor,sklearn,linear_model,_least_angle,LarsCV,,"Cross-validated Least Angle Regression model.  See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <least_angle_regression>`.  Parameters ---------- fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).  verbose : bool or int, default=False Sets the verbosity amount.  max_iter : int, default=500 Maximum number of iterations to perform.  precompute : bool, 'auto' or array-like , default='auto' Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X.  cv : int, cross-validation generator or an iterable, default=None Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here."
Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>,"Small, Medium",sklearn.linear_model._coordinate_descent.Lasso,regressor,sklearn,linear_model,_coordinate_descent,Lasso,,"Linear Model trained with L1 prior as regularizer (aka the Lasso).  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Technically the Lasso model is optimizing the same objective function as the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).  Read more in the :ref:`User Guide <lasso>`.  Parameters ---------- alpha : float, default=1.0 Constant that multiplies the L1 term, controlling regularization strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.  When `alpha = 0`, the objective is equivalent to ordinary least squares, solved by the :class:`LinearRegression` object. For numerical reasons, using `alpha = 0` with the `Lasso` object is not advised. Instead, you should use the :class:`LinearRegression` object.  fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).  precompute : bool or array-like of shape (n_features, n_features),                 default=False Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix can also be passed as argument. For sparse input this option is always ``False`` to preserve sparsity.  copy_X : bool, default=True If ``True``, X will be copied; else, it may be overwritten.  max_iter : int, default=1000 The maximum number of iterations.  tol : float, default=1e-4 The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``, see Notes below.  warm_start : bool, default=False When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.  positive : bool, default=False When set to ``True``, forces the coefficients to be positive.  random_state : int, RandomState instance, default=None The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.  selection : {'cyclic', 'random'}, default='cyclic' If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.  Attributes ---------- coef_ : ndarray of shape (n_features,) or (n_targets, n_features) Parameter vector (w in the cost function formula).  dual_gap_ : float or ndarray of shape (n_targets,) Given param alpha, the dual gaps at the end of the optimization, same shape as each observation of y.  sparse_coef_ : sparse matrix of shape (n_features, 1) or             (n_targets, n_features) Readonly property derived from ``coef_``.  intercept_ : float or ndarray of shape (n_targets,) Independent term in decision function.  n_iter_ : int or list of int Number of iterations run by the coordinate descent solver to reach the specified tolerance.  n_features_in_ : int Number of features seen during :term:`fit`."
LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>,"Small, Medium",sklearn.linear_model._coordinate_descent.LassoCV,regressor,sklearn,linear_model,_coordinate_descent,LassoCV,,"Lasso linear model with iterative fitting along a regularization path.  See glossary entry for :term:`cross-validation estimator`.  The best model is selected by cross-validation.  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <lasso>`.  Parameters ---------- eps : float, default=1e-3 Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``.  n_alphas : int, default=100 Number of alphas along the regularization path.  alphas : array-like, default=None List of alphas where to compute the models. If ``None`` alphas are set automatically.  fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).  precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto' Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.  max_iter : int, default=1000 The maximum number of iterations.  tol : float, default=1e-4 The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``.  copy_X : bool, default=True If ``True``, X will be copied; else, it may be overwritten.  cv : int, cross-validation generator or iterable, default=None Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - int, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For int/None inputs, :class:`~sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here."
LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>,"Small, Medium",sklearn.linear_model._least_angle.LassoLars,regressor,sklearn,linear_model,_least_angle,LassoLars,,"Lasso model fit with Least Angle Regression a.k.a. Lars.  It is a Linear Model trained with an L1 prior as regularizer.  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <least_angle_regression>`.  Parameters ---------- alpha : float, default=1.0 Constant that multiplies the penalty term. Defaults to 1.0. ``alpha = 0`` is equivalent to an ordinary least square, solved by :class:`LinearRegression`. For numerical reasons, using ``alpha = 0`` with the LassoLars object is not advised and you should prefer the LinearRegression object.  fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).  verbose : bool or int, default=False Sets the verbosity amount.  precompute : bool, 'auto' or array-like, default='auto' Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.  max_iter : int, default=500 Maximum number of iterations to perform.  eps : float, default=np.finfo(float).eps The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.  copy_X : bool, default=True If True, X will be copied; else, it may be overwritten.  fit_path : bool, default=True If ``True`` the full path is stored in the ``coef_path_`` attribute. If you compute the solution for a large problem or many targets, setting ``fit_path`` to ``False`` will lead to a speedup, especially with a small alpha.  positive : bool, default=False Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients will not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator.  jitter : float, default=None Upper bound on a uniform noise parameter to be added to the `y` values, to satisfy the model's assumption of one-at-a-time computations. Might help with stability."
LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>,"Small, Medium",sklearn.linear_model._least_angle.LassoLarsCV,regressor,sklearn,linear_model,_least_angle,LassoLarsCV,,"Cross-validated Lasso, using the LARS algorithm.  See glossary entry for :term:`cross-validation estimator`.  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <least_angle_regression>`.  Parameters ---------- fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).  verbose : bool or int, default=False Sets the verbosity amount.  max_iter : int, default=500 Maximum number of iterations to perform.  precompute : bool or 'auto' , default='auto' Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X.  cv : int, cross-validation generator or an iterable, default=None Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here."
LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>,"Small, Medium",sklearn.linear_model._least_angle.LassoLarsIC,regressor,sklearn,linear_model,_least_angle,LassoLarsIC,,"Lasso model fit with Lars using BIC or AIC for model selection.  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  AIC is the Akaike information criterion [2]_ and BIC is the Bayes Information criterion [3]_. Such criteria are useful to select the value of the regularization parameter by making a trade-off between the goodness of fit and the complexity of the model. A good model should explain well the data while being simple.  Read more in the :ref:`User Guide <lasso_lars_ic>`.  Parameters ---------- criterion : {'aic', 'bic'}, default='aic' The type of criterion to use.  fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).  verbose : bool or int, default=False Sets the verbosity amount.  precompute : bool, 'auto' or array-like, default='auto' Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.  max_iter : int, default=500 Maximum number of iterations to perform. Can be used for early stopping.  eps : float, default=np.finfo(float).eps The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.  copy_X : bool, default=True If True, X will be copied; else, it may be overwritten.  positive : bool, default=False Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients do not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator. As a consequence using LassoLarsIC only makes sense for problems where a sparse solution is expected and/or reached.  noise_variance : float, default=None The estimated noise variance of the data. If `None`, an unbiased estimate is computed by an OLS model. However, it is only possible in the case where `n_samples > n_features + fit_intercept`."
LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>,"Medium, Large",sklearn.decomposition._lda.LatentDirichletAllocation,transformer,sklearn,decomposition,_lda,LatentDirichletAllocation,,Latent Dirichlet Allocation with online variational Bayes algorithm.  The implementation is based on [1]_ and [2]_.
LedoitWolf,<class 'sklearn.covariance._shrunk_covariance.LedoitWolf'>,Small,sklearn.covariance._shrunk_covariance.LedoitWolf,unknown,sklearn,covariance,_shrunk_covariance,LedoitWolf,,"LedoitWolf Estimator.  Ledoit-Wolf is a particular form of shrinkage, where the shrinkage coefficient is computed using O. Ledoit and M. Wolf's formula as described in ""A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices"", Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.  Read more in the :ref:`User Guide <shrunk_covariance>`.  Parameters ---------- store_precision : bool, default=True Specify if the estimated precision is stored.  assume_centered : bool, default=False If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data will be centered before computation.  block_size : int, default=1000 Size of blocks into which the covariance matrix will be split during its Ledoit-Wolf estimation. This is purely a memory optimization and does not affect results.  Attributes ---------- covariance_ : ndarray of shape (n_features, n_features) Estimated covariance matrix.  location_ : ndarray of shape (n_features,) Estimated location, i.e. the estimated mean.  precision_ : ndarray of shape (n_features, n_features) Estimated pseudo inverse matrix. (stored only if store_precision is True)  shrinkage_ : float Coefficient in the convex combination used for the computation of the shrunk estimate. Range is [0, 1].  n_features_in_ : int Number of features seen during :term:`fit`."
LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>,Medium,sklearn.discriminant_analysis.LinearDiscriminantAnalysis,"classifier, transformer",sklearn,discriminant_analysis,LinearDiscriminantAnalysis,,,"Linear Discriminant Analysis.  A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes' rule.  The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.  The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the `transform` method."
LinearRegression,<class 'sklearn.linear_model._base.LinearRegression'>,"Medium, Large",sklearn.linear_model._base.LinearRegression,regressor,sklearn,linear_model,_base,LinearRegression,,Ordinary least squares Linear Regression.
LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>,Medium,sklearn.svm._classes.LinearSVC,classifier,sklearn,svm,_classes,LinearSVC,,"Linear Support Vector Classification.  Similar to SVC with parameter kernel='linear', but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.  The main differences between :class:`~sklearn.svm.LinearSVC` and :class:`~sklearn.svm.SVC` lie in the loss function used by default, and in the handling of intercept regularization between those two implementations.  This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.  Read more in the :ref:`User Guide <svm_classification>`.  Parameters ---------- penalty : {'l1', 'l2'}, default='l2' Specifies the norm used in the penalization. The 'l2' penalty is the standard used in SVC. The 'l1' leads to ``coef_`` vectors that are sparse.  loss : {'hinge', 'squared_hinge'}, default='squared_hinge' Specifies the loss function. 'hinge' is the standard SVM loss (used e.g. by the SVC class) while 'squared_hinge' is the square of the hinge loss. The combination of ``penalty='l1'`` and ``loss='hinge'`` is not supported.  dual : ""auto"" or bool, default=""auto"" Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features. `dual=""auto""` will choose the value of the parameter automatically, based on the values of `n_samples`, `n_features`, `loss`, `multi_class` and `penalty`. If `n_samples` < `n_features` and optimizer supports chosen `loss`, `multi_class` and `penalty`, then dual will be set to True, otherwise it will be set to False."
LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>,Medium,sklearn.svm._classes.LinearSVR,regressor,sklearn,svm,_classes,LinearSVR,,"Linear Support Vector Regression.  Similar to SVR with parameter kernel='linear', but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.  The main differences between :class:`~sklearn.svm.LinearSVR` and :class:`~sklearn.svm.SVR` lie in the loss function used by default, and in the handling of intercept regularization between those two implementations.  This class supports both dense and sparse input.  Read more in the :ref:`User Guide <svm_regression>`."
LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>,"Small, Medium",sklearn.neighbors._lof.LocalOutlierFactor,unknown,sklearn,neighbors,_lof,LocalOutlierFactor,,"Unsupervised Outlier Detection using the Local Outlier Factor (LOF).  The anomaly score of each sample is called the Local Outlier Factor. It measures the local deviation of the density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers."
LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>,Small,sklearn.manifold._locally_linear.LocallyLinearEmbedding,transformer,sklearn,manifold,_locally_linear,LocallyLinearEmbedding,,"Locally Linear Embedding.  Read more in the :ref:`User Guide <locally_linear_embedding>`.  Parameters ---------- n_neighbors : int, default=5 Number of neighbors to consider for each point.  n_components : int, default=2 Number of coordinates for the manifold.  reg : float, default=1e-3 Regularization constant, multiplies the trace of the local covariance matrix of the distances.  eigen_solver : {'auto', 'arpack', 'dense'}, default='auto' The solver used to compute the eigenvectors. The available options are:  - `'auto'` : algorithm will attempt to choose the best method for input data. - `'arpack'` : use arnoldi iteration in shift-invert mode. For this method, M may be a dense matrix, sparse matrix, or general linear operator. - `'dense'`  : use standard dense matrix operations for the eigenvalue decomposition. For this method, M must be an array or matrix type. This method should be avoided for large problems."
LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>,Large,sklearn.linear_model._logistic.LogisticRegression,classifier,sklearn,linear_model,_logistic,LogisticRegression,,"Logistic Regression (aka logit, MaxEnt) classifier.  This class implements regularized logistic regression using the 'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note that regularization is applied by default**. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization with primal formulation, or no regularization. The 'liblinear' solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. The Elastic-Net regularization is only supported by the 'saga' solver.  For :term:`multiclass` problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss. 'liblinear' and 'newton-cholesky' only handle binary classification but can be extended to handle multiclass by using :class:`~sklearn.multiclass.OneVsRestClassifier`.  Read more in the :ref:`User Guide <logistic_regression>`.  Parameters ---------- penalty : {'l1', 'l2', 'elasticnet', None}, default='l2' Specify the norm of the penalty:  - `None`: no penalty is added; - `'l2'`: add a L2 penalty term and it is the default choice; - `'l1'`: add a L1 penalty term; - `'elasticnet'`: both L1 and L2 penalty terms are added."
LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>,"Medium, Large",sklearn.linear_model._logistic.LogisticRegressionCV,classifier,sklearn,linear_model,_logistic,LogisticRegressionCV,,"Logistic Regression CV (aka logit, MaxEnt) classifier.  See glossary entry for :term:`cross-validation estimator`.  This class implements logistic regression using liblinear, newton-cg, sag or lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. Elastic-Net penalty is only supported by the saga solver.  For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter is selected by the cross-validator :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).  Read more in the :ref:`User Guide <logistic_regression>`.  Parameters ---------- Cs : int or list of floats, default=10 Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. Like in support vector machines, smaller values specify stronger regularization.  fit_intercept : bool, default=True Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.  cv : int or cross-validation generator, default=None The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module :mod:`sklearn.model_selection` module for the list of possible cross-validation objects."
MDS,<class 'sklearn.manifold._mds.MDS'>,Small,sklearn.manifold._mds.MDS,unknown,sklearn,manifold,_mds,MDS,,"Multidimensional scaling.  Read more in the :ref:`User Guide <multidimensional_scaling>`.  Parameters ---------- n_components : int, default=2 Number of dimensions in which to immerse the dissimilarities.  metric : bool, default=True If ``True``, perform metric MDS; otherwise, perform nonmetric MDS. When ``False`` (i.e. non-metric MDS), dissimilarities with 0 are considered as missing values.  n_init : int, default=4 Number of times the SMACOF algorithm will be run with different initializations. The final results will be the best output of the runs, determined by the run with the smallest final stress.  max_iter : int, default=300 Maximum number of iterations of the SMACOF algorithm for a single run.  verbose : int, default=0 Level of verbosity.  eps : float, default=1e-3 Relative tolerance with respect to stress at which to declare convergence. The value of `eps` should be tuned separately depending on whether or not `normalized_stress` is being used.  n_jobs : int, default=None The number of jobs to use for the computation. If multiple initializations are used (``n_init``), each run of the algorithm is computed in parallel.  ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.  random_state : int, RandomState instance or None, default=None Determines the random number generator used to initialize the centers. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.  dissimilarity : {'euclidean', 'precomputed'}, default='euclidean' Dissimilarity measure to use:  - 'euclidean': Pairwise Euclidean distances between points in the dataset.  - 'precomputed': Pre-computed dissimilarities are passed directly to ``fit`` and ``fit_transform``.  normalized_stress : bool or ""auto"" default=""auto"" Whether use and return normed stress value (Stress-1) instead of raw stress calculated by default. Only supported in non-metric MDS."
MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>,1,sklearn.neural_network._multilayer_perceptron.MLPClassifier,classifier,sklearn,neural_network,_multilayer_perceptron,MLPClassifier,,Multi-layer Perceptron classifier.  This model optimizes the log-loss function using LBFGS or stochastic gradient descent.
MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>,1,sklearn.neural_network._multilayer_perceptron.MLPClassifier,classifier,sklearn,neural_network,_multilayer_perceptron,MLPClassifier,,Multi-layer Perceptron classifier.  This model optimizes the log-loss function using LBFGS or stochastic gradient descent.
MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>,Medium,sklearn.neural_network._multilayer_perceptron.MLPClassifier,classifier,sklearn,neural_network,_multilayer_perceptron,MLPClassifier,,Multi-layer Perceptron classifier.  This model optimizes the log-loss function using LBFGS or stochastic gradient descent.
MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>,Medium,sklearn.neural_network._multilayer_perceptron.MLPClassifier,classifier,sklearn,neural_network,_multilayer_perceptron,MLPClassifier,,Multi-layer Perceptron classifier.  This model optimizes the log-loss function using LBFGS or stochastic gradient descent.
MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>,1,sklearn.neural_network._multilayer_perceptron.MLPRegressor,regressor,sklearn,neural_network,_multilayer_perceptron,MLPRegressor,,Multi-layer Perceptron regressor.  This model optimizes the squared error using LBFGS or stochastic gradient descent.
MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>,1,sklearn.neural_network._multilayer_perceptron.MLPRegressor,regressor,sklearn,neural_network,_multilayer_perceptron,MLPRegressor,,Multi-layer Perceptron regressor.  This model optimizes the squared error using LBFGS or stochastic gradient descent.
MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>,Medium,sklearn.neural_network._multilayer_perceptron.MLPRegressor,regressor,sklearn,neural_network,_multilayer_perceptron,MLPRegressor,,Multi-layer Perceptron regressor.  This model optimizes the squared error using LBFGS or stochastic gradient descent.
MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>,Medium,sklearn.neural_network._multilayer_perceptron.MLPRegressor,regressor,sklearn,neural_network,_multilayer_perceptron,MLPRegressor,,Multi-layer Perceptron regressor.  This model optimizes the squared error using LBFGS or stochastic gradient descent.
MaxAbsScaler,<class 'sklearn.preprocessing._data.MaxAbsScaler'>,Large,sklearn.preprocessing._data.MaxAbsScaler,transformer,sklearn,preprocessing,_data,MaxAbsScaler,,"Scale each feature by its maximum absolute value.  This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.  This scaler can also be applied to sparse CSR or CSC matrices.  `MaxAbsScaler` doesn't reduce the effect of outliers; it only linearly scales them down. For an example visualization, refer to :ref:`Compare MaxAbsScaler with other scalers <plot_all_scaling_max_abs_scaler_section>`."
MeanShift,<class 'sklearn.cluster._mean_shift.MeanShift'>,Small,sklearn.cluster._mean_shift.MeanShift,cluster,sklearn,cluster,_mean_shift,MeanShift,,"Mean shift clustering using a flat kernel.  Mean shift clustering aims to discover ""blobs"" in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.  Seeding is performed using a binning technique for scalability.  For an example of how to use MeanShift clustering, refer to: :ref:`sphx_glr_auto_examples_cluster_plot_mean_shift.py`.  Read more in the :ref:`User Guide <mean_shift>`.  Parameters ---------- bandwidth : float, default=None Bandwidth used in the flat kernel.  If not given, the bandwidth is estimated using sklearn.cluster.estimate_bandwidth; see the documentation for that function for hints on scalability (see also the Notes, below).  seeds : array-like of shape (n_samples, n_features), default=None Seeds used to initialize kernels. If not set, the seeds are calculated by clustering.get_bin_seeds with bandwidth as the grid size and default values for other parameters.  bin_seeding : bool, default=False If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. The default value is False. Ignored if seeds argument is not None.  min_bin_freq : int, default=1 To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds.  cluster_all : bool, default=True If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1.  n_jobs : int, default=None The number of jobs to use for the computation. The following tasks benefit from the parallelization:  - The search of nearest neighbors for bandwidth estimation and label assignments. See the details in the docstring of the ``NearestNeighbors`` class. - Hill-climbing optimization for all seeds.  See :term:`Glossary <n_jobs>` for more details.  ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.  max_iter : int, default=300 Maximum number of iterations, per seed point before the clustering operation terminates (for that seed point), if has not converged yet."
MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>,Small,sklearn.covariance._robust_covariance.MinCovDet,unknown,sklearn,covariance,_robust_covariance,MinCovDet,,"Minimum Covariance Determinant (MCD): robust estimator of covariance.  The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets.  Read more in the :ref:`User Guide <robust_covariance>`.  Parameters ---------- store_precision : bool, default=True Specify if the estimated precision is stored.  assume_centered : bool, default=False If True, the support of the robust location and the covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.  support_fraction : float, default=None The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: `(n_samples + n_features + 1) / 2 * n_samples`. The parameter must be in the range (0, 1].  random_state : int, RandomState instance or None, default=None Determines the pseudo random number generator for shuffling the data. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.  Attributes ---------- raw_location_ : ndarray of shape (n_features,) The raw robust estimated location before correction and re-weighting.  raw_covariance_ : ndarray of shape (n_features, n_features) The raw robust estimated covariance before correction and re-weighting.  raw_support_ : ndarray of shape (n_samples,) A mask of the observations that have been used to compute the raw robust estimates of location and shape, before correction and re-weighting.  location_ : ndarray of shape (n_features,) Estimated robust location.  covariance_ : ndarray of shape (n_features, n_features) Estimated robust covariance matrix.  precision_ : ndarray of shape (n_features, n_features) Estimated pseudo inverse matrix. (stored only if store_precision is True)  support_ : ndarray of shape (n_samples,) A mask of the observations that have been used to compute the robust estimates of location and shape.  dist_ : ndarray of shape (n_samples,) Mahalanobis distances of the training set (on which :meth:`fit` is called) observations.  n_features_in_ : int Number of features seen during :term:`fit`."
MinMaxScaler,<class 'sklearn.preprocessing._data.MinMaxScaler'>,Large,sklearn.preprocessing._data.MinMaxScaler,transformer,sklearn,preprocessing,_data,MinMaxScaler,,"Transform features by scaling each feature to a given range.  This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.  The transformation is given by::  X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) X_scaled = X_std * (max - min) + min  where min, max = feature_range.  This transformation is often used as an alternative to zero mean, unit variance scaling.  `MinMaxScaler` doesn't reduce the effect of outliers, but it linearly scales them down into a fixed range, where the largest occurring data point corresponds to the maximum value and the smallest one corresponds to the minimum value. For an example visualization, refer to :ref:`Compare MinMaxScaler with other scalers <plot_all_scaling_minmax_scaler_section>`.  Read more in the :ref:`User Guide <preprocessing_scaler>`.  Parameters ---------- feature_range : tuple (min, max), default=(0, 1) Desired range of transformed data.  copy : bool, default=True Set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array).  clip : bool, default=False Set to True to clip transformed values of held-out data to provided `feature range`."
MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>,"Medium, Large",sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning,transformer,sklearn,decomposition,_dict_learning,MiniBatchDictionaryLearning,,"Mini-batch dictionary learning.  Finds a dictionary (a set of atoms) that performs well at sparsely encoding the fitted data.  Solves the optimization problem::  (U^*,V^*) = argmin 0.5 || X - U V ||_Fro^2 + alpha * || U ||_1,1 (U,V) with || V_k ||_2 <= 1 for all  0 <= k < n_components  ||.||_Fro stands for the Frobenius norm and ||.||_1,1 stands for the entry-wise matrix norm which is the sum of the absolute values of all the entries in the matrix.  Read more in the :ref:`User Guide <DictionaryLearning>`.  Parameters ---------- n_components : int, default=None Number of dictionary elements to extract.  alpha : float, default=1 Sparsity controlling parameter.  max_iter : int, default=1_000 Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics."
MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>,Large,sklearn.cluster._kmeans.MiniBatchKMeans,"cluster, transformer",sklearn,cluster,_kmeans,MiniBatchKMeans,,"Mini-Batch K-Means clustering.  Read more in the :ref:`User Guide <mini_batch_kmeans>`.  Parameters ----------  n_clusters : int, default=8 The number of clusters to form as well as the number of centroids to generate.  init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++' Method for initialization:  'k-means++' : selects initial cluster centroids using sampling based on an empirical probability distribution of the points' contribution to the overall inertia. This technique speeds up convergence. The algorithm implemented is ""greedy k-means++"". It differs from the vanilla k-means++ by making several trials at each sampling step and choosing the best centroid among them.  'random': choose `n_clusters` observations (rows) at random from data for the initial centroids.  If an array is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.  If a callable is passed, it should take arguments X, n_clusters and a random state and return an initialization.  For an evaluation of the impact of initialization, see the example :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_stability_low_dim_dense.py`.  max_iter : int, default=100 Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics.  batch_size : int, default=1024 Size of the mini batches. For faster computations, you can set the ``batch_size`` greater than 256 * number of cores to enable parallelism on all cores."
MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>,Large,sklearn.decomposition._nmf.MiniBatchNMF,transformer,sklearn,decomposition,_nmf,MiniBatchNMF,,Mini-Batch Non-Negative Matrix Factorization (NMF).
MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>,Large,sklearn.decomposition._sparse_pca.MiniBatchSparsePCA,transformer,sklearn,decomposition,_sparse_pca,MiniBatchSparsePCA,,"Mini-batch Sparse Principal Components Analysis.  Finds the set of sparse components that can optimally reconstruct the data.  The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha.  For an example comparing sparse PCA to PCA, see :ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`  Read more in the :ref:`User Guide <SparsePCA>`.  Parameters ---------- n_components : int, default=None Number of sparse atoms to extract. If None, then ``n_components`` is set to ``n_features``.  alpha : int, default=1 Sparsity controlling parameter. Higher values lead to sparser components.  ridge_alpha : float, default=0.01 Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method.  max_iter : int, default=1_000 Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics."
MissingIndicator,<class 'sklearn.impute._base.MissingIndicator'>,Large,sklearn.impute._base.MissingIndicator,transformer,sklearn,impute,_base,MissingIndicator,,"Binary indicators for missing values.  Note that this component typically should not be used in a vanilla :class:`~sklearn.pipeline.Pipeline` consisting of transformers and a classifier, but rather could be added using a :class:`~sklearn.pipeline.FeatureUnion` or :class:`~sklearn.compose.ColumnTransformer`.  Read more in the :ref:`User Guide <impute>`."
MultiLabelBinarizer,<class 'sklearn.preprocessing._label.MultiLabelBinarizer'>,Large,sklearn.preprocessing._label.MultiLabelBinarizer,transformer,sklearn,preprocessing,_label,MultiLabelBinarizer,,"Transform between iterable of iterables and a multilabel format.  Although a list of sets or tuples is a very intuitive format for multilabel data, it is unwieldy to process. This transformer converts between this intuitive format and the supported multilabel format: a (samples x classes) binary matrix indicating the presence of a class label.  Parameters ---------- classes : array-like of shape (n_classes,), default=None Indicates an ordering for the class labels. All entries should be unique (cannot contain duplicate classes).  sparse_output : bool, default=False Set to True if output binary array is desired in CSR sparse format.  Attributes ---------- classes_ : ndarray of shape (n_classes,) A copy of the `classes` parameter when provided. Otherwise it corresponds to the sorted set of classes found when fitting.  See Also -------- OneHotEncoder : Encode categorical features using a one-hot aka one-of-K scheme.  Examples -------- >>> from sklearn.preprocessing import MultiLabelBinarizer >>> mlb = MultiLabelBinarizer() >>> mlb.fit_transform([(1, 2), (3,)]) array([[1, 1, 0], [0, 0, 1]]) >>> mlb.classes_ array([1, 2, 3])  >>> mlb.fit_transform([{'sci-fi', 'thriller'}, {'comedy'}]) array([[0, 1, 1], [1, 0, 0]]) >>> list(mlb.classes_) ['comedy', 'sci-fi', 'thriller']  A common mistake is to pass in a list, which leads to the following issue:  >>> mlb = MultiLabelBinarizer() >>> mlb.fit(['sci-fi', 'thriller', 'comedy']) MultiLabelBinarizer() >>> mlb.classes_ array(['-', 'c', 'd', 'e', 'f', 'h', 'i', 'l', 'm', 'o', 'r', 's', 't', 'y'], dtype=object)  To correct this, the list of labels should be passed in as:  >>> mlb = MultiLabelBinarizer() >>> mlb.fit([['sci-fi', 'thriller', 'comedy']]) MultiLabelBinarizer() >>> mlb.classes_ array(['comedy', 'sci-fi', 'thriller'], dtype=object)"
MultiOutputClassifier,<class 'sklearn.multioutput.MultiOutputClassifier'>,Medium,sklearn.multioutput.MultiOutputClassifier,classifier,sklearn,multioutput,MultiOutputClassifier,,,"Multi target classification.  This strategy consists of fitting one classifier per target. This is a simple strategy for extending classifiers that do not natively support multi-target classification.  Parameters ---------- estimator : estimator object An estimator object implementing :term:`fit` and :term:`predict`. A :term:`predict_proba` method will be exposed only if `estimator` implements it.  n_jobs : int or None, optional (default=None) The number of jobs to run in parallel. :meth:`fit`, :meth:`predict` and :meth:`partial_fit` (if supported by the passed estimator) will be parallelized for each target.  When individual estimators are fast to train or predict, using ``n_jobs > 1`` can result in slower performance due to the parallelism overhead.  ``None`` means `1` unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all available processes / threads. See :term:`Glossary <n_jobs>` for more details."
MultiOutputRegressor,<class 'sklearn.multioutput.MultiOutputRegressor'>,Medium,sklearn.multioutput.MultiOutputRegressor,regressor,sklearn,multioutput,MultiOutputRegressor,,,Multi target regression.  This strategy consists of fitting one regressor per target. This is a simple strategy for extending regressors that do not natively support multi-target regression.
MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>,"Small, Medium",sklearn.linear_model._coordinate_descent.MultiTaskElasticNet,regressor,sklearn,linear_model,_coordinate_descent,MultiTaskElasticNet,,"Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.  The optimization objective for MultiTaskElasticNet is::  (1 / (2 * n_samples)) * ||Y - XW||_Fro^2 + alpha * l1_ratio * ||W||_21 + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2  Where::  ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)  i.e. the sum of norms of each row.  Read more in the :ref:`User Guide <multi_task_elastic_net>`.  Parameters ---------- alpha : float, default=1.0 Constant that multiplies the L1/L2 term. Defaults to 1.0.  l1_ratio : float, default=0.5 The ElasticNet mixing parameter, with 0 < l1_ratio <= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.  fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).  copy_X : bool, default=True If ``True``, X will be copied; else, it may be overwritten.  max_iter : int, default=1000 The maximum number of iterations.  tol : float, default=1e-4 The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``.  warm_start : bool, default=False When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.  random_state : int, RandomState instance, default=None The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.  selection : {'cyclic', 'random'}, default='cyclic' If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.  Attributes ---------- intercept_ : ndarray of shape (n_targets,) Independent term in decision function.  coef_ : ndarray of shape (n_targets, n_features) Parameter vector (W in the cost function formula). If a 1D y is passed in at fit (non multi-task usage), ``coef_`` is then a 1D array. Note that ``coef_`` stores the transpose of ``W``, ``W.T``.  n_iter_ : int Number of iterations run by the coordinate descent solver to reach the specified tolerance.  dual_gap_ : float The dual gaps at the end of the optimization.  eps_ : float The tolerance scaled scaled by the variance of the target `y`.  sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features) Sparse representation of the `coef_`.  n_features_in_ : int Number of features seen during :term:`fit`."
MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>,"Small, Medium",sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV,regressor,sklearn,linear_model,_coordinate_descent,MultiTaskElasticNetCV,,Multi-task L1/L2 ElasticNet with built-in cross-validation.  See glossary entry for :term:`cross-validation estimator`.  The optimization objective for MultiTaskElasticNet is::  (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * l1_ratio * ||W||_21 + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2  Where::  ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_elastic_net>`.
MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>,"Small, Medium",sklearn.linear_model._coordinate_descent.MultiTaskLasso,regressor,sklearn,linear_model,_coordinate_descent,MultiTaskLasso,,"Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21  Where::  ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_lasso>`.  Parameters ---------- alpha : float, default=1.0 Constant that multiplies the L1/L2 term. Defaults to 1.0.  fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).  copy_X : bool, default=True If ``True``, X will be copied; else, it may be overwritten.  max_iter : int, default=1000 The maximum number of iterations.  tol : float, default=1e-4 The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``.  warm_start : bool, default=False When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.  random_state : int, RandomState instance, default=None The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.  selection : {'cyclic', 'random'}, default='cyclic' If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.  Attributes ---------- coef_ : ndarray of shape (n_targets, n_features) Parameter vector (W in the cost function formula). Note that ``coef_`` stores the transpose of ``W``, ``W.T``.  intercept_ : ndarray of shape (n_targets,) Independent term in decision function.  n_iter_ : int Number of iterations run by the coordinate descent solver to reach the specified tolerance.  dual_gap_ : ndarray of shape (n_alphas,) The dual gaps at the end of the optimization for each alpha.  eps_ : float The tolerance scaled scaled by the variance of the target `y`.  sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features) Sparse representation of the `coef_`.  n_features_in_ : int Number of features seen during :term:`fit`."
MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>,"Small, Medium",sklearn.linear_model._coordinate_descent.MultiTaskLassoCV,regressor,sklearn,linear_model,_coordinate_descent,MultiTaskLassoCV,,Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.  See glossary entry for :term:`cross-validation estimator`.  The optimization objective for MultiTaskLasso is::  (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21  Where::  ||W||_21 = \sum_i \sqrt{\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_lasso>`.
MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>,1,sklearn.naive_bayes.MultinomialNB,classifier,sklearn,naive_bayes,MultinomialNB,,,"Naive Bayes classifier for multinomial models.  The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.  Parameters ---------- alpha : float or array-like of shape (n_features,), default=1.0 Additive (Laplace/Lidstone) smoothing parameter (set alpha=0 and force_alpha=True, for no smoothing).  force_alpha : bool, default=True If False and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged. This may cause numerical errors if alpha is too close to 0."
MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>,1,sklearn.naive_bayes.MultinomialNB,classifier,sklearn,naive_bayes,MultinomialNB,,,"Naive Bayes classifier for multinomial models.  The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.  Parameters ---------- alpha : float or array-like of shape (n_features,), default=1.0 Additive (Laplace/Lidstone) smoothing parameter (set alpha=0 and force_alpha=True, for no smoothing).  force_alpha : bool, default=True If False and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged. This may cause numerical errors if alpha is too close to 0."
MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>,"Medium, Large",sklearn.naive_bayes.MultinomialNB,classifier,sklearn,naive_bayes,MultinomialNB,,,"Naive Bayes classifier for multinomial models.  The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.  Parameters ---------- alpha : float or array-like of shape (n_features,), default=1.0 Additive (Laplace/Lidstone) smoothing parameter (set alpha=0 and force_alpha=True, for no smoothing).  force_alpha : bool, default=True If False and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged. This may cause numerical errors if alpha is too close to 0."
MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>,"Medium, Large",sklearn.naive_bayes.MultinomialNB,classifier,sklearn,naive_bayes,MultinomialNB,,,"Naive Bayes classifier for multinomial models.  The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.  Parameters ---------- alpha : float or array-like of shape (n_features,), default=1.0 Additive (Laplace/Lidstone) smoothing parameter (set alpha=0 and force_alpha=True, for no smoothing).  force_alpha : bool, default=True If False and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged. This may cause numerical errors if alpha is too close to 0."
NMF,<class 'sklearn.decomposition._nmf.NMF'>,"Medium, Large",sklearn.decomposition._nmf.NMF,transformer,sklearn,decomposition,_nmf,NMF,,"Non-Negative Matrix Factorization (NMF).  Find two non-negative matrices, i.e. matrices with all non-negative elements, (W, H) whose product approximates the non-negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.  The objective function is:"
NearestCentroid,<class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>,Medium,sklearn.neighbors._nearest_centroid.NearestCentroid,classifier,sklearn,neighbors,_nearest_centroid,NearestCentroid,,"Nearest centroid classifier.  Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.  Read more in the :ref:`User Guide <nearest_centroid_classifier>`.  Parameters ---------- metric : {""euclidean"", ""manhattan""}, default=""euclidean"" Metric to use for distance computation.  If `metric=""euclidean""`, the centroid for the samples corresponding to each class is the arithmetic mean, which minimizes the sum of squared L1 distances. If `metric=""manhattan""`, the centroid is the feature-wise median, which minimizes the sum of L1 distances."
NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>,"Small, Medium",sklearn.neighbors._unsupervised.NearestNeighbors,unknown,sklearn,neighbors,_unsupervised,NearestNeighbors,,Unsupervised learner for implementing neighbor searches.  Read more in the :ref:`User Guide <unsupervised_neighbors>`.
NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>,Small,sklearn.neighbors._nca.NeighborhoodComponentsAnalysis,transformer,sklearn,neighbors,_nca,NeighborhoodComponentsAnalysis,,"Neighborhood Components Analysis.  Neighborhood Component Analysis (NCA) is a machine learning algorithm for metric learning. It learns a linear transformation in a supervised fashion to improve the classification accuracy of a stochastic nearest neighbors rule in the transformed space.  Read more in the :ref:`User Guide <nca>`.  Parameters ---------- n_components : int, default=None Preferred dimensionality of the projected space. If None it will be set to `n_features`.  init : {'auto', 'pca', 'lda', 'identity', 'random'} or ndarray of shape             (n_features_a, n_features_b), default='auto' Initialization of the linear transformation. Possible options are `'auto'`, `'pca'`, `'lda'`, `'identity'`, `'random'`, and a numpy array of shape `(n_features_a, n_features_b)`.  - `'auto'` Depending on `n_components`, the most reasonable initialization is chosen. If `n_components <= min(n_features, n_classes - 1)` we use `'lda'`, as it uses labels information. If not, but `n_components < min(n_features, n_samples)`, we use `'pca'`, as it projects data in meaningful directions (those of higher variance). Otherwise, we just use `'identity'`.  - `'pca'` `n_components` principal components of the inputs passed to :meth:`fit` will be used to initialize the transformation. (See :class:`~sklearn.decomposition.PCA`)  - `'lda'` `min(n_components, n_classes)` most discriminative components of the inputs passed to :meth:`fit` will be used to initialize the transformation. (If `n_components > n_classes`, the rest of the components will be zero.) (See :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)  - `'identity'` If `n_components` is strictly smaller than the dimensionality of the inputs passed to :meth:`fit`, the identity matrix will be truncated to the first `n_components` rows.  - `'random'` The initial transformation will be a random array of shape `(n_components, n_features)`. Each value is sampled from the standard normal distribution.  - numpy array `n_features_b` must match the dimensionality of the inputs passed to :meth:`fit` and n_features_a must be less than or equal to that. If `n_components` is not `None`, `n_features_a` must match it.  warm_start : bool, default=False If `True` and :meth:`fit` has been called before, the solution of the previous call to :meth:`fit` is used as the initial linear transformation (`n_components` and `init` will be ignored).  max_iter : int, default=50 Maximum number of iterations in the optimization.  tol : float, default=1e-5 Convergence tolerance for the optimization.  callback : callable, default=None If not `None`, this function is called after every iteration of the optimizer, taking as arguments the current solution (flattened transformation matrix) and the number of iterations. This might be useful in case one wants to examine or store the transformation found after each iteration.  verbose : int, default=0 If 0, no progress messages will be printed. If 1, progress messages will be printed to stdout. If > 1, progress messages will be printed and the `disp` parameter of :func:`scipy.optimize.minimize` will be set to `verbose - 2`.  random_state : int or numpy.RandomState, default=None A pseudo random number generator object or a seed for it if int. If `init='random'`, `random_state` is used to initialize the random transformation. If `init='pca'`, `random_state` is passed as an argument to PCA when initializing the transformation. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.  Attributes ---------- components_ : ndarray of shape (n_components, n_features) The linear transformation learned during fitting.  n_features_in_ : int Number of features seen during :term:`fit`."
Normalizer,<class 'sklearn.preprocessing._data.Normalizer'>,Large,sklearn.preprocessing._data.Normalizer,transformer,sklearn,preprocessing,_data,Normalizer,,"Normalize samples individually to unit norm.  Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1, l2 or inf) equals one.  This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion).  Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.  For an example visualization, refer to :ref:`Compare Normalizer with other scalers <plot_all_scaling_normalizer_section>`.  Read more in the :ref:`User Guide <preprocessing_normalization>`.  Parameters ---------- norm : {'l1', 'l2', 'max'}, default='l2' The norm to use to normalize each non zero sample. If norm='max' is used, values will be rescaled by the maximum of the absolute values.  copy : bool, default=True Set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix).  Attributes ---------- n_features_in_ : int Number of features seen during :term:`fit`."
NuSVC,<class 'sklearn.svm._classes.NuSVC'>,"Small, Medium",sklearn.svm._classes.NuSVC,classifier,sklearn,svm,_classes,NuSVC,,"Nu-Support Vector Classification.  Similar to SVC but uses a parameter to control the number of support vectors.  The implementation is based on libsvm.  Read more in the :ref:`User Guide <svm_classification>`.  Parameters ---------- nu : float, default=0.5 An upper bound on the fraction of margin errors (see :ref:`User Guide <nu_svc>`) and a lower bound of the fraction of support vectors. Should be in the interval (0, 1].  kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf' Specifies the kernel type to be used in the algorithm. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix. For an intuitive visualization of different kernel types see :ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`.  degree : int, default=3 Degree of the polynomial kernel function ('poly'). Must be non-negative. Ignored by all other kernels.  gamma : {'scale', 'auto'} or float, default='scale' Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.  - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features - if float, must be non-negative."
NuSVR,<class 'sklearn.svm._classes.NuSVR'>,"Small, Medium",sklearn.svm._classes.NuSVR,regressor,sklearn,svm,_classes,NuSVR,,"Nu Support Vector Regression.  Similar to NuSVC, for regression, uses a parameter nu to control the number of support vectors. However, unlike NuSVC, where nu replaces C, here nu replaces the parameter epsilon of epsilon-SVR.  The implementation is based on libsvm.  Read more in the :ref:`User Guide <svm_regression>`.  Parameters ---------- nu : float, default=0.5 An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1].  By default 0.5 will be taken.  C : float, default=1.0 Penalty parameter C of the error term. For an intuitive visualization of the effects of scaling the regularization parameter C, see :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.  kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf' Specifies the kernel type to be used in the algorithm. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix. For an intuitive visualization of different kernel types see See :ref:`sphx_glr_auto_examples_svm_plot_svm_regression.py`  degree : int, default=3 Degree of the polynomial kernel function ('poly'). Must be non-negative. Ignored by all other kernels.  gamma : {'scale', 'auto'} or float, default='scale' Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.  - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features - if float, must be non-negative."
Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>,Medium,sklearn.kernel_approximation.Nystroem,transformer,sklearn,kernel_approximation,Nystroem,,,Approximate a kernel map using a subset of the training data.  Constructs an approximate feature map for an arbitrary kernel using a subset of the data as basis.  Read more in the :ref:`User Guide <nystroem_kernel_approx>`.
OAS,<class 'sklearn.covariance._shrunk_covariance.OAS'>,Small,sklearn.covariance._shrunk_covariance.OAS,unknown,sklearn,covariance,_shrunk_covariance,OAS,,"Oracle Approximating Shrinkage Estimator.  Read more in the :ref:`User Guide <shrunk_covariance>`.  Parameters ---------- store_precision : bool, default=True Specify if the estimated precision is stored.  assume_centered : bool, default=False If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data will be centered before computation.  Attributes ---------- covariance_ : ndarray of shape (n_features, n_features) Estimated covariance matrix.  location_ : ndarray of shape (n_features,) Estimated location, i.e. the estimated mean.  precision_ : ndarray of shape (n_features, n_features) Estimated pseudo inverse matrix. (stored only if store_precision is True)  shrinkage_ : float coefficient in the convex combination used for the computation of the shrunk estimate. Range is [0, 1].  n_features_in_ : int Number of features seen during :term:`fit`."
OPTICS,<class 'sklearn.cluster._optics.OPTICS'>,"Small, Medium",sklearn.cluster._optics.OPTICS,cluster,sklearn,cluster,_optics,OPTICS,,"Estimate clustering structure from vector array.  OPTICS (Ordering Points To Identify the Clustering Structure), closely related to DBSCAN, finds core sample of high density and expands clusters from them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable neighborhood radius. Better suited for usage on large datasets than the current sklearn implementation of DBSCAN.  Clusters are then extracted using a DBSCAN-like method (cluster_method = 'dbscan') or an automatic technique proposed in [1]_ (cluster_method = 'xi').  This implementation deviates from the original OPTICS by first performing k-nearest-neighborhood searches on all points to identify core sizes, then computing only the distances to unprocessed points when constructing the cluster order. Note that we do not employ a heap to manage the expansion candidates, so the time complexity will be O(n^2).  Read more in the :ref:`User Guide <optics>`.  Parameters ---------- min_samples : int > 1 or float between 0 and 1, default=5 The number of samples in a neighborhood for a point to be considered as a core point. Also, up and down steep regions can't have more than ``min_samples`` consecutive non-steep points. Expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2).  max_eps : float, default=np.inf The maximum distance between two samples for one to be considered as in the neighborhood of the other. Default value of ``np.inf`` will identify clusters across all scales; reducing ``max_eps`` will result in shorter run times.  metric : str or callable, default='minkowski' Metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.  If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy's metrics, but is less efficient than passing the metric name as a string. If metric is ""precomputed"", `X` is assumed to be a distance matrix and must be square.  Valid values for metric are:  - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']  - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']  Sparse matrices are only supported by scikit-learn metrics. See the documentation for scipy.spatial.distance for details on these metrics."
OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>,Small,sklearn.svm._classes.OneClassSVM,unknown,sklearn,svm,_classes,OneClassSVM,,"Unsupervised Outlier Detection.  Estimate the support of a high-dimensional distribution.  The implementation is based on libsvm.  Read more in the :ref:`User Guide <outlier_detection>`.  Parameters ---------- kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf' Specifies the kernel type to be used in the algorithm. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix.  degree : int, default=3 Degree of the polynomial kernel function ('poly'). Must be non-negative. Ignored by all other kernels.  gamma : {'scale', 'auto'} or float, default='scale' Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.  - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features - if float, must be non-negative."
OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>,Large,sklearn.preprocessing._encoders.OneHotEncoder,transformer,sklearn,preprocessing,_encoders,OneHotEncoder,,"Encode categorical features as a one-hot numeric array.  The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka 'one-of-K' or 'dummy') encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the ``sparse_output`` parameter).  By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the `categories` manually.  This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.  Note: a one-hot encoding of y labels should use a LabelBinarizer instead.  Read more in the :ref:`User Guide <preprocessing_categorical_features>`. For a comparison of different encoders, refer to: :ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`.  Parameters ---------- categories : 'auto' or a list of array-like, default='auto' Categories (unique values) per feature:  - 'auto' : Determine categories automatically from the training data. - list : ``categories[i]`` holds the categories expected in the ith column. The passed categories should not mix strings and numeric values within a single feature, and should be sorted in case of numeric values.  The used categories can be found in the ``categories_`` attribute."
OneVsOneClassifier,<class 'sklearn.multiclass.OneVsOneClassifier'>,Medium,sklearn.multiclass.OneVsOneClassifier,classifier,sklearn,multiclass,OneVsOneClassifier,,,"One-vs-one multiclass strategy.  This strategy consists in fitting one classifier per class pair. At prediction time, the class which received the most votes is selected. Since it requires to fit `n_classes * (n_classes - 1) / 2` classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don't scale well with `n_samples`. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used `n_classes` times.  Read more in the :ref:`User Guide <ovo_classification>`.  Parameters ---------- estimator : estimator object A regressor or a classifier that implements :term:`fit`. When a classifier is passed, :term:`decision_function` will be used in priority and it will fallback to :term:`predict_proba` if it is not available. When a regressor is passed, :term:`predict` is used.  n_jobs : int, default=None The number of jobs to use for the computation: the `n_classes * ( n_classes - 1) / 2` OVO problems are computed in parallel.  ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.  Attributes ---------- estimators_ : list of ``n_classes * (n_classes - 1) / 2`` estimators Estimators used for predictions.  classes_ : numpy array of shape [n_classes] Array containing labels.  n_classes_ : int Number of classes.  pairwise_indices_ : list, length = ``len(estimators_)``, or ``None`` Indices of samples used when training the estimators. ``None`` when ``estimator``'s `pairwise` tag is False.  n_features_in_ : int Number of features seen during :term:`fit`."
OneVsRestClassifier,<class 'sklearn.multiclass.OneVsRestClassifier'>,Medium,sklearn.multiclass.OneVsRestClassifier,classifier,sklearn,multiclass,OneVsRestClassifier,,,"One-vs-the-rest (OvR) multiclass strategy.  Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only `n_classes` classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice.  OneVsRestClassifier can also be used for multilabel classification. To use this feature, provide an indicator matrix for the target `y` when calling `.fit`. In other words, the target labels should be formatted as a 2D binary (0/1) matrix, where [i, j] == 1 indicates the presence of label j in sample i. This estimator uses the binary relevance method to perform multilabel classification, which involves training one binary classifier independently for each label.  Read more in the :ref:`User Guide <ovr_classification>`.  Parameters ---------- estimator : estimator object A regressor or a classifier that implements :term:`fit`. When a classifier is passed, :term:`decision_function` will be used in priority and it will fallback to :term:`predict_proba` if it is not available. When a regressor is passed, :term:`predict` is used.  n_jobs : int, default=None The number of jobs to use for the computation: the `n_classes` one-vs-rest problems are computed in parallel.  ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details."
OrdinalEncoder,<class 'sklearn.preprocessing._encoders.OrdinalEncoder'>,Large,sklearn.preprocessing._encoders.OrdinalEncoder,transformer,sklearn,preprocessing,_encoders,OrdinalEncoder,,"Encode categorical features as an integer array.  The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature.  Read more in the :ref:`User Guide <preprocessing_categorical_features>`. For a comparison of different encoders, refer to: :ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`."
OrthogonalMatchingPursuit,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuit'>,"Small, Medium",sklearn.linear_model._omp.OrthogonalMatchingPursuit,regressor,sklearn,linear_model,_omp,OrthogonalMatchingPursuit,,"Orthogonal Matching Pursuit model (OMP).  Read more in the :ref:`User Guide <omp>`.  Parameters ---------- n_nonzero_coefs : int, default=None Desired number of non-zero entries in the solution. Ignored if `tol` is set. When `None` and `tol` is also `None`, this value is either set to 10% of `n_features` or 1, whichever is greater.  tol : float, default=None Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs.  fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).  precompute : 'auto' or bool, default='auto' Whether to use a precomputed Gram and Xy matrix to speed up calculations. Improves performance when :term:`n_targets` or :term:`n_samples` is very large. Note that if you already have such matrices, you can pass them directly to the fit method.  Attributes ---------- coef_ : ndarray of shape (n_features,) or (n_targets, n_features) Parameter vector (w in the formula).  intercept_ : float or ndarray of shape (n_targets,) Independent term in decision function.  n_iter_ : int or array-like Number of active features across every target.  n_nonzero_coefs_ : int or None The number of non-zero coefficients in the solution or `None` when `tol` is set. If `n_nonzero_coefs` is None and `tol` is None this value is either set to 10% of `n_features` or 1, whichever is greater.  n_features_in_ : int Number of features seen during :term:`fit`."
OrthogonalMatchingPursuitCV,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuitCV'>,"Small, Medium",sklearn.linear_model._omp.OrthogonalMatchingPursuitCV,regressor,sklearn,linear_model,_omp,OrthogonalMatchingPursuitCV,,"Cross-validated Orthogonal Matching Pursuit model (OMP).  See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <omp>`.  Parameters ---------- copy : bool, default=True Whether the design matrix X must be copied by the algorithm. A false value is only helpful if X is already Fortran-ordered, otherwise a copy is made anyway.  fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).  max_iter : int, default=None Maximum numbers of iterations to perform, therefore maximum features to include. 10% of ``n_features`` but at least 5 if available.  cv : int, cross-validation generator or iterable, default=None Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here."
OutputCodeClassifier,<class 'sklearn.multiclass.OutputCodeClassifier'>,Medium,sklearn.multiclass.OutputCodeClassifier,classifier,sklearn,multiclass,OutputCodeClassifier,,,"(Error-Correcting) Output-Code multiclass strategy.  Output-code based strategies consist in representing each class with a binary code (an array of 0s and 1s). At fitting time, one binary classifier per bit in the code book is fitted.  At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen. The main advantage of these strategies is that the number of classifiers used can be controlled by the user, either for compressing the model (0 < `code_size` < 1) or for making the model more robust to errors (`code_size` > 1). See the documentation for more details.  Read more in the :ref:`User Guide <ecoc>`.  Parameters ---------- estimator : estimator object An estimator object implementing :term:`fit` and one of :term:`decision_function` or :term:`predict_proba`.  code_size : float, default=1.5 Percentage of the number of classes to be used to create the code book. A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. A number greater than 1 will require more classifiers than one-vs-the-rest.  random_state : int, RandomState instance, default=None The generator used to initialize the codebook. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.  n_jobs : int, default=None The number of jobs to use for the computation: the multiclass problems are computed in parallel.  ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.  Attributes ---------- estimators_ : list of `int(n_classes * code_size)` estimators Estimators used for predictions.  classes_ : ndarray of shape (n_classes,) Array containing labels.  code_book_ : ndarray of shape (n_classes, `len(estimators_)`) Binary array containing the code of each class.  n_features_in_ : int Number of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit."
PCA,<class 'sklearn.decomposition._pca.PCA'>,Large,sklearn.decomposition._pca.PCA,transformer,sklearn,decomposition,_pca,PCA,,"Principal component analysis (PCA).  Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.  It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.  With sparse inputs, the ARPACK implementation of the truncated SVD can be used (i.e. through :func:`scipy.sparse.linalg.svds`). Alternatively, one may consider :class:`TruncatedSVD` where the data are not centered.  Notice that this class only supports sparse inputs for some solvers such as ""arpack"" and ""covariance_eigh"". See :class:`TruncatedSVD` for an alternative with sparse data.  For a usage example, see :ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py`  Read more in the :ref:`User Guide <PCA>`.  Parameters ---------- n_components : int, float or 'mle', default=None Number of components to keep. if n_components is not set all components are kept::  n_components == min(n_samples, n_features)  If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's MLE is used to guess the dimension. Use of ``n_components == 'mle'`` will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.  If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components.  If ``svd_solver == 'arpack'``, the number of components must be strictly less than the minimum of n_features and n_samples.  Hence, the None case results in::  n_components == min(n_samples, n_features) - 1  copy : bool, default=True If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.  whiten : bool, default=False When True (False by default) the `components_` vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.  Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions.  svd_solver : {'auto', 'full', 'covariance_eigh', 'arpack', 'randomized'},            default='auto' ""auto"" : The solver is selected by a default 'auto' policy is based on `X.shape` and `n_components`: if the input data has fewer than 1000 features and more than 10 times as many samples, then the ""covariance_eigh"" solver is used. Otherwise, if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient ""randomized"" method is selected. Otherwise the exact ""full"" SVD is computed and optionally truncated afterwards. ""full"" : Run exact full SVD calling the standard LAPACK solver via `scipy.linalg.svd` and select the components by postprocessing ""covariance_eigh"" : Precompute the covariance matrix (on centered data), run a classical eigenvalue decomposition on the covariance matrix typically using LAPACK and select the components by postprocessing. This solver is very efficient for n_samples >> n_features and small n_features. It is, however, not tractable otherwise for large n_features (large memory footprint required to materialize the covariance matrix). Also note that compared to the ""full"" solver, this solver effectively doubles the condition number and is therefore less numerical stable (e.g. on input data with a large range of singular values). ""arpack"" : Run SVD truncated to `n_components` calling ARPACK solver via `scipy.sparse.linalg.svds`. It requires strictly `0 < n_components < min(X.shape)` ""randomized"" : Run randomized SVD by the method of Halko et al."
PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>,Small,sklearn.cross_decomposition._pls.PLSCanonical,"regressor, transformer",sklearn,cross_decomposition,_pls,PLSCanonical,,"Partial Least Squares transformer and regressor.  For a comparison between other cross decomposition algorithms, see :ref:`sphx_glr_auto_examples_cross_decomposition_plot_compare_cross_decomposition.py`.  Read more in the :ref:`User Guide <cross_decomposition>`."
PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>,Small,sklearn.cross_decomposition._pls.PLSRegression,"regressor, transformer",sklearn,cross_decomposition,_pls,PLSRegression,,"PLS regression.  PLSRegression is also known as PLS2 or PLS1, depending on the number of targets.  For a comparison between other cross decomposition algorithms, see :ref:`sphx_glr_auto_examples_cross_decomposition_plot_compare_cross_decomposition.py`.  Read more in the :ref:`User Guide <cross_decomposition>`."
PLSSVD,<class 'sklearn.cross_decomposition._pls.PLSSVD'>,Small,sklearn.cross_decomposition._pls.PLSSVD,transformer,sklearn,cross_decomposition,_pls,PLSSVD,,"Partial Least Square SVD.  This transformer simply performs a SVD on the cross-covariance matrix `X'Y`. It is able to project both the training data `X` and the targets `Y`. The training data `X` is projected on the left singular vectors, while the targets are projected on the right singular vectors.  Read more in the :ref:`User Guide <cross_decomposition>`."
PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>,Large,sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier,classifier,sklearn,linear_model,_passive_aggressive,PassiveAggressiveClassifier,,"Passive Aggressive Classifier.  Read more in the :ref:`User Guide <passive_aggressive>`.  Parameters ---------- C : float, default=1.0 Maximum step size (regularization). Defaults to 1.0.  fit_intercept : bool, default=True Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.  max_iter : int, default=1000 The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`~sklearn.linear_model.PassiveAggressiveClassifier.partial_fit` method."
PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>,Large,sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor,regressor,sklearn,linear_model,_passive_aggressive,PassiveAggressiveRegressor,,"Passive Aggressive Regressor.  Read more in the :ref:`User Guide <passive_aggressive>`.  Parameters ----------  C : float, default=1.0 Maximum step size (regularization). Defaults to 1.0.  fit_intercept : bool, default=True Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.  max_iter : int, default=1000 The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`~sklearn.linear_model.PassiveAggressiveRegressor.partial_fit` method."
PatchExtractor,<class 'sklearn.feature_extraction.image.PatchExtractor'>,Large,sklearn.feature_extraction.image.PatchExtractor,transformer,sklearn,feature_extraction,image,PatchExtractor,,Extracts patches from a collection of images.  Read more in the :ref:`User Guide <image_feature_extraction>`.
Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>,1,sklearn.linear_model._perceptron.Perceptron,classifier,sklearn,linear_model,_perceptron,Perceptron,,"Linear perceptron classifier.  The implementation is a wrapper around :class:`~sklearn.linear_model.SGDClassifier` by fixing the `loss` and `learning_rate` parameters as::  SGDClassifier(loss=""perceptron"", learning_rate=""constant"")  Other available parameters are described below and are forwarded to :class:`~sklearn.linear_model.SGDClassifier`.  Read more in the :ref:`User Guide <perceptron>`.  Parameters ----------  penalty : {'l2','l1','elasticnet'}, default=None The penalty (aka regularization term) to be used.  alpha : float, default=0.0001 Constant that multiplies the regularization term if regularization is used.  l1_ratio : float, default=0.15 The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`. `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1. Only used if `penalty='elasticnet'`."
Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>,1,sklearn.linear_model._perceptron.Perceptron,classifier,sklearn,linear_model,_perceptron,Perceptron,,"Linear perceptron classifier.  The implementation is a wrapper around :class:`~sklearn.linear_model.SGDClassifier` by fixing the `loss` and `learning_rate` parameters as::  SGDClassifier(loss=""perceptron"", learning_rate=""constant"")  Other available parameters are described below and are forwarded to :class:`~sklearn.linear_model.SGDClassifier`.  Read more in the :ref:`User Guide <perceptron>`.  Parameters ----------  penalty : {'l2','l1','elasticnet'}, default=None The penalty (aka regularization term) to be used.  alpha : float, default=0.0001 Constant that multiplies the regularization term if regularization is used.  l1_ratio : float, default=0.15 The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`. `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1. Only used if `penalty='elasticnet'`."
Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>,Large,sklearn.linear_model._perceptron.Perceptron,classifier,sklearn,linear_model,_perceptron,Perceptron,,"Linear perceptron classifier.  The implementation is a wrapper around :class:`~sklearn.linear_model.SGDClassifier` by fixing the `loss` and `learning_rate` parameters as::  SGDClassifier(loss=""perceptron"", learning_rate=""constant"")  Other available parameters are described below and are forwarded to :class:`~sklearn.linear_model.SGDClassifier`.  Read more in the :ref:`User Guide <perceptron>`.  Parameters ----------  penalty : {'l2','l1','elasticnet'}, default=None The penalty (aka regularization term) to be used.  alpha : float, default=0.0001 Constant that multiplies the regularization term if regularization is used.  l1_ratio : float, default=0.15 The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`. `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1. Only used if `penalty='elasticnet'`."
Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>,Large,sklearn.linear_model._perceptron.Perceptron,classifier,sklearn,linear_model,_perceptron,Perceptron,,"Linear perceptron classifier.  The implementation is a wrapper around :class:`~sklearn.linear_model.SGDClassifier` by fixing the `loss` and `learning_rate` parameters as::  SGDClassifier(loss=""perceptron"", learning_rate=""constant"")  Other available parameters are described below and are forwarded to :class:`~sklearn.linear_model.SGDClassifier`.  Read more in the :ref:`User Guide <perceptron>`.  Parameters ----------  penalty : {'l2','l1','elasticnet'}, default=None The penalty (aka regularization term) to be used.  alpha : float, default=0.0001 Constant that multiplies the regularization term if regularization is used.  l1_ratio : float, default=0.15 The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`. `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1. Only used if `penalty='elasticnet'`."
Pipeline,<class 'sklearn.pipeline.Pipeline'>,"Small, Medium, Large",sklearn.pipeline.Pipeline,unknown,sklearn,pipeline,Pipeline,,,"A sequence of data transformers with an optional final predictor.  `Pipeline` allows you to sequentially apply a list of transformers to preprocess the data and, if desired, conclude the sequence with a final :term:`predictor` for predictive modeling.  Intermediate steps of the pipeline must be transformers, that is, they must implement `fit` and `transform` methods. The final :term:`estimator` only needs to implement `fit`. The transformers in the pipeline can be cached using ``memory`` argument.  The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a `'__'`, as in the example below. A step's estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting it to `'passthrough'` or `None`.  For an example use case of `Pipeline` combined with :class:`~sklearn.model_selection.GridSearchCV`, refer to :ref:`sphx_glr_auto_examples_compose_plot_compare_reduction.py`. The example :ref:`sphx_glr_auto_examples_compose_plot_digits_pipe.py` shows how to grid search on a pipeline using `'__'` as a separator in the parameter names.  Read more in the :ref:`User Guide <pipeline>`."
PoissonRegressor,<class 'sklearn.linear_model._glm.glm.PoissonRegressor'>,Medium,sklearn.linear_model._glm.glm.PoissonRegressor,regressor,sklearn,linear_model,_glm,glm,PoissonRegressor,Generalized Linear Model with a Poisson distribution.  This regressor uses the 'log' link function.  Read more in the :ref:`User Guide <Generalized_linear_models>`.
PolynomialCountSketch,<class 'sklearn.kernel_approximation.PolynomialCountSketch'>,Large,sklearn.kernel_approximation.PolynomialCountSketch,transformer,sklearn,kernel_approximation,PolynomialCountSketch,,,"Polynomial kernel approximation via Tensor Sketch.  Implements Tensor Sketch, which approximates the feature map of the polynomial kernel::  K(X, Y) = (gamma * <X, Y> + coef0)^degree  by efficiently computing a Count Sketch of the outer product of a vector with itself using Fast Fourier Transforms (FFT). Read more in the :ref:`User Guide <polynomial_kernel_approx>`."
PolynomialFeatures,<class 'sklearn.preprocessing._polynomial.PolynomialFeatures'>,Medium,sklearn.preprocessing._polynomial.PolynomialFeatures,transformer,sklearn,preprocessing,_polynomial,PolynomialFeatures,,"Generate polynomial and interaction features.  Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].  Read more in the :ref:`User Guide <polynomial_features>`.  Parameters ---------- degree : int or tuple (min_degree, max_degree), default=2 If a single int is given, it specifies the maximal degree of the polynomial features. If a tuple `(min_degree, max_degree)` is passed, then `min_degree` is the minimum and `max_degree` is the maximum polynomial degree of the generated features. Note that `min_degree=0` and `min_degree=1` are equivalent as outputting the degree zero term is determined by `include_bias`.  interaction_only : bool, default=False If `True`, only interaction features are produced: features that are products of at most `degree` *distinct* input features, i.e. terms with power of 2 or higher of the same input feature are excluded:  - included: `x[0]`, `x[1]`, `x[0] * x[1]`, etc. - excluded: `x[0] ** 2`, `x[0] ** 2 * x[1]`, etc.  include_bias : bool, default=True If `True` (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model).  order : {'C', 'F'}, default='C' Order of output array in the dense case. `'F'` order is faster to compute, but may slow down subsequent estimators."
PowerTransformer,<class 'sklearn.preprocessing._data.PowerTransformer'>,Large,sklearn.preprocessing._data.PowerTransformer,transformer,sklearn,preprocessing,_data,PowerTransformer,,"Apply a power transform featurewise to make data more Gaussian-like.  Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired.  Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood.  Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data.  By default, zero-mean, unit-variance normalization is applied to the transformed data.  For an example visualization, refer to :ref:`Compare PowerTransformer with other scalers <plot_all_scaling_power_transformer_section>`. To see the effect of Box-Cox and Yeo-Johnson transformations on different distributions, see: :ref:`sphx_glr_auto_examples_preprocessing_plot_map_data_to_normal.py`.  Read more in the :ref:`User Guide <preprocessing_transformer>`."
QuadraticDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>,"Small, Medium",sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis,classifier,sklearn,discriminant_analysis,QuadraticDiscriminantAnalysis,,,"Quadratic Discriminant Analysis.  A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes' rule.  The model fits a Gaussian density to each class."
QuantileRegressor,<class 'sklearn.linear_model._quantile.QuantileRegressor'>,"Small, Medium",sklearn.linear_model._quantile.QuantileRegressor,regressor,sklearn,linear_model,_quantile,QuantileRegressor,,Linear regression model that predicts conditional quantiles.  The linear :class:`QuantileRegressor` optimizes the pinball loss for a desired `quantile` and is robust to outliers.  This model uses an L1 regularization like :class:`~sklearn.linear_model.Lasso`.  Read more in the :ref:`User Guide <quantile_regression>`.
QuantileTransformer,<class 'sklearn.preprocessing._data.QuantileTransformer'>,Large,sklearn.preprocessing._data.QuantileTransformer,transformer,sklearn,preprocessing,_data,QuantileTransformer,,"Transform features using quantiles information.  This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme.  The transformation is applied on each feature independently. First an estimate of the cumulative distribution function of a feature is used to map the original values to a uniform distribution. The obtained values are then mapped to the desired output distribution using the associated quantile function. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.  For example visualizations, refer to :ref:`Compare QuantileTransformer with other scalers <plot_all_scaling_quantile_transformer_section>`.  Read more in the :ref:`User Guide <preprocessing_transformer>`."
RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>,Small,sklearn.linear_model._ransac.RANSACRegressor,regressor,sklearn,linear_model,_ransac,RANSACRegressor,,"RANSAC (RANdom SAmple Consensus) algorithm.  RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set.  Read more in the :ref:`User Guide <ransac_regression>`.  Parameters ---------- estimator : object, default=None Base estimator object which implements the following methods:  * `fit(X, y)`: Fit model to given training data and target values. * `score(X, y)`: Returns the mean accuracy on the given test data, which is used for the stop criterion defined by `stop_score`. Additionally, the score is used to decide which of two equally large consensus sets is chosen as the better one. * `predict(X)`: Returns predicted values using the linear model, which is used to compute residual error using loss function.  If `estimator` is None, then :class:`~sklearn.linear_model.LinearRegression` is used for target values of dtype float.  Note that the current implementation only supports regression estimators.  min_samples : int (>= 1) or float ([0, 1]), default=None Minimum number of samples chosen randomly from original data. Treated as an absolute number of samples for `min_samples >= 1`, treated as a relative number `ceil(min_samples * X.shape[0])` for `min_samples < 1`. This is typically chosen as the minimal number of samples necessary to estimate the given `estimator`. By default a :class:`~sklearn.linear_model.LinearRegression` estimator is assumed and `min_samples` is chosen as ``X.shape[1] + 1``. This parameter is highly dependent upon the model, so if a `estimator` other than :class:`~sklearn.linear_model.LinearRegression` is used, the user must provide a value.  residual_threshold : float, default=None Maximum residual for a data sample to be classified as an inlier. By default the threshold is chosen as the MAD (median absolute deviation) of the target values `y`. Points whose residuals are strictly equal to the threshold are considered as inliers.  is_data_valid : callable, default=None This function is called with the randomly selected data before the model is fitted to it: `is_data_valid(X, y)`. If its return value is False the current randomly chosen sub-sample is skipped.  is_model_valid : callable, default=None This function is called with the estimated model and the randomly selected data: `is_model_valid(model, X, y)`. If its return value is False the current randomly chosen sub-sample is skipped. Rejecting samples with this function is computationally costlier than with `is_data_valid`. `is_model_valid` should therefore only be used if the estimated model is needed for making the rejection decision.  max_trials : int, default=100 Maximum number of iterations for random sample selection.  max_skips : int, default=np.inf Maximum number of iterations that can be skipped due to finding zero inliers or invalid data defined by ``is_data_valid`` or invalid models defined by ``is_model_valid``."
RBFSampler,<class 'sklearn.kernel_approximation.RBFSampler'>,Large,sklearn.kernel_approximation.RBFSampler,transformer,sklearn,kernel_approximation,RBFSampler,,,"Approximate a RBF kernel feature map using random Fourier features.  It implements a variant of Random Kitchen Sinks.[1]  Read more in the :ref:`User Guide <rbf_kernel_approx>`.  Parameters ---------- gamma : 'scale' or float, default=1.0 Parameter of RBF kernel: exp(-gamma * x^2). If ``gamma='scale'`` is passed then it uses 1 / (n_features * X.var()) as value of gamma."
RFE,<class 'sklearn.feature_selection._rfe.RFE'>,"Small, Medium",sklearn.feature_selection._rfe.RFE,transformer,sklearn,feature_selection,_rfe,RFE,,"Feature ranking with recursive feature elimination.  Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.  Read more in the :ref:`User Guide <rfe>`.  Parameters ---------- estimator : ``Estimator`` instance A supervised learning estimator with a ``fit`` method that provides information about feature importance (e.g. `coef_`, `feature_importances_`).  n_features_to_select : int or float, default=None The number of features to select. If `None`, half of the features are selected. If integer, the parameter is the absolute number of features to select. If float between 0 and 1, it is the fraction of features to select."
RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>,"Small, Medium",sklearn.feature_selection._rfe.RFECV,transformer,sklearn,feature_selection,_rfe,RFECV,,"Recursive feature elimination with cross-validation to select features.  The number of features selected is tuned automatically by fitting an :class:`RFE` selector on the different cross-validation splits (provided by the `cv` parameter). The performance of the :class:`RFE` selector are evaluated using `scorer` for different number of selected features and aggregated together. Finally, the scores are averaged across folds and the number of features selected is set to the number of features that maximize the cross-validation score. See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <rfe>`.  Parameters ---------- estimator : ``Estimator`` instance A supervised learning estimator with a ``fit`` method that provides information about feature importance either through a ``coef_`` attribute or through a ``feature_importances_`` attribute.  step : int or float, default=1 If greater than or equal to 1, then ``step`` corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then ``step`` corresponds to the percentage (rounded down) of features to remove at each iteration. Note that the last iteration may remove fewer than ``step`` features in order to reach ``min_features_to_select``.  min_features_to_select : int, default=1 The minimum number of features to be selected. This number of features will always be scored, even if the difference between the original feature count and ``min_features_to_select`` isn't divisible by ``step``."
RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>,Small,sklearn.neighbors._classification.RadiusNeighborsClassifier,classifier,sklearn,neighbors,_classification,RadiusNeighborsClassifier,,"Classifier implementing a vote among neighbors within a given radius.  Read more in the :ref:`User Guide <classification>`.  Parameters ---------- radius : float, default=1.0 Range of parameter space to use by default for :meth:`radius_neighbors` queries.  weights : {'uniform', 'distance'}, callable or None, default='uniform' Weight function used in prediction.  Possible values:  - 'uniform' : uniform weights.  All points in each neighborhood are weighted equally. - 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. - [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.  Uniform weights are used by default.  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto' Algorithm used to compute the nearest neighbors:  - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDTree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method.  Note: fitting on sparse input will override the setting of this parameter, using brute force.  leaf_size : int, default=30 Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem.  p : float, default=2 Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected to be positive.  metric : str or callable, default='minkowski' Metric to use for distance computation. Default is ""minkowski"", which results in the standard Euclidean distance when p = 2. See the documentation of `scipy.spatial.distance <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric values.  If metric is ""precomputed"", X is assumed to be a distance matrix and must be square during fit. X may be a :term:`sparse graph`, in which case only ""nonzero"" elements may be considered neighbors.  If metric is a callable function, it takes two arrays representing 1D vectors as inputs and must return one value indicating the distance between those vectors. This works for Scipy's metrics, but is less efficient than passing the metric name as a string.  outlier_label : {manual label, 'most_frequent'}, default=None Label for outlier samples (samples with no neighbors in given radius).  - manual label: str or int label (should be the same type as y) or list of manual labels if multi-output is used. - 'most_frequent' : assign the most frequent label of y to outliers. - None : when any outlier is detected, ValueError will be raised.  The outlier label should be selected from among the unique 'Y' labels. If it is specified with a different value a warning will be raised and all class probabilities of outliers will be assigned to be 0.  metric_params : dict, default=None Additional keyword arguments for the metric function.  n_jobs : int, default=None The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.  Attributes ---------- classes_ : ndarray of shape (n_classes,) Class labels known to the classifier.  effective_metric_ : str or callable The distance metric used. It will be same as the `metric` parameter or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to 'minkowski' and `p` parameter set to 2.  effective_metric_params_ : dict Additional keyword arguments for the metric function. For most metrics will be same with `metric_params` parameter, but may also contain the `p` parameter value if the `effective_metric_` attribute is set to 'minkowski'.  n_features_in_ : int Number of features seen during :term:`fit`."
RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>,Small,sklearn.neighbors._regression.RadiusNeighborsRegressor,regressor,sklearn,neighbors,_regression,RadiusNeighborsRegressor,,Regression based on neighbors within a fixed radius.  The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.  Read more in the :ref:`User Guide <regression>`.
RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>,Small,sklearn.neighbors._graph.RadiusNeighborsTransformer,transformer,sklearn,neighbors,_graph,RadiusNeighborsTransformer,,Transform X into a (weighted) graph of neighbors nearer than a radius.  The transformed data is a sparse graph as returned by `radius_neighbors_graph`.  Read more in the :ref:`User Guide <neighbors_transformer>`.
RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>,1,sklearn.ensemble._forest.RandomForestClassifier,classifier,sklearn,ensemble,_forest,RandomForestClassifier,,"A random forest classifier.  A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Trees in the forest use the best split strategy, i.e. equivalent to passing `splitter=""best""` to the underlying :class:`~sklearn.tree.DecisionTreeClassifier`. The sub-sample size is controlled with the `max_samples` parameter if `bootstrap=True` (default), otherwise the whole dataset is used to build each tree.  For a comparison between tree-based ensemble models see the example :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : int, default=100 The number of trees in the forest."
RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>,1,sklearn.ensemble._forest.RandomForestClassifier,classifier,sklearn,ensemble,_forest,RandomForestClassifier,,"A random forest classifier.  A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Trees in the forest use the best split strategy, i.e. equivalent to passing `splitter=""best""` to the underlying :class:`~sklearn.tree.DecisionTreeClassifier`. The sub-sample size is controlled with the `max_samples` parameter if `bootstrap=True` (default), otherwise the whole dataset is used to build each tree.  For a comparison between tree-based ensemble models see the example :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : int, default=100 The number of trees in the forest."
RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>,"Medium, Large",sklearn.ensemble._forest.RandomForestClassifier,classifier,sklearn,ensemble,_forest,RandomForestClassifier,,"A random forest classifier.  A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Trees in the forest use the best split strategy, i.e. equivalent to passing `splitter=""best""` to the underlying :class:`~sklearn.tree.DecisionTreeClassifier`. The sub-sample size is controlled with the `max_samples` parameter if `bootstrap=True` (default), otherwise the whole dataset is used to build each tree.  For a comparison between tree-based ensemble models see the example :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : int, default=100 The number of trees in the forest."
RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>,"Medium, Large",sklearn.ensemble._forest.RandomForestClassifier,classifier,sklearn,ensemble,_forest,RandomForestClassifier,,"A random forest classifier.  A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Trees in the forest use the best split strategy, i.e. equivalent to passing `splitter=""best""` to the underlying :class:`~sklearn.tree.DecisionTreeClassifier`. The sub-sample size is controlled with the `max_samples` parameter if `bootstrap=True` (default), otherwise the whole dataset is used to build each tree.  For a comparison between tree-based ensemble models see the example :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : int, default=100 The number of trees in the forest."
RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>,"Medium, Large",sklearn.ensemble._forest.RandomForestRegressor,regressor,sklearn,ensemble,_forest,RandomForestRegressor,,"A random forest regressor.  A random forest is a meta estimator that fits a number of decision tree regressors on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Trees in the forest use the best split strategy, i.e. equivalent to passing `splitter=""best""` to the underlying :class:`~sklearn.tree.DecisionTreeRegressor`. The sub-sample size is controlled with the `max_samples` parameter if `bootstrap=True` (default), otherwise the whole dataset is used to build each tree.  For a comparison between tree-based ensemble models see the example :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.  Read more in the :ref:`User Guide <forest>`.  Parameters ---------- n_estimators : int, default=100 The number of trees in the forest."
RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>,Large,sklearn.ensemble._forest.RandomTreesEmbedding,transformer,sklearn,ensemble,_forest,RandomTreesEmbedding,,"An ensemble of totally random trees.  An unsupervised transformation of a dataset to a high-dimensional sparse representation. A datapoint is coded according to which leaf of each tree it is sorted into. Using a one-hot encoding of the leaves, this leads to a binary coding with as many ones as there are trees in the forest.  The dimensionality of the resulting representation is ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``, the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.  For an example of applying Random Trees Embedding to non-linear classification, see :ref:`sphx_glr_auto_examples_ensemble_plot_random_forest_embedding.py`.  Read more in the :ref:`User Guide <random_trees_embedding>`.  Parameters ---------- n_estimators : int, default=100 Number of trees in the forest."
RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>,"Small, Medium",sklearn.model_selection._search.RandomizedSearchCV,unknown,sklearn,model_selection,_search,RandomizedSearchCV,,"Randomized search on hyper parameters.  RandomizedSearchCV implements a ""fit"" and a ""score"" method. It also implements ""score_samples"", ""predict"", ""predict_proba"", ""decision_function"", ""transform"" and ""inverse_transform"" if they are implemented in the estimator used.  The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.  In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.  If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters.  Read more in the :ref:`User Guide <randomized_parameter_search>`."
RegressorChain,<class 'sklearn.multioutput.RegressorChain'>,Medium,sklearn.multioutput.RegressorChain,regressor,sklearn,multioutput,RegressorChain,,,A multi-label model that arranges regressions into a chain.  Each model makes a prediction in the order specified by the chain using all of the available features provided to the model plus the predictions of models that are earlier in the chain.  Read more in the :ref:`User Guide <regressorchain>`.
Ridge,<class 'sklearn.linear_model._ridge.Ridge'>,"Medium, Large",sklearn.linear_model._ridge.Ridge,regressor,sklearn,linear_model,_ridge,Ridge,,"Linear least squares with l2 regularization.  Minimizes the objective function::  ||y - Xw||^2_2 + alpha * ||w||^2_2  This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape (n_samples, n_targets)).  Read more in the :ref:`User Guide <ridge_regression>`.  Parameters ---------- alpha : {float, ndarray of shape (n_targets,)}, default=1.0 Constant that multiplies the L2 term, controlling regularization strength. `alpha` must be a non-negative float i.e. in `[0, inf)`.  When `alpha = 0`, the objective is equivalent to ordinary least squares, solved by the :class:`LinearRegression` object. For numerical reasons, using `alpha = 0` with the `Ridge` object is not advised. Instead, you should use the :class:`LinearRegression` object.  If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.  fit_intercept : bool, default=True Whether to fit the intercept for this model. If set to false, no intercept will be used in calculations (i.e. ``X`` and ``y`` are expected to be centered).  copy_X : bool, default=True If True, X will be copied; else, it may be overwritten.  max_iter : int, default=None Maximum number of iterations for conjugate gradient solver. For 'sparse_cg' and 'lsqr' solvers, the default value is determined by scipy.sparse.linalg. For 'sag' solver, the default value is 1000. For 'lbfgs' solver, the default value is 15000.  tol : float, default=1e-4 The precision of the solution (`coef_`) is determined by `tol` which specifies a different convergence criterion for each solver:  - 'svd': `tol` has no impact.  - 'cholesky': `tol` has no impact.  - 'sparse_cg': norm of residuals smaller than `tol`.  - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr, which control the norm of the residual vector in terms of the norms of matrix and coefficients.  - 'sag' and 'saga': relative change of coef smaller than `tol`.  - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals| smaller than `tol`."
RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>,"Medium, Large",sklearn.linear_model._ridge.RidgeCV,regressor,sklearn,linear_model,_ridge,RidgeCV,,"Ridge regression with built-in cross-validation.  See glossary entry for :term:`cross-validation estimator`.  By default, it performs efficient Leave-One-Out Cross-Validation.  Read more in the :ref:`User Guide <ridge_regression>`.  Parameters ---------- alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0) Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear models such as :class:`~sklearn.linear_model.LogisticRegression` or :class:`~sklearn.svm.LinearSVC`. If using Leave-One-Out cross-validation, alphas must be strictly positive.  fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).  scoring : str, callable, default=None A string (see :ref:`scoring_parameter`) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. If None, the negative mean squared error if cv is 'auto' or None (i.e. when using leave-one-out cross-validation), and r2 score otherwise.  cv : int, cross-validation generator or an iterable, default=None Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the efficient Leave-One-Out cross-validation - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  For integer/None inputs, if ``y`` is binary or multiclass, :class:`~sklearn.model_selection.StratifiedKFold` is used, else, :class:`~sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  gcv_mode : {'auto', 'svd', 'eigen'}, default='auto' Flag indicating which strategy to use when performing Leave-One-Out Cross-Validation. Options are::  'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen' 'svd' : force use of singular value decomposition of X when X is dense, eigenvalue decomposition of X^T.X when X is sparse. 'eigen' : force computation via eigendecomposition of X.X^T  The 'auto' mode is the default and is intended to pick the cheaper option of the two depending on the shape of the training data.  store_cv_results : bool, default=False Flag indicating if the cross-validation values corresponding to each alpha should be stored in the ``cv_results_`` attribute (see below). This flag is only compatible with ``cv=None`` (i.e. using Leave-One-Out Cross-Validation)."
RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>,"Medium, Large",sklearn.linear_model._ridge.RidgeClassifier,classifier,sklearn,linear_model,_ridge,RidgeClassifier,,"Classifier using Ridge regression.  This classifier first converts the target values into ``{-1, 1}`` and then treats the problem as a regression task (multi-output regression in the multiclass case).  Read more in the :ref:`User Guide <ridge_regression>`.  Parameters ---------- alpha : float, default=1.0 Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear models such as :class:`~sklearn.linear_model.LogisticRegression` or :class:`~sklearn.svm.LinearSVC`.  fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).  copy_X : bool, default=True If True, X will be copied; else, it may be overwritten.  max_iter : int, default=None Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg.  tol : float, default=1e-4 The precision of the solution (`coef_`) is determined by `tol` which specifies a different convergence criterion for each solver:  - 'svd': `tol` has no impact.  - 'cholesky': `tol` has no impact.  - 'sparse_cg': norm of residuals smaller than `tol`.  - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr, which control the norm of the residual vector in terms of the norms of matrix and coefficients.  - 'sag' and 'saga': relative change of coef smaller than `tol`.  - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals| smaller than `tol`."
RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>,"Medium, Large",sklearn.linear_model._ridge.RidgeClassifierCV,classifier,sklearn,linear_model,_ridge,RidgeClassifierCV,,"Ridge classifier with built-in cross-validation.  See glossary entry for :term:`cross-validation estimator`.  By default, it performs Leave-One-Out Cross-Validation. Currently, only the n_features > n_samples case is handled efficiently.  Read more in the :ref:`User Guide <ridge_regression>`.  Parameters ---------- alphas : array-like of shape (n_alphas,), default=(0.1, 1.0, 10.0) Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear models such as :class:`~sklearn.linear_model.LogisticRegression` or :class:`~sklearn.svm.LinearSVC`. If using Leave-One-Out cross-validation, alphas must be strictly positive.  fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).  scoring : str, callable, default=None A string (see :ref:`scoring_parameter`) or a scorer callable object / function with signature ``scorer(estimator, X, y)``.  cv : int, cross-validation generator or an iterable, default=None Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the efficient Leave-One-Out cross-validation - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  class_weight : dict or 'balanced', default=None Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.  store_cv_results : bool, default=False Flag indicating if the cross-validation results corresponding to each alpha should be stored in the ``cv_results_`` attribute (see below). This flag is only compatible with ``cv=None`` (i.e. using Leave-One-Out Cross-Validation)."
RobustScaler,<class 'sklearn.preprocessing._data.RobustScaler'>,Large,sklearn.preprocessing._data.RobustScaler,transformer,sklearn,preprocessing,_data,RobustScaler,,"Scale features using statistics that are robust to outliers.  This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).  Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the :meth:`transform` method.  Standardization of a dataset is a common preprocessing for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, using the median and the interquartile range often give better results. For an example visualization and comparison to other scalers, refer to :ref:`Compare RobustScaler with other scalers <plot_all_scaling_robust_scaler_section>`."
SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>,Large,sklearn.linear_model._stochastic_gradient.SGDClassifier,classifier,sklearn,linear_model,_stochastic_gradient,SGDClassifier,,"Linear classifiers (SVM, logistic regression, etc.) with SGD training.  This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning via the `partial_fit` method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.  This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).  The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.  Read more in the :ref:`User Guide <sgd>`.  Parameters ---------- loss : {'hinge', 'log_loss', 'modified_huber', 'squared_hinge',        'perceptron', 'squared_error', 'huber', 'epsilon_insensitive',        'squared_epsilon_insensitive'}, default='hinge' The loss function to be used.  - 'hinge' gives a linear SVM. - 'log_loss' gives logistic regression, a probabilistic classifier. - 'modified_huber' is another smooth loss that brings tolerance to outliers as well as probability estimates. - 'squared_hinge' is like hinge but is quadratically penalized. - 'perceptron' is the linear loss used by the perceptron algorithm. - The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and 'squared_epsilon_insensitive' are designed for regression but can be useful in classification as well; see :class:`~sklearn.linear_model.SGDRegressor` for a description.  More details about the losses formulas can be found in the :ref:`User Guide <sgd_mathematical_formulation>` and you can find a visualisation of the loss functions in :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_loss_functions.py`.  penalty : {'l2', 'l1', 'elasticnet', None}, default='l2' The penalty (aka regularization term) to be used. Defaults to 'l2' which is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might bring sparsity to the model (feature selection) not achievable with 'l2'. No penalty is added when set to `None`.  You can see a visualisation of the penalties in :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`.  alpha : float, default=0.0001 Constant that multiplies the regularization term. The higher the value, the stronger the regularization. Also used to compute the learning rate when `learning_rate` is set to 'optimal'. Values must be in the range `[0.0, inf)`.  l1_ratio : float, default=0.15 The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Only used if `penalty` is 'elasticnet'. Values must be in the range `[0.0, 1.0]`.  fit_intercept : bool, default=True Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.  max_iter : int, default=1000 The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method. Values must be in the range `[1, inf)`."
SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>,"Medium, Large",sklearn.linear_model._stochastic_gradient.SGDOneClassSVM,unknown,sklearn,linear_model,_stochastic_gradient,SGDOneClassSVM,,Solves linear One-Class SVM using Stochastic Gradient Descent.  This implementation is meant to be used with a kernel approximation technique (e.g. `sklearn.kernel_approximation.Nystroem`) to obtain results similar to `sklearn.svm.OneClassSVM` which uses a Gaussian kernel by default.  Read more in the :ref:`User Guide <sgd_online_one_class_svm>`.
SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>,Large,sklearn.linear_model._stochastic_gradient.SGDRegressor,regressor,sklearn,linear_model,_stochastic_gradient,SGDRegressor,,"Linear model fitted by minimizing a regularized empirical loss with SGD.  SGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).  The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.  This implementation works with data represented as dense numpy arrays of floating point values for the features.  Read more in the :ref:`User Guide <sgd>`.  Parameters ---------- loss : str, default='squared_error' The loss function to be used. The possible values are 'squared_error', 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'  The 'squared_error' refers to the ordinary least squares fit. 'huber' modifies 'squared_error' to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is linear past that; this is the loss function used in SVR. 'squared_epsilon_insensitive' is the same but becomes squared loss past a tolerance of epsilon.  More details about the losses formulas can be found in the :ref:`User Guide <sgd_mathematical_formulation>`.  penalty : {'l2', 'l1', 'elasticnet', None}, default='l2' The penalty (aka regularization term) to be used. Defaults to 'l2' which is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might bring sparsity to the model (feature selection) not achievable with 'l2'. No penalty is added when set to `None`.  You can see a visualisation of the penalties in :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`.  alpha : float, default=0.0001 Constant that multiplies the regularization term. The higher the value, the stronger the regularization. Also used to compute the learning rate when `learning_rate` is set to 'optimal'. Values must be in the range `[0.0, inf)`.  l1_ratio : float, default=0.15 The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Only used if `penalty` is 'elasticnet'. Values must be in the range `[0.0, 1.0]`.  fit_intercept : bool, default=True Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.  max_iter : int, default=1000 The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method. Values must be in the range `[1, inf)`."
SVC,<class 'sklearn.svm._classes.SVC'>,"Small, Medium",sklearn.svm._classes.SVC,classifier,sklearn,svm,_classes,SVC,,"C-Support Vector Classification.  The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using :class:`~sklearn.svm.LinearSVC` or :class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a :class:`~sklearn.kernel_approximation.Nystroem` transformer or other :ref:`kernel_approximation`.  The multiclass support is handled according to a one-vs-one scheme.  For details on the precise mathematical formulation of the provided kernel functions and how `gamma`, `coef0` and `degree` affect each other, see the corresponding section in the narrative documentation: :ref:`svm_kernels`.  To learn how to tune SVC's hyperparameters, see the following example: :ref:`sphx_glr_auto_examples_model_selection_plot_nested_cross_validation_iris.py`  Read more in the :ref:`User Guide <svm_classification>`.  Parameters ---------- C : float, default=1.0 Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty. For an intuitive visualization of the effects of scaling the regularization parameter C, see :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.  kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf' Specifies the kernel type to be used in the algorithm. If none is given, 'rbf' will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape ``(n_samples, n_samples)``. For an intuitive visualization of different kernel types see :ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`.  degree : int, default=3 Degree of the polynomial kernel function ('poly'). Must be non-negative. Ignored by all other kernels.  gamma : {'scale', 'auto'} or float, default='scale' Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.  - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features - if float, must be non-negative."
SVR,<class 'sklearn.svm._classes.SVR'>,"Small, Medium",sklearn.svm._classes.SVR,regressor,sklearn,svm,_classes,SVR,,"Epsilon-Support Vector Regression.  The free parameters in the model are C and epsilon.  The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets consider using :class:`~sklearn.svm.LinearSVR` or :class:`~sklearn.linear_model.SGDRegressor` instead, possibly after a :class:`~sklearn.kernel_approximation.Nystroem` transformer or other :ref:`kernel_approximation`.  Read more in the :ref:`User Guide <svm_regression>`.  Parameters ---------- kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf' Specifies the kernel type to be used in the algorithm. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix. For an intuitive visualization of different kernel types see :ref:`sphx_glr_auto_examples_svm_plot_svm_regression.py`  degree : int, default=3 Degree of the polynomial kernel function ('poly'). Must be non-negative. Ignored by all other kernels.  gamma : {'scale', 'auto'} or float, default='scale' Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.  - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features - if float, must be non-negative."
SelectFdr,<class 'sklearn.feature_selection._univariate_selection.SelectFdr'>,Large,sklearn.feature_selection._univariate_selection.SelectFdr,transformer,sklearn,feature_selection,_univariate_selection,SelectFdr,,"Filter: Select the p-values for an estimated false discovery rate.  This uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound on the expected false discovery rate.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable, default=f_classif Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below ""See Also""). The default function only works with classification tasks.  alpha : float, default=5e-2 The highest uncorrected p-value for features to keep.  Attributes ---------- scores_ : array-like of shape (n_features,) Scores of features.  pvalues_ : array-like of shape (n_features,) p-values of feature scores.  n_features_in_ : int Number of features seen during :term:`fit`."
SelectFpr,<class 'sklearn.feature_selection._univariate_selection.SelectFpr'>,Large,sklearn.feature_selection._univariate_selection.SelectFpr,transformer,sklearn,feature_selection,_univariate_selection,SelectFpr,,"Filter: Select the pvalues below alpha based on a FPR test.  FPR test stands for False Positive Rate test. It controls the total amount of false detections.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable, default=f_classif Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below ""See Also""). The default function only works with classification tasks.  alpha : float, default=5e-2 Features with p-values less than `alpha` are selected.  Attributes ---------- scores_ : array-like of shape (n_features,) Scores of features.  pvalues_ : array-like of shape (n_features,) p-values of feature scores.  n_features_in_ : int Number of features seen during :term:`fit`."
SelectFromModel,<class 'sklearn.feature_selection._from_model.SelectFromModel'>,Large,sklearn.feature_selection._from_model.SelectFromModel,transformer,sklearn,feature_selection,_from_model,SelectFromModel,,Meta-transformer for selecting features based on importance weights.
SelectFwe,<class 'sklearn.feature_selection._univariate_selection.SelectFwe'>,Large,sklearn.feature_selection._univariate_selection.SelectFwe,transformer,sklearn,feature_selection,_univariate_selection,SelectFwe,,"Filter: Select the p-values corresponding to Family-wise error rate.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable, default=f_classif Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below ""See Also""). The default function only works with classification tasks.  alpha : float, default=5e-2 The highest uncorrected p-value for features to keep.  Attributes ---------- scores_ : array-like of shape (n_features,) Scores of features.  pvalues_ : array-like of shape (n_features,) p-values of feature scores.  n_features_in_ : int Number of features seen during :term:`fit`."
SelectKBest,<class 'sklearn.feature_selection._univariate_selection.SelectKBest'>,Large,sklearn.feature_selection._univariate_selection.SelectKBest,transformer,sklearn,feature_selection,_univariate_selection,SelectKBest,,"Select features according to the k highest scores.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable, default=f_classif Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below ""See Also""). The default function only works with classification tasks."
SelectPercentile,<class 'sklearn.feature_selection._univariate_selection.SelectPercentile'>,Large,sklearn.feature_selection._univariate_selection.SelectPercentile,transformer,sklearn,feature_selection,_univariate_selection,SelectPercentile,,"Select features according to a percentile of the highest scores.  Read more in the :ref:`User Guide <univariate_feature_selection>`.  Parameters ---------- score_func : callable, default=f_classif Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below ""See Also""). The default function only works with classification tasks."
SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>,Medium,sklearn.semi_supervised._self_training.SelfTrainingClassifier,classifier,sklearn,semi_supervised,_self_training,SelfTrainingClassifier,,"Self-training classifier.  This :term:`metaestimator` allows a given supervised classifier to function as a semi-supervised classifier, allowing it to learn from unlabeled data. It does this by iteratively predicting pseudo-labels for the unlabeled data and adding them to the training set.  The classifier will continue iterating until either max_iter is reached, or no pseudo-labels were added to the training set in the previous iteration.  Read more in the :ref:`User Guide <self_training>`.  Parameters ---------- estimator : estimator object An estimator object implementing `fit` and `predict_proba`. Invoking the `fit` method will fit a clone of the passed estimator, which will be stored in the `estimator_` attribute."
SequentialFeatureSelector,<class 'sklearn.feature_selection._sequential.SequentialFeatureSelector'>,"Small, Medium",sklearn.feature_selection._sequential.SequentialFeatureSelector,transformer,sklearn,feature_selection,_sequential,SequentialFeatureSelector,,"Transformer that performs Sequential Feature Selection.  This Sequential Feature Selector adds (forward selection) or removes (backward selection) features to form a feature subset in a greedy fashion. At each stage, this estimator chooses the best feature to add or remove based on the cross-validation score of an estimator. In the case of unsupervised learning, this Sequential Feature Selector looks only at the features (X), not the desired outputs (y).  Read more in the :ref:`User Guide <sequential_feature_selection>`."
ShrunkCovariance,<class 'sklearn.covariance._shrunk_covariance.ShrunkCovariance'>,Small,sklearn.covariance._shrunk_covariance.ShrunkCovariance,unknown,sklearn,covariance,_shrunk_covariance,ShrunkCovariance,,"Covariance estimator with shrinkage.  Read more in the :ref:`User Guide <shrunk_covariance>`.  Parameters ---------- store_precision : bool, default=True Specify if the estimated precision is stored.  assume_centered : bool, default=False If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data will be centered before computation.  shrinkage : float, default=0.1 Coefficient in the convex combination used for the computation of the shrunk estimate. Range is [0, 1].  Attributes ---------- covariance_ : ndarray of shape (n_features, n_features) Estimated covariance matrix  location_ : ndarray of shape (n_features,) Estimated location, i.e. the estimated mean.  precision_ : ndarray of shape (n_features, n_features) Estimated pseudo inverse matrix. (stored only if store_precision is True)  n_features_in_ : int Number of features seen during :term:`fit`."
SimpleImputer,<class 'sklearn.impute._base.SimpleImputer'>,Large,sklearn.impute._base.SimpleImputer,transformer,sklearn,impute,_base,SimpleImputer,,"Univariate imputer for completing missing values with simple strategies.  Replace missing values using a descriptive statistic (e.g. mean, median, or most frequent) along each column, or using a constant value.  Read more in the :ref:`User Guide <impute>`."
SkewedChi2Sampler,<class 'sklearn.kernel_approximation.SkewedChi2Sampler'>,Large,sklearn.kernel_approximation.SkewedChi2Sampler,transformer,sklearn,kernel_approximation,SkewedChi2Sampler,,,"Approximate feature map for ""skewed chi-squared"" kernel.  Read more in the :ref:`User Guide <skewed_chi_kernel_approx>`.  Parameters ---------- skewedness : float, default=1.0 ""skewedness"" parameter of the kernel. Needs to be cross-validated.  n_components : int, default=100 Number of Monte Carlo samples per original feature. Equals the dimensionality of the computed feature space.  random_state : int, RandomState instance or None, default=None Pseudo-random number generator to control the generation of the random weights and random offset when fitting the training data. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.  Attributes ---------- random_weights_ : ndarray of shape (n_features, n_components) Weight array, sampled from a secant hyperbolic distribution, which will be used to linearly transform the log of the data.  random_offset_ : ndarray of shape (n_features, n_components) Bias term, which will be added to the data. It is uniformly distributed between 0 and 2*pi.  n_features_in_ : int Number of features seen during :term:`fit`."
SparseCoder,<class 'sklearn.decomposition._dict_learning.SparseCoder'>,"Small, Medium",sklearn.decomposition._dict_learning.SparseCoder,transformer,sklearn,decomposition,_dict_learning,SparseCoder,,"Sparse coding.  Finds a sparse representation of data against a fixed, precomputed dictionary.  Each row of the result is the solution to a sparse coding problem. The goal is to find a sparse array `code` such that::  X ~= code * dictionary  Read more in the :ref:`User Guide <SparseCoder>`.  Parameters ---------- dictionary : ndarray of shape (n_components, n_features) The dictionary atoms used for sparse coding. Lines are assumed to be normalized to unit norm.  transform_algorithm : {'lasso_lars', 'lasso_cd', 'lars', 'omp',             'threshold'}, default='omp' Algorithm used to transform the data:  - `'lars'`: uses the least angle regression method (`linear_model.lars_path`); - `'lasso_lars'`: uses Lars to compute the Lasso solution; - `'lasso_cd'`: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if the estimated components are sparse; - `'omp'`: uses orthogonal matching pursuit to estimate the sparse solution; - `'threshold'`: squashes to zero all coefficients less than alpha from the projection ``dictionary * X'``.  transform_n_nonzero_coefs : int, default=None Number of nonzero coefficients to target in each column of the solution. This is only used by `algorithm='lars'` and `algorithm='omp'` and is overridden by `alpha` in the `omp` case. If `None`, then `transform_n_nonzero_coefs=int(n_features / 10)`.  transform_alpha : float, default=None If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the penalty applied to the L1 norm. If `algorithm='threshold'`, `alpha` is the absolute value of the threshold below which coefficients will be squashed to zero. If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides `n_nonzero_coefs`. If `None`, default to 1.  split_sign : bool, default=False Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers.  n_jobs : int, default=None Number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.  positive_code : bool, default=False Whether to enforce positivity when finding the code."
SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>,"Medium, Large",sklearn.decomposition._sparse_pca.SparsePCA,transformer,sklearn,decomposition,_sparse_pca,SparsePCA,,"Sparse Principal Components Analysis (SparsePCA).  Finds the set of sparse components that can optimally reconstruct the data.  The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha.  Read more in the :ref:`User Guide <SparsePCA>`.  Parameters ---------- n_components : int, default=None Number of sparse atoms to extract. If None, then ``n_components`` is set to ``n_features``.  alpha : float, default=1 Sparsity controlling parameter. Higher values lead to sparser components.  ridge_alpha : float, default=0.01 Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method.  max_iter : int, default=1000 Maximum number of iterations to perform.  tol : float, default=1e-8 Tolerance for the stopping condition.  method : {'lars', 'cd'}, default='lars' Method to be used for optimization. lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.  n_jobs : int, default=None Number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.  U_init : ndarray of shape (n_samples, n_components), default=None Initial values for the loadings for warm restart scenarios. Only used if `U_init` and `V_init` are not None.  V_init : ndarray of shape (n_components, n_features), default=None Initial values for the components for warm restart scenarios. Only used if `U_init` and `V_init` are not None.  verbose : int or bool, default=False Controls the verbosity; the higher, the more messages. Defaults to 0.  random_state : int, RandomState instance or None, default=None Used during dictionary learning. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.  Attributes ---------- components_ : ndarray of shape (n_components, n_features) Sparse components extracted from the data.  error_ : ndarray Vector of errors at each iteration.  n_components_ : int Estimated number of components."
SparseRandomProjection,<class 'sklearn.random_projection.SparseRandomProjection'>,Large,sklearn.random_projection.SparseRandomProjection,transformer,sklearn,random_projection,SparseRandomProjection,,,Reduce dimensionality through sparse random projection.  Sparse random matrix is an alternative to dense random projection matrix that guarantees similar embedding quality while being much more memory efficient and allowing faster computation of the projected data.  If we note `s = 1 / density` the components of the random matrix are drawn from:
SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>,"Small, Medium",sklearn.cluster._bicluster.SpectralBiclustering,unknown,sklearn,cluster,_bicluster,SpectralBiclustering,,"Spectral biclustering (Kluger, 2003).  Partitions rows and columns under the assumption that the data has an underlying checkerboard structure. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters. The outer product of the corresponding row and column label vectors gives this checkerboard structure.  Read more in the :ref:`User Guide <spectral_biclustering>`.  Parameters ---------- n_clusters : int or tuple (n_row_clusters, n_column_clusters), default=3 The number of row and column clusters in the checkerboard structure.  method : {'bistochastic', 'scale', 'log'}, default='bistochastic' Method of normalizing and converting singular vectors into biclusters. May be one of 'scale', 'bistochastic', or 'log'. The authors recommend using 'log'. If the data is sparse, however, log normalization will not work, which is why the default is 'bistochastic'."
SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>,"Small, Medium",sklearn.cluster._spectral.SpectralClustering,cluster,sklearn,cluster,_spectral,SpectralClustering,,"Apply clustering to a projection of the normalized Laplacian.  In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex, or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster, such as when clusters are nested circles on the 2D plane.  If the affinity matrix is the adjacency matrix of a graph, this method can be used to find normalized graph cuts [1]_, [2]_.  When calling ``fit``, an affinity matrix is constructed using either a kernel function such the Gaussian (aka RBF) kernel with Euclidean distance ``d(X, X)``::  np.exp(-gamma * d(X,X) ** 2)  or a k-nearest neighbors connectivity matrix.  Alternatively, a user-provided affinity matrix can be specified by setting ``affinity='precomputed'``.  Read more in the :ref:`User Guide <spectral_clustering>`.  Parameters ---------- n_clusters : int, default=8 The dimension of the projection subspace.  eigen_solver : {'arpack', 'lobpcg', 'amg'}, default=None The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities. If None, then ``'arpack'`` is used. See [4]_ for more details regarding `'lobpcg'`.  n_components : int, default=None Number of eigenvectors to use for the spectral embedding. If None, defaults to `n_clusters`.  random_state : int, RandomState instance, default=None A pseudo random number generator used for the initialization of the lobpcg eigenvectors decomposition when `eigen_solver == 'amg'`, and for the K-Means initialization. Use an int to make the results deterministic across calls (See :term:`Glossary <random_state>`)."
SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>,"Small, Medium",sklearn.cluster._bicluster.SpectralCoclustering,unknown,sklearn,cluster,_bicluster,SpectralCoclustering,,"Spectral Co-Clustering algorithm (Dhillon, 2001).  Clusters rows and columns of an array `X` to solve the relaxed normalized cut of the bipartite graph created from `X` as follows: the edge between row vertex `i` and column vertex `j` has weight `X[i, j]`.  The resulting bicluster structure is block-diagonal, since each row and each column belongs to exactly one bicluster.  Supports sparse matrices, as long as they are nonnegative.  Read more in the :ref:`User Guide <spectral_coclustering>`.  Parameters ---------- n_clusters : int, default=3 The number of biclusters to find.  svd_method : {'randomized', 'arpack'}, default='randomized' Selects the algorithm for finding singular vectors. May be 'randomized' or 'arpack'. If 'randomized', use :func:`sklearn.utils.extmath.randomized_svd`, which may be faster for large matrices. If 'arpack', use :func:`scipy.sparse.linalg.svds`, which is more accurate, but possibly slower in some cases.  n_svd_vecs : int, default=None Number of vectors to use in calculating the SVD. Corresponds to `ncv` when `svd_method=arpack` and `n_oversamples` when `svd_method` is 'randomized`.  mini_batch : bool, default=False Whether to use mini-batch k-means, which is faster but may get different results.  init : {'k-means++', 'random'}, or ndarray of shape             (n_clusters, n_features), default='k-means++' Method for initialization of k-means algorithm; defaults to 'k-means++'.  n_init : int, default=10 Number of random initializations that are tried with the k-means algorithm.  If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen.  random_state : int, RandomState instance, default=None Used for randomizing the singular value decomposition and the k-means initialization. Use an int to make the randomness deterministic. See :term:`Glossary <random_state>`.  Attributes ---------- rows_ : array-like of shape (n_row_clusters, n_rows) Results of the clustering. `rows[i, r]` is True if cluster `i` contains row `r`. Available only after calling ``fit``.  columns_ : array-like of shape (n_column_clusters, n_columns) Results of the clustering, like `rows`.  row_labels_ : array-like of shape (n_rows,) The bicluster label of each row.  column_labels_ : array-like of shape (n_cols,) The bicluster label of each column.  biclusters_ : tuple of two ndarrays The tuple contains the `rows_` and `columns_` arrays.  n_features_in_ : int Number of features seen during :term:`fit`."
SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>,Small,sklearn.manifold._spectral_embedding.SpectralEmbedding,unknown,sklearn,manifold,_spectral_embedding,SpectralEmbedding,,"Spectral embedding for non-linear dimensionality reduction.  Forms an affinity matrix given by the specified function and applies spectral decomposition to the corresponding graph laplacian. The resulting transformation is given by the value of the eigenvectors for each data point.  Note : Laplacian Eigenmaps is the actual algorithm implemented here.  Read more in the :ref:`User Guide <spectral_embedding>`.  Parameters ---------- n_components : int, default=2 The dimension of the projected subspace.  affinity : {'nearest_neighbors', 'rbf', 'precomputed',                 'precomputed_nearest_neighbors'} or callable,                 default='nearest_neighbors' How to construct the affinity matrix. - 'nearest_neighbors' : construct the affinity matrix by computing a graph of nearest neighbors. - 'rbf' : construct the affinity matrix by computing a radial basis function (RBF) kernel. - 'precomputed' : interpret ``X`` as a precomputed affinity matrix. - 'precomputed_nearest_neighbors' : interpret ``X`` as a sparse graph of precomputed nearest neighbors, and constructs the affinity matrix by selecting the ``n_neighbors`` nearest neighbors. - callable : use passed in function as affinity the function takes in data matrix (n_samples, n_features) and return affinity matrix (n_samples, n_samples).  gamma : float, default=None Kernel coefficient for rbf kernel. If None, gamma will be set to 1/n_features.  random_state : int, RandomState instance or None, default=None A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when `eigen_solver == 'amg'`, and for the K-Means initialization. Use an int to make the results deterministic across calls (See :term:`Glossary <random_state>`)."
SplineTransformer,<class 'sklearn.preprocessing._polynomial.SplineTransformer'>,Large,sklearn.preprocessing._polynomial.SplineTransformer,transformer,sklearn,preprocessing,_polynomial,SplineTransformer,,"Generate univariate B-spline bases for features.  Generate a new feature matrix consisting of `n_splines=n_knots + degree - 1` (`n_knots - 1` for `extrapolation=""periodic""`) spline basis functions (B-splines) of polynomial order=`degree` for each feature.  In order to learn more about the SplineTransformer class go to: :ref:`sphx_glr_auto_examples_applications_plot_cyclical_feature_engineering.py`  Read more in the :ref:`User Guide <spline_transformer>`."
StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>,Medium,sklearn.ensemble._stacking.StackingClassifier,"classifier, transformer",sklearn,ensemble,_stacking,StackingClassifier,,Stack of estimators with a final classifier.  Stacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.  Note that `estimators_` are fitted on the full `X` while `final_estimator_` is trained using cross-validated predictions of the base estimators using `cross_val_predict`.  Read more in the :ref:`User Guide <stacking>`.
StackingRegressor,<class 'sklearn.ensemble._stacking.StackingRegressor'>,Medium,sklearn.ensemble._stacking.StackingRegressor,"regressor, transformer",sklearn,ensemble,_stacking,StackingRegressor,,Stack of estimators with a final regressor.  Stacked generalization consists in stacking the output of individual estimator and use a regressor to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.  Note that `estimators_` are fitted on the full `X` while `final_estimator_` is trained using cross-validated predictions of the base estimators using `cross_val_predict`.  Read more in the :ref:`User Guide <stacking>`.
StandardScaler,<class 'sklearn.preprocessing._data.StandardScaler'>,Large,sklearn.preprocessing._data.StandardScaler,transformer,sklearn,preprocessing,_data,StandardScaler,,Standardize features by removing the mean and scaling to unit variance.  The standard score of a sample `x` is calculated as:
TSNE,<class 'sklearn.manifold._t_sne.TSNE'>,Small,sklearn.manifold._t_sne.TSNE,transformer,sklearn,manifold,_t_sne,TSNE,,"T-distributed Stochastic Neighbor Embedding.  t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.  It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. For more tips see Laurens van der Maaten's FAQ [2].  Read more in the :ref:`User Guide <t_sne>`.  Parameters ---------- n_components : int, default=2 Dimension of the embedded space.  perplexity : float, default=30.0 The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significantly different results. The perplexity must be less than the number of samples.  early_exaggeration : float, default=12.0 Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. For larger values, the space between natural clusters will be larger in the embedded space. Again, the choice of this parameter is not very critical. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high.  learning_rate : float or ""auto"", default=""auto"" The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a 'ball' with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help. Note that many other t-SNE implementations (bhtsne, FIt-SNE, openTSNE, etc.) use a definition of learning_rate that is 4 times smaller than ours. So our learning_rate=200 corresponds to learning_rate=800 in those other implementations. The 'auto' option sets the learning_rate to `max(N / early_exaggeration / 4, 50)` where N is the sample size, following [4] and [5]."
TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>,Large,sklearn.preprocessing._target_encoder.TargetEncoder,transformer,sklearn,preprocessing,_target_encoder,TargetEncoder,,"Target Encoder for regression and classification targets.  Each category is encoded based on a shrunk estimate of the average target values for observations belonging to the category. The encoding scheme mixes the global target mean with the target mean conditioned on the value of the category (see [MIC]_).  When the target type is ""multiclass"", encodings are based on the conditional probability estimate for each class. The target is first binarized using the ""one-vs-all"" scheme via :class:`~sklearn.preprocessing.LabelBinarizer`, then the average target value for each class and each category is used for encoding, resulting in `n_features` * `n_classes` encoded output features.  :class:`TargetEncoder` considers missing values, such as `np.nan` or `None`, as another category and encodes them like any other category. Categories that are not seen during :meth:`fit` are encoded with the target mean, i.e. `target_mean_`.  For a demo on the importance of the `TargetEncoder` internal cross-fitting, see :ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder_cross_val.py`. For a comparison of different encoders, refer to :ref:`sphx_glr_auto_examples_preprocessing_plot_target_encoder.py`. Read more in the :ref:`User Guide <target_encoder>`."
TfidfTransformer,<class 'sklearn.feature_extraction.text.TfidfTransformer'>,Large,sklearn.feature_extraction.text.TfidfTransformer,transformer,sklearn,feature_extraction,text,TfidfTransformer,,"Transform a count matrix to a normalized tf or tf-idf representation.  Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.  The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.  The formula that is used to compute the tf-idf for a term t of a document d in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where n is the total number of documents in the document set and df(t) is the document frequency of t; the document frequency is the number of documents in the document set that contain the term t. The effect of adding ""1"" to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(t) = log [ n / (df(t) + 1) ]).  If ``smooth_idf=True`` (the default), the constant ""1"" is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.  Furthermore, the formulas used to compute tf and idf depend on parameter settings that correspond to the SMART notation used in IR as follows:  Tf is ""n"" (natural) by default, ""l"" (logarithmic) when ``sublinear_tf=True``. Idf is ""t"" when use_idf is given, ""n"" (none) otherwise. Normalization is ""c"" (cosine) when ``norm='l2'``, ""n"" (none) when ``norm=None``.  Read more in the :ref:`User Guide <text_feature_extraction>`.  Parameters ---------- norm : {'l1', 'l2'} or None, default='l2' Each output row will have unit norm, either:  - 'l2': Sum of squares of vector elements is 1. The cosine similarity between two vectors is their dot product when l2 norm has been applied. - 'l1': Sum of absolute values of vector elements is 1. See :func:`~sklearn.preprocessing.normalize`. - None: No normalization.  use_idf : bool, default=True Enable inverse-document-frequency reweighting. If False, idf(t) = 1.  smooth_idf : bool, default=True Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.  sublinear_tf : bool, default=False Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).  Attributes ---------- idf_ : array of shape (n_features) The inverse document frequency (IDF) vector; only defined if  ``use_idf`` is True."
TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>,Large,sklearn.feature_extraction.text.TfidfVectorizer,unknown,sklearn,feature_extraction,text,TfidfVectorizer,,"Convert a collection of raw documents to a matrix of TF-IDF features.  Equivalent to :class:`CountVectorizer` followed by :class:`TfidfTransformer`.  For an example of usage, see :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`.  For an efficiency comparison of the different feature extractors, see :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.  For an example of document clustering and comparison with :class:`~sklearn.feature_extraction.text.HashingVectorizer`, see :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`.  Read more in the :ref:`User Guide <text_feature_extraction>`.  Parameters ---------- input : {'filename', 'file', 'content'}, default='content' - If `'filename'`, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.  - If `'file'`, the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory.  - If `'content'`, the input is expected to be a sequence of items that can be of type string or byte.  encoding : str, default='utf-8' If bytes or files are given to analyze, this encoding is used to decode.  decode_error : {'strict', 'ignore', 'replace'}, default='strict' Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'.  strip_accents : {'ascii', 'unicode'} or callable, default=None Remove accents and perform other character normalization during the preprocessing step. 'ascii' is a fast method that only works on characters that have a direct ASCII mapping. 'unicode' is a slightly slower method that works on any characters. None (default) means no character normalization is performed.  Both 'ascii' and 'unicode' use NFKD normalization from :func:`unicodedata.normalize`.  lowercase : bool, default=True Convert all characters to lowercase before tokenizing.  preprocessor : callable, default=None Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if ``analyzer`` is not callable.  tokenizer : callable, default=None Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if ``analyzer == 'word'``.  analyzer : {'word', 'char', 'char_wb'} or callable, default='word' Whether the feature should be made of word or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.  If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input."
TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>,"Small, Medium",sklearn.linear_model._theil_sen.TheilSenRegressor,regressor,sklearn,linear_model,_theil_sen,TheilSenRegressor,,"Theil-Sen Estimator: robust multivariate regression model.  The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is ""n_samples choose n_subsamples"", it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions.  Read more in the :ref:`User Guide <theil_sen_regression>`.  Parameters ---------- fit_intercept : bool, default=True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations.  copy_X : bool, default=True If True, X will be copied; else, it may be overwritten."
TransformedTargetRegressor,<class 'sklearn.compose._target.TransformedTargetRegressor'>,"Small, Medium, Large",sklearn.compose._target.TransformedTargetRegressor,regressor,sklearn,compose,_target,TransformedTargetRegressor,,"Meta-estimator to regress on a transformed target.  Useful for applying a non-linear transformation to the target `y` in regression problems. This transformation can be given as a Transformer such as the :class:`~sklearn.preprocessing.QuantileTransformer` or as a function and its inverse such as `np.log` and `np.exp`.  The computation during :meth:`fit` is::  regressor.fit(X, func(y))  or::  regressor.fit(X, transformer.transform(y))  The computation during :meth:`predict` is::  inverse_func(regressor.predict(X))  or::  transformer.inverse_transform(regressor.predict(X))  Read more in the :ref:`User Guide <transformed_target_regressor>`."
TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>,Large,sklearn.decomposition._truncated_svd.TruncatedSVD,transformer,sklearn,decomposition,_truncated_svd,TruncatedSVD,,"Dimensionality reduction using truncated SVD (aka LSA).  This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with sparse matrices efficiently.  In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In that context, it is known as latent semantic analysis (LSA).  This estimator supports two algorithms: a fast randomized SVD solver, and a ""naive"" algorithm that uses ARPACK as an eigensolver on `X * X.T` or `X.T * X`, whichever is more efficient.  Read more in the :ref:`User Guide <LSA>`.  Parameters ---------- n_components : int, default=2 Desired dimensionality of output data. If algorithm='arpack', must be strictly less than the number of features. If algorithm='randomized', must be less than or equal to the number of features. The default value is useful for visualisation. For LSA, a value of 100 is recommended.  algorithm : {'arpack', 'randomized'}, default='randomized' SVD solver to use. Either ""arpack"" for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ""randomized"" for the randomized algorithm due to Halko (2009).  n_iter : int, default=5 Number of iterations for randomized SVD solver. Not used by ARPACK. The default is larger than the default in :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse matrices that may have large slowly decaying spectrum.  n_oversamples : int, default=10 Number of oversamples for randomized SVD solver. Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd` for a complete description."
TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>,Unknown,sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV,classifier,sklearn,model_selection,_classification_threshold,TunedThresholdClassifierCV,,"Classifier that post-tunes the decision threshold using cross-validation.  This estimator post-tunes the decision threshold (cut-off point) that is used for converting posterior probability estimates (i.e. output of `predict_proba`) or decision scores (i.e. output of `decision_function`) into a class label. The tuning is done by optimizing a binary metric, potentially constrained by a another metric.  Read more in the :ref:`User Guide <TunedThresholdClassifierCV>`."
TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>,Medium,sklearn.linear_model._glm.glm.TweedieRegressor,regressor,sklearn,linear_model,_glm,glm,TweedieRegressor,"Generalized Linear Model with a Tweedie distribution.  This estimator can be used to model different GLMs depending on the ``power`` parameter, which determines the underlying distribution.  Read more in the :ref:`User Guide <Generalized_linear_models>`."
VarianceThreshold,<class 'sklearn.feature_selection._variance_threshold.VarianceThreshold'>,Large,sklearn.feature_selection._variance_threshold.VarianceThreshold,transformer,sklearn,feature_selection,_variance_threshold,VarianceThreshold,,"Feature selector that removes all low-variance features.  This feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.  Read more in the :ref:`User Guide <variance_threshold>`.  Parameters ---------- threshold : float, default=0 Features with a training-set variance lower than this threshold will be removed. The default is to keep all features with non-zero variance, i.e. remove the features that have the same value in all samples.  Attributes ---------- variances_ : array, shape (n_features,) Variances of individual features.  n_features_in_ : int Number of features seen during :term:`fit`."
VotingClassifier,<class 'sklearn.ensemble._voting.VotingClassifier'>,"Medium, Large",sklearn.ensemble._voting.VotingClassifier,"classifier, transformer",sklearn,ensemble,_voting,VotingClassifier,,Soft Voting/Majority Rule classifier for unfitted estimators.  Read more in the :ref:`User Guide <voting_classifier>`.
VotingRegressor,<class 'sklearn.ensemble._voting.VotingRegressor'>,"Medium, Large",sklearn.ensemble._voting.VotingRegressor,"regressor, transformer",sklearn,ensemble,_voting,VotingRegressor,,"Prediction voting regressor for unfitted estimators.  A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction.  For a detailed example, refer to :ref:`sphx_glr_auto_examples_ensemble_plot_voting_regressor.py`.  Read more in the :ref:`User Guide <voting_regressor>`."
