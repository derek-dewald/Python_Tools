Date,Historical_Score,Word,Process,Categorization,Definition,Notes,Link,Image,Markdown Equation,Dataset Size,Learning Type,Algorithm Classification,Model Type
2026-01-07,-2,SGDClassifier,ML Model,Algorithm,"SGDClassifier is a linear classification algorithm that trains models using stochastic gradient descent, updating parameters incrementally with each batch or observation. It is important because it scales efficiently to very large datasets and supports multiple loss functions (e.g., logistic regression, hinge loss). SGDClassifier is commonly used when data is high-dimensional, streaming, or too large for batch optimization methods. It trades some stability for speed and scalability.",,,,,Large,Supervised Learning,Classification,Linear
2026-01-07,-5,VotingClassifier,ML Model,Algorithm,"VotingClassifier is an ensemble method that combines predictions from multiple different models into a single prediction, using either majority voting (hard voting) or averaged probabilities (soft voting). It is important because combining diverse models often improves robustness and generalization compared to any single model. Voting classifiers are typically used when several reasonably strong models exist and their errors are not perfectly correlated. They are especially useful in production systems where stability is valued over interpretability.",,,,,"Medium, Large",Supervised Learning,Classification,Meta-Model
2026-01-07,-3,Quantile Random Forest,ML Model,Algorithm,"Quantile Random Forest is an extension of random forests that estimates conditional quantiles of the target variable instead of only the mean. It is important because it provides uncertainty information and prediction intervals, not just point estimates. This method is commonly used in risk modeling, forecasting, and decision-making scenarios where understanding the distribution of outcomes matters. It is particularly useful when residuals are heteroskedastic or non-Gaussian.",,,,,,Supervised Learning,Regression,Tree-Based
2026-01-07,-5,RandomTreesEmbedding,ML Model,Algorithm,"RandomTreesEmbedding is an unsupervised transformation technique that maps data into a high-dimensional sparse feature space using randomly generated decision trees. It originates from ensemble tree methods and was introduced to create nonlinear feature representations. Its importance lies in enabling downstream linear models to capture complex, nonlinear structures. Unlike Random Forests, it does not use target labels during training. It is commonly used for feature engineering, clustering, and anomaly detection.",,,,,Large,Unsupervised Learning,Feature Transformation,Tree-Based
2026-01-07,-3,Area Under the Curve,Model Evaluation,Feature Selection,"AUC (Area Under the Curve) measures a model’s ability to distinguish between classes by summarizing the performance of the ROC (Receiver Operating Characteristic) curve. It represents the probability that the model ranks a randomly chosen positive example higher than a randomly chosen negative one. AUC ranges from 0.5 (no better than chance) to 1.0 (perfect separation), with higher values indicating better classification performance.
AUC is primarily a binary classification metric, so when using against a multivariate challenge must determine how to score, OVR, OVO. One Vs Rest, One vs One. Using MNist as an example, OVR evaluates 10 0 vs (1,2,3,4,5,6,7,8,9), 1 vs () ... etc, Where OVO measures 45 0 Vs 1, 0 vs 2, etc..",,,,,,Assessment,Evaluation,
2026-01-08,-3,Perceptron,ML Model,Algorithm,"The Perceptron is one of the earliest linear classification algorithms, learning a decision boundary by iteratively adjusting weights based on misclassified examples. It is important historically as the foundation of modern neural networks and gradient-based learning. In practice, it is used mainly for educational purposes or as a fast baseline on linearly separable data. Its simplicity limits its performance on noisy or non-linear problems.",,,,,Large,Supervised Learning,Classification,Linear
2026-01-08,-5,RidgeCV,ML Model,Algorithm,"RidgeCV is a regression model that combines ridge regression (L2 regularization) with cross-validation to automatically select the optimal regularization strength. It is important because it reduces overfitting while maintaining stability in the presence of multicollinearity. RidgeCV is commonly used when predictors are highly correlated and predictive performance is prioritized over feature sparsity. It provides a principled, automated way to tune regularization.",,,,,"Medium, Large",Supervised Learning,Regression,Linear
2026-01-08,-2,Regularization,ML Model,Definition,"Technique used to prevent overfitting by adding a penalty term to the loss function. It discourages the model from fitting too closely to the training data, ensuring better generalization to unseen data. Traditionnaly results in bias, at benefit of materially lowering Variance. Regularization is a Constraint.",,,,,,,,
2026-01-08,-3,SelectFpr,ML Model,Algorithm,SelectFpr is a feature selection method that selects features based on statistical tests while controlling the false positive rate. It is important because it helps remove irrelevant features while limiting the probability of including noise variables. SelectFpr is typically used in high-dimensional settings such as genomics or text data. It is appropriate when interpretability and statistical control are priorities.,,,,,Large,,,
2026-01-08,-5,AdaBoostClassifier,ML Model,Algorithm,"AdaBoostClassifier is an ensemble boosting algorithm that builds a strong classifier by sequentially combining many weak learners, typically decision stumps. It is important because it focuses learning on previously misclassified examples, often achieving high accuracy with simple base models. AdaBoost is commonly used when data is moderately sized and relatively clean. It can be sensitive to noise and outliers due to its reweighting mechanism.",,,,,"Small, Medium",Supervised Learning,Classification,Linear
2026-01-13,-5,Adagrad,Training,Optimizer,"Variation of Gradient Descent, which adapts the learning rate for each parameter by scaling it inversely proportional to the sum of past squared gradients. 
",,,,"$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{G_t + \epsilon}} \nabla J(\theta_t)
$$

where:
- $( G_t = \sum_{i=1}^{t} \nabla J(\theta_i) \odot \nabla J(\theta_i) )$ (element-wise sum of squared gradients),
- $( \epsilon )$ is a small constant to prevent division by zero.",,,,
2026-01-13,-2,NMF,ML Model,Algorithm,"Non-negative Matrix Factorization is a matrix decomposition technique that factorizes a non-negative data matrix into two lower-rank non-negative matrices, typically interpreted as parts-based representations of the original data. It was popularized in the late 1990s by Daniel D. Lee and H. Sebastian Seung, who showed that the non-negativity constraint leads to more interpretable components than methods like PCA or SVD. NMF is important because it aligns well with many real-world datasets where values are naturally non-negative (e.g., counts, intensities, frequencies) and because its components often correspond to meaningful latent factors. It is widely used in unsupervised learning for dimensionality reduction and feature extraction. Common applications include topic modeling in text analysis, image and signal processing, bioinformatics (e.g., gene expression analysis), and customer behavior segmentation.",,,,,"Medium, Large",,,
2026-01-13,-2,Cache,Data Engineering,Definition,"Cache takes load away from critical systems. Databases. Things that are looked up every second, minute. Can Cache at every layer.  Memory Buffers exist everywhere. Authentication is terrible caching. Don't Cache purchases and refunds. Search Queries, too broad. Can't cache transaction.",,,,,,,,
2026-01-13,-2,Grapth Theory,Graph Theory,Definition,,,,,,,,,
2026-01-14,-2,HuberRegressor,ML Model,Algorithm,"HuberRegressor is a robust linear regression model that uses the Huber loss function, which behaves like squared error for small rsiduals and absolute error for large residuals, reducing sensitivity to outliers. It originates from Peter J. Huber’s work in the 1960s on robust statistics, where he introduced the Huber loss as a compromise between least squares and least absolute deviations. Its importance lies in providing stable, reliable parameter estimates when datasets contain noise or outliers that would disproportionately influence ordinary linear regression. HuberRegressor is commonly used in supervised learning for regression problems where data quality is uneven or heavy-tailed. Typical applications include finance, economics, sensor data analysis, and any real-world modeling scenario where robustness matters more than strict optimality under ideal assumptions",,,,,Small,Supervised Learning,Regression,Meta-Model
2026-01-14,-2,RandomForestClassifier,ML Model,Algorithm,"RandomForestClassifier is an ensemble learning algorithm that builds many decision trees using bootstrapped samples of the data and aggregates their predictions via majority voting. It was formalized by Leo Breiman (2001), building on earlier work in decision trees and bagging to reduce variance and overfitting. Its importance lies in its strong out-of-the-box performance, robustness to noise, and ability to model complex non-linear relationships without heavy feature engineering. The model is widely used because it balances predictive power with interpretability through feature importance measures. In practice, it is a common baseline and production model across many applied machine-learning domains.",,,,,"Medium, Large",Supervised Learning,Classification,Tree-Based
2026-01-14,-3,DBSCAN,ML Model,Algorithm,DBSCAN is a density-based clustering algorithm that groups points based on local neighborhood density without requiring a predefined number of clusters. It is especially useful for identifying anomalies and non-spherical clusters. DBSCAN’s strength is its ability to label noise explicitly and handle irregular cluster shapes. Its main weakness is poor scalability and difficulty tuning parameters when data density varies widely.,,,,,"Small, Medium",Unsupervised Learning,Clustering,Probabilistic
2026-01-14,-2,RFE,ML Model,Algorithm,,,,,,,,,
2026-01-14,-3,Core Variables,Data Preparation,Functional Role ,,,,,,,,,
2026-02-08,-3,Instance Based Learning,Mathematics,Definition,"Instance-Based Learning is a learning paradigm where predictions or inferences are made by directly comparing new observations to stored examples rather than learning an explicit global model. It answers the question: “What past observations most closely resemble this one?” Its strengths are simplicity, transparency, and flexibility, as it makes few assumptions about the underlying data structure. Its weaknesses include sensitivity to feature scaling and noise, lack of global summarization, and higher computational cost at inference time for large datasets",,,,,,,,
2026-02-08,-2,LatentDirichletAllocation,ML Model,Algorithm,,,,,,"Medium, Large",,,
2026-02-08,-2,NuSVC,ML Model,Algorithm,,,,,,"Small, Medium",,,
2026-02-08,1,Pareto Principle,Theorem,Definition,"80/20 Rule, is a principle named after the Italian economist Vilfredo Pareto, who observed that roughly 80% of the effects come from 20% of the causes.",,,,,,,,
2026-02-08,-3,Evaluation,ML Project,Pipeline Stage,"
As a general term, Evaluation has multiple meanings, so care is required to be explicit about its usage. In the context of machine learning, Evaluation has two distinct and valid meanings: one as an Algorithm Classification and one as an ML Pipeline Stage. These uses are related but serve different conceptual purposes.

Pipeline Step: Measures how well the final model performs against defined success metrics on a truly unseen test dataset. Metrics depend on the problem type (e.g., accuracy, AUC, RMSE, precision–recall). This step provides an objective assessment of whether the model meets business or research requirements. Evaluation results often determine whether a model is production-ready.

Algorithm Class: Methods that quantify the performance, quality, or reliability of models or learned structures. These techniques produce metrics, scores, or diagnostic summaries rather than predictions or feature transformations. Evaluation methods support model comparison, validation, and selection but do not directly influence how models are trained or how decisions are made.",,,,,,,,
