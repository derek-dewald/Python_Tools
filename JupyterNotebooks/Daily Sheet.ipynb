{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351918b-6aa3-4128-ace3-bb75f8e9be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate Sheets in Streamlit.\n",
    "# Have Read Me and Clear DOcumented process to remind self.\n",
    "\n",
    "# Consolidate Notes such that it's also exportable and ordered.\n",
    "\n",
    "## Need a process for Looking at a .py File and then ensuring I have a reference usage here so I can remember.\n",
    "## Need to also Classify the models based on usage and Dates so I can see changes.\n",
    "## Need to determine a plan to migrate to a single usage.\n",
    "\n",
    "# Add Filter on Definition Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9bc9fdb-c7fb-4b09-abc3-42a3b72ff883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")\n",
    "\n",
    "#!bash /Users/derekdewald/scripts/daily_shell_job.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8aaafc-d417-4a89-b196-926f038ba490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from daily_processes import daily_test\n",
    "\n",
    "#review_test_results()\n",
    "daily_test()\n",
    "review_test_results()\n",
    "\n",
    "#df = pd.read_csv('/Users/derekdewald/Documents/Python/Github_Repo/Data/daily_test_results.csv')\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c01798-7063-4af8-a97b-b171d3945b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "from data_d_strings import gpt_question\n",
    "gpt_question(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c064e23-2bfe-4d72-a5a3-1976fbcd74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from daily_processes import generate_dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd6d627-c385-48e6-b257-56535bbb3357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Definition', 'Guiding Principle', 'Algorithm', 'Consideration', 'Process Step', 'Model Type', 'Process', 'Requirement', 'Project Implementation', 'Project Conceputalization', 'Expected Outcomes', 'Functional Role', 'Regularization', 'Semantic Type', 'Function', 'Model Architecture', 'Optimizer', 'Procedure', 'Parameter', 'Automating Python', 'Tooling', 'Evaluation', 'Pipeline Stage', 'Theorem', 'Concept', 'Hadoop', 'Paradigm', 'Spark']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")\n",
    "\n",
    "from shared_folder import read_directory,text_file_import,parse_dot_py_file\n",
    "from input_functions_ignore import input1,input2,input3\n",
    "import data_d_dicts,data_d_lists,data_d_strings\n",
    "from data_d_dicts import function_table_dictionary,links\n",
    "from dict_processing import dict_to_dataframe\n",
    "from list_processing import list_to_dataframe\n",
    "from data_validation import column_segmenter\n",
    "\n",
    "def generate_dictionary(notes_df=None,\n",
    "                        definition_df=None,\n",
    "                        export_location=''):\n",
    "\n",
    "    '''\n",
    "    Function Used to Generate D Learning Notes, which is a consolidated view of D Notes and D Definitions.\n",
    "    Used as a crtical input to D Streamlit Dashboard.\n",
    "\n",
    "    CSV Generated from this process saved to Github and Used for Streamlit Dashboard.\n",
    "\n",
    "    Approach takes D Notes as the Primary Source and then merges in Definitions, based on a 4 step integration process.\n",
    "\n",
    "    Approach uses some Filtering Logic\n",
    "    \n",
    "    Step1: \n",
    "        When Word in Both Sheets is Identical, then utilize word as definition.\n",
    "        If Word Does not have a Definition in the D Notes, then replace blank with the definition. \n",
    "        If word has a definition, then add a new record.\n",
    "        \n",
    "    Step2:\n",
    "        Utilzing Function notes_word_equals_word_definition_categorization, integration where notes.word=definitions.categorization\n",
    "        It also begins to replace Word.\n",
    "        \n",
    "    Step3:\n",
    "        Utilizing Function notes_word_equals_definition_process, integration where notes.word=definitions.process\n",
    "        \n",
    "    Step4: \n",
    "        Merge all other Definitions into Notes Sheet. \n",
    "\n",
    "    Parameters:\n",
    "        notes_df (df): If Blank, then Function will generate call\n",
    "        definitions_df(df): If Blank Function will call.\n",
    "        export_Location (str): If populated, location where .csv file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        Object Type\n",
    "\n",
    "    date_created:12-Jan-26\n",
    "    date_last_modified: 12-Jan-26\n",
    "    classification:TBD\n",
    "    sub_classification:TBD\n",
    "    usage:\n",
    "        notes_df,examine_further= generate_dictionary()\n",
    "        notes_df,insert_records,insert_records2,insert_records3,examine_further= generate_dictionary()\n",
    "\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def notes_word_equals_word_definition_categorization(df,notes_df):\n",
    "        df = df.copy()\n",
    "        # Create Definition where Definition Categorization Equals Word.\n",
    "        df['WORD_IS_ZATION'] = np.where(df['Categorization_DEF'].isin(notes_df['Word'].unique().tolist()),1,0)\n",
    "        \n",
    "        merge_ = df[df['WORD_IS_ZATION']==1].copy()\n",
    "        residual_values= df[df['WORD_IS_ZATION']!=1].copy()\n",
    "        \n",
    "        merge_ = merge_[['Categorization_DEF','Word','Definition_DEF']].copy()\n",
    "        merge_['Definition'] = merge_['Word'] + \": \" + merge_['Definition_DEF']\n",
    "        merge_ = merge_.drop(['Definition_DEF','Word'],axis=1).rename(columns={'Categorization_DEF':\"Word\"})\n",
    "        merge_['Categorization'] = 'Definition'\n",
    "    \n",
    "        merge_ = merge_.merge(notes_df.drop_duplicates('Word')[['Word','Process']],on='Word',how='left')\n",
    "\n",
    "        residual_values = residual_values[['Process_DEF','Categorization_DEF','Word','Definition_DEF']].rename(columns={'Process_DEF':'Process',\n",
    "                                                                                             'Categorization_DEF':'Categorization',\n",
    "                                                                                             'Definition_DEF':'Definition'})\n",
    "        return merge_,residual_values\n",
    "\n",
    "    def notes_word_equals_definition_process(examine_further,notes_df):\n",
    "        '''\n",
    "            \n",
    "        '''\n",
    "    \n",
    "        df = examine_further.copy()\n",
    "        notes_df = notes_df.copy()\n",
    "    \n",
    "        # Make List\n",
    "        notes_word_list = notes_df['Word'].unique().tolist()\n",
    "    \n",
    "        examine_further = df[~df['Process'].isin(notes_word_list)].copy()\n",
    "        insert_df = df[df['Process'].isin(notes_word_list)].copy()\n",
    "    \n",
    "        # Clean Up Insert DF so it meets Notes DF Structure, Not Definition DF Structure.\n",
    "        # Definition Updated to Include Word\n",
    "        # Word is Updated to Include Process.\n",
    "        # Categorization Does not Change\n",
    "        # Process Takes on Whatever Value in Notes is, as this is now a input into that Process.\n",
    "        \n",
    "        # Word is Updated to include Categorization (needs to happen After Definition is Change)\n",
    "        \n",
    "        insert_df['Definition'] = insert_df['Word'] + \": \" +  insert_df['Definition'] \n",
    "        insert_df['Word'] = insert_df['Process'].copy()\n",
    "        insert_df.drop(['Process'],axis=1,inplace=True)\n",
    "        insert_df = insert_df.merge(notes_df.drop_duplicates('Word')[['Word','Process']],on='Word',how='left')\n",
    "    \n",
    "        return examine_further,insert_df\n",
    "\n",
    "        \n",
    "    from data_d_dicts import links\n",
    "\n",
    "    try:\n",
    "        notes_df = notes.copy()\n",
    "    except:\n",
    "        notes_df = pd.read_csv(links['google_notes_csv'])\n",
    "        \n",
    "    try:\n",
    "        definition_df = definition_df.copy()\n",
    "        definition_df.rename(columns={'Process':'Process_DEF','Categorization':\"Categorization_DEF\",'Definition':\"Definition_DEF\"},inplace=True)\n",
    "    except:\n",
    "        definition_df = pd.read_csv(links['google_definition_csv']).rename(columns={'Process':'Process_DEF','Categorization':\"Categorization_DEF\",'Definition':\"Definition_DEF\"})\n",
    "        definition_df.sort_values(['Process_DEF','Categorization_DEF','Word'],inplace=True)\n",
    "        \n",
    "    # Step 1: Merge Definitions into Words where they explicitly Match. No logic required.\n",
    "    # Identify Where there is a Record. \n",
    "        # Example 1: ML Project >> Process Step >> Problem Definition\n",
    "            # This is a Definition to a process which has Steps. Need to Merge a NEW RECORD.\n",
    "        # Example 2: Best Linear Unbiased Estimator\n",
    "            # This is a Example which needs Definitions Merged in, some of which arent direct Definitions. \n",
    "    \n",
    "    # We will create a modified Definition DF. To identify where direct matches exist.\n",
    "    # Need to distribute information from temp_def until Empty. Need Data Quality View Steps.\n",
    "\n",
    "    temp_def = definition_df[['Process_DEF','Categorization_DEF','Word','Definition_DEF']].merge(notes_df[['Process','Categorization','Word','Definition']].drop_duplicates('Word'),on='Word',how='left',indicator=True)\n",
    "    merge = temp_def[temp_def['_merge']=='both'].drop('_merge',axis=1)\n",
    "    examine_further = temp_def[temp_def['_merge']!='both'].drop('_merge',axis=1)\n",
    "    # Naming for trouble shooting\n",
    "\n",
    "    # Two Types of Merge\n",
    "    # Inserting New Records, specifically Where the existing Note has a Definition, this means there is an existing Process. So this\n",
    "    # Value Represents a Definition, and there is a New Record Insert\n",
    "\n",
    "    # Merging Definition. WHere the existing note has No Definition, then we will incorporate a definition. This technically could be \n",
    "    # Ignored, but by doing this, I can indirectly influence Order without direct assignment by assuming the Notes Order is Reference Point.\n",
    "\n",
    "    # Insert Record\n",
    "    insert_records = merge[merge['Definition'].notnull()].drop(['Process_DEF','Categorization_DEF','Definition','Categorization'],axis=1).rename(columns={'Definition_DEF':\"Definition\"})\n",
    "    insert_records['Categorization'] = 'Definition'\n",
    "    insert_records['Source'] = 'Insert Records'\n",
    "    \n",
    "    merge_records  = merge[merge['Definition'].isnull()]\n",
    "    \n",
    "    # WAIT TO MERGE UNTIL END IF POSSIBLE. THAT WAY I ONLY NEED TO RANK ONCE.\n",
    "    \n",
    "    notes_df =  notes_df.merge(merge_records[['Word','Definition_DEF']],on='Word',how='left')\n",
    "    notes_df['Definition'] = notes_df['Definition'].fillna(notes_df['Definition_DEF'])\n",
    "    notes_df.drop('Definition_DEF',axis=1,inplace=True)\n",
    "    notes_df['Source'] = 'Notes DF'\n",
    "    #### Step 2: \n",
    "\n",
    "    # Insert Instances where Word In Notes is Equal to Categorization in Definition. \n",
    "    # When a Process as Steps which are defined and need to be included, but Also Represent a Definition unto themselves.\n",
    "    # Example Bias, Bias Variance Trade Off, they are STEPS to in the Feature Selection Process, but also key terms which \n",
    "    # Deserve their own definition and explanation.\n",
    "\n",
    "    insert_records2, examine_further = notes_word_equals_word_definition_categorization(examine_further,notes_df)\n",
    "    insert_records2['Source'] = 'Insert Records2'\n",
    "    # Start To Bring Ordering to the Data Set.\n",
    "\n",
    "    c1_df = notes_df[['Process']].drop_duplicates().reset_index(drop=True).assign(C1_ORDER=lambda df: df.index+1)\n",
    "    c12_df = notes_df[['Process','Categorization']].drop_duplicates().reset_index(drop=True).assign(C12_ORDER=lambda df: df.index+1)\n",
    "    c3_df = notes_df[['Word']].drop_duplicates().reset_index(drop=True).assign(C3_ORDER=lambda df: df.index+1)\n",
    "\n",
    "    examine_further,insert_records3 = notes_word_equals_definition_process(examine_further,notes_df)\n",
    "    insert_records3['Source'] = 'Insert Records3'\n",
    "    examine_further['Source'] = 'Examine Further'\n",
    "    \n",
    "    # Determine Order for Sorting.\n",
    "    notes_df = notes_df.merge(c1_df,on='Process',how='left')\n",
    "    notes_df = notes_df.merge(c3_df,on=['Word'],how='left')\n",
    "\n",
    "    insert_records = pd.concat([\n",
    "        insert_records,\n",
    "        insert_records2,\n",
    "        insert_records3,\n",
    "        examine_further\n",
    "    ])\n",
    "\n",
    "    # Determine COL1 and COL3 Sort Order\n",
    "    insert_records = insert_records.merge(c1_df,on='Process',how='left')\n",
    "    insert_records = insert_records.merge(c3_df,on='Word',how='left')\n",
    "\n",
    "    # Combine ALl DF\n",
    "    notes_df = pd.concat([\n",
    "        notes_df,\n",
    "        insert_records\n",
    "    ])\n",
    "\n",
    "    d_notes_categorization_order = ['Definition',\n",
    "                                'Guiding Principle',\n",
    "                                    \n",
    "                                'Algorithm',\n",
    "                                'Consideration',\n",
    "                                'Process Step']\n",
    "    \n",
    "    residual_list = [x for x in notes_df['Categorization'].unique() if (x not in d_notes_categorization_order) & (pd.notna(x))]\n",
    "    d_notes_categorization_order.extend(residual_list)\n",
    "\n",
    "    print(d_notes_categorization_order)\n",
    "    \n",
    "    # Determine COL2 Sort Order, which is based on Mapping and Can be done once when consolidated.\n",
    "    col2_order_dict = {d_notes_categorization_order[x]:x+1 for x in range(len(d_notes_categorization_order))}\n",
    "    notes_df['C2_ORDER'] = notes_df['Categorization'].map(col2_order_dict)\n",
    "    notes_df['C2_ORDER'] = notes_df['C2_ORDER'].fillna(1000)   \n",
    "\n",
    "    notes_df =  notes_df.sort_values(['C1_ORDER','C2_ORDER','C3_ORDER'])\n",
    "\n",
    "    # Need to insure that every record \n",
    "    \n",
    "    #return notes_df,insert_records,insert_records2,insert_records3,examine_further\n",
    "  \n",
    "    notes_df = notes_df.drop(['C1_ORDER','C3_ORDER','C2_ORDER'],axis=1)\n",
    "\n",
    "    if export_location:\n",
    "        notes_df.to_csv(export_location,index=False)\n",
    "\n",
    "    return notes_df,examine_further\n",
    "\n",
    "\n",
    "notes_df, examine_further = generate_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99274473-789e-42fd-839d-8172a4d5c99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9882c19c-1c7e-47b6-9191-bd1431ccc1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categorization\n",
       "Algorithm                    211\n",
       "Definition                   125\n",
       "Process Step                  48\n",
       "Consideration                 40\n",
       "Guiding Principle             33\n",
       "Procedure                     22\n",
       "Parameter                     19\n",
       "Semantic Type                 16\n",
       "Spark                          8\n",
       "Project Conceputalization      7\n",
       "Project Implementation         7\n",
       "Optimizer                      7\n",
       "Model Type                     6\n",
       "Requirement                    5\n",
       "Theorem                        4\n",
       "Function                       4\n",
       "Automating Python              3\n",
       "Regularization                 3\n",
       "Functional Role                3\n",
       "Process                        3\n",
       "Pipeline Stage                 2\n",
       "Model Architecture             2\n",
       "Expected Outcomes              1\n",
       "Concept                        1\n",
       "Hadoop                         1\n",
       "Paradigm                       1\n",
       "Evaluation                     1\n",
       "Tooling                        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes_df['Categorization'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8f83e99-37c0-408f-92bf-a5a40d2e3692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Categorization</th>\n",
       "      <th>Word</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Guiding Principle</td>\n",
       "      <td>Behavioural Clustering Goal</td>\n",
       "      <td>Must Clarify this clearly, specifically the di...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Guiding Principle</td>\n",
       "      <td>Behavioural Clustering Goal</td>\n",
       "      <td>Small Number of Stable, Interpretable, Member ...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Guiding Principle</td>\n",
       "      <td>Expected Outcomes</td>\n",
       "      <td>Stability over Uniqueness</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Guiding Principle</td>\n",
       "      <td>Expected Outcomes</td>\n",
       "      <td>Interpretability over novelty</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Guiding Principle</td>\n",
       "      <td>Expected Outcomes</td>\n",
       "      <td>Repeatability over perfection.</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Guiding Principle</td>\n",
       "      <td>Expected Outcomes</td>\n",
       "      <td>Consistent</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Guiding Principle</td>\n",
       "      <td>Expected Outcomes</td>\n",
       "      <td>Repeatable</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Guiding Principle</td>\n",
       "      <td>What is it?</td>\n",
       "      <td>Calculation, utilizing mathematical precision ...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Guiding Principle</td>\n",
       "      <td>How to Decide?</td>\n",
       "      <td>Understand the impact of components, can preve...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Guiding Principle</td>\n",
       "      <td>Benefit</td>\n",
       "      <td>Strategic Conversations become diagnostic not ...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Guiding Principle</td>\n",
       "      <td>Benefit</td>\n",
       "      <td>Precision without Personalization</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Guiding Principle</td>\n",
       "      <td>Benefit</td>\n",
       "      <td>Differenitation without Discrimination</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Guiding Principle</td>\n",
       "      <td>Benefit</td>\n",
       "      <td>Optimization without cumbersome rules</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Implementation</td>\n",
       "      <td>Step 1</td>\n",
       "      <td>Ensure Data is Labelled and Categorized.</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Implementation</td>\n",
       "      <td>Step 2</td>\n",
       "      <td>Review Semantic Classifications, insure suffic...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Implementation</td>\n",
       "      <td>Step 3</td>\n",
       "      <td>Run Model</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Implementation</td>\n",
       "      <td>Step 4</td>\n",
       "      <td>Correlation &amp; Redundancy Analysis. If Greater ...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Implementation</td>\n",
       "      <td>Step 5</td>\n",
       "      <td>Variance &amp; Stability Contribution: Variance In...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Implementation</td>\n",
       "      <td>Step 6</td>\n",
       "      <td>Clustering Sensitivity / Stability Tests\\n1. T...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Implementation</td>\n",
       "      <td>Step 7</td>\n",
       "      <td>4. Incremental Value via Feature Importance in...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Conceputalization</td>\n",
       "      <td>Step 1</td>\n",
       "      <td>Identify and validate the behavioural attribut...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Conceputalization</td>\n",
       "      <td>Step 2</td>\n",
       "      <td>Understand the relative contribution and sensi...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Conceputalization</td>\n",
       "      <td>Step 3</td>\n",
       "      <td>Identify and define distinct, stable member ar...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Conceputalization</td>\n",
       "      <td>Step 4</td>\n",
       "      <td>Characterize each archetype using clear, inter...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Conceputalization</td>\n",
       "      <td>Step 5</td>\n",
       "      <td>Define the intended use, boundaries, and decis...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Conceputalization</td>\n",
       "      <td>Step 6</td>\n",
       "      <td>Design and apply differentiated engagement str...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Project Conceputalization</td>\n",
       "      <td>Step 7</td>\n",
       "      <td>Monitor archetype-level outcomes, migration, a...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Clustering</td>\n",
       "      <td>Expected Outcomes</td>\n",
       "      <td>Behavioural Clustering</td>\n",
       "      <td>Identify explicit groups of members which poss...</td>\n",
       "      <td>Notes DF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Process             Categorization                         Word  \\\n",
       "70   Clustering          Guiding Principle  Behavioural Clustering Goal   \n",
       "71   Clustering          Guiding Principle  Behavioural Clustering Goal   \n",
       "72   Clustering          Guiding Principle            Expected Outcomes   \n",
       "73   Clustering          Guiding Principle            Expected Outcomes   \n",
       "74   Clustering          Guiding Principle            Expected Outcomes   \n",
       "117  Clustering          Guiding Principle            Expected Outcomes   \n",
       "118  Clustering          Guiding Principle            Expected Outcomes   \n",
       "115  Clustering          Guiding Principle                  What is it?   \n",
       "116  Clustering          Guiding Principle               How to Decide?   \n",
       "119  Clustering          Guiding Principle                      Benefit   \n",
       "120  Clustering          Guiding Principle                      Benefit   \n",
       "121  Clustering          Guiding Principle                      Benefit   \n",
       "122  Clustering          Guiding Principle                      Benefit   \n",
       "123  Clustering     Project Implementation                       Step 1   \n",
       "124  Clustering     Project Implementation                       Step 2   \n",
       "125  Clustering     Project Implementation                       Step 3   \n",
       "126  Clustering     Project Implementation                       Step 4   \n",
       "127  Clustering     Project Implementation                       Step 5   \n",
       "128  Clustering     Project Implementation                       Step 6   \n",
       "129  Clustering     Project Implementation                       Step 7   \n",
       "130  Clustering  Project Conceputalization                       Step 1   \n",
       "131  Clustering  Project Conceputalization                       Step 2   \n",
       "132  Clustering  Project Conceputalization                       Step 3   \n",
       "133  Clustering  Project Conceputalization                       Step 4   \n",
       "134  Clustering  Project Conceputalization                       Step 5   \n",
       "135  Clustering  Project Conceputalization                       Step 6   \n",
       "136  Clustering  Project Conceputalization                       Step 7   \n",
       "137  Clustering          Expected Outcomes       Behavioural Clustering   \n",
       "\n",
       "                                            Definition    Source  \n",
       "70   Must Clarify this clearly, specifically the di...  Notes DF  \n",
       "71   Small Number of Stable, Interpretable, Member ...  Notes DF  \n",
       "72                           Stability over Uniqueness  Notes DF  \n",
       "73                       Interpretability over novelty  Notes DF  \n",
       "74                      Repeatability over perfection.  Notes DF  \n",
       "117                                         Consistent  Notes DF  \n",
       "118                                         Repeatable  Notes DF  \n",
       "115  Calculation, utilizing mathematical precision ...  Notes DF  \n",
       "116  Understand the impact of components, can preve...  Notes DF  \n",
       "119  Strategic Conversations become diagnostic not ...  Notes DF  \n",
       "120                  Precision without Personalization  Notes DF  \n",
       "121             Differenitation without Discrimination  Notes DF  \n",
       "122              Optimization without cumbersome rules  Notes DF  \n",
       "123           Ensure Data is Labelled and Categorized.  Notes DF  \n",
       "124  Review Semantic Classifications, insure suffic...  Notes DF  \n",
       "125                                          Run Model  Notes DF  \n",
       "126  Correlation & Redundancy Analysis. If Greater ...  Notes DF  \n",
       "127  Variance & Stability Contribution: Variance In...  Notes DF  \n",
       "128  Clustering Sensitivity / Stability Tests\\n1. T...  Notes DF  \n",
       "129  4. Incremental Value via Feature Importance in...  Notes DF  \n",
       "130  Identify and validate the behavioural attribut...  Notes DF  \n",
       "131  Understand the relative contribution and sensi...  Notes DF  \n",
       "132  Identify and define distinct, stable member ar...  Notes DF  \n",
       "133  Characterize each archetype using clear, inter...  Notes DF  \n",
       "134  Define the intended use, boundaries, and decis...  Notes DF  \n",
       "135  Design and apply differentiated engagement str...  Notes DF  \n",
       "136  Monitor archetype-level outcomes, migration, a...  Notes DF  \n",
       "137  Identify explicit groups of members which poss...  Notes DF  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes_df[notes_df['Process'].fillna(\"\").str.contains('Clu')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
