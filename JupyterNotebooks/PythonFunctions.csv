,Function Name,Description,Return,Code,File
0,IterateThroughListuntilText,"Function which Iterates through a list of python objects, not stopping until it finds an object which shares the same text as it. Created to iterate through sklearn model documentation . Parameters: lines (list of str): Input lines",,"def IterateThroughListuntilText(list_,text):
    """"""
    Function which Iterates through a list of python objects, not stopping until it finds an object which shares the same text as it.
    Created to iterate through sklearn model documentation . 

    Parameters:
        lines (list of str): Input lines

    Returns:
        str: Concatenated result up to (but not including) line with '..'
    """"""
    result = []
    for line in list_:
        if text in line:
            break
        result.append(line.strip())
    return ' '.join(result).strip()",TextFunctions.py
1,extract_doc_sections_all,"Function which iterates through text, looking for breakpoints and specific indentation. Created for the purposes of extracting Parameters and Attributes from SKlearn Library. Parameters: estimator_class: A scikit-learn class (e.g., LogisticRegression) sections: A tuple of section names to extract",,"def extract_doc_sections_all(estimator_class, sections=('Parameters', 'Attributes'),model_name=None):
    """"""
    Function which iterates through text, looking for breakpoints and specific indentation. 
    Created for the purposes of extracting Parameters and Attributes from SKlearn Library.

    Parameters:
        estimator_class: A scikit-learn class (e.g., LogisticRegression)
        sections: A tuple of section names to extract

    Returns:
        pd.DataFrame with columns: ['Section', 'Name', 'Type', 'Default', 'Description']
    """"""
    doc = inspect.getdoc(estimator_class)
    lines = doc.split('\n')

    results = []
    section = None
    current_name = None
    current_type = ''
    current_default = ''
    current_description = []

    for line in lines:
        stripped = line.strip()

        # Section start
        if stripped in sections:
            section = stripped
            continue
        elif stripped in ['Parameters', 'Attributes', 'Returns', 'Examples', 'See Also', 'Notes', 'References']:
            if section in sections:
                section = None  # exit current section
            continue

        if section is None:
            continue

        # Param/attribute line
        if re.match(r'^\S[^:]*\s*:\s*.+$', line):
            if current_name:
                results.append({
                    'Section': section,
                    'Name': current_name,
                    'Type': current_type,
                    'Default': current_default,
                    'Description': ' '.join(current_description).strip()
                })

            # Start new field
            current_description = []
            colon_index = line.find(':')
            current_name = line[:colon_index].strip()
            rest = line[colon_index+1:].strip()

            current_type = ''
            current_default = ''
            if ',' in rest:
                parts = [p.strip() for p in rest.split(',')]
                current_type = parts[0]
                for p in parts[1:]:
                    if p.startswith('default='):
                        current_default = p.split('=')[1].strip()
            else:
                current_type = rest

        elif line.startswith('    ') and not line.strip().startswith('..'):
            current_description.append(line.strip())

    # Final item
    if current_name:
        results.append({
            'Section': section,
            'Name': current_name,
            'Type': current_type,
            'Default': current_default,
            'Description': ' '.join(current_description).strip()
        })

    df = pd.DataFrame(results)
    if not model_name:
        model_name = lines[0]
    df['Model'] =model_name
    
    df['Estimator'] = estimator_class

    sklearn_desc = pd.DataFrame([[model_name,IterateThroughListuntilText(lines,'..')]],columns=['Model Name','Sklearn Desc'])
    
    return df,sklearn_desc",TextFunctions.py
2,CountWordsinDFColumn,"Function which reads a Dataframe Column and returns a value count of either, Phrases, Words or Both. Used for understanding Text frequency within a Column, utilizes Counter, from Collections Library. Parameters: df (dataframe) column_name (str): Name of Column to Iterate through output_type (str): phrase, word, both. phrase: Returns a count of the entire phrase word: Returns a count of each individual word contained within the text. both: Returns",,"def CountWordsinDFColumn(df,
                         column_name,
                         output_type='both'):
    '''
    Function which reads a Dataframe Column and returns a value count of either, Phrases, Words or Both.
    Used for understanding Text frequency within a Column, utilizes Counter, from Collections Library.
    
    Parameters:
        df (dataframe)
        column_name (str): Name of Column to Iterate through
        output_type (str): phrase, word, both.
            phrase: Returns a count of the entire phrase
            word: Returns a count of each individual word contained within the text.
            both: Returns
    
    Returns:
        df
    

    
    '''
    
    col = df[(df[column_name].notnull())][column_name].astype(str).values.flatten()
    total_col = Counter(col)
    
    if (output_type.lower()=='phrase')|(output_type.lower()=='both'):
        col_count_df = pd.DataFrame(total_col.items(),columns=['Text','Count']).sort_values('Count',ascending=False)
        col_count_df['Type'] = 'Phrase'
    else:
        col_count_df = pd.DataFrame()
        
    if (output_type.lower()=='word')|(output_type.lower()=='both'):
        words = []
        for phrase in total_col:
            words.extend(phrase.split())
        
        total_word = Counter(words)
        word_count_df = pd.DataFrame(total_word.items(),columns=['Text','Count']).sort_values('Count',ascending=False)
        word_count_df['Type'] = 'Word'
    else:
        word_count_df = pd.DataFrame()
    
    if output_type.lower() =='phrase':
        return col_count_df
    
    elif output_type.lower() == 'word':
        return word_count_df
    
    else:
        return pd.concat([col_count_df,word_count_df])",TextFunctions.py
3,RemovePatternFromDFColumn,"Removes text matching a regex pattern from a DataFrame column. Optionally logs removed text in a new or existing column. Args: df (pd.DataFrame): Input DataFrame. column (str): Column to clean. pattern (str): Regex pattern to remove. new_column_name (str): Optional column name to store removed values. ignore_case (bool): If True, pattern match is case-insensitive. clean_whitespace (bool): If True, strips and collapses whitespace.",,"def RemovePatternFromDFColumn(df,
                               column,
                               pattern,
                               new_column_name=None,
                               ignore_case=True,
                               clean_whitespace=True):
    """"""
    Removes text matching a regex pattern from a DataFrame column.
    Optionally logs removed text in a new or existing column.
    
    Args:
        df (pd.DataFrame): Input DataFrame.
        column (str): Column to clean.
        pattern (str): Regex pattern to remove.
        new_column_name (str): Optional column name to store removed values.
        ignore_case (bool): If True, pattern match is case-insensitive.
        clean_whitespace (bool): If True, strips and collapses whitespace.
    
    Returns:
        pd.DataFrame: Modified DataFrame with cleaned and optionally logged matches.
    """"""
    df = df.copy()
    flags = re.IGNORECASE if ignore_case else 0

    # Safely fill NaNs
    original_text = df[column].fillna('')

    # Find all matches (full pattern matches, not just inner substrings)
    found_matches = original_text.str.findall(pattern, flags=flags)
    match_strings = found_matches.apply(lambda x: ', '.join(x))

    # Add or update the removal log
    if new_column_name:
        if new_column_name in df.columns:
            df[new_column_name] = df[new_column_name].fillna('')
            df[new_column_name] = df.apply(
                lambda row: ', '.join(filter(None, [row[new_column_name], match_strings[row.name]])).strip(', '),
                axis=1
            )
        else:
            df[new_column_name] = match_strings

    # Remove the matched patterns from the text
    cleaned_text = original_text.str.replace(pattern, '', flags=flags, regex=True)
    
    if clean_whitespace:
        cleaned_text = (
            cleaned_text.str.replace(r'\s+', ' ', regex=True).str.strip()
        )
    
    df[column] = cleaned_text

    return df",TextFunctions.py
4,RemoveWordfromDFColumn,,None,"def RemoveWordfromDFColumn(df,
                           column,
                           word,
                           new_column_name=None):
    
    '''
    
    '''
    
    df = df.copy()
    
    if new_column_name:
        try:
            df[new_column_name] = np.where(df[column].fillna("""").str.contains(word,case=False),word,df[new_column_name])
        except:
            df[new_column_name] = np.where(df[column].fillna("""").str.contains(word,case=False),word,"""")
        
    df[column] = df[column].fillna("""").str.replace(word,"""",regex=True,case=False).str.strip()
        
    return df",TextFunctions.py
5,GenericStrClean,,None,"def GenericStrClean(df,
                    column_name,
                    trim=True,
                    lower_case=True,
                    remove_punc=False,
                    remove_letters=False,
                    remove_numbers=False,
                    new_column_name=None):
    '''


    '''
    
    df= df.copy()
    
    if not new_column_name:
        new_column_name = f""{column_name}_CLEAN""
    
    result = df[column_name].astype(str).copy()
    
    
    if lower_case:
        result = result.str.lower()
    
    if remove_letters:
        result = result.str.replace(r'[a-zA-Z]',"""",regex=True)
    
    if remove_punc:
        result = result.str.replace(r'[^\w\s]','',regex=True)
        
    if remove_numbers:
        result = result.str.replace(r'\d','',regex=True)
        
    if trim:
        result = result.str.strip()
                
    df[new_column_name] = result
    
    return df",TextFunctions.py
6,TextClean,"Applies selected text cleaning operations to specified DataFrame columns. Parameters: df (pd.DataFrame): DataFrame to clean. column_list (list): Columns to clean. Cleaning Options (all default to False): remove_newlines (bool): Remove  and characters. strip_whitespace (bool): Trim leading and trailing whitespace. normalize_whitespace (bool): Collapse multiple spaces/tabs into one space. remove_punctuation (bool): Remove punctuation characters. only_digits (bool): Remove all non-digit characters and convert to numeric. only_letters (bool): Remove all non-letter characters and convert to string.",,"def TextClean(
    df,
    column_list,
    lower_case=False,
    remove_newlines=False,
    strip_whitespace=False,
    normalize_whitespace=False,
    remove_punctuation=False,
    only_digits=False,
    only_letters=False):
    
    
    """"""
    Applies selected text cleaning operations to specified DataFrame columns.

    Parameters:
        df (pd.DataFrame): DataFrame to clean.
        column_list (list): Columns to clean.
        
        Cleaning Options (all default to False):
            remove_newlines (bool): Remove \r and \n characters.
            strip_whitespace (bool): Trim leading and trailing whitespace.
            normalize_whitespace (bool): Collapse multiple spaces/tabs into one space.
            remove_punctuation (bool): Remove punctuation characters.
            only_digits (bool): Remove all non-digit characters and convert to numeric.
            only_letters (bool): Remove all non-letter characters and convert to string.

    Returns:
        pd.DataFrame: Cleaned DataFrame.
    """"""
    for col in column_list:
        if col not in df.columns:
            continue

        series = df[col].astype(str)
        
        if lower_case:
            series = series.str.lower()
        if remove_newlines:
            series = series.str.replace(r'[\r\n]+', '', regex=True)
        if normalize_whitespace:
            series = series.str.replace(r'\s+', ' ', regex=True)
        if strip_whitespace:
            series = series.str.strip()
        if only_digits:
            series = series.str.replace(r'[^\d.]', '', regex=True)
            series = pd.to_numeric(series, errors='coerce')
        if only_letters:
            series = series.str.replace(r'[^a-zA-Z]', '', regex=True)
        if remove_punctuation:
            series = series.str.translate(str.maketrans('', '', string.punctuation))

        df[col] = series

    return df",TextFunctions.py
7,generate_polynomial_features,Generates polynomial features for all numerical columns except the target. Args: df (pd.DataFrame): The input DataFrame. target_col (str): The name of the target column to exclude from transformation. degree (int): The polynomial degree (default = 2). include_bias (bool): Whether to include a bias column (default = False).,,"def generate_polynomial_features(df, target_col='Target', degree=2, include_bias=False):
    """"""
    Generates polynomial features for all numerical columns except the target.

    Args:
    df (pd.DataFrame): The input DataFrame.
    target_col (str): The name of the target column to exclude from transformation.
    degree (int): The polynomial degree (default = 2).
    include_bias (bool): Whether to include a bias column (default = False).

    Returns:
    pd.DataFrame: A new DataFrame with named polynomial features.
    """"""
    
    # Select numeric columns excluding the target variable
    numeric_features = df.drop(columns=[target_col]).select_dtypes(include=['number'])
    
    # Initialize PolynomialFeatures transformer
    poly = PolynomialFeatures(degree=degree, include_bias=include_bias)
    
    # Fit and transform the numerical features
    X_poly = poly.fit_transform(numeric_features)
    
    # Get feature names
    feature_names = poly.get_feature_names_out(numeric_features.columns)
    
    # Create new DataFrame with transformed features
    df_poly = pd.DataFrame(X_poly, columns=feature_names, index=df.index)
    
    # Add back the target column
    df_poly[target_col] = df[target_col]
    
    return df_poly",FeatureEngineering.py
8,BinaryColumnCreator,,None,"def BinaryColumnCreator(df,
                        column_name,
                        new_column_name,
                        value,
                        calculation):
  
  '''
  
  
  
  '''
  
  if calculation=='>':
    df[new_column_name] = np.where(df[column_name]>value,1,0)
  elif calculation =='=':
    df[new_column_name] = np.where(df[column_name]==value,1,0)
  elif calculation =='<':
    df[new_column_name] = np.where(df[column_name]<value,1,0)
  elif calculation =='isin':
    df[new_column_name] = np.where(df[column_name].isin(value),1,0)
  elif calculation =='contains':
    df[new_column_name] = np.where(df[column_name].str.contains(value),1,0)
  elif calculation == 'dict':
    for key,value in value.items():
        try:
          df[new_column_name] = np.where((df[key]==value)&(df[new_column_name]==1),1,0)
        except:
          df[new_column_name] = np.where(df[key]==value,1,0)",FeatureEngineering.py
9,TextClean,,None,"def TextClean(df, 
              column_list, 
              clean_type='only_digits'):
    '''
    
    
    '''

    if clean_type == 'only_digits':
        for column in column_list:
            df[column] = df[column].astype(str).str.replace(r'[^\d.]', """", regex=True)
            df[column] = pd.to_numeric(df[column], errors='coerce')

    return df",FeatureEngineering.py
10,FillFromAbove,Force Fill Information from above defined values when Blank. Args:,,"def FillFromAbove(df,
                 column_name,
                 new_column_name=''):
    
    '''
    Force Fill Information from above defined values when Blank. 

    Args:

    
    Returns:

     
    df = pd.DataFrame({'Test': [None, 'Value1', None, 'Value2', None, None, 'Value3', None]})
    FillFromAbove(df,'Test','Test1')
    
    
    '''
    if new_column_name=="""":
        new_column_name = column_name
    
    df[new_column_name] = df[column_name].ffill()",FeatureEngineering.py
11,BracketColumn,"Function Created to analyze the contents of a Column in a Dataframe and return a summarized view of the distribution. Function differs from ColumnPartioner, in that it does not look for uniformity, a higher level summary. Parameters: df {dataframe} column_name (str): Column Name of DF column to review new_column_name (str): Name of Column to be created in Dataframe as result of Function bins [list]: Number of Bins to include in Segmentation. Need atleast 2, no upper bound limit. formating (str): Value which can be appended for str output in return value text. $ Only Viable in current iteration assign_cat (int): Binary Flag which allows user to save the column in Categorical Order to ease filtering. this does impact how groupby works and size of data, so proceed with caution. last_bin_lowst_value (int): Binary Flag, allows user to either Place a Floor on bottom limit based on Bin Value, or search everything below bottom limit. EI, if you have 0 for deposit it would not include balances below 0 in the count if 1.",,"def BracketColumn(df,
                  column_name,
                  new_column_name,
                  bins=[0,10,20,30,40,50,60,70,80,90,100],
                  formating="""",
                  assign_cat=0,
                  last_bin_lowest_value=1,):
    
    '''
    Function Created to analyze the contents of a Column in a Dataframe and return a summarized view of the distribution.
    Function differs from ColumnPartioner, in that it does not look for uniformity, a higher level summary.
    
    Parameters:
        df {dataframe}
        
        column_name (str): Column Name of DF column to review
        
        new_column_name (str): Name of Column to be created in Dataframe as result of Function
        
        bins [list]: Number of Bins to include in Segmentation. Need atleast 2, no upper bound limit.
        
        formating (str): Value which can be appended for str output in return value text. $ Only Viable in current iteration 
        
        assign_cat (int): Binary Flag which allows user to save the column in Categorical Order to ease filtering.
        this does impact how groupby works and size of data, so proceed with caution.
    
        last_bin_lowst_value (int): Binary Flag, allows user to either Place a Floor on bottom limit based on Bin Value, or 
        search everything below bottom limit. EI, if you have 0 for deposit it would not include balances below 0 in the count if 1.
    
    Returns:
        Dataframe with New Column
    
    '''
    
    df[column_name] = df[column_name].fillna(0)
    
    condition_list = []
    value_list = []
    
    for count,value in enumerate(bins):
        if count == 0:
            if last_bin_lowest_value==1:
                condition_list.append(df[column_name]==value)
                value_list.append(f""{count+1})Equal to {formating}{value:,}"")
            else:
                condition_list.append(df[column_name]<value)
                value_list.append(f""{count+1})Less than {formating}{value:,}"")
        
        elif count < len(bins)-1:
            condition_list.append(df[column_name]<=value)
            value_list.append(f""{count+1})Between {formating}{bins[count-1]:,} and {formating}{bins[count]:,}"")
        
        else:
            condition_list.append(df[column_name]<=value)
            value_list.append(f""{count+1})Between {formating}{bins[count-1]:,} and {formating}{bins[count]:,}"")
            
            condition_list.append(df[column_name]>value)
            value_list.append(f""{count+1})Greater than {formating}{value:,}"")
            
        df[new_column_name]=np.select(condition_list,value_list,'Problem')
        
        if assign_cat ==1:
            df[new_column_name] = pd.Categorical(df[new_column_name], categories=value_list)
    
    print(df[new_column_name].value_counts())",FeatureEngineering.py
12,VarianceInflationFactor,Computes Variance Inflation Factor (VIF) for each numerical feature. Parameters: df (pd.DataFrame): DataFrame containing numerical predictors.,,"def VarianceInflationFactor(df):
    """"""
    Computes Variance Inflation Factor (VIF) for each numerical feature.
    
    Parameters:
        df (pd.DataFrame): DataFrame containing numerical predictors.
    
    Returns:
        pd.DataFrame: VIF values for each feature.
    """"""
    
    vif_data = pd.DataFrame()
    vif_data[""Feature""] = df.columns
    vif_data[""VIF""] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]

    condition = [vif_data['VIF']<=1,
                 vif_data['VIF']<=5,
                 vif_data['VIF']<=10,
                 vif_data['VIF']>10]
    
    values = ['No Multicollinearity',
              'Low to Moderate Multicollinearity',
              'High Multicollinearity',
              'Severe Multicollinearity']
        
    vif_data['Assessement'] = np.select(condition,values,default=""NA"")
    vif_data['Action'] = np.where(vif_data['VIF']>5,'Review','Pass')
    
    
    return vif_data",FeatureEngineering.py
13,BalanceTargetDistribution,"Function to support reduction of Dataset based on desire to Target weight the percentage of observations. Parameters: df (DataFrame) Target (str): Column Name of Target desired_percentage (float): Number between 0 - 1, which is desired weighting of Target between 1 and 0. Target Value (int): Value used to balance, perferably Int, can be str.",,"def BalanceTargetDistribution(df,
                              Target,
                              desired_percentage,
                              TargetValue=1):
    
    '''
    Function to support reduction of Dataset based on desire to Target weight the percentage of observations.
    
    Parameters:
        df (DataFrame)
        Target (str): Column Name of Target
        desired_percentage (float): Number between 0 - 1, which is desired weighting of Target between 1 and 0.
        Target Value (int): Value used to balance, perferably Int, can be str.
        
        
    Returns:
        Dataset
    
    '''
    df = df.copy()

    df0 = df[df[Target]==TargetValue].copy()
    df1 = df[df[Target]!=TargetValue].copy()
    
    if (desired_percentage>0)&(desired_percentage<1):
        req_columns = int(round(len(df0)/desired_percentage,0))
        
        return pd.concat([df0,df1.sample(req_columns)]).sample(frac=1).reset_index(drop=True)
        
    else:
        print('Desired Percentage Outside of Allowable Range (0-1), Please Select a New Value')",FeatureEngineering.py
14,CompareTextColumns,"Function to Create a Binary Flag when to Dataframe Columns are Equal. Parameters df (dataframe) col1 (str): Name of Column 1 col2 (str): Name of Column 2 (to be compared with column1) new_column_name (str): Name of New Column, if left blank then it will be Default to BINARY_MATCH_",None,"def CompareTextColumns(df,
                       col1,
                       col2,
                       new_column_name=''):
    '''
    Function to Create a Binary Flag when to Dataframe Columns are Equal.


    Parameters
        df (dataframe)
        col1 (str): Name of Column 1
        col2 (str): Name of Column 2 (to be compared with column1)
        new_column_name (str): Name of New Column, if left blank then it will be Default to BINARY_MATCH_
    
    
    '''
    
    if new_column_name=='':
        new_column_name = f'BINARY_MATCH_{col1}_{col2}'
    
    df[new_column_name] = np.where(df[col1].str.strip().str.lower()==df[col2].str.strip().str.lower(),1,0)",FeatureEngineering.py
15,CategorizeBinaryChange,"Function to Simply Apply a Condition to generate whether a particular column, which is meant to be a Change over a time series dataset, has Increased, Decreased or Stayed the Same. Can be stand alone, created for use in ColumnElementalChange Args: change_column (str): Name of Column Created.",,"def CategorizeBinaryChange(df,
                           change_column='VARIANCE'):
    '''
    Function to Simply Apply a Condition to generate whether a particular column, which is meant to be a Change over a 
    time series dataset, has Increased, Decreased or Stayed the Same. Can be stand alone, created for use in ColumnElementalChange
    
    Args:
        change_column (str): Name of Column Created. 
        
    Returns:
        dataframe, with change_column added
        
    '''
    
    condition = [
        df[change_column]>0,
        df[change_column]<0,
        df[change_column]==0
    ]
        
    value = ['Records Increasing',
             'Records Decreasing',
             'Records Not Changing']
        
    df['CHANGE_CLASSIFICATION'] = np.select(condition,value,'Null Value Present')
    
    try:
        # Supplement Change to increase visability of granularity so we can understand true variance, in addition to Add/Losses
        df['CHANGE_CLASSIFICATION'] = np.where(df['_merge']=='right_only','Records Lost',df['CHANGE_CLASSIFICATION'])
        df['CHANGE_CLASSIFICATION'] = np.where(df['_merge']=='left_only','Records Added',df['CHANGE_CLASSIFICATION'])
        
    except:
        pass",FeatureEngineering.py
16,ConvertToBinary,"Function to convert a column into a Binary Variable. Specifically useful when aggregating counts, and in place you'd like to return a simple binary indicator of the possession of a particular attribute. Parameters df (DataFrame) column_name (str): The column to be converted. return_value  (int): Binary Flag, 1 for Int, 'Y' for text. Returns Modifies the DataFrame in place.",None,"def ConvertToBinary(df, column_name, return_value=1):
    '''
    Function to convert a column into a Binary Variable. Specifically useful when aggregating counts, and in place you'd like to return
    a simple binary indicator of the possession of a particular attribute.

    Parameters
    df (DataFrame)
    column_name (str): The column to be converted.
    return_value  (int): Binary Flag, 1 for Int, 'Y' for text.

    Returns
        Modifies the DataFrame in place.
    '''
    if str(return_value) == '1':
        df[column_name] = np.where(df[column_name] > 0, 1, 0)
    else:
        df[column_name] = np.where(df[column_name] > 0, 'Y', 'N')",FeatureEngineering.py
17,FirstObservanceFlag,Function to creates a binary flag column indicating the first observation of a Value in a Column. Reteurns a 0 for all subsequent values Parameters df (DataFrame) column_name (str): The column name where values will be counted Returns Modifies the DataFrame in place.,None,"def FirstObservanceFlag(df, column_name):
    '''
    Function to creates a binary flag column indicating the first observation of a Value in a Column. Reteurns a 0 for all subsequent values

    Parameters
    df (DataFrame)
    column_name (str): The column name where values will be counted

    Returns
        Modifies the DataFrame in place.
    '''
    df['FirstObs'] = (~df[column_name].duplicated()).astype(int)",FeatureEngineering.py
18,CreateRandomDFColumn,Function to Create a New Column in a Dataframe by randomly Selecting values from a list Parameters: df (Dataframe) value_list (list): List of values to be randomly choosen from new_column_name: Name of New Column,,"def CreateRandomDFColumn(df, value_list, new_column_name):
    '''
    Function to Create a New Column in a Dataframe by randomly Selecting values from a list

    Parameters:
    df (Dataframe)
    value_list (list): List of values to be randomly choosen from
    new_column_name: Name of New Column
    
    Returns:
        new dataframe column
    
    '''
    
    df[new_column_name] = np.random.choice(value_list, size=len(df))",FeatureEngineering.py
19,BinaryComplexEquivlancey,"Function to Compare Whether two Columns are Equal. Equivalancy in this broadened definition includes Null to Null, Null to 0, """" to Null. and 0 to 0. Parameters: df (dataframe) col(str): Column Name from Dataframe col1(str): Column Name from Dataframe (should new_column_name (str): Name of New Binary Column",,"def BinaryComplexEquivlancey(df, col, col1, new_column_name):
    '''
    Function to Compare Whether two Columns are Equal. 
    Equivalancy in this broadened definition includes Null to Null, Null to 0, """" to Null. and 0 to 0.

    Parameters:
        df (dataframe)
        col(str): Column Name from Dataframe
        col1(str): Column Name from Dataframe (should 
        new_column_name (str): Name of New Binary Column

    Returns:
        New Dataframe Column, NEW_COLUMN_NAME.
     
    '''
    try:
        # Try numeric comparison
        df[new_column_name] = np.where(
            (df[col].isna() & df[col1].isna()) |
            ((df[col].fillna(0) == 0) & df[col1].isna()) |
            ((df[col1].fillna(0) == 0) & df[col].isna()) |
            (df[col].fillna(0) == df[col1].fillna(0)),
            1, 0
        )
    except Exception:
        # Fallback to string comparison
        df[new_column_name] = np.where(
            (df[col].isna() & df[col1].isna()) |
            ((df[col].fillna('') == '') & df[col1].isna()) |
            ((df[col1].fillna('') == '') & df[col].isna()) |
            (df[col].fillna('').astype(str).str.strip().str.lower() ==
             df[col1].fillna('').astype(str).str.strip().str.lower()),
            1, 0
        )",FeatureEngineering.py
20,ConvertDate,"Function to Convert Str to Datetime for Dataframe Column Parameters: column_name (str): Name of Column to Convert new_column_name (str): If populated, it will create a new column Name, otherwise it will replace column_name normalize (int): Binary Flag, if 0 then no normalization, if 1 then .dt.normalize applied.",,"def ConvertDate(df,
                column_name,
                new_column_name="""",
                normalize=0):
    '''
    Function to Convert Str to Datetime for Dataframe Column
    
    Parameters:
        column_name (str): Name of Column to Convert
        new_column_name (str): If populated, it will create a new column Name, otherwise it will replace column_name
        normalize (int): Binary Flag, if 0 then no normalization, if 1 then .dt.normalize applied.
        
    Returns:
        Nil 
    
    
    
    '''
    
    if new_column_name =="""":
        new_column_name = column_name
    
    if normalize==1:
        df[new_column_name] = pd.to_datetime(df[column_name],errors='coerce').dt.normalize()
    else:
        df[new_column_name] = pd.to_datetime(df[column_name],errors='coerce')",DateFunctions.py
21,ConvertDateColumns,Function which applies the ConverDate function to all Columns in DF with word DATE. It overwrites Existing Data. Parameters: Nil,,"def ConvertDateColumns(df,normalize=0):
    
    '''
    Function which applies the ConverDate function to all Columns in DF with word DATE. It overwrites Existing Data.
    
    Parameters:
        Nil
        
    Returns:
        Nil 
    
    '''
    
    
    for column in df.columns:
        if column.upper().find('DATE')!=-1:
            ConvertDate(df,column,normalize=normalize)",DateFunctions.py
22,CalculateDaysFromtoday,"Function to Simply Calculate the Number of Days in Years between 2 points in time. First Date is populated via Datafarme, Second date is Calculated by Refence of Month_int. Parameters: df (dataframe) column_name (str): Name of Column in Dataframe. Should be datetime format before utilization. new_column_name (str): Name of Column which will be created (Defaults to DaysSinceDate month_int: Reference point of time, Please refer to MonthSelector for additional insight as to how this is calculated.",,"def CalculateDaysFromtoday(df,
                           column_name,
                           new_column_name='DaysSinceDate',
                           month_int=0):
    
    '''
    Function to Simply Calculate the Number of Days in Years between 2 points in time. 
    First Date is populated via Datafarme, Second date is Calculated by Refence of Month_int.
    
    Parameters:
        df (dataframe)
        column_name (str): Name of Column in Dataframe. Should be datetime format before utilization.
        new_column_name (str): Name of Column which will be created (Defaults to DaysSinceDate
        month_int: Reference point of time, Please refer to MonthSelector for additional insight as to how this is 
        calculated.
        
    Returns:
        Dataframe, with new column. 
    
    
    '''
    
    reference_date = MonthSelector(month_int)
    
    df[new_column_name] = (reference_date-df[column_name])/datetime.timedelta(365.25)",DateFunctions.py
23,MonthEndDate,Function to take a Random Date in a specific format and convert it into dt. Primarily used with Str Jan-25. Included import function in code as this is a rarely used file and preventing the need to import in library of commonly used functions. Parameters: Return:,None,"def MonthEndDate(x,og_format):
    
        
    '''
    Function to take a Random Date in a specific format and convert it into dt. Primarily used with Str Jan-25.
    Included import function in code as this is a rarely used file and preventing the need to import in library of commonly used functions.
    
    
    Parameters:
        
    
    Return:
        
    '''

    x = datetime.datetime.strptime(x,og_format)
    day = calendar.monthrange(x.year,x.month)[1]
    return datetime.datetime(x.year,x.month,day)",DateFunctions.py
24,ManualConvertDate,Function to Take a Existing Dataframe Data Column and Convert it to a New Column Parameters: df: column_name: og_format: new_format:,,"def ManualConvertDate(df,
                    column_name,
                    new_column_name="""",
                    og_format='month_str',
                    new_format='month_dt'):
    
    '''
    Function to Take a Existing Dataframe Data Column and Convert it to a New Column
    
    Parameters:
        df:
        column_name:
        og_format:
        new_format:
            
    Returns:
        Dataframe with New Column (or if not included, then Updated Format of column_name)
        
    '''
    
    if new_column_name=='':
        new_column_name = column_name
        
    og_list = df[column_name].unique().tolist()
    
    if og_format =='month_str':
        if new_format=='month_dt':
            new_list = [datetime.datetime.strptime(x,""%b-%y"") for x in og_list]
        elif new_format =='month':
            new_list = [MonthEndDate(x,'%b-%y').strftime('%d-%b-%y') for x in og_list]
        else:
            new_list = og_list
    
    elif og_format =='month':
        if new_format=='month_dt':
            new_list = [MonthEndDate(x,""%d-%b-%y"") for x in og_list]
        elif new_format =='month_str':
            new_list = [datetime.datetime.strptime(x,""%d-%b-%y"").strftime('%b-%y') for x in og_list]
        else:
            new_list = og_list
            
    elif og_format =='month_dt':
        og_list = pd.to_datetime(df[column_name].unique())
        if new_format=='month':
            new_list = [datetime.datetime.strftime(x,""%d-%b-%y"") for x in og_list]
        elif new_format =='month_str':
            new_list = [datetime.datetime.strftime(x,""%b-%y"") for x in og_list]
        else:
            new_list = new_list = df[column_name].unique().tolist()
        
    temp_df = pd.DataFrame(new_list,columns=[new_format]).merge(pd.DataFrame(og_list,columns=[og_format]),left_index=True,right_index=True,how='left').rename(columns={new:new_column_name})
    
    return df.merge(temp_df,on=column_name,how='left')",DateFunctions.py
25,generate_day_list,No description available,None,"def generate_day_list(start_date=datetime.datetime(2025,1,1),end_date= None):
    if end_date is None:
        end_date = datetime.datetime.now() - datetime.timedelta(days=1)
    
    # Generate the list of dates
    date_list = [start_date + datetime.timedelta(days=i) for i in range((end_date - start_date).days + 1)]
    
    return date_list",DateFunctions.py
26,print_current_time,No description available,None,"def print_current_time():
    print(f""Current Time: {datetime.datetime.now().strftime('%H:%M:%S')}"")",DateFunctions.py
27,FIRST_DAY_OF_MONTH,No description available,None,"def FIRST_DAY_OF_MONTH(x,):
    try:
        x = pd.to_datetime(x,errors='coerce').replace(day=1)
    except:
        pass
    return x.date()",DateFunctions.py
28,LAST_DAY_PREVIOUS_MONTH,,None,"def LAST_DAY_PREVIOUS_MONTH(date_dt=None,
                            return_value='month_dt'):
    '''
    
    
    '''
    if date_dt==None:
        date_dt = datetime.datetime.now()
    
    new_date = date_dt.replace(day=1,hour=0,minute=0,second=0,microsecond=0)-datetime.timedelta(days=1)
    
    if return_value=='month_dt':
        return new_date
    
    elif return_value == 'month_str':
        return new_date.strftime('%d-%b-%y')",DateFunctions.py
29,CreateMonthList,No description available,None,"def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value=""month_dt""):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    month_list = list(month_list)
    
    if month_int ==0:
        month_list.append(pd.Timestamp.today())
    
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime().replace(hour=0,minute=0,second=0,microsecond=0) for x in month_list]
    elif return_value == 'date':
        month_list = [x.to_pydatetime().date() for x in month_list]

    if sort_ascending == 0:
        month_list.reverse()

    return month_list[month_int:final_months]",DateFunctions.py
30,MonthSelector,"Function to return the Date based on Binary Int. 1 Represents month end last month, 0 Today (can change to yesterday with default binary flag) Parameters: month_int (int): Representative of the desired month 0 (today), 1 (last month end), 2 (2 Months, If in FebX it would be 31DecX) end_date_yesterday (int): OPtional function to overwrite today to be yesterday when 0 for SQL db queries. Return:",None,"def MonthSelector(month_int=1,
                  return_value='month_dt',
                  end_date_yesterday=1):
    
        
    '''
    Function to return the Date based on Binary Int. 1 Represents month end last month, 0 Today (can change to yesterday with default binary flag)
    
    
    Parameters:
        month_int (int): Representative of the desired month 0 (today), 1 (last month end), 2 (2 Months, If in FebX it would be 31DecX)
        end_date_yesterday (int): OPtional function to overwrite today to be yesterday when 0 for SQL db queries.
    
    Return:
        
    '''
    
    if (month_int==0) & (end_date_yesterday==1):
        value_ = (datetime.datetime.now()-datetime.timedelta(days=1)).replace(hour=0,minute=0,second=0,microsecond=0)
        if return_value=='month_dt':
            return value_
        elif return_value =='month_str':
            return value_.strftime('%b-%y')
        elif return_value =='month':
            return value_.strftime('%d-%b-%y')
    else:
        return CreateMonthList(month_int=month_int,months=1,return_value=return_value)[0]",DateFunctions.py
31,extract_function_details_ast,Extracts structured function details from a Python script using AST. Parameters: file_content (str): The raw string content of a Python script.,,"def extract_function_details_ast(file_content,file_name):
    '''
    Extracts structured function details from a Python script using AST.
    
    Parameters:
    file_content (str): The raw string content of a Python script.

    Returns:
    dict: A dictionary with function names as keys and metadata (description, args, return, code).

    '''
    
    tree = ast.parse(file_content)  # Parse the script into an AST
    function_data = {}

    for node in tree.body:
        if isinstance(node, ast.FunctionDef):  # Identify function definitions
            function_name = node.name
            docstring = ast.get_docstring(node) or ""No description available""

            # Extract arguments from function signature
            args = [arg.arg for arg in node.args.args]

            # Extract return type annotation if present
            return_type = ast.unparse(node.returns) if node.returns else ""None""

            # Extract function code using AST
            function_code = ast.get_source_segment(file_content, node).strip()

            # Process the docstring to separate description, args, and return
            description_text = []
            args_text = []
            return_text = ""None""

            if docstring:
                doc_lines = docstring.split(""\n"")
                found_args = False

                for line in doc_lines:
                    stripped = line.strip()

                    if stripped.lower().startswith((""args:"", ""parameters:"")):  # Start of args
                        found_args = True
                        continue
                    elif stripped.lower().startswith(""returns:""):  # Start of return
                        return_text = stripped.replace(""Returns:"", """").strip()
                        found_args = False
                        continue

                    if not found_args:
                        description_text.append(stripped)
                    else:
                        args_text.append(stripped)
                        
            function_data[function_name] = {
                ""Description"": ""\n"".join(description_text).strip(),
                ""Arguments"": args_text if args_text else args,
                ""Return"": return_text,
                ""Code"":function_code,
                'File':file_name
            }
            
    return function_data",Organization.py
32,ReadPythonFiles,"Function which reads a Folder, iterating through all files saved, looking for .py files utilizing athe Extract Function Details AST Args: location (str): Folder",,"def ReadPythonFiles(location='/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/'):
    
    '''
    Function which reads a Folder, iterating through all files saved, looking for .py files utilizing athe Extract Function Details AST 
 
 
     Args:
         location (str): Folder
         
    Returns:
        Dataframe of Functions with description.

    '''    
    py_file_dict = {}
    
    for file_name in [x for x in ReadDirectory(location) if x.find('.py')!=-1 and x.find('__')==-1]:
        with open(f""{location}{file_name}"", ""r"", encoding=""utf-8"") as file:
            data = file.read()
            py_file_dict.update(extract_function_details_ast(data,file_name))
            
    return pd.DataFrame(py_file_dict).T",Organization.py
33,CreateMarkdown,"Create a Markdown string from a DataFrame with columns: Title, Header, Description",None,"def CreateMarkdown(df, return_value=""""):
    '''
    Create a Markdown string from a DataFrame with columns: Title, Header, Description
    '''
    try:
        df1 = df[['Title', 'Header', 'Description']]
    except:
        print('DataFrame must contain: Title, Header, Description')
        return ''

    title = """"
    step_number = 1
    text = """"

    for _, row in df1.iterrows():
        # New section
        if title != row['Title']:
            if title != """":
                text += ""\n""  # separate sections
            title = row['Title']
            text += f""### {step_number}. {title}\n""
            step_number += 1

        # Level 2 bullet
        if isinstance(row['Header'], str) and row['Header'].strip():
            text += f""- {row['Header'].strip()}\n""

            # Level 3 bullet (indented)
            if isinstance(row['Description'], str) and row['Description'].strip():
                text += f""    - {row['Description'].strip()}\n""

    return text",Organization.py
34,CreateMarkdownfromProcess,"Function to call Process Map from Google sheet with a single reference to the name of the process. Args: process_name (str): Process Name as defined in Google Sheet return_value (str): Value Desired to be returned, as required input from CreateMarkdown",,"def CreateMarkdownfromProcess(process_name=None,
                              return_value=""""):
    '''
    Function to call Process Map from Google sheet with a single reference to the name of the process.
    
    Args:
        process_name (str): Process Name as defined in Google Sheet
        return_value (str): Value Desired to be returned, as required input from CreateMarkdown
    
    Returns:
        If return_value == """", then Displayed Markdown
        If return_value == ""text"", HTML formatted markdown text
        otherwise, returns """"
    '''
    # Call Parameter Mapping function to return DF of Process Sheet

    df = pd.read_csv(ParamterMapping('ProcessSheet')['CSV'].item())
    
    if process_name !=None:
        df1 = df[df['Process']==process_name].drop('Process',axis=1)
    else:
        return df
        
    try:
        if return_value =="""":
            return CreateMarkdown(df1)
        elif return_value == ""text"":
            return CreateMarkdown(df1,'text')
    
    except:
        print(""Could Not Format Data"")
        return df",Organization.py
35,DownloadFilesFromGit,Function to Download Files from Github to a dedicated folder. Specifically used when i DO NOT want to formally link to Github. Parameters: User: Repo: folder: output_folder:,,"def DownloadFilesFromGit(user='derek-dewald',
                        repo='Python_Tools',
                        folder='d_py_functions',
                        output_folder=""""):
    '''
    Function to Download Files from Github to a dedicated folder. Specifically used when i DO NOT want to formally link to Github.
    
    Parameters:
        User:
        Repo:
        folder:
        output_folder:
        
    Returns:
        Saves files to Output Folder.
    

    '''
    
    if len(output_folder) == 0:
        output_folder = os.getcwd()
    
    api_url = f""https://api.github.com/repos/{user}/{repo}/contents/{folder}""
    response = requests.get(api_url)

    if response.status_code == 200:
        files = response.json()
        py_files = [file for file in files if file['name'].endswith('.py')]

        for file in py_files:
            file_url = file['download_url']
            file_name = file['name']
            file_response = requests.get(file_url)

            if file_response.status_code == 200:
                with open(os.path.join(output_folder, file_name), ""w"", encoding=""utf-8"") as f:
                    f.write(file_response.text)
                    
        return True
    else:
        return False",Connections.py
36,ParamterMapping,"Function to Google Mapping Sheet, which is used to store Mappings, Links, etc. For both simplicity and Organization Args: Definition (Str): Key word used to Access individual elements",,"def ParamterMapping(Definition=""""):
    
    '''
    Function to Google Mapping Sheet, which is used to store Mappings, Links, etc.
    For both simplicity and Organization
    
    Args:
        Definition (Str): Key word used to Access individual elements
        
    Returns:
        Dataframe, unless Definition is defined, in which case it might be Str.
    
    '''

    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSwDznLz-GKgWFT1uN0XZYm3bsos899I9MS-pSvEoDC-Cjqo9CWeEuSdjitxjqzF3O39LmjJB0_Fg-B/pub?output=csv')
    
    # If user has not included a definition, the return entire DF
    if len(Definition)==0:
        return df
    else:
        try:
            df1 = df[df['Definition']==Definition]
            if len(df1)==1:
                if df1['TYPE'].item()=='csv':
                    return pd.read_csv(df1['VALUE'].item())
                else:
                    return df1['VALUE'].item()
        except:
            return df[df['Definition']==Definition]",Connections.py
37,BackUpGoogleSheets,Function to Create a Backup of Information Stored in Google Sheets. Parameters: None,,"def BackUpGoogleSheets(location='/Users/derekdewald/Documents/Python/Github_Repo/CSV Backup Files/'):
    '''
    Function to Create a Backup of Information Stored in Google Sheets.
    
    Parameters:
        None
        
    Returns:
        CSV Files 
    
    '''
    
    df = ParamterMapping()
    
    for row in range(len(df)):
        try:
            file_name = df['Definition'][row]
            file_location = df['CSV'][row]
            month = datetime.datetime.now().strftime('%b-%y')
            
            temp_df = pd.read_csv(file_location)
            temp_df.to_csv(f'{location}{file_name}_{month}.csv',index=False)
            print(f'Back Up Saved, {location}{file_name}')
        except:
            print(f'Counld Not Print Record {row}')",Connections.py
38,GoogleProcessSheetLinks,"Function to Google Mapping Sheet, Navigate to Specific Sites. Provides Options, Enable Selection based on inputs. Parameters:",,"def GoogleProcessSheetLinks():
     
    '''
    Function to Google Mapping Sheet, Navigate to Specific Sites.
    Provides Options, Enable Selection based on inputs.
    
    Parameters:
    
        
    Returns:
        
    
    '''

    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSwDznLz-GKgWFT1uN0XZYm3bsos899I9MS-pSvEoDC-Cjqo9CWeEuSdjitxjqzF3O39LmjJB0_Fg-B/pub?output=csv')
    
    display(df)
    
    p = input('Which Process would You like to review?')
    v = input('What would you like to return?')
    
    df1 = df[df['Definition']==p]
    
    if v.lower() =='link':
        webbrowser.open(df1['Link'].item())
    elif v.lower() == 'csv':
        return pd.read_csv(df1['CSV'].item())
    elif v.lower()=='streamlit':
        webbrowser.open(df1['Streamlit'].item())",Connections.py
39,NavigateUsingDMap,"Function to Google Mapping Sheet, Navigate to Specific Sites. Provides Options, Enable Selection based on inputs. Parameters:",,"def NavigateUsingDMap():
     
    '''
    Function to Google Mapping Sheet, Navigate to Specific Sites.
    Provides Options, Enable Selection based on inputs.
    
    Parameters:
    
        
    Returns:
        
    
    '''

    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSwDznLz-GKgWFT1uN0XZYm3bsos899I9MS-pSvEoDC-Cjqo9CWeEuSdjitxjqzF3O39LmjJB0_Fg-B/pub?output=csv')
    
    display(df)
    
    p = input('Which Process would You like to review?')
    v = input('What would you like to return?')
    
    df1 = df[df['Definition']==p]
    
    if v.lower() =='link':
        webbrowser.open(df1['Link'].item())
    elif v.lower() == 'csv':
        return pd.read_csv(df1['CSV'].item())
    elif v.lower()=='streamlit':
        webbrowser.open(df1['Streamlit'].item())",Connections.py
40,CreateTableAnalytics,"Function utilized to Run Script Created from Function: generate_create_table_sql Because the formating in generate_create_table_sql is not beyond reproach, it is a good practice to keep these items distinct and ensure a manual step before Table is created. Parameters: sql(text): SQL Text, generated primarily exclusively by generate_create_table_sql",,"def CreateTableAnalytics(sql):
    '''
    Function utilized to Run Script Created from Function: generate_create_table_sql
    
    Because the formating in generate_create_table_sql is not beyond reproach, it is a good practice to keep these 
    items distinct and ensure a manual step before Table is created.
    
    Parameters:
        sql(text): SQL Text, generated primarily exclusively by generate_create_table_sql
    
    Returns:
        Creates a New table in Analytics Database
    
    
    '''
    cnxn = Analytics()
    cursor = cnxn.cursor()
    
    cursor.execute(sql)
    cnxn.commit()
    
    cursor.close()
    cnxn.close()",Connections.py
41,DeleteAnalyticalTable,No description available,None,"def DeleteAnalyticalTable(table_name,
                          condition=None,
                          by_pass_validation=0,
                          remove_table=0):
    
    cnxn = Analytics()
    cursor = cnxn.cursor()
    
    if remove_table==1:
        proceed = input('Do you want to Delete Table. Only Way forward is Yes')
        if proceed !='Yes':
            print(""User Defined Escape"")
            return
        delete_sql = f'DROP TABLE {table_name}'
    
    else:
        delete_sql = f'Delete from {table_name}'

        if by_pass_validation==0:
            proceed = input('Do you REALLY want to delete Data from this table. Only Way forward is Yes')

            if proceed !='Yes':
                print(""User Defined Escape"")
                return
        if condition:
            delete_sql +=  f"" {condition}"" 
    try:
        cursor.execute(delete_sql)
        cnxn.commit()
    except Exception as e:
        print('Error during Insert',e)
        cnxn.rollback()
    finally:
        cursor.close()
        cnxn.close()",Connections.py
42,IterativeTestInsertColumnByColumn,Attempts to insert each column from the first row individually into a test table with one column. Used to pinpoint formatting/casting issues.,None,"def IterativeTestInsertColumnByColumn(df, table_name,total_rows=5):
    """"""
    Attempts to insert each column from the first row individually into a test table
    with one column. Used to pinpoint formatting/casting issues.
    """"""
    

    columns = []
    
    df = df[:total_rows].copy()
            
    for cols in df.columns:
        columns.append(cols)
        print(f""Attempting to Insert:{cols}"")
        INSERT_STATEMENT(df[columns],table_name)
        print(f'Successfully Inserted:{cols}')",Connections.py
43,UpdateAnalyticsTable,Function to insert data into a SQL Server table in batches or all at once. Handles nulls and datetime values.,None,"def UpdateAnalyticsTable(df, table_name, batch_size=0):
    '''
    Function to insert data into a SQL Server table in batches or all at once.
    Handles nulls and datetime values.
    '''
    start_time = timeit.default_timer()

    # Replace NaN and NaT with None
    df = df.where(pd.notnull(df), None).copy()

    if batch_size == 0:
        # Insert all rows at once
        try:
            INSERT_STATEMENT(df, table_name)
            print(f""Inserted all {len(df)} records. Elapsed Time: {timeit.default_timer() - start_time:.2f} seconds"")
        except Exception as e:
            print(f""Could not insert data: {e}"")
    else:
        # Insert in batches
        for start in range(0, len(df), batch_size):
            end = min(start + batch_size, len(df))
            try:
                INSERT_STATEMENT(df.iloc[start:end], table_name)
                print(f""Inserted records {start} to {end - 1}. Elapsed Time: {timeit.default_timer() - start_time:.2f} seconds"")
            except Exception as e:
                print(f""Could not insert records {start} to {end - 1}: {e}"")",Connections.py
44,INSERT_STATEMENT,Function Used to Insert Data Into a Table in MS SQL.,None,"def INSERT_STATEMENT(df,table_name):
    
    '''
    Function Used to Insert Data Into a Table in MS SQL.
    
    
    
    '''
    cnxn = Analytics()
    cursor = cnxn.cursor()
    
    cursor.fast_executemany=True
    
    columns = ', '.join(f""[{col}]"" for col in df.columns)
    placeholders = ', '.join(['?'] * len(df.columns))
    insert_sql = f""INSERT INTO {table_name} ({columns}) VALUES ({placeholders})""
    
    data = list(df.itertuples(index=False,name=None))
    
    try:
        cursor.executemany(insert_sql,data)
        cnxn.commit()
    except Exception as e:
        print('Error during Insert',e)
        cnxn.rollback()
    finally:
        cursor.close()
        cnxn.close()",Connections.py
45,get_varchar_bucket,Function Used for generate_create_table_sql to round VARCHAR VALUES,None,"def get_varchar_bucket(length):
    '''
    Function Used for generate_create_table_sql to round VARCHAR VALUES
    '''
    thresholds = [8, 16, 32, 64, 128, 255]
    for t in thresholds:
        if length <= t:
            return t
    return 255",Connections.py
46,TableColumnCleaner,No description available,None,"def TableColumnCleaner(df,clean_df=0):
    df = df.copy()

    # Auto-detect if not provided
    inferred_text = []
    inferred_number = []
    inferred_date = []
    failed_cols = []
    
    for col in df.columns:
        dtype = df[col].dtype

        if col == ""MEMBERNBR"":
            inferred_number.append(col)
        elif col == ""ACCTNBR"":
            inferred_number.append(col)
        elif col.lower().find('date')!=-1:
            inferred_date.append(col)
        elif col.lower().find('flag')!=-1:
            inferred_number.append(col)
        elif pd.api.types.is_string_dtype(dtype):
            inferred_text.append(col)
        elif pd.api.types.is_numeric_dtype(dtype):
            inferred_number.append(col)
        elif pd.api.types.is_datetime64_any_dtype(dtype):
            inferred_date.append(col)
        else:
            failed_cols.append(col)
            
    print(f""Text Columns:{inferred_text}"")
    print(f""Numeric Columns:{inferred_number}"")
    print(f""Date Columns:{inferred_date}"")
    print(f""Failed Columns:{failed_cols}"")

    if clean_df==1:

        # Clean text
        for col in inferred_text:
            df[col] = df[col].fillna("""")

        # Clean numeric
        for col in inferred_number:
            df[col] = df[col].fillna(0)

        # Clean date
        for col in inferred_date:
            try:
                ConvertDate(df, col, col, 1)
            except Exception as e:
                print(f""Failed to convert date column {col}: {e}"")
                failed_cols.append(col)
    return df",Connections.py
47,ViewDF,"Function to Assist in the Viewing of a Dataframe in a User Friendly Manner, showing all rows, columns and Numbers. Parameters: df (dataframe) global (int): Binary Flag, if 1 it applies the preferences to the Workbook, if 0 it displays the single dataframe, not exporting the perferences to the workbook",None,"def ViewDF(df=None,update_decimal=1):
    '''
    Function to Assist in the Viewing of a Dataframe in a User Friendly Manner, showing all rows, columns and Numbers.

    Parameters:
        df (dataframe)
        global (int): Binary Flag, if 1 it applies the preferences to the Workbook, if 0 it displays the single dataframe, not exporting the perferences to the workbook
    
    
    '''
    if update_decimal:
        pd.options.display.float_format = '{:,.2f}'.format

    if df:
      with pd.option_context(
            'display.max_colwidth',None,
            'display.max_columns',None,
            'display.expand_frame_repr',False):
            display(df)
    else:
        pd.set_option('display.max_colwidth',None)
        pd.set_option('display.max_columns',None,)
        pd.set_option('display.expand_frame_repr',False)",UtilityFunctions.py
48,InspectFunction,No description available,None,"def InspectFunction(function_name):
    print(inspect.getsource(function_name))",UtilityFunctions.py
49,password_generator,"Purpose: Create a Randomly Generated Password of a predetermined number of CHaracters. Input: Minimum: Number of Minimum Characters. Maximum: Number of Maximum Characters. Default: Minimum: 8 Characters Maximum: 10 Characters. Notes: First Character is always a Letter which is captilized. The last 1-3 letters are special Characters, randomly assinged.",None,"def password_generator(minimum=8,maximum=10):
    
    '''
    Purpose: Create a Randomly Generated Password of a predetermined number of CHaracters. 
    
    Input: 
    
    Minimum: Number of Minimum Characters. 
    Maximum: Number of Maximum Characters.
    
    Default:
    Minimum: 8 Characters
    Maximum: 10 Characters.
    
    Notes:
    First Character is always a Letter which is captilized.
    The last 1-3 letters are special Characters, randomly assinged. 
    
    '''

    letter = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
    char = ['!','@','#','$','%','&','*',')',""(""]
    num = ['1','2','3','4','5','6','7','8','9','0']
    
    # select a random number between 8 and 12
    total_char  = random.randint(minimum,maximum)
    #print(total_char)

    # Select a random Number of Special Characters, always at the End for simplicity
    special_char = random.randint(1,3)
        
    alpha_num = total_char - special_char 
    
    # ALways start with a Capital
    password = random.choice(letter).upper()
    
    for i in range(0,alpha_num-1):
        temp = random.choice([letter,num])
        password += random.choice(temp)
    
    for i in range(0,special_char):
        password += random.choice(char)

    #print(len(password))
    return password",UtilityFunctions.py
50,sklearn_dataset,No description available,None,"def sklearn_dataset(df):
    # Load the diabetes dataset
    df1 = pd.DataFrame(df['data'],columns=df['feature_names'])
    df1 = pd.concat([df1,pd.DataFrame(df['target'],columns=['Target'])],axis=1)
    return df1",DataSets.py
51,FakeBaseballstats,No description available,None,"def FakeBaseballstats():
    Members = ['Roger Dorn','Rick Vaughn','Lou Brown','Pedro Cerrano','Rachel Phelps','Jake Taylor']
    Team = ['Montreal Expos','Boston Red Sox','New York Yankees','Toronto Blue Jays']
    years = [1990,1991,1992,1993,1994]
    games = 162

    final_df = pd.DataFrame()

    for year in years:
        for day in range(0,games):
            for player in Members:
                ab = np.random.randint(1,6)
                hits = np.random.randint(0,ab)
                temp_df = pd.DataFrame([[player,np.random.choice(Team),hits,ab,year]],columns=['Player','Opponent','Hits','Abs','Year'])
                final_df = pd.concat([final_df,temp_df])
        final_df = final_df.reset_index(drop=True)
    return final_df",DataSets.py
52,GrowthRate,"Function to Generate a Synthic Growth Percentage, used to support GenerateFakeMember",None,"def GrowthRate(value,outlook):
    '''
    Function to Generate a Synthic Growth Percentage, used to support GenerateFakeMember
    
    
    '''
    if outlook==-2:
        return max(0,value - np.random.uniform(.15,.3)*value)
    elif outlook==-1:
        return max(0,value - np.random.uniform(.05,.15)*value)
    elif outlook==0:
        return max(0,value + np.random.uniform(-.05,.05)*value)
    elif outlook==1:
        return max(0,value + np.random.uniform(0,.15)*value)
    elif outlook==2:
        return max(0,value + np.random.uniform(.15,.3)*value)",DataSets.py
53,GenerateFakeMemberDF,Function to Create a Fake Customer Dataset for Analyzing Over Time Series.,None,"def GenerateFakeMemberDF(mbrs,months,attrition_perc=.05,growth_max_perc=.1):
    '''
    Function to Create a Fake Customer Dataset for Analyzing Over Time Series.
    
    
    '''
    members = {}
    final_df = pd.DataFrame()
    current_id = 0

    for _ in range(mbrs):
        m = Member(current_id, 1,0)
        members[current_id] = m
        final_df = pd.concat([final_df, pd.DataFrame([m.to_dict(0)])])
        current_id += 1

    for month in range(1, months+1):  # months 1 to 18 inclusive
        monthly_records = []

        for m in members.values():
            updated = m.update_month(month=month, GrowthRateFunc=GrowthRate,attrition_perc=attrition_perc)
            monthly_records.append(updated)

        new_member_count = np.random.randint(0, mbrs*growth_max_perc)
        for _ in range(new_member_count):
            m = Member(current_id,1,month)
            members[current_id] = m
            monthly_records.append(m.to_dict(month))
            current_id += 1

        # 3. Append everything for this month to the final DataFrame
        final_df = pd.concat([final_df, pd.DataFrame(monthly_records)], ignore_index=True)  
    return final_df",DataSets.py
54,MNIST_SKLEARN,"Loads the MNIST dataset from OpenML and returns train/test sets. Parameters: normalize (bool): If True, scale pixel values to [0, 1]. flatten (bool): If True, keep images as (784,) instead of (28, 28). random_state (int): Random seed for reproducibility.",,"def MNIST_SKLEARN(normalize=True, flatten=False, random_state=42,return_value=None):
    """"""
    Loads the MNIST dataset from OpenML and returns train/test sets.

    Parameters:
        normalize (bool): If True, scale pixel values to [0, 1].
        flatten (bool): If True, keep images as (784,) instead of (28, 28).
        random_state (int): Random seed for reproducibility.

    Returns:
        
    """"""
    print(""Downloading MNIST from OpenML..."")
    mnist = fetch_openml('mnist_784', version=1, as_frame=False)

    # Raw data and labels
    X, y = mnist[""data""], mnist[""target""]

    # Convert labels to integers
    y = y.astype(np.uint8)

    # Normalize pixel values
    if normalize:
        X = X / 255.0

    # Reshape to (28, 28) images unless flatten=True
    if not flatten:
        X = X.reshape(-1, 28, 28)

    if return_value =='df':
        return pd.concat([pd.DataFrame(X),pd.DataFrame(y,columns=['Target'])],axis=1),pd.DataFrame()
    else:
        return X,y",DataSets.py
55,GenerateSKModelDoc,No description available,None,"def GenerateSKModelDoc():

    sklearn_model_df = SKLearnModelList()
       
    param_df = pd.DataFrame()
    desc_df = pd.DataFrame()

    for index,row in sklearn_model_df.iterrows():
        temp_para,temp_desc = extract_doc_sections_all(row['Estimator Class'],model_name=row['Model Name'])
        param_df = pd.concat([param_df,temp_para])
        desc_df = pd.concat([desc_df,temp_desc])

    sklearn_model_df = sklearn_model_df.merge(desc_df,on='Model Name',how='left') 

    sklearn_model_df.to_csv('SKLearnModels.csv',index=False)
    param_df.to_csv('SKlearnParameterList.csv',index=False)
    
    return sklearn_model_df,param_df",DataSets.py
56,DuplicateFileorFolder,Function to copy a file or folder to another location while handling errors. Parameters: source_path (str): Path to the file or folder to copy. destination_path (str): Destination path where the file or folder should be stored.,,"def DuplicateFileorFolder(source_path, destination_path):
    """"""
    Function to copy a file or folder to another location while handling errors.

    Parameters:
        source_path (str): Path to the file or folder to copy.
        destination_path (str): Destination path where the file or folder should be stored.
    
    Returns:
        None
    
    """"""
    if not os.path.exists(source_path):
        raise FileNotFoundError(f""Source path '{source_path}' does not exist."")

    if os.path.isfile(source_path):
        try:
            shutil.copy2(source_path, destination_path)
        except PermissionError:
            print(f""Skipped (Permission Denied): {source_path}"")
        except Exception as e:
            print(f""Failed to copy file: {source_path}. Error: {e}"")

    elif os.path.isdir(source_path):
        if not os.path.exists(destination_path):
            os.makedirs(destination_path, exist_ok=True)  # Ensure destination directory exists

        for root, dirs, files in os.walk(source_path):
            # **Skip `.git` directories**
            if "".git"" in root.split(os.sep):
                print(f"" Skipping `.git` folder: {root}"")
                continue  

            for dir_name in dirs:
                source_dir = os.path.join(root, dir_name)
                dest_dir = os.path.join(destination_path, os.path.relpath(source_dir, source_path))

                try:
                    os.makedirs(dest_dir, exist_ok=True)
                except PermissionError:
                    print(f""Skipped directory (Permission Denied): {dest_dir}"")
                except Exception as e:
                    print(f""Failed to create directory: {dest_dir}. Error: {e}"")

            for file_name in files:
                source_file = os.path.join(root, file_name)
                dest_file = os.path.join(destination_path, os.path.relpath(source_file, source_path))

                try:
                    shutil.copy2(source_file, dest_file)
                except PermissionError:
                    print(f""Skipped file (Permission Denied): {source_file}"")
                except Exception as e:
                    print(f""Failed to copy file: {source_file}. Error: {e}"")

    else:
        raise ValueError(f""Source path '{source_path}' is neither a file nor a folder."")

    print(f""Finished copying '{source_path}' to '{destination_path}'."")",SharedFolder.py
57,ReadDirectory,"Function which reads reads a directory and returns a list of files included within Parameters: folder (str): The path to the directory. Defaults to the current working directory if not provided. file_type (str): The file extension or type to filter by (e.g., '.ipynb'). If empty, returns all files.",,"def ReadDirectory(location=None,
                  file_type=None,
                  match_str=None,
                  create_df=0,
                  number_of_CPUs=1):
                  
    """"""
    Function which reads reads a directory and returns a list of files included within

    Parameters:
    folder (str): The path to the directory. Defaults to the current working directory if not provided.
    file_type (str): The file extension or type to filter by (e.g., '.ipynb'). If empty, returns all files.

    Returns:
    list: A list of files from the directory, optionally filtered by file type.
    """"""
    
    # If no folder is provided, use the current working directory
    if location ==None:
        location = os.getcwd() +""\\""
    
    file_list = os.listdir(location)
        
    # If no file type is provided, return all files in the directory
    if file_type !=None:
        file_list = [x for x in file_list if file_type in x]
    
    if match_str !=None:
        file_list = [x for x in file_list if x.find(match_str)!=-1]
        
    if create_df ==0:                  
        # Return files that match the specified file type
        return file_list
    else:
        if number_of_CPUs==1:
            final_df = pd.DataFrame()
            for file in file_list:
                if file_type=='csv':
                    final_df = pd.concat([final_df,pd.read_csv(f""{location}{file}"")])
                elif file_type == 'xlsx':
                    final_df = pd.concat([final_df,pd.read_excel(f""{location}{file}"")])
            return final_df
        
        else:
            if file_type=='csv':
                def read_file(file):
                    return pd.read_csv(file)
            else:
                def read_file(file):
                    return pd.read_excel(file)
            
            file_list = [f""{location}{x}"" for x in file_list]
            
            with ThreadPoolExecutor(max_workers=number_of_CPUs) as executor:
                dfs = list(executor.map(read_file,file_list))
            
            return pd.concat(dfs,ignore_index=True)",SharedFolder.py
58,crawl_directory_with_progress,"Recursively crawl through a directory, track progress every `progress_step` percent. Parameters: root_dir (str): Root directory to start from. progress_step (int): Percent steps at which to report progress (e.g. 5 for 5%).",,"def crawl_directory_with_progress(root_dir, progress_step=5,print_=1):
    """"""
    Recursively crawl through a directory, track progress every `progress_step` percent.
    
    Parameters:
        root_dir (str): Root directory to start from.
        progress_step (int): Percent steps at which to report progress (e.g. 5 for 5%).

    Returns:
        pd.DataFrame: DataFrame with file path, name, and type.
    """"""
    start_time = time.time()
    file_records = []

    # Step 1: Count total directories to visit (quick)
    total_dirs = sum(1 for _ in os.walk(root_dir))
    
    print(total_dirs)
    
    if total_dirs == 0:
        print(""No directories found."")
        return pd.DataFrame()

    # Progress tracking
    next_progress_mark = progress_step
    visited_dirs = 0

    for dirpath, dirnames, filenames in os.walk(root_dir):
        visited_dirs += 1

        for file in filenames:
            file_path = os.path.join(dirpath, file)
            file_name = os.path.basename(file)
            file_ext = os.path.splitext(file)[1].lower().lstrip('.')  # remove dot
            
            file_records.append({
                'file_path': file_path,
                'file_name': file_name,
                'file_type': file_ext
            })

        # Print progress every 5%
        if print_==1:
            percent_complete = (visited_dirs / total_dirs) * 100
            if percent_complete >= next_progress_mark:
                elapsed = time.time() - start_time
                estimated_total = elapsed / (percent_complete / 100)
                remaining = estimated_total - elapsed

                print(f""[{percent_complete:.1f}%] Done - ""
                      f""Elapsed: {elapsed:.1f}s - ""
                      f""ETA: {remaining:.1f}s remaining ""
                      f""(Total est: {estimated_total:.1f}s)"")

                next_progress_mark += progress_step

    total_time = time.time() - start_time
    if print_==1:
        print(f"" Done. Total time: {total_time:.2f} seconds. Files found: {len(file_records)}"")
    return pd.DataFrame(file_records)",SharedFolder.py
59,MakeFolder,No description available,None,"def MakeFolder(folder,
               path_):
    
    location = f""{path_}{folder}\\""
    
    if os.path.exists(f""{location}""):
        print(f""{location} exits"")
    else:
        os.makedirs(f""{location}"")
        print('New Folder Created')",SharedFolder.py
60,SimpleBar,"Function to simplify the creation of a Bar Chart. Note, currently the binary_split_col has not been tested. Parameters:",,"def SimpleBar(df,
              x_axis,
              y_axis,
              title="""",
              binary_split_col="""",
              figsize=(10,5),
              legend=0,
              auto_scale_y='',
              export_location="""",
              return_value='graph'):
    '''
    Function to simplify the creation of a Bar Chart.
    
    Note, currently the binary_split_col has not been tested.
    
    
    Parameters:
        
        
        
    Returns:
        
    

    
    '''
    
    plt.figure(figsize=figsize)
      
    if binary_split_col!="""":
        plt.plot(df[x_axis],df[y_axis],label='Population',alpha=.2)
        on = df[df[binary_split_col]==1]
        off = df[df[binary_split_col]==0]
        plt.plot(on[x_axis],on[y_axis],label='Target',alpha=.8)
        plt.plot(off[x_axis],off[y_axis],label='Not Target',alpha=.5)
    
    elif isinstance(y_axis,list):
        x_pos = np.arange(len(df[x_axis]))
        bar_width = 1 / (len(y_axis) + 1)
        
        for i, axis in enumerate(y_axis):
            plt.bar(x_pos + (i - len(y_axis) // 2) * bar_width, df[axis], bar_width, label=axis)
        
        plt.xticks(x_pos, df[x_axis], rotation=45)
            
    else:
        plt.bar(df[x_axis],df[y_axis],label='Population')

    if legend==1:
        plt.legend()
    
    plt.title(title, fontsize=16, ha='center', fontweight='bold', color='black')
    
    if auto_scale_y != """":
        try:
            plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, pos: ConvertAxisValue(x, auto_scale_y)))
        except Exception as e:
            print(f""Error in formatting Y-axis: {e}"")
    
    if len(export_location)!=0:
        plt.savefig(export_location,bbox_inches='tight',transparent=False, facecolor='white')
    
    if return_value == 'graph':
        plt.show()
    else:
        return fig",Visualization.py
61,Heatmap,"Function Which Generates a Heatmap Parameters: Dataframe column_name (list): If included, will only show certain columns on the Horizontal Axis.",,"def Heatmap(df,
            column_list=[],
            title='Heat Map of Correlation',
            cmap='coolwarm',
            figsize=(10,10)):
    
    '''
    Function Which Generates a Heatmap
    
    Parameters:
        Dataframe
        column_name (list): If included, will only show certain columns on the Horizontal Axis.
    
    Returns:
        matlplot plot.
    
    '''
    
    sns.set(style='white')
    
    # View column with Abbreviated title or full. Abbreviated displays nicer.
    corr = df.corr()
    
    if len(column_list)!=0:
        corr = corr[column_list]
    
    mask= np.zeros_like(corr,dtype=bool)
    mask[np.triu_indices_from(mask)]=True
    f,ax = plt.subplots(figsize=figsize)
    sns.heatmap(corr,mask=mask,cmap=cmap,center=0,square=True,linewidths=1,annot=True)
    
    plt.title(title)
    plt.show()",Visualization.py
62,plot_histograms,Plots histograms for each column in a DataFrame. Parameters: df (pd.DataFrame): The input DataFrame. bins (int): Number of bins for the histograms (default=30).,None,"def plot_histograms(df, bins=30):
    """"""
    Plots histograms for each column in a DataFrame.
    
    Parameters:
    df (pd.DataFrame): The input DataFrame.
    bins (int): Number of bins for the histograms (default=30).
    """"""
    num_columns = len(df.columns)
    num_rows = math.ceil(num_columns / 4)  # Determine rows for 4 columns
    
    fig, axes = plt.subplots(num_rows, 4, figsize=(16, 4 * num_rows))
    axes = axes.flatten()  # Flatten in case of fewer than 4 columns

    for i, col in enumerate(df.columns):
        axes[i].hist(df[col], bins=bins, color=""skyblue"", edgecolor=""black"")
        axes[i].set_title(f""Histogram of {col}"")
        axes[i].set_xlabel(col)
        axes[i].set_ylabel(""Frequency"")

    # Hide any unused subplots
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()",Visualization.py
63,plot_scatter_matrix,Plots scatter plots of each feature against the target variable. Parameters: df (pd.DataFrame): The input DataFrame. target_col (str): The name of the target variable.,None,"def plot_scatter_matrix(df, target_col='Target'):
    """"""
    Plots scatter plots of each feature against the target variable.
    
    Parameters:
    df (pd.DataFrame): The input DataFrame.
    target_col (str): The name of the target variable.
    """"""
    num_features = df.drop(columns=[target_col]).shape[1]
    num_rows = (num_features // 4) + 1
    
    fig, axes = plt.subplots(num_rows, 4, figsize=(15, 5 * num_rows))
    axes = axes.flatten()

    for i, col in enumerate(df.columns):
        if col != target_col:
            sns.scatterplot(x=df[col], y=df[target_col], ax=axes[i])
            axes[i].set_title(f""{col} vs. {target_col}"")

    # Hide empty subplots
    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    plt.show()",Visualization.py
64,ConvertAxisValue,Function to Simply support Formating on Matplotlib Object. To be applied as lambda function. Function written by Chatgpt.,None,"def ConvertAxisValue(x, scale):
    '''
    Function to Simply support Formating on Matplotlib Object. To be applied as lambda function.
    Function written by Chatgpt. 
    
    '''
    
    
    if scale == 'M':
        return f'{x * 1e-6:.1f}M'
    elif scale == 'K':
        return f'{x * 1e-3:.1f}K'  # Convert to thousands
    else:
        return f'{x:.1f}'",Visualization.py
65,JupyterNotebookMarkdown,"Function to Create a Markdown file from Process DF, which is a data frame of the structure, Title, Header, Description Args: df (DataFrame): Must include columns Title, Header, Description return_value (str): If """", renders HTML in notebook. If text, returns HTML Markdown string.",,"def JupyterNotebookMarkdown(df, return_value=""""):
    '''
    Function to Create a Markdown file from Process DF, which is a data frame of the structure, 
    Title, Header, Description

    Args:
        df (DataFrame): Must include columns Title, Header, Description
        return_value (str): 
            If """", renders HTML in notebook.
            If text, returns HTML Markdown string.
    
    Returns:
        str or display: Based on return_value
    '''
    try:
        df1 = df[['Title', 'Header', 'Description']]
    except:
        print('DataFrame must include columns: Title, Header, Description')
        return ''

    text = """"
    step_number = 1
    last_title = None
    last_header = None
    open_l2 = False  # Track if L2 <ul> is open
    open_l3 = False  # Track if L3 <ul> is open

    for _, row in df1.iterrows():
        curr_title = row['Title']
        curr_header = row['Header']
        curr_description = row['Description']

        # If new Title
        if curr_title != last_title:
            if open_l3:
                text += ""</ul>\n""
                open_l3 = False
            if open_l2:
                text += ""</ul>\n""
                open_l2 = False
            if last_title is not None:
                text += ""</ul>\n""  # Close previous title's outer <ul>

            text += f""<h4>{step_number}. {curr_title}</h4>\n<ul>\n""
            step_number += 1
            last_title = curr_title
            last_header = None  # Reset header context

        # If new Header
        if curr_header != last_header and isinstance(curr_header, str) and curr_header.strip():
            if open_l3:
                text += ""</ul>\n""
                open_l3 = False
            if open_l2:
                text += ""</ul>\n""
                open_l2 = False

            text += f""  <ul><li>{curr_header}</li>\n""
            open_l2 = True
            last_header = curr_header

        # If Description exists
        if isinstance(curr_description, str) and curr_description.strip():
            if not open_l3:
                text += ""    <ul>\n""
                open_l3 = True
            text += f""      <li>{curr_description}</li>\n""

    # Close any open lists at the end
    if open_l3:
        text += ""    </ul>\n""
    if open_l2:
        text += ""  </ul>\n""
    text += ""</ul>\n""

    if return_value == """":
        display(HTML(text))
    else:
        return text",Visualization.py
66,Scatter,No description available,None,"def Scatter(df,
            X,
            y,
            title='Scatter Plot',
            x_axis='Feature',
            y_axis='Target'):
    
    plt.scatter(df[X], df[y], color=""blue"", alpha=0.5)
    plt.title(title)
    plt.xlabel(x_axis)
    plt.ylabel(y_axis)
    plt.grid(True)
    plt.show()",Visualization.py
67,SimpleOverTimeGraph,,None,"def SimpleOverTimeGraph(df, x, y, z=None, title='', cols=4):

    '''
    
    
    '''
    

    df = df.copy()
    df[x] = pd.to_datetime(df[x])
    df = df.sort_values(by=x)

    if z:
        categories = df[z].unique()
        total = len(categories)
        rows = math.ceil(total / cols)

        fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))
        axes = axes.flatten()

        for idx, cat in enumerate(categories):
            ax = axes[idx]
            temp_df = df[df[z] == cat].sort_values(x)
            ax.plot(temp_df[x], temp_df[y], marker='o', linestyle='-')
            ax.set_title(f'{cat}')
            ax.set_xlabel(""Date"")
            ax.set_ylabel(""Count"")
            ax.grid(True)
            ax.tick_params(axis='x', rotation=45)
            
        # Hide unused subplots
        for idx in range(total, len(axes)):
            fig.delaxes(axes[idx])
        
            

        fig.suptitle(title, fontsize=16)
        plt.tight_layout(rect=[0, 0, 1, 0.95])

    else:
        plt.figure(figsize=(12, 6))
        plt.plot(df[x], df[y], marker='o', linestyle='-')
        plt.xlabel(""Date"")
        plt.ylabel(""Count"")
        plt.title(title)
        plt.grid(True)
        plt.tight_layout()
        plt.xticks(rotation=45)
        
        plt.show()",Visualization.py
68,TrainingTestPerformanceChart,Function to Graphically Depict Testing and Training of ML Function. Parameters training_errors (list): List of Training Errors test_errors (list): List of Test Errors LossType(str): Type of Loss which was calculated (Default Mean Squre Error) Returns Matplotlib Visualization,None,"def TrainingTestPerformanceChart(train_errors,test_errors,LossType='Mean Squared Error'):
    '''
    Function to Graphically Depict Testing and Training of ML Function.

    Parameters
        training_errors (list): List of Training Errors
        test_errors (list): List of Test Errors
        LossType(str): Type of Loss which was calculated (Default Mean Squre Error)

    Returns
        Matplotlib Visualization
        
    '''

    records = len(train_errors)
    
    plt.plot(records,train_errors,label='Training Errors')
    plt.plot(records,test_errors,label='Test Errors')
    plt.xlabel('Observations')
    plt.ylabel(LossType)
    plt.legend()",Visualization.py
69,visualize_hex_color,Purpose: Function to Review a distribution of colors from a particular list. Primarily Utilizing a predetermined ordered list. Parameters:,,"def visualize_hex_color(hex_color_list,columns=15,rows=10):
    
    '''
    Purpose: Function to Review a distribution of colors from a particular list. 
    Primarily Utilizing a predetermined ordered list.
        
    Parameters:
    
    Returns:
    
    visualize_hex_color(hex_color_list[:150],15)
  
    '''
    
    total_iterations = rows*columns
    
    if len(hex_color_list) == 0:
        
        hex_color_list = hex_color_list[:total_iterations]
            
    num_rows = (total_iterations+(columns-1))//columns
    num_cols = min(columns,total_iterations)
    
    fig, axs = plt.subplots(num_rows,num_cols,figsize=(20,20))
    
    for i,hex_color in enumerate(hex_color_list):
        row = i//num_cols
        col = i% num_cols
        ax = axs[row,col] if total_iterations>1 else axs
        ax.add_patch(plt.Rectangle((0, 0), 1, 1, color=hex_color))
        ax.set_title(hex_color)
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_aspect('equal')

    plt.show()",Visualization.py
70,pixelate_image,"Function to take a .jpg file and Pixelate it, Developed for Playing a ""Fun"" Game. Parameters: image_path: Location of JPG File PixelSizeList: List of Pixelation Levels to Apply, 1 being a near replica of the image, 200 being a near unrecognizable generalist Blob.",,"def pixelate_image(image_path, PixelSizeList=[10,50,75,100,125]):
    '''
    Function to take a .jpg file and Pixelate it, Developed for Playing a ""Fun"" Game.
    
    Parameters:
        image_path: Location of JPG File
        PixelSizeList: List of Pixelation Levels to Apply, 1 being a near replica of the image, 200 being a near unrecognizable generalist Blob.

    Returns:
        pix
    
    '''

    import cv2

    # Load the image
    image = cv2.imread(image_path)
    
    # Get the dimensions of the image
    height, width = image.shape[:2]
    
    for pixel_size in PixelSizeList:

        # Resize to a smaller version
        small_image = cv2.resize(image, (width // pixel_size, height // pixel_size), interpolation=cv2.INTER_LINEAR)

        # Resize back to original size (creates pixelation effect)
        pixelated_image = cv2.resize(small_image, (width, height), interpolation=cv2.INTER_NEAREST)

        # Save or display the pixelated image
        pixelated_path = f""pixelated_image_{pixel_size}.jpg""
        cv2.imwrite(pixelated_path, pixelated_image)
    
    print(f""Pixelated image saved at: {image_path}"")
    
    return pixelated_path",Visualization.py
71,DailyTest,No description available,None,"def DailyTest(questions=20,updates=5):
    
    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vQq1-3cTas8DCWBa2NKYhVFXpl8kLaFDohg0zMfNTAU_Fiw6aIFLWfA5zRem4eSaGPa7UiQvkz05loW/pub?output=csv')
    
    updates = df[df['Definition'].isnull()].sample(updates)
    print(updates['Word'].tolist())
    
    questions = df[df['Definition'].notnull()].sample(questions)
    
    for row in range(len(df)):
        display_term_latex_dynamic(questions.iloc[row])",D_Testing.py
72,display_term_latex_dynamic,Function Created to Sample Data Dictionary Rows and present information in incremental Format Parameters: Series,,"def display_term_latex_dynamic(row):
    '''
    Function Created to Sample Data Dictionary Rows and present information in incremental Format
    
    
    Parameters:
        Series
        
    Returns:
        Nil
    

    '''
    print(f""\n=== {row['Word']} ==="")
    print(f""Category: {row['Category']}"")
    input()
    print(f""Sub Category: {row['Sub Categorization']}"")
    input()
    print(f""Definition: {row['Definition']}\n"")
    input()
    if pd.notna(row['Markdown Equation']):
        eq_text = row['Markdown Equation']
        
        # Extract equation
        main_eq = re.search(r""\$\$(.*?)\$\$"", eq_text, re.DOTALL)
        if main_eq:
            display(Markdown(""**Equation:**""))
            display(Math(main_eq.group(1).strip()))
        
        # Extract ""where"" section
        where_part = re.split(r""\bwhere:\b"", eq_text, flags=re.IGNORECASE)
        if len(where_part) > 1:
            display(Markdown(""**Where:**""))
            where_lines = where_part[1].strip().splitlines()
            for line in where_lines:
                cleaned = line.strip(""- "").strip()
                if cleaned:
                    display(Math(cleaned))
    if pd.notna(row['Link']):
        display(Markdown(f""[More Info]({row['Link']})""))",D_Testing.py
73,analyze_distribution,"Analyzes skewness, kurtosis, and visualizes the distribution of all numeric columns. Parameters: df (pd.DataFrame): The input DataFrame.",,"def analyze_distribution(df):
    """"""
    Analyzes skewness, kurtosis, and visualizes the distribution of all numeric columns.

    Parameters:
    df (pd.DataFrame): The input DataFrame.

    Returns:
    None (displays plots and summary in an X by 3 format)
    """"""
    numeric_cols = df.select_dtypes(include=['number']).columns
    num_vars = len(numeric_cols)
    
    fig, axes = plt.subplots(num_vars, 3, figsize=(18, 6 * num_vars))
    
    if num_vars == 1:
        axes = [axes]  # Ensure iterable for single variable
    
    for i, col in enumerate(numeric_cols):
        target_data = df[col].dropna()
        
        # Compute skewness & kurtosis
        skewness = skew(target_data)
        kurt = kurtosis(target_data)
        
        if abs(skewness) > 1:
            skewness_comment = ""Highly Skewed, Consider Transformation""
        elif skewness > 0:
            skewness_comment = ""Right Skewed""
        elif skewness < 0:
            skewness_comment = ""Left Skewed""
        else:
            skewness_comment = ""Symmetric""

        # Correct Kurtosis Comments
        if kurt >= 2.75 and kurt <= 3.25:
            kurt_comment = ""Mesokurtic (Normally Distributed), No Action Necessary.""
        elif kurt > 3.25:
            kurt_comment = ""Leptokurtic (High Kurtosis), More Extreme Values""
        elif kurt < 2.75:
            kurt_comment = ""Platykurtic (Low Kurtosis), Less Extreme Values""
        
        summary_text = f""Skewness: {skewness:.2f}\n{skewness_comment}\nKurtosis: {kurt:.2f}\n{kurt_comment}""
        
        # Plot Histogram
        sns.histplot(target_data, kde=True, bins=30, color=""blue"", ax=axes[i][0])
        axes[i][0].set_title(f""Histogram of {col} (Skew: {skewness:.2f})"")
        
        # Plot Boxplot
        sns.boxplot(x=target_data, color=""red"", ax=axes[i][1])
        axes[i][1].set_title(f""Boxplot of {col}"")
        
        # Display Text
        axes[i][2].text(0.5, 0.5, summary_text, fontsize=12, va='center', ha='center', bbox=dict(facecolor='white', alpha=0.8))
        axes[i][2].axis(""off"")
        
    plt.tight_layout()
    plt.show()",EDA.py
74,TranposeNonTimeSeriesDF,"Transposes a non-time-series DataFrame from wide to long format by melting specified columns. This is especially useful for flattening columns into a single column to support tools like Power BI, where long format enables dynamic pivoting and aggregation. Parameters: df (DataFrame): The input pandas DataFrame. index (list): Columns to retain as identifiers (will remain unchanged). columns (list): Columns to unpivot into key-value pairs.",,"def TranposeNonTimeSeriesDF(df, index, columns=None):
    '''
    Transposes a non-time-series DataFrame from wide to long format by melting specified columns.

    This is especially useful for flattening columns into a single column to support tools 
    like Power BI, where long format enables dynamic pivoting and aggregation.

    Parameters:
        df (DataFrame): The input pandas DataFrame.
        index (list): Columns to retain as identifiers (will remain unchanged).
        columns (list): Columns to unpivot into key-value pairs.

    Returns:
        DataFrame: A long-format DataFrame with 'variable' and 'value' columns.
    '''
    if not columns:
        columns = [col for col in final_df1.columns if (isinstance(col, pd.Timestamp))|(isinstance(col, datetime.datetime))]
    
    return df.melt(id_vars=index, value_vars=columns)",TimeSeries_WIP.py
75,CreatePivotTableFromTimeSeries,Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.,None,"def CreatePivotTableFromTimeSeries(df,
                                   index,
                                   columns,
                                   values,
                                   aggfunc='sum',
                                   skipna=True):
    
    '''
    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.
    
    
    
    '''
    
    # 1. Pivot
    if index==None:
        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)
    else:
        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)

    # 2. Capture original month columns IMMEDIATELY after pivot
    month_cols = df1.columns.tolist()
 
    # 3. Add rolling window stats
    if len(month_cols) >= 3:
        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)
        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]
        try:
            df1['PERC_CHG_3M'] = df1['CHG_3M']/df1[month_cols[-3]]
        except:
            df1['PERC_CHG_3M'] = 0
    
    if len(month_cols) >= 6:
        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)
        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]
        try:
            df1['PERC_CHG_6M'] = df1['CHG_6M']/df1[month_cols[-6]]
        except:
            df1['PERC_CHG_6M'] = 0
            
    if len(month_cols) >= 12:
        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)
        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]
        try:
            df1['PERC_CHG_12M'] = df1['CHG_12M']/df1[month_cols[-12]]
        except:
            df1['PERC_CHG_12M'] = 0

    df1['CHG_DF']  = df1[month_cols[-1]]-df1[month_cols[0]]
    df1['AVG_DF'] = df1[month_cols[-1:]].mean(axis=1, skipna=skipna)
    df1['PERC_CHG_DF'] = df1['AVG_DF']/df1[month_cols[-1]]

    
    # 4. Now calculate global stats **only using the original month columns**
    stats = pd.DataFrame({
        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),
        'STD': df1[month_cols].std(axis=1, skipna=skipna),
        'MAX': df1[month_cols].max(axis=1, skipna=skipna),
        'MIN': df1[month_cols].min(axis=1, skipna=skipna),
        'COUNT': df1[month_cols].count(axis=1)
    })

    # 5. Merge the stats
    df1 = pd.concat([df1, stats], axis=1)
    
    return df1.fillna(0)",TimeSeries_WIP.py
76,CreateMultiplePivotTableFromTimeSeries,"Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's",None,"def CreateMultiplePivotTableFromTimeSeries(df,
                                           index_list,
                                           metric_list,
                                           column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    

    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    
    for metric in metric_list:
        print(f'Attempting to Process:{metric}')
        try:
            all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') 
            cols = list(all_df.columns)
            all_df = all_df.reset_index(drop=True)
            all_df['METRIC'] = metric
            cols.insert(0,'METRIC')

            for key in index_list:
                cols.insert(0,key)
                all_df[key] = 'All'

            final_df = pd.concat([final_df,all_df[cols]])
            # Iterate through all Index Items Individually
            for key in index_list:
                temp = CreatePivotTableFromTimeSeries(df,
                                                      index=key,
                                                      values=metric,
                                                      columns=column).reset_index() 
                for missing in [x for x in index_list if x != key]:
                    temp[missing] = 'All'
                temp['METRIC'] = metric
                final_df = pd.concat([final_df,temp])

            # Add Value for Metric with Entire Index Combination
            temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        except:
            print(f'Could Not Process Metric:{metric}.')

    return final_df",TimeSeries_WIP.py
77,CreateMultiplePivotTableFromTimeSeries,"Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's",None,"def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:
        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') 
        cols = list(all_df.columns)
        all_df = all_df.reset_index(drop=True)
        all_df['METRIC'] = metric
        cols.insert(0,'METRIC')

        for key in index:
            cols.insert(0,key)
            all_df[key] = 'All'

        final_df = pd.concat([final_df,all_df[cols]])

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df",TimeSeries_WIP.py
78,SummarizeTimeSeriesDf,"Function to Summarize a Time Series dataframe based on a finite number of identified Columns. Parameters df (Dataframe): TimeSeries in Nature summary_cols (List): List of Columns which are to be included in SUmmary primary_key_list (list): Primary Key of Dataframe Returns temp_df1: Raw Data of SUmmary Cols with a Count of Observations. If include Month Variable Easy to add to Pivot Table summary: Summary (Excluding Primary Key). including Total Observations, MEan, Max, Min.",None,"def SummarizeTimeSeriesDf(df,
                          summary_cols,
                          primary_key_list):
    '''
    Function to Summarize a Time Series dataframe based on a finite number of identified Columns.
    
    Parameters
        df (Dataframe): TimeSeries in Nature
        summary_cols (List): List of Columns which are to be included in SUmmary
        primary_key_list (list): Primary Key of Dataframe
    
    Returns
        temp_df1: Raw Data of SUmmary Cols with a Count of Observations. If include Month Variable Easy to add to Pivot Table
        summary: Summary (Excluding Primary Key). including Total Observations, MEan, Max, Min.
    
    
    '''
    temp_df = df[summary_cols].copy()
    temp_df['COUNT'] = 1
    
    # Unique Occurances by Pivot Criteria. Important to Include Month
    temp_df1 = temp_df.groupby(summary_cols).sum().reset_index().rename(columns={'COUNT':'TOTAL_DAYS'})
    
    pivot_columns1 = [x for x in summary_cols if x not in primary_key_list]
    
    summary = temp_df1.groupby(pivot_columns1).agg(
        TOTAL=('TOTAL_DAYS', 'count'),
        AVG_DAYS_OPEN=('TOTAL_DAYS', 'mean'),
        MAX_OBS=('TOTAL_DAYS', 'min'),
        MIN_OBS=('TOTAL_DAYS', 'max')).reset_index()
    
    return temp_df1,summary",TimeSeries_WIP.py
79,JupyterNotebookMarkdown,"Function to Create a Markdown file from Process DF, which is a data frame of the structure, Title,Header,Description Args: Dataframe( Must be of format, Title, Header, Description) return_value (str: """" or text): If Blank, will render text in HTML Format. If text, then will return text for rendering in HTML Markdown",,"def JupyterNotebookMarkdown(df,return_value=""""):
    
    '''
    Function to Create a Markdown file from Process DF, which is a data frame of the structure, 
    Title,Header,Description
    
    Args:
        Dataframe( Must be of format, Title, Header, Description)
        return_value (str: """" or text):
            If Blank, will render text in HTML Format. 
            If text, then will return text for rendering in HTML Markdown
    
    Returns:
        Conditional on Return Value. Please read Args.
    
    
    '''
    
    try:
        df1 = df[['Title','Header','Description']]
    
    except:
        
        print('DataFrame does not meet structure requirement, which must include 3 Column: Title, Header, Description')
        return ''
    
    title= """"
    step_number = 1
    text = """"

    l2_bullet = '-'  # Level 2 Bullet
    l3_bullet = '*'  # Level 3 Bullet

    for index, row in df1.iterrows():
        # Ensure previous list is closed before starting a new title
        if title and title != row.iloc[0]:  
            text += ""</ul>\n""  # Close the last unordered list before switching to a new title

        # If it's a new title, start a new section
        if title == """" or title != row.iloc[0]:
            text += f""<h4>{step_number}. {row.iloc[0]}</h4>\n<ul>\n""  # Reset indentation
            step_number += 1
            title = row.iloc[0]  # Store the new title

        # Add Level 2 content (Column 2)
        if isinstance(row.iloc[1], str) and row.iloc[1].strip():
            text += f""  <li>{row.iloc[1]}</li>\n""  # L2 starts here

            # Add Level 3 content (Column 3) only if it exists
            if isinstance(row.iloc[2], str) and row.iloc[2].strip():
                text += f""    <ul><li>{row.iloc[2]}</li></ul>\n""  # L3 indented under L2

    text += ""</ul>\n""  # Close any remaining lists

    if return_value =="""":# Display the formatted HTML output in Jupyter Notebook
        display(HTML(text))
        
    else:
        return text",OtherFunctions.py
80,DataFrameFromProcess,"Function to Extract Data from Process Sheet and Return Markdown Text in Jupyter Notebook. Created because Streamlit functionality Changed and could not support HTML Display functionality. Parameters: process_name (str): Value in Column Process, From Google SHeet Process. return_value (str): Markdown (returns Markdown value), otherwise it returns filtered dataframe",,"def DataFrameFromProcess(process_name=None,
                        return_value = 'Markdown'):
    
    '''
    Function to Extract Data from Process Sheet and Return Markdown Text in Jupyter Notebook.
    Created because Streamlit functionality Changed and could not support HTML Display functionality.
    
    Parameters:
        process_name (str): Value in Column Process, From Google SHeet Process.
        return_value (str): Markdown (returns Markdown value), otherwise it returns filtered dataframe
    
    Returns: 
        Based on return_value and process name. Dataframe, Filtered Data Frame or HTML Markdown text.
    
    '''
    
    try:
        df = pd.read_csv(ParamterMapping('ProcessSheet')['CSV'].item())
    except:
        print('Could Not Extract DataFrame')
    
    if process_name ==None:
        return JupyterNotebookMarkdown(df)
    
    try:
        df1 = df[df['Process']==process_name].copy()
    except:
        print('Could Not Filter Process Name')
        return df
    
    if return_value == 'Markdown':
        try:
            return JupyterNotebookMarkdown(df1)
        except:
            print('Could Not Render JupyterNotebookMarkdown')
            return df1",OtherFunctions.py
81,CombineLists,"Function to generate an exhaustive list from multiple lists. Handles both Cartesian product (list of lists) and flat combinations/permutations. If List of Lists, it by-passes Combniation and Permutations by making items equal to 1. Parameters: list_ (list): Flat list or list of lists add_metric (any): Optional value to append to each sublist combo (int): 1 = combinations, 0 = permutations r (int): size of each combination/permutation return_value (str): 'list_' or 'df'",,"def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    If List of Lists, it by-passes Combniation and Permutations by making items equal to 1.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value).drop_duplicates()",DFProcessing.py
82,FilterDataframe,No description available,None,"def FilterDataframe(df,
                    binary_include={},
                    binary_exclude={},
                   ):
    
    temp_df = df.copy()
    
    if len(binary_include)!=0:
        for key, value in binary_include.items():
            temp_df = temp_df[temp_df[key]==value].copy()
            
    if len(binary_exclude)!=0:
        for key, value in binary_exclude.items():
            temp_df = temp_df[temp_df[key]!=value].copy()

    return temp_df",DFProcessing.py
83,TransposePivotTable,Function to take Pivot Table and Turn it into a Vertically Stacked List. Parameters: reset_index(int): Binary Flag to determine whether intex needs to be reset or not. x_axis (str): Column Naming from X_axis. Default X_Axis y_axis (str): value (str): Returns Dataframe,None,"def TransposePivotTable(df,
                        reset_index=1,
                       x_axis='X_Axis',
                       y_axis='Y_Axis',
                       value='VALUE'):
    '''
    Function to take Pivot Table and Turn it into a Vertically Stacked List.
    
    Parameters:
        reset_index(int): Binary Flag to determine whether intex needs to be reset or not.
        x_axis (str): Column Naming from X_axis. Default X_Axis
        y_axis (str):
        value (str):
        
    Returns
        Dataframe
    
    '''
    
    if reset_index==1:
        df1 = df.stack().reset_index()
    else:
        df1 = df.stack()
    try:
        df1 =  df1.rename(columns={'level_0':x_axis,'level_1':y_axis,0:value})
    except:
        pass
    
    df1 = df1[df1[x_axis] < df1[y_axis]].copy()
    
    return df1[df1[x_axis]!=df1[y_axis]].sort_values(value)",DFProcessing.py
84,GenerateBinaryChange,"Function to Simply Apply a Condition to generate whether a particular column, which is meant to be a Change over a time series dataset, has Increased, Decreased or Stayed the Same. Can be stand alone, created for use in ColumnElementalChange Parameters: change_column (str): Name of Column Created.",,"def GenerateBinaryChange(df,
                           change_column='VARIANCE',
                           print_=1):
    '''
    Function to Simply Apply a Condition to generate whether a particular column, which is meant to be a Change over a 
    time series dataset, has Increased, Decreased or Stayed the Same. Can be stand alone, created for use in ColumnElementalChange
    
    Parameters:
        change_column (str): Name of Column Created. 
        
    Returns:
        dataframe, with change_column added
        
    '''
    
    condition = [
        df[change_column]>0,
        df[change_column]<0,
        df[change_column]==0
    ]
        
    value = ['Records Increasing',
             'Records Decreasing',
             'Records Not Changing']
        
    df['CHANGE_CLASSIFICATION'] = np.select(condition,value,'Null Value Present')
    if print_==1:
        print(df['CHANGE_CLASSIFICATION'].value_counts())",DFProcessing.py
85,ColumnStatisticalCompare,Args:,,"def ColumnStatisticalCompare(df,df1,column_name):
    '''
    

    Args:

    Returns:

    
    
    '''

    a = ColumnStatisticalReview(df,column_name).rename(columns={column_name:f'{column_name}_DF'})
    b = ColumnStatisticalReview(df1,column_name).rename(columns={column_name:f'{column_name}_DF1'})
    
    temp_df = pd.concat([a,b],axis=1)
    temp_df[f'{column_name}_VAR_AMT'] = temp_df[f'{column_name}_DF'] - temp_df[f'{column_name}_DF1'] 
    temp_df[f'{column_name}_VAR_PERC'] = temp_df[f'{column_name}_VAR_AMT']/temp_df[f'{column_name}_DF1']
    temp_df[f'{column_name}_VAR_PERC'] = temp_df[f'{column_name}_VAR_PERC'].fillna(0)
    
    return temp_df",DFProcessing.py
86,DFStructureReview,"Function to Create a simplified view of the overall structure of a dataframe, or where 2 similiar dataframes are present, to help understand the difference between. Args: primary_key(list): Primary Keys between",,"def DFStructureReview(df,
                      primary_key='',
                      df1=""""):
    
    '''
    Function to Create a simplified view of the overall structure of a dataframe, or where 2 similiar dataframes are 
    present, to help understand the difference between.
    
    Args:
        primary_key(list): Primary Keys between 
    
    Returns:
        dictionary
        
    '''
    
    prop_dict = {}
    
    prop_dict['TotalRecords_DF']= len(df)
    prop_dict['TotalColumns_DF'] = len(df.columns)
    
    try:
        prop_dict['UniqueRecords_DF']= len(df.drop_duplicates(primary_key))
    except:
        pass
    
    # If there are 2 dataframes:
    
    if len(df1)!=0:
        cols_df =  set(df.columns.values)
        cols_df1 = set(df1.columns.values)
    
        prop_dict['cols_missing_df']  = [x for x in list(cols_df1) if x not in cols_df]
        prop_dict['cols_missing_df1'] = [x for x in list(cols_df) if x not in cols_df1]
        prop_dict['TotalRecords_DF_1'] = len(df1)
        prop_dict['TotalColumns_DF1']  = len(df1.columns)
        
        try:
            temp_df = df[primary_key].merge(df1[primary_key],on=primary_key,how='outer',indicator=True)
            prop_dict['UniqueRecords_DF1'] = len(df1.drop_duplicates(primary_key))
            prop_dict['SharedRecords']     = len(temp_df[temp_df['_merge']=='both'])
            prop_dict['Only_DF']           = len(temp_df[temp_df['_merge']=='left_only'])
            prop_dict['Only_DF1']          = len(temp_df[temp_df['_merge']=='right_only'])
        
        except:
            pass

    return prop_dict",DFProcessing.py
87,ColumnStatisticalReview,"Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution of values. Args: column_name (str): Name of Column partitions (int): Number of partitions to include (Decile 10) exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination. If blank values are excluded it gives a better representation for the members of the set, however it might provide a misleading representation of the population. exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as such it can include both blanks and true 0 values.",None,"def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])
    
    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df
    
    try:

        # Add top X records Based on Frequency
        if top_x_records>0:
            top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index().rename(columns={column_name:'count','index':column_name})
            if len(top_instances)>0:
                top_instances[column_name] = top_instances.apply(lambda row: f""Value: {row[column_name]}, Frequency: {row['count']}"", axis=1)
                top_instances['index'] = [f""Top {x+1}"" for x in range(len(top_instances[column_name]))]
                top_instances = top_instances.drop('count',axis=1).set_index('index')
                temp_df = pd.concat([temp_df,top_instances])

        if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
            segment_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='')
            seg_val_df = ColumnPartitioner(df=df,
                                               column_name=column_name,
                                               partitions=partitions,
                                               exclude_blanks=exclude_blanks_from_segments,
                                               exclude_zeros=exclude_zeroes_from_segments,
                                               return_value='agg_value').rename(columns={'VALUE':column_name})
            return pd.concat([temp_df,segment_df.T,seg_val_df])
    except:
        pass
            
    return temp_df",DFProcessing.py
88,CountBlanksZeroes,"Function to Count the Number of Blanks, Zereos and Nulls in a Dataframe. Parameters: df (dataframe) Returns Dataframe, with all columns as column and 3 rows, with count of observed values.",None,"def CountBlanksZeroes(df):
    '''
    Function to Count the Number of Blanks, Zereos and Nulls in a Dataframe. 

    Parameters:
        df (dataframe)

    Returns
        Dataframe, with all columns as column and 3 rows, with count of observed values.
    
    
    '''

    final_dict = {}
    
    for column in df.columns:
        final_dict[column] = {'Blanks':len(df[df[column]==""""]),
                            'Zeros':len(df[(df[column]==0)|(df[column]==""0"")]),
                            'Null':len(df[df[column].isnull()])}
        
    return pd.DataFrame(final_dict)",DFProcessing.py
89,ColumnPartitioner,"Function to create partions from Float or INT column which returns the Upper Partion Bound for a Column in a DataFrame. Inspired by the Decile Analysis, it quickly highlights the distribution of a given dataset. Args: partitions: Total Number of desired Partitions. Default 10 as a homage to DR and his love of the Decile Analysis. Exclude Blanks: Binary flag to determine whether null value records  are to be considered in the Analysis. If 1 then they are excluded, otherwise, they are given a value of 0 and included. Note that this can Materially Impact Distribution and Inference, so should be carefully considered. Exclude Zeros: Binary flag to determine whether 0 value records are to be considered in the analysis. If 1 then they are excluded, otherwise they are included. Note that this can Materially Impact Distribution and Inference, so should be carefully considered. Return Value: Value to be returned: default (""""):       DF of Value at Individual Partition Locations list_index(list):   Returns list of Index Locations in Dataframe list_value(list):   List of Value at Individual Partition Locations merge(df):          New Column in existing DF which is numerical value of segment which value belongs agg_value(df):      DF of Aggregate Value total Impact of Each Segment position_value(df)  DF of Position (Transposed Default DF) and agg_value dataframe. New Column Name: Name of New Column if original Partition is choosen. By Default, Parition is choosen.",None,"def ColumnPartitioner(df,
                      column_name,
                      new_column_name='Partition',
                      new_value_column='Total Balance in Partion',
                      partitions=10,
                      exclude_blanks=1,
                      exclude_zeros=0,
                      return_value=''):
    '''
    Function to create partions from Float or INT column which returns the Upper Partion Bound for a Column in a DataFrame. 
    Inspired by the Decile Analysis, it quickly highlights the distribution of a given dataset.
    
    Args:
        partitions:
            Total Number of desired Partitions. Default 10 as a homage to DR and his love of the Decile Analysis.
    
        Exclude Blanks:
            Binary flag to determine whether null value records  are to be considered in the Analysis. If 1 then 
            they are excluded, otherwise, they are given a value of 0 and included. Note that this can Materially 
            Impact Distribution and Inference, so should be carefully considered.
        
        Exclude Zeros:
            Binary flag to determine whether 0 value records are to be considered in the analysis. If 1 then they are excluded,
            otherwise they are included. Note that this can Materially Impact Distribution and Inference, so should be carefully
            considered.
        
        Return Value:
            Value to be returned:
            default (""""):       DF of Value at Individual Partition Locations
            list_index(list):   Returns list of Index Locations in Dataframe
            list_value(list):   List of Value at Individual Partition Locations 
            merge(df):          New Column in existing DF which is numerical value of segment which value belongs
            agg_value(df):      DF of Aggregate Value total Impact of Each Segment
            position_value(df)  DF of Position (Transposed Default DF) and agg_value dataframe.
            

        New Column Name:
            Name of New Column if original Partition is choosen. By Default, Parition is choosen.
            
    '''
    if partitions <2:
        return print('Requries a Minimum of 2 partitions, recommends no less than 3 partitions')
    
    # Make a copy to ensure no overwriting
    temp_df = df.copy()
    
    # Clean Dataset 
    if exclude_blanks ==1:
        blanks_removed = len(temp_df[temp_df[column_name].isnull()])
        #print(f""Blank Entries Removed: {blanks_removed}"")
        temp_df = temp_df[temp_df[column_name].notnull()]
    else:
        temp_df[column_name] = temp_df[column_name].fillna(0)
        
    if exclude_zeros ==1:
        zeroes_removed = len(temp_df[temp_df[column_name]==0])
        #print(f""Zero Entries Removed: {zeroes_removed}"")
        temp_df = temp_df[temp_df[column_name]!=0]
        
    column_list = temp_df[column_name].tolist()
    column_list.sort()
    length_of_df = len(column_list)
    break_point = math.ceil(length_of_df/partitions)
    
    if partitions >=length_of_df:
        return print(f'Sample Size insufficient to Warrant Calculation for column {column_name}, please review data')
        
    record_position = list(range(0,length_of_df,break_point))
    record_value = [column_list[x] for x in record_position]
    #print(record_value)
    
    
    # Parition Value DF
    
    partition_df = pd.DataFrame(record_value,index=[f""{new_column_name} {x+1}"" for x in range(len(record_value))],columns=[column_name]).T
            
    if return_value == '':
        return partition_df
    elif return_value == 'list_value':
        return record_value
    elif return_value == 'list_index':
        return record_position
    elif (return_value == 'merge')|(return_value == 'agg_value')|(return_value=='position_value'):
        temp_df = temp_df.sort_values(column_name).reset_index(drop=True)
        temp_df[new_column_name] = np.searchsorted(record_position,temp_df.index,side='right')
        
        if (return_value == 'agg_value')|(return_value=='position_value'):
            agg_impact = temp_df[['Partition',column_name]].groupby('Partition').sum()[column_name].values
            agg_impact_df = pd.DataFrame(agg_impact,
                                         columns=['VALUE'],
                                         index=[f""{new_value_column} {x+1}"" for x in range(len(agg_impact))])
            
            if return_value=='position_value':
                agg_impact_df.reset_index(drop='True',inplace=True)
                agg_impact_df['index'] = [f""{new_column_name} {x+1}"" for x in range(len(agg_impact_df))]
                agg_impact_df.set_index('index',inplace=True)
                
                temp_df1 =  partition_df.T.merge(agg_impact_df,
                                                 left_index=True,
                                                 right_index=True,
                                                 how='left').rename(columns={'VALUE':""AGGREGATE_VALUE"",""VARIANCE"":""PARTITION"",})
                return temp_df1
            return agg_impact_df
        return temp_df",DFProcessing.py
90,CalculateColumnWiseCorrelation,"Function to summarize Correlation Coeffient within dataframe Columns (or sample of columns if defined). Differs from Heatmap, in that it is a numerical lead summary. Parameters df (dataframe) column_name (list): List of Column Names to be included in summary, default to include all as indicated by None tail_head_count (int): Number of Records to return Top and Bottom (specifically includes in seperate DF) Return Multiple DataFrames",None,"def CalculateColumnWiseCorrelation(df,
                                   column_name=None,
                                   tail_head_count=5):
    
    '''
    Function to summarize Correlation Coeffient within dataframe Columns (or sample of columns if defined).
    Differs from Heatmap, in that it is a numerical lead summary.
    
    Parameters
        df (dataframe)
        column_name (list): List of Column Names to be included in summary, default to include all as indicated by None
        tail_head_count (int): Number of Records to return Top and Bottom (specifically includes in seperate DF)
    
    Return
        Multiple DataFrames
        
    '''
    
    if column_name == None:
        column_name = df.columns.tolist()
       
    df1 = TransposePivotTable(df[column_name].fillna(0).corr(),value=""CorrelationCoefficient"")
    
    BracketColumn(df1,'CorrelationCoefficient','Segment',[-1,-.5,0,.5,1])
    
    bot = df1.head(tail_head_count).copy()
    bot['Description'] = [f'Top {x} Negative Correlation' for x in range(1,tail_head_count+1)]
    
    top = df1.tail(tail_head_count).copy()
    top['Description'] = [f'Top {x-1} Positive Correlation' for x in range(tail_head_count+1,1,-1)]
    
    temp_ = pd.concat([bot,top]).reset_index(drop=True)
    
    display(temp_)
    
    return df1,temp_",DFProcessing.py
91,DFStructureReview,"Function to Create a simplified view of the overall structure of a dataframe, or where 2 similiar dataframes are present, to help understand the difference between. Args: primary_key(list): Primary Keys between",,"def DFStructureReview(df,
                      primary_key='',
                      df1=""""):
    
    '''
    Function to Create a simplified view of the overall structure of a dataframe, or where 2 similiar dataframes are 
    present, to help understand the difference between.
    
    Args:
        primary_key(list): Primary Keys between 
    
    Returns:
        dictionary
        
    '''
    
    prop_dict = {}
    
    prop_dict['TotalRecords_DF']= len(df)
    prop_dict['TotalColumns_DF'] = len(df.columns)
    
    try:
        prop_dict['UniqueRecords_DF']= len(df.drop_duplicates(primary_key))
    except:
        pass
    
    # If there are 2 dataframes:
    
    if len(df1)!=0:
        cols_df =  set(df.columns.values)
        cols_df1 = set(df1.columns.values)
    
        prop_dict['cols_missing_df']  = [x for x in list(cols_df1) if x not in cols_df]
        prop_dict['cols_missing_df1'] = [x for x in list(cols_df) if x not in cols_df1]
        prop_dict['TotalRecords_DF_1'] = len(df1)
        prop_dict['TotalColumns_DF1']  = len(df1.columns)
        
        try:
            temp_df = df[primary_key].merge(df1[primary_key],on=primary_key,how='outer',indicator=True)
            prop_dict['UniqueRecords_DF1'] = len(df1.drop_duplicates(primary_key))
            prop_dict['SharedRecords']     = len(temp_df[temp_df['_merge']=='both'])
            prop_dict['Only_DF']           = len(temp_df[temp_df['_merge']=='left_only'])
            prop_dict['Only_DF1']          = len(temp_df[temp_df['_merge']=='right_only'])
        
        except:
            pass

    return prop_dict",DFProcessing.py
92,ColumnStatisticalCompare,"Function to Compare Two Columns, primarily derived for TimeSeries Analysis. Parameters:",,"def ColumnStatisticalCompare(df,df1,column_name):
    '''
    Function to Compare Two Columns, primarily derived for TimeSeries Analysis.


    Parameters:

    
    Returns:

    
    
    '''

    a = ColumnStatisticalReview(df,column_name,top_x_records=0).rename(columns={column_name:f'{column_name}_DF'})
    b = ColumnStatisticalReview(df1,column_name,top_x_records=0).rename(columns={column_name:f'{column_name}_DF1'})
    
    temp_df = pd.concat([a,b],axis=1)
    temp_df[f'{column_name}_VAR_AMT'] = temp_df[f'{column_name}_DF'] - temp_df[f'{column_name}_DF1'] 
    temp_df[f'{column_name}_VAR_PERC'] = temp_df[f'{column_name}_VAR_AMT']/temp_df[f'{column_name}_DF1']
    temp_df[f'{column_name}_VAR_PERC'] = temp_df[f'{column_name}_VAR_PERC'].fillna(0)
    
    return temp_df",DFProcessing.py
93,DataFrameColumnObservations,Function to provide a quick summary of how a Dataframe is distributed amongst a set of determined columns. Parameters: columns (list): Columns to which you wish to be invluded include_obs (int): Binary Flag to indicate whether column Observation Count Totals are included include_perc (int): Binary Flag to indicate whether column Percentage Calculations are to be included,,"def DataFrameColumnObservations(df,
                                columns,
                                include_obs=1,
                                include_perc=1):
    
    '''
    Function to provide a quick summary of how a Dataframe is distributed amongst a set of determined columns.
    
    
    Parameters:
        columns (list): Columns to which you wish to be invluded
        include_obs (int): Binary Flag to indicate whether column Observation Count Totals are included
        include_perc (int): Binary Flag to indicate whether column Percentage Calculations are to be included
    
    Returns:
        Dataframe
    
    '''

    temp_df = df[columns].copy()
    temp_df['COUNT'] = 1
    
    final_df = temp_df.fillna("""").groupby(columns).sum().reset_index()
    
    for column in columns:
        temp_df1 = temp_df[[column,'COUNT']].groupby(column).sum().reset_index().rename(columns={'COUNT':f""{column}_OBS""})
        final_df = final_df.merge(temp_df1,on=column,how='left')

    if include_perc==1:
        final_df['PERC_DATA'] = final_df['COUNT']/len(temp_df)
        final_df = final_df.sort_values('COUNT',ascending=False)
        final_df['CUMMULATIVE_PERC'] = final_df['PERC_DATA'].cumsum()
        
        for column in columns:
            final_df[f""{column}_PERC""] = final_df[f""{column}_OBS""]/len(temp_df)
        
    if include_obs==0:
        return final_df.drop([f""{column}_OBS"" for x in columns],axis=1)
    
    else:
        return final_df",DFProcessing.py
94,MissingCartesianProducts,"Function which Looks at the Combination of Two Lists and explores all possible Combinations. Developed for the purpose of understanding how many combinations exist and generating a list of Values which do not exist, this list can be valueable for pending to previously Aggregagted Datasets to insure all possible records have representation Parameters: list1_ (list) list2_ (list) columns (Columns to be included, should represent the expected Column Name of list1_ and list2_) merge_df (Dataframe): To be used to Validate the number of missing records, if not included, then returns only combination.",,"def MissingCartesianProducts(list1_,
                             list2_,
                             columns,
                             merge_df=None,
                             remove_values=['0',"""",'N/A']):
    '''
    Function which Looks at the Combination of Two Lists and explores all possible Combinations. 
    Developed for the purpose of understanding how many combinations exist and generating a list of Values which do not
    exist, this list can be valueable for pending to previously Aggregagted Datasets to insure all possible records have
    representation
    
    Parameters:
        list1_ (list)
        list2_ (list)
        columns (Columns to be included, should represent the expected Column Name of list1_ and list2_)
        merge_df (Dataframe): To be used to Validate the number of missing records, if not included, then 
        returns only combination.
    
    Returns:
    
    
    '''

    from DFProcessing import CombineLists
    
    list1_ = [x for x in list1_ if x not in remove_values]
    list2_ = [x for x in list2_ if x not in remove_values]
    
    required_records = CombineLists([list1_,list2_])
    df = pd.DataFrame(required_records,columns=columns)

    
    if len(merge_df)==0:
        return df
    
    else:
        df = df.merge(merge_df[columns].drop_duplicates(),on=columns,how='left',indicator=True)
        print(f""Distribution of Records and Missing Records:\n{df['_merge'].value_counts()}"")
        df = df[df['_merge']=='left_only'].drop('_merge',axis=1)
        return df",DFProcessing.py
95,TranposeNonTimeSeriesDF,"Transposes a non-time-series DataFrame from wide to long format by melting specified columns. This is especially useful for flattening columns into a single column to support tools like Power BI, where long format enables dynamic pivoting and aggregation. Parameters: df (DataFrame): The input pandas DataFrame. index (list): Columns to retain as identifiers (will remain unchanged). columns (list): Columns to unpivot into key-value pairs.",,"def TranposeNonTimeSeriesDF(df, index, columns):
    '''
    Transposes a non-time-series DataFrame from wide to long format by melting specified columns.

    This is especially useful for flattening columns into a single column to support tools 
    like Power BI, where long format enables dynamic pivoting and aggregation.

    Parameters:
        df (DataFrame): The input pandas DataFrame.
        index (list): Columns to retain as identifiers (will remain unchanged).
        columns (list): Columns to unpivot into key-value pairs.

    Returns:
        DataFrame: A long-format DataFrame with 'variable' and 'value' columns.
    '''
    return df.melt(id_vars=index, value_vars=columns)",DFProcessing.py
96,ColumnElementalChange,"Function to Compare the Element Level change of two dataframes. Includes: Summary of Records Increasing and Decreasing, A summary of where the respective partitions are Args: column_name (str): Name of Column to which you wish to analyze primary_key (list): List of Primary Key(s), used for purposes of Merging df and df1",,"def ColumnElementalChange(df,
                          df1,
                          column_name,
                          primary_key=['MEMBERNBR']):
    '''
    Function to Compare the Element Level change of two dataframes. Includes: Summary of Records Increasing and Decreasing,
    A summary of where the respective partitions are 
    
    Args:
        column_name (str): Name of Column to which you wish to analyze
        primary_key (list): List of Primary Key(s), used for purposes of Merging df and df1
        
    Returns:
        Single Rowed Dataframe including a summary of Record Changing Count (Increase, Decrease, No Change), 
        Position of respective Partitions in Dataset (remembering Position 0 is MIN value and position -1 is 90% value)
        Total Value Change which is attributed to respective Partition.
        
    '''
    
    # Using primary Key and Column Name create conditions for Merge
    key = primary_key.copy()
    key.append(column_name)    
    
    # Merge DF and DF1 for record level comparison 
    temp_df = df[key].rename(columns={column_name:f""{column_name}_DF""}).merge(df1[key].rename(columns={column_name:f""{column_name}_DF1""}),
                                                                                                       on=primary_key,
                                                                                                       how='outer',
                                                                                                       indicator=True)
    # Create a Column to Track Change at Record Level 
    
    try:
        temp_df['VARIANCE'] = temp_df[f""{column_name}_DF""].fillna(0) - temp_df[f""{column_name}_DF1""].fillna(0)
        
        # Create a variable which defines change in text terms, Increase, Decrease, No Change, Null Value Detered for VALUES
        CategorizeBinaryChange(temp_df,'VARIANCE')
    
    except:
        temp_df['VARIANCE'] = np.where(temp_df[f""{column_name}_DF""].fillna(0) != temp_df[f""{column_name}_DF1""].fillna(0),1,0)
        print(temp_df['VARIANCE'].sum())

    # Developing Components that deal with text.
    return temp_df
        
    
    # Add Partition Column for Purposes of Calculating Aggregate Change
    agg_impact_df = ColumnPartitioner(temp_df,'VARIANCE',return_value='agg_value')
    
    # Create a Dataframe Stacked Vertically for Increase, Decrease, No CHange
    chg_val_df = pd.DataFrame(temp_df['CHANGE_CLASSIFICATION'].value_counts()).rename(columns={'CHANGE_CLASSIFICATION':'VALUE'})

    # Create a DataFrame Stacked Vertically for Decile Partions
    partition_val_df = ColumnPartitioner(temp_df,'VARIANCE',new_column_name='Variance Partition').T.rename(columns={'VARIANCE':'VALUE'})

    # Add all individual elements
    final_df = pd.concat([chg_val_df,
                          partition_val_df,
                          agg_impact_df])
                        
    return final_df",DFProcessing.py
97,ConvertDicttoDF,Function to convert a straight Dictionary into a Dataframe. Parameters dict_ (dict) key_name (str): Name of the column for dictionary keys. Default is 'KEY'. value_name (str): Name of the column for dictionary values. Default is 'VALUE'. Returns DataFrame,None,"def ConvertDicttoDF(dict_, key_name=""KEY"", value_name=""VALUE""):
    '''
    Function to convert a straight Dictionary into a Dataframe.

    Parameters
    dict_ (dict)
    key_name (str): Name of the column for dictionary keys. Default is 'KEY'.
    value_name (str): Name of the column for dictionary values. Default is 'VALUE'.

    Returns
        DataFrame
    '''
    return pd.DataFrame.from_dict(dict_, orient='index', columns=[value_name]).reset_index().rename(columns={'index': key_name})",DFProcessing.py
98,ConvertListstoDF,"Function to Take a Dictionary of Lists and convert into a Dataframe. dict_lists = {'Column1':list1,'Column2':List2} Parameter: Dictionary of Lists, Keys will become column Name Returns DataFrame",None,"def ConvertListstoDF(dict_lists):
    '''
    Function to Take a Dictionary of Lists and convert into a Dataframe. 

    dict_lists = {'Column1':list1,'Column2':List2}

    Parameter:
        Dictionary of Lists, Keys will become column Name

    Returns
        DataFrame
    
    

    '''

    return pd.DataFrame(dict_lists)",DFProcessing.py
99,SummarizedDataSetforBITool,Builds a DataFrame with all combinations of ALL-level rollups across the specified dimensions and metrics. Parameters: df (pd.DataFrame): Input DataFrame. dimensions (list of str): Dimension column names. metrics (list of str): Metric column names to aggregate.,,"def SummarizedDataSetforBITool(df, dimensions, metrics):
    """"""
    Builds a DataFrame with all combinations of ALL-level rollups 
    across the specified dimensions and metrics.

    Parameters:
        df (pd.DataFrame): Input DataFrame.
        dimensions (list of str): Dimension column names.
        metrics (list of str): Metric column names to aggregate.

    Returns:
        pd.DataFrame: Aggregated DataFrame with 'ALL' rollups.
    """"""
    result_frames = []
    
    available_metrics = [x for x in metrics if x in df.columns]

    for r in range(len(dimensions) + 1):
        for dims in itertools.combinations(dimensions, r):
            group_cols = list(dims)
            
            # Aggregate metrics with or without groupby
            if group_cols:
                agg_df = df.groupby(group_cols, dropna=False)[available_metrics].sum().reset_index()
            else:
                # Grand total (ALL for all dims)
                sums = df[available_metrics].sum().to_frame().T
                agg_df = sums
                for col in dimensions:
                    agg_df[col] = 'ALL'

            # Fill missing dimension columns with 'ALL'
            for col in dimensions:
                if col not in group_cols:
                    agg_df[col] = 'ALL'

            # Ensure consistent column order
            agg_df = agg_df[dimensions + available_metrics]
            result_frames.append(agg_df)

    final_df = pd.concat(result_frames, ignore_index=True)
    return final_df",DFProcessing.py
100,CreateCalculatedField,"calc_instructions = [ {'type': 'sum', 'value1': 'LENDING', 'name': 'TOTAL_LENDING'}, {'type': 'weighted_average', 'value1': 'LENDING', 'value2': 'INTEREST_RATE', 'name': 'WEIGHTED_INTEREST'}, {'type': 'ratio', 'value1': 'RENEWED_AMOUNT', 'value2': 'MATURED_AMOUNT', 'name': 'RENEWAL_RATE'} ] output = CreateCalculatedField(final_df, ['BRANCHNAME', 'CITY', 'LOB', 'DURATION'], calc_instructions) Create as initially Used in Data Management Dashboard Accumulation. Was not pursed Overtly Complex.",None,"def CreateCalculatedField(df, primary_key, calc_instructions, include_all=1):

    '''
    calc_instructions = [
    {'type': 'sum', 'value1': 'LENDING', 'name': 'TOTAL_LENDING'},
    {'type': 'weighted_average', 'value1': 'LENDING', 'value2': 'INTEREST_RATE', 'name': 'WEIGHTED_INTEREST'},
    {'type': 'ratio', 'value1': 'RENEWED_AMOUNT', 'value2': 'MATURED_AMOUNT', 'name': 'RENEWAL_RATE'}
    ]

    output = CreateCalculatedField(final_df, ['BRANCHNAME', 'CITY', 'LOB', 'DURATION'], calc_instructions)

    

    Create as initially Used in Data Management Dashboard Accumulation. Was not pursed Overtly Complex.

    '''

    base_aggs = {}
    
    # Collect all fields we need
    for calc in calc_instructions:
        if calc['type'] == 'sum':
            base_aggs[calc['name']] = (calc['value1'], 'sum')
        elif calc['type'] == 'weighted_average':
            base_aggs[f""__{calc['name']}_NUM""] = (
                calc['value2'], lambda x, col=calc['value1']: (df.loc[x.index, col] * x).sum()
            )
            base_aggs[f""__{calc['name']}_DEN""] = (calc['value1'], 'sum')
        elif calc['type'] == 'ratio':
            base_aggs[f""__{calc['name']}_NUM""] = (calc['value1'], 'sum')
            base_aggs[f""__{calc['name']}_DEN""] = (calc['value2'], 'sum')

    # Base groupby
    grouped = df.groupby(primary_key, dropna=False).agg(**base_aggs).reset_index()

    # Compute post-aggregates
    for calc in calc_instructions:
        if calc['type'] == 'sum':
            continue
        elif calc['type'] == 'weighted_average':
            num = grouped[f""__{calc['name']}_NUM""]
            den = grouped[f""__{calc['name']}_DEN""]
            grouped[calc['name']] = np.where(den != 0, num / den, np.nan)
            grouped.drop(columns=[f""__{calc['name']}_NUM"", f""__{calc['name']}_DEN""], inplace=True)
        elif calc['type'] == 'ratio':
            num = grouped[f""__{calc['name']}_NUM""]
            den = grouped[f""__{calc['name']}_DEN""]
            grouped[calc['name']] = np.where(den != 0, num / den, np.nan)
            grouped.drop(columns=[f""__{calc['name']}_NUM"", f""__{calc['name']}_DEN""], inplace=True)

    result_frames = [grouped.copy()]

    # Rollup combinations
    if include_all:
        for r in range(1, len(primary_key)):
            for group_cols in combinations(primary_key, r):
                temp = df.copy()
                for col in primary_key:
                    if col not in group_cols:
                        temp[col] = 'All'
                temp_group = CreateCalculatedField(temp, primary_key, calc_instructions, include_all=0)
                result_frames.append(temp_group)

        # Full 'All' row
        temp = df.copy()
        for col in primary_key:
            temp[col] = 'All'
        temp_group = CreateCalculatedField(temp, primary_key, calc_instructions, include_all=0)
        result_frames.append(temp_group)

    return pd.concat(result_frames, ignore_index=True)",ArchivedFunctions.py
101,ClassificationMetrics,"Function to generate summary statitics related to a ML Model Run. Removed Classification of TP,FN, FP, TN from df as when running many models quantum of data is high ,also it doesn't make a ton of sense in Multivariante classifications. Parameters: df (pDataFrame): DataFrame with predictions and actual values. prediction (str): Name of prediction column. actual (str): Name of actual/true label column. AUC_Score (float): Optional AUC. new_column_name (str): Name of column to store TP/TN/FP/FN tags.",,"def ClassificationMetrics(df,
                          prediction='PREDICTION',
                          actual='ACTUAL',
                          AUC_Score=0,
                          new_column_name='RESULT'):
    """"""
    Function to generate summary statitics related to a ML Model Run.
    Removed Classification of TP,FN, FP, TN from df as when running many models quantum of data is high ,also it doesn't make a ton of sense in Multivariante classifications.
    
    Parameters:
        df (pDataFrame): DataFrame with predictions and actual values.
        prediction (str): Name of prediction column.
        actual (str): Name of actual/true label column.
        AUC_Score (float): Optional AUC.
        new_column_name (str): Name of column to store TP/TN/FP/FN tags.

    Returns:
        metrics_summary_df (DataFrame)
    """"""

    classes = sorted(df[actual].unique())
    metrics_list = []
    df = df.copy()

    for cls in classes:
        # Create One-vs-Rest binary views
        is_actual = df[actual] == cls
        is_pred   = df[prediction] == cls

        # Aggregate metrics per class
        TP = ( is_pred &  is_actual).sum()
        FP = ( is_pred & ~is_actual).sum()
        FN = (~is_pred &  is_actual).sum()
        TN = (~is_pred & ~is_actual).sum()

        precision = TP / (TP + FP) if (TP + FP) > 0 else 0
        recall    = TP / (TP + FN) if (TP + FN) > 0 else 0
        f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        accuracy  = (TP + TN) / (TP + FP + FN + TN) if (TP + FP + FN + TN) > 0 else 0

        metrics_list.append({
            ""Class"": cls,
            ""TP"": TP,
            ""FP"": FP,
            ""FN"": FN,
            ""TN"": TN,
            ""Precision"": precision,
            ""Recall"": recall,
            ""F1"": f1,
            ""Accuracy"": accuracy,
            ""AUC"": AUC_Score
        })

    result_df = pd.DataFrame(metrics_list)
    
    result_df.loc['Total',:] = result_df.mean()
    result_df['Class'] = result_df['Class'].astype(str)
    result_df.loc['Total','Class'] = 'Macro'
    
    return result_df",MLPipeline.py
102,apply_scaling,"Applies optional scaling to training and test datasets. Parameters: X_train (np.array or pd.DataFrame): Training features. X_test (np.array or pd.DataFrame): Test features. scaler (str or None): Type of scaling to apply. Options: 'standard', 'normalization', or None.",,"def apply_scaling(X_train, X_test, scaler=None):
    """"""
    Applies optional scaling to training and test datasets.
    
    Parameters:
        X_train (np.array or pd.DataFrame): Training features.
        X_test (np.array or pd.DataFrame): Test features.
        scaler (str or None): Type of scaling to apply. 
                              Options: 'standard', 'normalization', or None.
    
    Returns:
        Scaled X_train and X_test.
    """"""
    scalers = {
        'standard': StandardScaler(),
        'normal': MinMaxScaler()
    }

    if scaler in scalers:
        scaler_instance = scalers[scaler]
        X_train = scaler_instance.fit_transform(X_train)
        X_test = scaler_instance.transform(X_test)

    return X_train, X_test, scaler_instance",MLPipeline.py
103,SKLearnModelList,"Function to generate a list of all scikit-learn estimators with type classification. Imports List from all_estimators (sklearn.utils) Imports D-Data Dashboard and Merges Column Dataset Size, to help filter for model processes. Parameter: Regressor_type: returns a list of relevant regressors if a specific regressor type is Picked. Types include:",,"def SKLearnModelList(regressor_type=None):
    '''
    Function to generate a list of all scikit-learn estimators with type classification.
    
    Imports List from all_estimators (sklearn.utils) 
    Imports D-Data Dashboard and Merges Column Dataset Size, to help filter for model processes.

    Parameter:
        Regressor_type: returns a list of relevant regressors if a specific regressor type is Picked. Types include: 
    
    Returns:
        df_final (pd.DataFrame): DataFrame of all estimators with class path, type labels, and module split.
    '''
    
    # 1. Get all estimators
    estimators = all_estimators(type_filter=None)
    
    # 2. Convert to DataFrame
    df = pd.DataFrame(estimators, columns=['Model Name', 'Estimator Class'])

    # Merge In Dataset Size to prevent running of REALLY slow models.
    temp = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vQq1-3cTas8DCWBa2NKYhVFXpl8kLaFDohg0zMfNTAU_Fiw6aIFLWfA5zRem4eSaGPa7UiQvkz05loW/pub?output=csv')[['Word','Dataset Size']].rename(columns={'Word':'Model Name'})
    df = df.merge(temp,on='Model Name',how='left')
    
    # 3. Extract full class path
    df['Full Class Path'] = df['Estimator Class'].astype(str).str.replace(""<class '"", """", regex=False).str.replace(""'>"", """", regex=False)
    
    # 4. Extract class type via mixin inspection
    def get_estimator_types(cls):
        types = []
        try:
            if issubclass(cls, ClassifierMixin):
                types.append(""classifier"")
            if issubclass(cls, RegressorMixin):
                types.append(""regressor"")
            if issubclass(cls, ClusterMixin):
                types.append(""cluster"")
            if issubclass(cls, TransformerMixin):
                types.append(""transformer"")
        except:
            types.append(""unknown"")
        return ', '.join(types) if types else 'unknown'
    
    df['Estimator Type'] = df['Estimator Class'].apply(get_estimator_types)
    
    # 5. Split module path
    path_split = df['Full Class Path'].str.split('.', expand=True)
    path_split.columns = [f'Part_{i+1}' for i in range(path_split.shape[1])]
    
    # 6. Combine and return
    df_final = pd.concat([df, path_split], axis=1)

    if regressor_type==None:
        return df_final
    else:
        return df_final[df_final['Estimator Type'].str.contains(regressor_type)]",MLPipeline.py
104,CalculateRegressionPerformance,"Function to Generate a series of Performance Metrics for Regression Models. Parameters: df (Dataframe) p (int): Predictors, Number of X Variables (Independent Variables in Model) y (float): Target Value, Actual Observed Value y_pred (float) : Y-Hat, Model Predicted Value",None,"def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df[y].to_numpy()
    y_pred = df[y_pred].to_numpy()
    
    dict_ = {}
    dict_['MAE'] = mean_absolute_error(y, y_pred)
    dict_['MSE'] = mean_squared_error(y, y_pred)
    dict_['RMSE'] = np.sqrt(dict_['MSE'])
    dict_['R2'] = r2_score(y, y_pred)
    dict_['MAPE'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['SMAPE'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['ADJ_R2'] = 1 - (1 - dict_['R2']) * (n - 1) / (n - p - 1)

    return pd.DataFrame(dict_.values(),index=dict_.keys(),columns=['VALUES']).T",MLPipeline.py
105,MLPipelineSample,"Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. Models are selected from SKlearn library based on model Type. This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well. Parameters: df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change). scaler (str): One of None, 'normal', or 'standard'. ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer' target_column (str): Name of the target column. sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size. run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models. test_size (float): Proportion of data used for testing.",,"def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """"""
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """"""

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]

    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()
        
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, ""predict_proba""):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class=""ovr"")
                    print(AUC_Score)
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X.columns),y='ACTUAL',y_pred='PREDICTION')
            
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f""Model Generation and Results Failed: {e}\n"")
    return results_df",MLPipeline.py
106,ConvertListtoSQLText,"Function to convert a python list into SQL code, of various formatinng. If return_value is CTE, then Python List will be turning into a SQL statement, with Column Name of COLUMN_NAME, and then a SQL query to merge various tables from whatever the statement is. Parameters: List_(list): Python List return_vaule(str): Indicator to control If statements in Function, default to return a raw list. column_name (str): In combination with Return Value, name of column in SQL Table Creation. sql_query: Statement after the CTE table statement.",None,"def ConvertListtoSQLText(list_, 
                         return_value=None, 
                         column_name=None,
                         sql_query=None):
    """"""
    Function to convert a python list into SQL code, of various formatinng. 

    If return_value is CTE, then Python List will be turning into a SQL statement, with Column Name of COLUMN_NAME, and then a SQL query to merge various 
    tables from whatever the statement is.


    Parameters:
            List_(list): Python List
            return_vaule(str): Indicator to control If statements in Function, default to return a raw list.
            column_name (str): In combination with Return Value, name of column in SQL Table Creation.
            sql_query: Statement after the CTE table statement.
    

    """"""
    if return_value == 'INT':
        return ', '.join(f""'{int(x)}'"" for x in list_)

    elif return_value == 'CTE':
        
        if not sql_query:
            sql_query = 'select * from CTE_TABLE'
        column_data  = "" UNION ALL\n"".join(f""SELECT {int(x)}"" for x in list_)
        sql1 = f"""""" WITH CTE_TABLE ({column_name}) AS ( {column_data} ) {sql_query}""""""
        
        return TIME_SQL(sql1)
        
    else:
        # String version
        return ', '.join(f""'{str(x)}'"" for x in list_)",SQL.py
107,generate_create_table_sql,"Function to create a SQL Statement to Create a New Table. Function Utilizes a review of the DataFrame to recommend Column Names, Formating and appropriate Column Sizing. Function is NOT BEYOND REPROACH, requires some manual review and should not be automated. Parameters: df(df): Any DataFrame table_name(str): desired name for the SQL table schema(str): Target Schema Name db(str): Database Name (function designed for Analytics, but can be applied to any MS SQL)",,"def generate_create_table_sql(df, table_name, schema, db='ANALYTICS'):
    """"""
    Function to create a SQL Statement to Create a New Table.
    Function Utilizes a review of the DataFrame to recommend Column Names, Formating and appropriate Column Sizing.
    
    Function is NOT BEYOND REPROACH, requires some manual review and should not be automated.
    

    Parameters:
    df(df): Any DataFrame
    table_name(str): desired name for the SQL table
    schema(str): Target Schema Name
    db(str): Database Name (function designed for Analytics, but can be applied to any MS SQL)

    Returns:
    - str: SQL CREATE TABLE statement
    
    """"""

    type_mapping = {
        ""int64"": ""BIGINT"",
        ""int32"": ""INT"",
        ""float64"": ""FLOAT"",
        ""float32"": ""FLOAT"",
        ""bool"": ""VARCHAR(5)"",
        ""datetime64[ns]"": ""DATE"",
        ""object"": ""VARCHAR""
    }

    max_len = max(len(col) for col in df.columns)

    column_defs = []
    for col in df.columns:
        dtype = str(df[col].dtype)

        if col == ""MEMBERNBR"":
            sql_type = 'BIGINT'
        elif col == ""ACCTNBR"":
            sql_type = 'BIGINT'
        elif col.lower().find('date')!=-1:
            sql_type = 'DATE'
        elif col.lower().find('flag')!=-1:
            sql_type = 'BIT'
        
        elif dtype == 'object':
            max_str_len = df[col].astype(str).map(len).max()
            varchar_len = get_varchar_bucket(min(max_str_len + 10, 255))
            sql_type = f""VARCHAR({varchar_len})""
        else:
            sql_type = type_mapping.get(dtype, ""VARCHAR(100)"")  # fallback for unexpected types

        # Use [col] instead of 'col' for SQL Server compatibility
        column_defs.append(f""    [{col}] {sql_type}"")

    column_sql = "",\n"".join(column_defs)
    full_table_name = f""[{db}].[{schema}].[{table_name}]""
    create_stmt = (
        f""CREATE TABLE {full_table_name} (\n""
        f""{column_sql}\n);\n""
        f""GO\n""
        f""GRANT SELECT, UPDATE, DELETE ON {full_table_name} TO [Python_User];""
    )

    print(create_stmt)",SQL.py
108,TableRecordCountByDate,"Generate a SQL Server query to count records by day for each table and pivot them. Parameters: dates (List[datetime.datetime]): List of dates to pivot on. table_dict (Dict[str, str]): Mapping of table name -> date column name.",,"def TableRecordCountByDate(table_dict,
                           end_date=None,
                           total_days=15):
    
    """"""
    Generate a SQL Server query to count records by day for each table and pivot them.
    
    Parameters:
        dates (List[datetime.datetime]): List of dates to pivot on.
        table_dict (Dict[str, str]): Mapping of table name -> date column name.
        
    Returns:
        str: A full SQL query string.
    """"""
    
    if not end_date:
        end_date = datetime.datetime.now().replace(hour=0,minute=0,second=0,microsecond=0)
    
    start_date = end_date - datetime.timedelta(days=total_days)
    
    dates = generate_day_list(start_date=start_date,end_date=end_date)
    
    min_date = dates[0].strftime('%Y-%m-%d')
    max_date = dates[-1].strftime('%Y-%m-%d')

    # Generate the UNION ALL query block
    union_parts = []
    for table, date_column in table_dict.items():
        block = f""""""
        SELECT 
            '{table}' AS TableName,
            CAST({date_column} AS DATE) AS CreatedDate,
            COUNT(*) AS Cnt
        FROM {table}
        WHERE {date_column} BETWEEN '{min_date}' AND '{max_date}'
        GROUP BY CAST({date_column} AS DATE)
        """"""
        union_parts.append(block.strip())

    union_query = ""\n    UNION ALL\n    "".join(union_parts)

    # Format dates as [YYYY-MM-DD] for pivot columns
    pivot_columns = "", "".join(f""[{d.strftime('%Y-%m-%d')}]"" for d in dates)

    # Combine full query
    full_query = f""""""
    SELECT TableName, {pivot_columns}
    FROM (
        {union_query}
    ) AS SourceData
    PIVOT (
        SUM(Cnt)
        FOR CreatedDate IN ({pivot_columns})
    ) AS PivotResult
    ORDER BY TableName;
    """""".strip()

    return full_query",SQL.py
109,SampleDataFrame,"Returns a random sample from a DataFrame based on confidence level and margin of error. Parameters: df (pd.DataFrame): The dataset to sample from. conf(float): Desired Confidence Percentage Level (e.g., 90, 95, 99). me (float): Margin of Error, (default is 5%). mv (float): Maximum Variability (Expected Level of Default)",,"def SampleDataFrame(df, 
                    conf=.95, 
                    me=0.05,
                    mv=0.5,
                    print_=0,
                    new_column_name=""""):
    """"""
    Returns a random sample from a DataFrame based on confidence level and margin of error.

    Parameters:
        df (pd.DataFrame): The dataset to sample from.
        conf(float): Desired Confidence Percentage Level (e.g., 90, 95, 99).
        me (float): Margin of Error, (default is 5%).
        mv (float): Maximum Variability (Expected Level of Default)

    Returns:
        pd.DataFrame: A random sample of the required size.
    """"""
    df = df.copy()
    
    if not 0 <= mv <= 1:
        raise ValueError(""mv (failure rate) must be between 0 and 1."")

    N = len(df)
    if N == 0:
        raise ValueError(""DataFrame is empty"")

    # Calculate the Z-score based on the confidence level
    z = norm.ppf(1 - (1 - conf) / 2)
    

    # Calculate the initial sample size (without finite population correction)
    n0 = (z**2 * mv * (1 - mv)) / (me**2)
    
    # Apply finite population correction if the population is smaller than 100,000
    if N >= 10000:  # For large populations, skip the correction
        n = int(n0)
    else:
        n = int((n0 * N) / (n0 + N - 1))

    if print_==1:
        print(f""Z-score: {z}"")  # Debug Z-score
        print(f""Initial sample size (n0): {n0}"")  # Debug n0
        print(f""Sample size with FPC: {n}"")  # Debug final sample size
    
    sample = df.sample(n=n, random_state=42)
    
    if len(new_column_name)==0:
        return sample 

    else:
        sample_index = sample.index
        df[new_column_name] = 0
        df.loc[sample_index, new_column_name] = 1
        return df",Validation.py
110,ReviewEntireDataframe,No description available,None,"def ReviewEntireDataframe(df,file_name=None):
    
    final_df = pd.DataFrame()
    
    for column in df.columns:
        start_time = timeit.default_timer()
        temp_df = ColumnStatisticalReview(df,column)
        print(f'Elapsed time to process {column}:{timeit.default_timer() - start_time:,.2f}')
        final_df = pd.concat([final_df,temp_df],axis=1)
    if file_name:
        final_df.to_csv(f""{file_name}.csv"")
        
    return final_df",Validation.py
111,ColumnStatisticalReview,"Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution of values. Args: column_name (str): Name of Column partitions (int): Number of partitions to include (Decile 10) exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination. If blank values are excluded it gives a better representation for the members of the set, however it might provide a misleading representation of the population. exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as such it can include both blanks and true 0 values.",None,"def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])
    
    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df

    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index().rename(columns={column_name:'count','index':column_name})
        if len(top_instances)>0:
            top_instances[column_name] = top_instances.apply(lambda row: f""Value: {row[column_name]}, Frequency: {row['count']}"", axis=1)
            top_instances['index'] = [f""Top {x+1}"" for x in range(len(top_instances[column_name]))]
            top_instances = top_instances.drop('count',axis=1).set_index('index')
            temp_df = pd.concat([temp_df,top_instances])
        
    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})

        return pd.concat([temp_df,segment_df.T,seg_val_df])
    return temp_df",Validation.py
