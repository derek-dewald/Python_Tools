{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486b7b1-2bfe-41cd-a76c-0b27a2e15034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_similarity_matrix(snap: pd.DataFrame, preprocessor) -> tuple[np.ndarray, dict]:\n",
    "    member_ids = snap[\"MEMBERNBR\"].to_numpy()\n",
    "    X = preprocessor.fit_transform(snap)\n",
    "    S = cosine_similarity(X)\n",
    "    member_index = {mid: i for i, mid in enumerate(member_ids)}\n",
    "    return S, member_index\n",
    "\n",
    "def topk_similar_members(S: np.ndarray, member_index: dict, member_id: int, k: int = 10):\n",
    "    idx = member_index[member_id]\n",
    "    scores = S[idx].copy()\n",
    "    scores[idx] = -1  # exclude self\n",
    "    topk_idx = np.argsort(-scores)[:k]\n",
    "    return topk_idx, scores[topk_idx]\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def fit_knn_index(snap: pd.DataFrame, preprocessor, metric=\"cosine\"):\n",
    "    member_ids = snap[\"MEMBERNBR\"].to_numpy()\n",
    "    X = preprocessor.fit_transform(snap)\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=50, metric=metric, algorithm=\"auto\")\n",
    "    nn.fit(X)\n",
    "    member_index = {mid: i for i, mid in enumerate(member_ids)}\n",
    "    return nn, member_index, member_ids, X\n",
    "\n",
    "def knn_topk(nn, member_index, member_ids, X, member_id, k=10):\n",
    "    \n",
    "    idx = member_index[member_id]\n",
    "    distances, indices = nn.kneighbors(X[idx:idx+1], n_neighbors=k+1)  # includes self\n",
    "    \n",
    "    indices = indices.ravel()\n",
    "    distances = distances.ravel()\n",
    "    \n",
    "    mask =  indices != idx\n",
    "\n",
    "    # drop self (first hit)\n",
    "    indices = indices[mask][:k]\n",
    "    distances = distances[mask][:k]\n",
    "    sim = 1 - distances \n",
    "    return member_ids[indices], sim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _month_floor(s):\n",
    "    # works for python date or datetime-like\n",
    "    return pd.to_datetime(s).dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "def _slope(y):\n",
    "    \"\"\"Simple linear slope vs time index 0..n-1; returns 0 if too short/flat.\"\"\"\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    n = len(y)\n",
    "    if n < 2 or np.allclose(y, y[0]):\n",
    "        return 0.0\n",
    "    x = np.arange(n, dtype=float)\n",
    "    # slope of least squares line\n",
    "    return float(np.cov(x, y, bias=True)[0, 1] / np.var(x))\n",
    "\n",
    "\n",
    "def iterate_function_over_dataframe(df,\n",
    "                                    function,\n",
    "                                    column_exclusion_list=[],\n",
    "                                    print_=True,\n",
    "                                    pause_threshold=20,\n",
    "                                    **kwargs):\n",
    "    \n",
    "    '''\n",
    "    Function to Apply ColumnStatisticalReview to a entire Dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "        df(Dataframe): DataFrame to apply entire dataframe to ColumnStatisticalREview Function\n",
    "        file_name(Str): Optional Argument to produce a .csv File OUtput\n",
    "        print(bin): Optional Argument to generate a summary note of how long individual refresh took.\n",
    "        time_check: Optional Argument to place Opt out to prevent program from Timing out. If you pass, the program will add\n",
    "        5 seconds each time between prompting, based on the understanding that we want to minimize hte number of promptings\n",
    "        and if you were willing to wait the previous time, then waiting 5 more seconds is likely.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame\n",
    "        (Optional .csv File in default directory, can also include specific path in file name)\n",
    "        \n",
    "    '''\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for column in [x for x in df.columns if x not in column_exclusion_list]:\n",
    "        start_time = timeit.default_timer()\n",
    "        temp_df = function(df,column,**kwargs)\n",
    "        elapsed_time = timeit.default_timer() - start_time\n",
    "        final_df = pd.concat([final_df,temp_df],axis=1)\n",
    "        time_check = PauseProcess(elapsed_time,pause_threshold,column)\n",
    "        if print_:\n",
    "            print(f'Elapsed time to process {column}:{timeit.default_timer() - start_time:,.2f}')\n",
    "                    \n",
    "    return final_df.T\n",
    "\n",
    "\n",
    "def compute_vif(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute VIF for each column in a numeric dataframe using statsmodels.\n",
    "    Assumes df is numeric and has no NaNs/Infs.\n",
    "    \"\"\"\n",
    "    X = df.values\n",
    "    vif_values = []\n",
    "    for j in range(X.shape[1]):\n",
    "        vif_values.append(variance_inflation_factor(X, j))\n",
    "    return pd.DataFrame({\"feature\": df.columns, \"VIF\": vif_values}).sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "\n",
    "def fast_vif(df, eps=1e-9):\n",
    "    \"\"\"\n",
    "    Compute VIF using inverse of the correlation matrix.\n",
    "    This is extremely fast and ideal for high multicollinearity.\n",
    "    \"\"\"\n",
    "    # Standardize to avoid scale issues\n",
    "    X = (df - df.mean()) / (df.std() + eps)\n",
    "\n",
    "    # Correlation matrix\n",
    "    R = X.corr().values\n",
    "\n",
    "    # Try inverting; add small ridge if nearly singular\n",
    "    try:\n",
    "        R_inv = np.linalg.inv(R)\n",
    "    except np.linalg.LinAlgError:\n",
    "        R_inv = np.linalg.inv(R + np.eye(R.shape[0]) * eps)\n",
    "\n",
    "    vif = np.diag(R_inv)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"feature\": df.columns,\n",
    "        \"VIF\": vif\n",
    "    }).sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "\n",
    "def filter_dataframe_on_str(df, column_name, include_list=[], exclude_list=[], whole_words=True, case=False):\n",
    "    \"\"\"\n",
    "    Vectorized mask: include any of include_words AND exclude none of exclude_words.\n",
    "    \n",
    "    include_words: str or list[str]\n",
    "    exclude_words: None, str, or list[str]\n",
    "    whole_words: if True, use \\\\b word boundaries\n",
    "    case: if False, case-insensitive (recommended for text mining)\n",
    "\n",
    "\n",
    "    trust = filter_dataframe_on_str(df=org_df,\n",
    "                                column_name='ORGNAME',\n",
    "                                include_list=['trust'],\n",
    "                                exclude_list=['notary','strata','commission','company']).head(40)\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    # Escape user-provided terms so special chars don't break regex (e.g., \"C++\")\n",
    "    inc = \"|\".join(map(re.escape, include_list))\n",
    "    exc = \"|\".join(map(re.escape, exclude_list)) if exclude_list else None\n",
    "\n",
    "    boundary = r\"\\b\" if whole_words else \"\"\n",
    "    include_pat = rf\"{boundary}(?:{inc}){boundary}\"\n",
    "\n",
    "    if exc:\n",
    "        exclude_pat = rf\"{boundary}(?:{exc}){boundary}\"\n",
    "        pattern = rf\"^(?!.*{exclude_pat}).*{include_pat}\"\n",
    "    else:\n",
    "        pattern = include_pat\n",
    "\n",
    "    return df[df[column_name].str.contains(pattern, case=case, na=False, regex=True)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e951796-6e9e-4b88-a7d9-268875a8cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompareFunction(func1,func2,additional_records=20):\n",
    "    \n",
    "    '''\n",
    "    Function which Compares 2 Functions and determines if they are different. Specifically, it can help to easily\n",
    "    Manage Version control of Functions outside of a More robust environment such as GIT.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    list1 = FunctionToSTR(func1)\n",
    "    list2 = FunctionToSTR(func2)\n",
    "    \n",
    "    length = max(len(list1),len(list2))\n",
    "    \n",
    "    for record in range(0,length):\n",
    "        if list1[record]==list2[record]:\n",
    "            if record == (length-1):\n",
    "                print(\"All Records Reconcile\")\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                print(list1[record:record+additional_records])\n",
    "                print(list2[record:record+additional_records])\n",
    "            except:\n",
    "                print(list1[record:record:])\n",
    "                print(list2[record:record:])\n",
    "            break\n",
    "\n",
    "\n",
    "CompareFunction(test,TranposeNonTimeSeriesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ef98f-fbbf-4311-8999-36365c4edf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "\n",
    "# Action Add DQ Item.\n",
    "# Add Read Me, which Explanation of process and Definitions of all Categorizations.\n",
    "\n",
    "# DQ Validation Component\n",
    "\n",
    "# Question I need to ask Myself.\n",
    "# Is it a Definition.\n",
    "# Is it a Step in a Process\n",
    "# Is it a Process\n",
    "# Is it information Related to a Process.\n",
    "\n",
    "# DQ Process 1: Need to Avoid Duplication of Word in ALL SHEETS.\n",
    "# Definitions: Are Technical Definitions, descriptions and eloborate Notes describing a Term. \n",
    "# Notes: Merge in Definitions.\n",
    "# Take Longer Listings and organize them, Models, BLUE, \n",
    "\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf37a5a-1f78-43ae-84f2-8aae69ad6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_read_me():\n",
    "\n",
    "    from data_d_dicts import links\n",
    "    df = pd.read_csv(links['google_definition_csv'])\n",
    "\n",
    "    read_me_df = pd.DataFrame(df.columns,columns=['W'])\n",
    "\n",
    "    return read_me_df\n",
    "    \n",
    "read_me_df = pd.DataFrame(df.columns,columns=['Word'])\n",
    "read_me_df.merge(df[['Category','Categorization','Definition','Word']],on='Word',how='left')\n",
    "\n",
    "read_me_df = pd.DataFrame(df.columns,columns=['Word'])\n",
    "read_me_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f7f3a5-8c81-4abf-b52c-b30e68909afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Do I have a Good Search for Format Type Function?\n",
    "    1. DFColumnTypeDict technically is, but not great.\n",
    "    2. Review Recently implemented at work for _____ (I can remember \n",
    "\n",
    "gpt_question(word_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33100b-3acd-4e45-95a2-fc73712481f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_list_to_str(list_,\n",
    "                        start_text='',\n",
    "                        additional_text=', ',\n",
    "                        ending_text=\"\"):\n",
    "    text = start_text\n",
    "    \n",
    "    for count,word in enumerate(list_):\n",
    "        if text==\"\":\n",
    "            text += word\n",
    "        else:\n",
    "            text += additional_text + word \n",
    "    \n",
    "    return text\n",
    "\n",
    "iterate_list_to_str(['Learning Type','Algorithm Classification', \"Model Type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb980b-baf5-46eb-8fe8-209709e286e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df_documentation(df,\n",
    "                              data_dictionary=None,\n",
    "                              column_name=\"\",\n",
    "                              match_on=\"Word\",\n",
    "                              text_description='Definition',):\n",
    "    \n",
    "    '''\n",
    "\n",
    "    \n",
    "\n",
    "    '''\n",
    "\n",
    "    # Make sure there is a Dictionary for Information.\n",
    "    try:\n",
    "        dd_df = data_dictionary.copy()\n",
    "    except:\n",
    "        dd_df = pd.read_csv(links['google_definition_csv'])\n",
    "        dd_df = dd_df[[match_on,text_description]].copy()\n",
    "\n",
    "    if len(column_name) == 0:\n",
    "        # Making a Dictionary of the Dataframe Column Values\n",
    "        final_df = pd.DataFrame(df.columns.tolist(),columns=[match_on])\n",
    "        final_df = final_df.merge(dd_df,on=match_on,how='left')\n",
    "        \n",
    "    else:\n",
    "        # Making a dictionary of the Column Name \n",
    "\n",
    "        col_values = sorted(\n",
    "            df[column_name]\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .loc[lambda s: s != \"\"]\n",
    "            .unique())\n",
    "        \n",
    "        final_df = pd.DataFrame(col_values,columns=[match_on])\n",
    "\n",
    "    return final_df.merge(dd_df[[text_description,match_on]],on=match_on,how='left')\n",
    "\n",
    "generate_df_documentation(df,\n",
    "                          data_dictionary=df,\n",
    "                          column_name='Learning Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f97faf-cd8d-4e04-b199-ba6d337d561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation_checklist(df=pd.DataFrame(),word_list=[]):\n",
    "    if len(df)==0:\n",
    "        df = pd.read_csv(links['d_learning_notes_url'])\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for word in word_list:\n",
    "        temp_df = df[(df['Definition'].fillna(\"\").str.contains(word,case=False))|(df['Categorization']==word)]\n",
    "        final_df = pd.concat([final_df,temp_df])\n",
    "\n",
    "    final_df = final_df.drop(['Source','Process','Categorization'],axis=1)\n",
    "    display(final_df)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "data_preparation_checklist(df1,word_list=['Semantic Type','Functional Role']).to_excel('delete.xlsx',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2fbbc0-1b77-4fa7-bc05-ab43669cac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SummarizedDataSetforBITool(df, dimensions, metrics,apply_tranpose=False):\n",
    "    \"\"\"\n",
    "    Builds a DataFrame with all combinations of ALL-level rollups \n",
    "    across the specified dimensions and metrics.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        dimensions (list of str): Dimension column names.\n",
    "        metrics (list of str): Metric column names to aggregate.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated DataFrame with 'ALL' rollups.\n",
    "        \n",
    "        \n",
    "    Date Created:\n",
    "    Date Last Maintained: 26-Sep-25\n",
    "    Updated Apply Transpose, to include Option to Transpose DF.\n",
    "        \n",
    "    \"\"\"\n",
    "    result_frames = []\n",
    "    \n",
    "    available_metrics = [x for x in metrics if x in df.columns]\n",
    "\n",
    "    for r in range(len(dimensions) + 1):\n",
    "        for dims in itertools.combinations(dimensions, r):\n",
    "            group_cols = list(dims)\n",
    "            \n",
    "            # Aggregate metrics with or without groupby\n",
    "            if group_cols:\n",
    "                agg_df = df.groupby(group_cols, dropna=False)[available_metrics].sum().reset_index()\n",
    "            else:\n",
    "                # Grand total (ALL for all dims)\n",
    "                sums = df[available_metrics].sum().to_frame().T\n",
    "                agg_df = sums\n",
    "                for col in dimensions:\n",
    "                    agg_df[col] = 'ALL'\n",
    "\n",
    "            # Fill missing dimension columns with 'ALL'\n",
    "            for col in dimensions:\n",
    "                if col not in group_cols:\n",
    "                    agg_df[col] = 'ALL'\n",
    "\n",
    "            # Ensure consistent column order\n",
    "            agg_df = agg_df[dimensions + available_metrics]\n",
    "            result_frames.append(agg_df)\n",
    "\n",
    "    final_df = pd.concat(result_frames, ignore_index=True)\n",
    "    \n",
    "    if apply_tranpose:\n",
    "        return TranposeDF(final_df,dimensions).rename(columns={'variable':'METRIC','value':\"VALUE\"})\n",
    "    else:\n",
    "        return final_df\n",
    "\n",
    "\n",
    "def DFColumnManualSortOrder(df,column_name,column_order):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df[column_name] = pd.Categorical(df[column_name], categories=column_order, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a2bf4-428c-4364-bfcb-c4345fcbf557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(x):\n",
    "    import ast\n",
    "    if isinstance(x, str):\n",
    "        return ast.literal_eval(x)\n",
    "    return x\n",
    "\n",
    "df['type'] = df['type'].apply(lambda x:string_to_list(x))\n",
    "df['charged_moves'] = df['charged_moves'].apply(lambda x:string_to_list(x))\n",
    "df['fast_moves'] = df['fast_moves'].apply(lambda x:string_to_list(x))\n",
    "\n",
    "df.head(2)\n",
    "\n",
    "\n",
    "def split_list_column_to_fixed_columns(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    prefix: str = \"Column\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a DataFrame list column into a fixed number of positional columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    column : str\n",
    "        Column containing list values\n",
    "    max_columns : int\n",
    "        Number of output columns to create\n",
    "    prefix : str\n",
    "        Prefix for column names (default 'Column')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with fixed positional columns\n",
    "    \"\"\"\n",
    "\n",
    "    max_columns = df['type'].apply(lambda x:len(x)).max()\n",
    "    \n",
    "    new_cols = [\n",
    "        f\"{prefix}{i+1}\"\n",
    "        for i in range(max_columns)\n",
    "    ]\n",
    "\n",
    "    expanded = pd.DataFrame(\n",
    "        df[column].apply(\n",
    "            lambda x: x[:max_columns] + [None] * (max_columns - len(x))\n",
    "            if isinstance(x, list)\n",
    "            else [None] * max_columns\n",
    "        ).tolist(),\n",
    "        columns=new_cols,\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    return expanded\n",
    "\n",
    "\n",
    "\n",
    "def expand_list_column_to_columns(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    prefix: str = None,\n",
    "    fill_value: int = 0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expand a DataFrame column containing lists into unique indicator columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    column : str\n",
    "        Column containing list values\n",
    "    prefix : str, optional\n",
    "        Optional prefix for new columns\n",
    "    fill_value : int\n",
    "        Value for absence (default 0)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        New DataFrame with one column per unique item\n",
    "    \"\"\"\n",
    "    # Step 1: Explode into long format\n",
    "    exploded = df[[column]].explode(column)\n",
    "\n",
    "    # Step 2: Create indicator column\n",
    "    exploded[\"_value\"] = 1\n",
    "\n",
    "    # Step 3: Pivot to wide format\n",
    "    wide = (\n",
    "        exploded\n",
    "        .pivot_table(\n",
    "            index=exploded.index,\n",
    "            columns=column,\n",
    "            values=\"_value\",\n",
    "            fill_value=fill_value\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Optional prefix\n",
    "    if prefix:\n",
    "        wide = wide.add_prefix(f\"{prefix}_\")\n",
    "\n",
    "    return wide.reset_index(drop=True)\n",
    "\n",
    "def unique_items_from_list_column(df, column):\n",
    "    \"\"\"\n",
    "    Extract unique items from a DataFrame column containing lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    column : str\n",
    "        Column name containing list values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Sorted list of unique items\n",
    "    \"\"\"\n",
    "    return sorted({item for sublist in df[column] for item in sublist})\n",
    "\n",
    "\n",
    "def single_column_compare(df,\n",
    "                          column_name,\n",
    "                          column_name1,\n",
    "                          additional_filter=None,\n",
    "                          bracketing=[-10000,-1000,-100,-1,0,1,100,1000,10000]):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Function which takes a dataframe with 2 Columns which are identical and attempts to Compare.\n",
    "    Function df_column_compare applies this function to an Entire Dataframe\n",
    "    \n",
    "    Parameters:\n",
    "        column_name (str):\n",
    "        column_name1 (str): For One Off Use, to compare different name\n",
    "        additional_filter (str): Default parameter to distinguish combined dataframes, also used in MergeAndRenameColumnsDf\n",
    "        bracketing(list): Value to create Distintion when Calculated Difference between columns is numeric.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of 3 Dataframes, Account, Summary and Group By.\n",
    "    \n",
    "    Values:\n",
    "    \n",
    "    \n",
    "    Date Created: August 21, 2025\n",
    "    Date Last Modified:\n",
    "    \n",
    "\n",
    "\n",
    "    '''\n",
    "    if not column_name1:\n",
    "        column_name1 = f\"{column_name}{column_distinction}\"    \n",
    "    \n",
    "    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.\n",
    "    temp_df = df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()\n",
    "    \n",
    "    output_dict = {}\n",
    "    \n",
    "    temp_df['COLUMN_NAME'] = column_name\n",
    "    \n",
    "    BinaryComplexEquivlancey(temp_df,'DF','DF1','VALUES_EQUAL')\n",
    "    \n",
    "    temp_df['VALUES_NOT_EQUAL'] = np.where(temp_df['VALUES_EQUAL']==0,1,0)\n",
    "    temp_df['NULL_RECORD_DF'] = np.where(temp_df['DF'].isnull(),1,0)\n",
    "    temp_df['NULL_RECORD_DF1'] = np.where(temp_df['DF1'].isnull(),1,0)\n",
    "    \n",
    "    try:\n",
    "        temp_df['DIFFERENCE'] = temp_df['DF'].fillna(0)-temp_df['DF1'].fillna(0)\n",
    "    except:        \n",
    "        temp_df['DIFFERENCE'] = 0\n",
    "        \n",
    "    try:\n",
    "        BracketColumn(temp_df,'DIFFERENCE','DIFF_SEGMENT',bracketing)\n",
    "    except:\n",
    "        temp_df['DIFF_SEGMENT'] = 'Could Not Calculate'\n",
    "    \n",
    "    # Removed Column Partitioner as it wasn't being Used.\n",
    "    \n",
    "    temp_df1 = temp_df.copy()\n",
    "    temp_df1['RECORD_COUNT']=1\n",
    "    \n",
    "    if additional_filter:\n",
    "        output_dict['groupby_df'] = temp_df1[[additional_filter,'COLUMN_NAME','DF','DF1','RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].groupby([additional_filter,'COLUMN_NAME','DF','DF1'],dropna=False).sum().sort_values('VALUES_EQUAL',ascending=False).head(20).reset_index()\n",
    "        \n",
    "    else:\n",
    "        output_dict['groupby_df'] = temp_df1[['COLUMN_NAME','DF','DF1','RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].groupby(['COLUMN_NAME','DF','DF1'],dropna=False).sum().sort_values('VALUES_EQUAL',ascending=False).head(20).reset_index()\n",
    "    \n",
    "    if additional_filter:\n",
    "        summary_df = pd.DataFrame()\n",
    "        for value in temp_df[additional_filter].unique():\n",
    "            temp = temp_df[temp_df[additional_filter]==value]\n",
    "            value_dict = {\n",
    "                'Total Combined Records':len(temp),\n",
    "                'Values Equal':temp['VALUES_EQUAL'].sum(),\n",
    "                'Values Not Equal':len(temp[temp['VALUES_EQUAL']==0]),\n",
    "                'Percent Values Equal': (temp['VALUES_EQUAL'].sum()/len(temp))*100,\n",
    "                'Null Records DF':temp['NULL_RECORD_DF'].sum(),\n",
    "                'Null Records DF1':temp['NULL_RECORD_DF1'].sum()}\n",
    "            \n",
    "            try:\n",
    "                value_dict['Total Difference']=temp['DIFFERENCE'].sum()\n",
    "            except:\n",
    "                value_dict['Total Difference']=0\n",
    "                \n",
    "            sum_df = pd.DataFrame(value_dict.values(),index=value_dict.keys(),columns=[column_name]).T.reset_index().rename(columns={'index':\"COLUMN_NAME\"})\n",
    "            sum_df[additional_filter] = value\n",
    "            summary_df = pd.concat([summary_df,sum_df])\n",
    "    else:\n",
    "        value_dict = {\n",
    "            'Total Combined Records':len(temp_df),\n",
    "            'Values Equal':temp_df['VALUES_EQUAL'].sum(),\n",
    "            'Values Not Equal':len(temp_df[temp_df['VALUES_EQUAL']==0]),\n",
    "            'Percent Values Equal': (temp_df['VALUES_EQUAL'].sum()/len(temp_df))*100,\n",
    "            'Null Records DF':temp['NULL_RECORD_DF'].sum(),\n",
    "            'Null Records DF1':temp['NULL_RECORD_DF1'].sum()}\n",
    "        \n",
    "        try:\n",
    "            value_dict['Total Difference']=temp['DIFFERENCE'].sum()\n",
    "        except:\n",
    "            value_dict['Total Difference']=0\n",
    "            \n",
    "        summary_df = pd.DataFrame(value_dict.values(),index=value_dict.keys(),columns=[column_name]).T.reset_index().rename(columns={'index':\"COLUMN_NAME\"})\n",
    "        \n",
    "    output_dict['summary_df'] = summary_df\n",
    "    output_dict['account_df'] = temp_df\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "def DfDqComparison(df,\n",
    "                   primary_key_list,\n",
    "                   additional_filter,\n",
    "                   column_distinction='_',\n",
    "                   bracketing=[-10000,-1000,-100,-1,0,1,100,1000,10000],\n",
    "                   file_name=None):\n",
    "    \n",
    "    '''\n",
    "    Function to Apply ColumnDQComparison against DataFrame.\n",
    "    Assumes you start with a Dataframe with Multiple Columns Different only by Column Distinction.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame)\n",
    "        primary_key_list (list): List of Primary Keys, which are REMOVE from comparison Loop.\n",
    "        additional_filter (str): Filter Used to Create a distinct Dimension. Currently DOES NOT accept List\n",
    "        column_distinction (str): String which is expected to compare Columns. Added as default with MergeIdenticalDF\n",
    "        bracketing (list): Numbers which can be used to Calculate a Bracketed difference Column in COmparison\n",
    "        file_name (str): If Included, it will generate Excel Copies (Excel Used as CSV had issues uploading to DF)\n",
    "        \n",
    "    Return:\n",
    "        DataFrame of Groupby, Account and Summary calculations.\n",
    "        \n",
    "        Account: Listing of All Account Values, with Calculations\n",
    "        Summary: A summary Calculation Speaking to Overall Comparison\n",
    "        Groupby: List of Equivalent Values, to compare Material Record Change/Consistency\n",
    "    \n",
    "    Date Created: August 21, 2025\n",
    "    Date Last Modified: \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Only Need to Test Common Records Can do a Simple Dataframe Analysis on Non Common Records.\n",
    "    \n",
    "    #Iterate Through All Columns in Common to create Final Values.\n",
    "    \n",
    "    account_df = pd.DataFrame()\n",
    "    groupby_df =  pd.DataFrame()\n",
    "    summary_df = pd.DataFrame()\n",
    "\n",
    "    for column_name in [x for x in df.columns if (x not in primary_key_list)&(x[-1]!=column_distinction)]:\n",
    "        column_name1 = f\"{column_name}{column_distinction}\"\n",
    "        try:\n",
    "            temp_dict = IdenticalColumnDQValidation(df=df[['ACCTNBR',additional_filter,column_name,column_name1]],\n",
    "                                                    column_name=column_name,\n",
    "                                                    additional_filter=additional_filter,\n",
    "                                                    bracketing=bracketing)\n",
    "    \n",
    "            account_df = pd.concat([account_df,temp_dict['account_df']])\n",
    "            summary_df = pd.concat([summary_df,temp_dict['summary_df']])\n",
    "            groupby_df = pd.concat([groupby_df,temp_dict['groupby_df']])\n",
    "            \n",
    "        except:\n",
    "            print(f'Could Not Compute: {column_name}') \n",
    "            \n",
    "    if file_name:\n",
    "        account_df.to_csv(f\"{file_name}_ACCOUNT.csv\",index=False)\n",
    "        summary_df.to_csv(f\"{file_name}_SUMMARY.csv\",index=False)\n",
    "        groupby_df.to_csv(f\"{file_name}_GROUPBY.csv\",index=False)\n",
    "\n",
    "    return account_df,summary_df,groupby_df\n",
    "\n",
    "\n",
    "def string_to_list(x):\n",
    "    import ast\n",
    "    if isinstance(x, str):\n",
    "        return ast.literal_eval(x)\n",
    "    return x\n",
    "\n",
    "df['type'] = df['type'].apply(lambda x:string_to_list(x))\n",
    "df['charged_moves'] = df['charged_moves'].apply(lambda x:string_to_list(x))\n",
    "df['fast_moves'] = df['fast_moves'].apply(lambda x:string_to_list(x))\n",
    "\n",
    "df.head(2)\n",
    "\n",
    "\n",
    "def split_list_column_to_fixed_columns(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    prefix: str = \"Column\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a DataFrame list column into a fixed number of positional columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    column : str\n",
    "        Column containing list values\n",
    "    max_columns : int\n",
    "        Number of output columns to create\n",
    "    prefix : str\n",
    "        Prefix for column names (default 'Column')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with fixed positional columns\n",
    "    \"\"\"\n",
    "\n",
    "    max_columns = df['type'].apply(lambda x:len(x)).max()\n",
    "    \n",
    "    new_cols = [\n",
    "        f\"{prefix}{i+1}\"\n",
    "        for i in range(max_columns)\n",
    "    ]\n",
    "\n",
    "    expanded = pd.DataFrame(\n",
    "        df[column].apply(\n",
    "            lambda x: x[:max_columns] + [None] * (max_columns - len(x))\n",
    "            if isinstance(x, list)\n",
    "            else [None] * max_columns\n",
    "        ).tolist(),\n",
    "        columns=new_cols,\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    return expanded\n",
    "\n",
    "\n",
    "\n",
    "def expand_list_column_to_columns(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    prefix: str = None,\n",
    "    fill_value: int = 0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expand a DataFrame column containing lists into unique indicator columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    column : str\n",
    "        Column containing list values\n",
    "    prefix : str, optional\n",
    "        Optional prefix for new columns\n",
    "    fill_value : int\n",
    "        Value for absence (default 0)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        New DataFrame with one column per unique item\n",
    "    \"\"\"\n",
    "    # Step 1: Explode into long format\n",
    "    exploded = df[[column]].explode(column)\n",
    "\n",
    "    # Step 2: Create indicator column\n",
    "    exploded[\"_value\"] = 1\n",
    "\n",
    "    # Step 3: Pivot to wide format\n",
    "    wide = (\n",
    "        exploded\n",
    "        .pivot_table(\n",
    "            index=exploded.index,\n",
    "            columns=column,\n",
    "            values=\"_value\",\n",
    "            fill_value=fill_value\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Optional prefix\n",
    "    if prefix:\n",
    "        wide = wide.add_prefix(f\"{prefix}_\")\n",
    "\n",
    "    return wide.reset_index(drop=True)\n",
    "\n",
    "def unique_items_from_list_column(df, column):\n",
    "    \"\"\"\n",
    "    Extract unique items from a DataFrame column containing lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    column : str\n",
    "        Column name containing list values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Sorted list of unique items\n",
    "    \"\"\"\n",
    "    return sorted({item for sublist in df[column] for item in sublist})\n",
    "\n",
    "\n",
    "def single_column_compare(df,\n",
    "                          column_name,\n",
    "                          column_name1,\n",
    "                          additional_filter=None,\n",
    "                          bracketing=[-10000,-1000,-100,-1,0,1,100,1000,10000]):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Function which takes a dataframe with 2 Columns which are identical and attempts to Compare.\n",
    "    Function df_column_compare applies this function to an Entire Dataframe\n",
    "    \n",
    "    Parameters:\n",
    "        column_name (str):\n",
    "        column_name1 (str): For One Off Use, to compare different name\n",
    "        additional_filter (str): Default parameter to distinguish combined dataframes, also used in MergeAndRenameColumnsDf\n",
    "        bracketing(list): Value to create Distintion when Calculated Difference between columns is numeric.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of 3 Dataframes, Account, Summary and Group By.\n",
    "    \n",
    "    Values:\n",
    "    \n",
    "    \n",
    "    Date Created: August 21, 2025\n",
    "    Date Last Modified:\n",
    "    \n",
    "\n",
    "\n",
    "    '''\n",
    "    if not column_name1:\n",
    "        column_name1 = f\"{column_name}{column_distinction}\"    \n",
    "    \n",
    "    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.\n",
    "    temp_df = df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()\n",
    "    \n",
    "    output_dict = {}\n",
    "    \n",
    "    temp_df['COLUMN_NAME'] = column_name\n",
    "    \n",
    "    BinaryComplexEquivlancey(temp_df,'DF','DF1','VALUES_EQUAL')\n",
    "    \n",
    "    temp_df['VALUES_NOT_EQUAL'] = np.where(temp_df['VALUES_EQUAL']==0,1,0)\n",
    "    temp_df['NULL_RECORD_DF'] = np.where(temp_df['DF'].isnull(),1,0)\n",
    "    temp_df['NULL_RECORD_DF1'] = np.where(temp_df['DF1'].isnull(),1,0)\n",
    "    \n",
    "    try:\n",
    "        temp_df['DIFFERENCE'] = temp_df['DF'].fillna(0)-temp_df['DF1'].fillna(0)\n",
    "    except:        \n",
    "        temp_df['DIFFERENCE'] = 0\n",
    "        \n",
    "    try:\n",
    "        BracketColumn(temp_df,'DIFFERENCE','DIFF_SEGMENT',bracketing)\n",
    "    except:\n",
    "        temp_df['DIFF_SEGMENT'] = 'Could Not Calculate'\n",
    "    \n",
    "    # Removed Column Partitioner as it wasn't being Used.\n",
    "    \n",
    "    temp_df1 = temp_df.copy()\n",
    "    temp_df1['RECORD_COUNT']=1\n",
    "    \n",
    "    if additional_filter:\n",
    "        output_dict['groupby_df'] = temp_df1[[additional_filter,'COLUMN_NAME','DF','DF1','RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].groupby([additional_filter,'COLUMN_NAME','DF','DF1'],dropna=False).sum().sort_values('VALUES_EQUAL',ascending=False).head(20).reset_index()\n",
    "        \n",
    "    else:\n",
    "        output_dict['groupby_df'] = temp_df1[['COLUMN_NAME','DF','DF1','RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].groupby(['COLUMN_NAME','DF','DF1'],dropna=False).sum().sort_values('VALUES_EQUAL',ascending=False).head(20).reset_index()\n",
    "    \n",
    "    if additional_filter:\n",
    "        summary_df = pd.DataFrame()\n",
    "        for value in temp_df[additional_filter].unique():\n",
    "            temp = temp_df[temp_df[additional_filter]==value]\n",
    "            value_dict = {\n",
    "                'Total Combined Records':len(temp),\n",
    "                'Values Equal':temp['VALUES_EQUAL'].sum(),\n",
    "                'Values Not Equal':len(temp[temp['VALUES_EQUAL']==0]),\n",
    "                'Percent Values Equal': (temp['VALUES_EQUAL'].sum()/len(temp))*100,\n",
    "                'Null Records DF':temp['NULL_RECORD_DF'].sum(),\n",
    "                'Null Records DF1':temp['NULL_RECORD_DF1'].sum()}\n",
    "            \n",
    "            try:\n",
    "                value_dict['Total Difference']=temp['DIFFERENCE'].sum()\n",
    "            except:\n",
    "                value_dict['Total Difference']=0\n",
    "                \n",
    "            sum_df = pd.DataFrame(value_dict.values(),index=value_dict.keys(),columns=[column_name]).T.reset_index().rename(columns={'index':\"COLUMN_NAME\"})\n",
    "            sum_df[additional_filter] = value\n",
    "            summary_df = pd.concat([summary_df,sum_df])\n",
    "    else:\n",
    "        value_dict = {\n",
    "            'Total Combined Records':len(temp_df),\n",
    "            'Values Equal':temp_df['VALUES_EQUAL'].sum(),\n",
    "            'Values Not Equal':len(temp_df[temp_df['VALUES_EQUAL']==0]),\n",
    "            'Percent Values Equal': (temp_df['VALUES_EQUAL'].sum()/len(temp_df))*100,\n",
    "            'Null Records DF':temp['NULL_RECORD_DF'].sum(),\n",
    "            'Null Records DF1':temp['NULL_RECORD_DF1'].sum()}\n",
    "        \n",
    "        try:\n",
    "            value_dict['Total Difference']=temp['DIFFERENCE'].sum()\n",
    "        except:\n",
    "            value_dict['Total Difference']=0\n",
    "            \n",
    "        summary_df = pd.DataFrame(value_dict.values(),index=value_dict.keys(),columns=[column_name]).T.reset_index().rename(columns={'index':\"COLUMN_NAME\"})\n",
    "        \n",
    "    output_dict['summary_df'] = summary_df\n",
    "    output_dict['account_df'] = temp_df\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "def DfDqComparison(df,\n",
    "                   primary_key_list,\n",
    "                   additional_filter,\n",
    "                   column_distinction='_',\n",
    "                   bracketing=[-10000,-1000,-100,-1,0,1,100,1000,10000],\n",
    "                   file_name=None):\n",
    "    \n",
    "    '''\n",
    "    Function to Apply ColumnDQComparison against DataFrame.\n",
    "    Assumes you start with a Dataframe with Multiple Columns Different only by Column Distinction.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame)\n",
    "        primary_key_list (list): List of Primary Keys, which are REMOVE from comparison Loop.\n",
    "        additional_filter (str): Filter Used to Create a distinct Dimension. Currently DOES NOT accept List\n",
    "        column_distinction (str): String which is expected to compare Columns. Added as default with MergeIdenticalDF\n",
    "        bracketing (list): Numbers which can be used to Calculate a Bracketed difference Column in COmparison\n",
    "        file_name (str): If Included, it will generate Excel Copies (Excel Used as CSV had issues uploading to DF)\n",
    "        \n",
    "    Return:\n",
    "        DataFrame of Groupby, Account and Summary calculations.\n",
    "        \n",
    "        Account: Listing of All Account Values, with Calculations\n",
    "        Summary: A summary Calculation Speaking to Overall Comparison\n",
    "        Groupby: List of Equivalent Values, to compare Material Record Change/Consistency\n",
    "    \n",
    "    Date Created: August 21, 2025\n",
    "    Date Last Modified: \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Only Need to Test Common Records Can do a Simple Dataframe Analysis on Non Common Records.\n",
    "    \n",
    "    #Iterate Through All Columns in Common to create Final Values.\n",
    "    \n",
    "    account_df = pd.DataFrame()\n",
    "    groupby_df =  pd.DataFrame()\n",
    "    summary_df = pd.DataFrame()\n",
    "\n",
    "    for column_name in [x for x in df.columns if (x not in primary_key_list)&(x[-1]!=column_distinction)]:\n",
    "        column_name1 = f\"{column_name}{column_distinction}\"\n",
    "        try:\n",
    "            temp_dict = IdenticalColumnDQValidation(df=df[['ACCTNBR',additional_filter,column_name,column_name1]],\n",
    "                                                    column_name=column_name,\n",
    "                                                    additional_filter=additional_filter,\n",
    "                                                    bracketing=bracketing)\n",
    "    \n",
    "            account_df = pd.concat([account_df,temp_dict['account_df']])\n",
    "            summary_df = pd.concat([summary_df,temp_dict['summary_df']])\n",
    "            groupby_df = pd.concat([groupby_df,temp_dict['groupby_df']])\n",
    "            \n",
    "        except:\n",
    "            print(f'Could Not Compute: {column_name}') \n",
    "            \n",
    "    if file_name:\n",
    "        account_df.to_csv(f\"{file_name}_ACCOUNT.csv\",index=False)\n",
    "        summary_df.to_csv(f\"{file_name}_SUMMARY.csv\",index=False)\n",
    "        groupby_df.to_csv(f\"{file_name}_GROUPBY.csv\",index=False)\n",
    "\n",
    "    return account_df,summary_df,groupby_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def TranposeNonTimeSeriesDF(df, index, columns=None):\n",
    "    '''\n",
    "    Transposes a non-time-series DataFrame from wide to long format by melting specified columns.\n",
    "\n",
    "    This is especially useful for flattening columns into a single column to support tools \n",
    "    like Power BI, where long format enables dynamic pivoting and aggregation.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input pandas DataFrame.\n",
    "        index (list): Columns to retain as identifiers (will remain unchanged).\n",
    "        columns (list): Columns to unpivot into key-value pairs.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A long-format DataFrame with 'variable' and 'value' columns.\n",
    "    '''\n",
    "    if not columns:\n",
    "        columns = [col for col in final_df1.columns if (isinstance(col, pd.Timestamp))|(isinstance(col, datetime.datetime))]\n",
    "    \n",
    "    return df.melt(id_vars=index, value_vars=columns)\n",
    "\n",
    "def CreatePivotTableFromTimeSeries(df,\n",
    "                                   index,\n",
    "                                   columns,\n",
    "                                   values,\n",
    "                                   aggfunc='sum',\n",
    "                                   skipna=True):\n",
    "    \n",
    "    '''\n",
    "    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 1. Pivot\n",
    "    if index==None:\n",
    "        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)\n",
    "    else:\n",
    "        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)\n",
    "\n",
    "    # 2. Capture original month columns IMMEDIATELY after pivot\n",
    "    month_cols = df1.columns.tolist()\n",
    " \n",
    "    # 3. Add rolling window stats\n",
    "    if len(month_cols) >= 3:\n",
    "        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]\n",
    "        try:\n",
    "            df1['PERC_CHG_3M'] = df1['CHG_3M']/df1[month_cols[-3]]\n",
    "        except:\n",
    "            df1['PERC_CHG_3M'] = 0\n",
    "    \n",
    "    if len(month_cols) >= 6:\n",
    "        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]\n",
    "        try:\n",
    "            df1['PERC_CHG_6M'] = df1['CHG_6M']/df1[month_cols[-6]]\n",
    "        except:\n",
    "            df1['PERC_CHG_6M'] = 0\n",
    "            \n",
    "    if len(month_cols) >= 12:\n",
    "        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]\n",
    "        try:\n",
    "            df1['PERC_CHG_12M'] = df1['CHG_12M']/df1[month_cols[-12]]\n",
    "        except:\n",
    "            df1['PERC_CHG_12M'] = 0\n",
    "\n",
    "    df1['CHG_DF']  = df1[month_cols[-1]]-df1[month_cols[0]]\n",
    "    df1['AVG_DF'] = df1[month_cols[-1:]].mean(axis=1, skipna=skipna)\n",
    "    df1['PERC_CHG_DF'] = df1['AVG_DF']/df1[month_cols[-1]]\n",
    "\n",
    "    \n",
    "    # 4. Now calculate global stats **only using the original month columns**\n",
    "    stats = pd.DataFrame({\n",
    "        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),\n",
    "        'STD': df1[month_cols].std(axis=1, skipna=skipna),\n",
    "        'MAX': df1[month_cols].max(axis=1, skipna=skipna),\n",
    "        'MIN': df1[month_cols].min(axis=1, skipna=skipna),\n",
    "        'COUNT': df1[month_cols].count(axis=1)\n",
    "    })\n",
    "\n",
    "    # 5. Merge the stats\n",
    "    df1 = pd.concat([df1, stats], axis=1)\n",
    "    \n",
    "    return df1.fillna(0)\n",
    "\n",
    "\n",
    "def CreateMultiplePivotTableFromTimeSeries(df,\n",
    "                                           index_list,\n",
    "                                           metric_list,\n",
    "                                           column):\n",
    "    '''\n",
    "    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through all Possible Metrics Selected.\n",
    "    \n",
    "    for metric in metric_list:\n",
    "        print(f'Attempting to Process:{metric}')\n",
    "        try:\n",
    "            all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') \n",
    "            cols = list(all_df.columns)\n",
    "            all_df = all_df.reset_index(drop=True)\n",
    "            all_df['METRIC'] = metric\n",
    "            cols.insert(0,'METRIC')\n",
    "\n",
    "            for key in index_list:\n",
    "                cols.insert(0,key)\n",
    "                all_df[key] = 'All'\n",
    "\n",
    "            final_df = pd.concat([final_df,all_df[cols]])\n",
    "            # Iterate through all Index Items Individually\n",
    "            for key in index_list:\n",
    "                temp = CreatePivotTableFromTimeSeries(df,\n",
    "                                                      index=key,\n",
    "                                                      values=metric,\n",
    "                                                      columns=column).reset_index() \n",
    "                for missing in [x for x in index_list if x != key]:\n",
    "                    temp[missing] = 'All'\n",
    "                temp['METRIC'] = metric\n",
    "                final_df = pd.concat([final_df,temp])\n",
    "\n",
    "            # Add Value for Metric with Entire Index Combination\n",
    "            temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()\n",
    "            temp['METRIC'] = metric\n",
    "            final_df = pd.concat([final_df,temp])\n",
    "        except:\n",
    "            print(f'Could Not Process Metric:{metric}.')\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):\n",
    "    '''\n",
    "    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through all Possible Metrics Selected.\n",
    "    for metric in metric_list:\n",
    "        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') \n",
    "        cols = list(all_df.columns)\n",
    "        all_df = all_df.reset_index(drop=True)\n",
    "        all_df['METRIC'] = metric\n",
    "        cols.insert(0,'METRIC')\n",
    "\n",
    "        for key in index:\n",
    "            cols.insert(0,key)\n",
    "            all_df[key] = 'All'\n",
    "\n",
    "        final_df = pd.concat([final_df,all_df[cols]])\n",
    "\n",
    "        # Iterate through all Index Items Individually\n",
    "        for key in index_list:\n",
    "            temp = CreatePivotTableFromTimeSeries(df,index=key,\n",
    "                                                  values=metric,\n",
    "                                                  columns=column).reset_index() \n",
    "            for missing in [x for x in index if x != key]:\n",
    "                temp[missing] = 'All'\n",
    "            temp['METRIC'] = metric\n",
    "            final_df = pd.concat([final_df,temp])\n",
    "        \n",
    "        # Add Value for Metric with Entire Index Combination\n",
    "        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()\n",
    "        temp['METRIC'] = metric\n",
    "        final_df = pd.concat([final_df,temp])\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def SummarizeTimeSeriesDf(df,\n",
    "                          summary_cols,\n",
    "                          primary_key_list):\n",
    "    '''\n",
    "    Function to Summarize a Time Series dataframe based on a finite number of identified Columns.\n",
    "    \n",
    "    Parameters\n",
    "        df (Dataframe): TimeSeries in Nature\n",
    "        summary_cols (List): List of Columns which are to be included in SUmmary\n",
    "        primary_key_list (list): Primary Key of Dataframe\n",
    "    \n",
    "    Returns\n",
    "        temp_df1: Raw Data of SUmmary Cols with a Count of Observations. If include Month Variable Easy to add to Pivot Table\n",
    "        summary: Summary (Excluding Primary Key). including Total Observations, MEan, Max, Min.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    temp_df = df[summary_cols].copy()\n",
    "    temp_df['COUNT'] = 1\n",
    "    \n",
    "    # Unique Occurances by Pivot Criteria. Important to Include Month\n",
    "    temp_df1 = temp_df.groupby(summary_cols).sum().reset_index().rename(columns={'COUNT':'TOTAL_DAYS'})\n",
    "    \n",
    "    pivot_columns1 = [x for x in summary_cols if x not in primary_key_list]\n",
    "    \n",
    "    summary = temp_df1.groupby(pivot_columns1).agg(\n",
    "        TOTAL=('TOTAL_DAYS', 'count'),\n",
    "        AVG_DAYS_OPEN=('TOTAL_DAYS', 'mean'),\n",
    "        MAX_OBS=('TOTAL_DAYS', 'min'),\n",
    "        MIN_OBS=('TOTAL_DAYS', 'max')).reset_index()\n",
    "    \n",
    "    return temp_df1,summary\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def CompareFunction(func1,func2,additional_records=20):\n",
    "    \n",
    "    '''\n",
    "    Function which Compares 2 Functions and determines if they are different. Specifically, it can help to easily\n",
    "    Manage Version control of Functions outside of a More robust environment such as GIT.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    list1 = FunctionToSTR(func1)\n",
    "    list2 = FunctionToSTR(func2)\n",
    "    \n",
    "    length = max(len(list1),len(list2))\n",
    "    \n",
    "    for record in range(0,length):\n",
    "        if list1[record]==list2[record]:\n",
    "            if record == (length-1):\n",
    "                print(\"All Records Reconcile\")\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                print(list1[record:record+additional_records])\n",
    "                print(list2[record:record+additional_records])\n",
    "            except:\n",
    "                print(list1[record:record:])\n",
    "                print(list2[record:record:])\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3354294-a64e-436d-b85a-dbfeb95b2d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed Multithread Processing from Read Directory. Need to have displine and Principles related to \n",
    "# Creation of Functions, Create Individual functions which do components, opposed to long ones. \n",
    "\n",
    "google_definition_csv = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQq1-3cTas8DCWBa2NKYhVFXpl8kLaFDohg0zMfNTAU_Fiw6aIFLWfA5zRem4eSaGPa7UiQvkz05loW/pub?output=csv'\n",
    "df = pd.read_csv(google_definition_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
