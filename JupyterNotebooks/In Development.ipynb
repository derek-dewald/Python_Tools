{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a522a1-e60e-4b23-aff5-49720c5bf183",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_openml() - Real Dataset\n",
    "Load_* - Toy Datasets\n",
    "make_* - Make New Datasets\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from transformers import pipeline \n",
    "import torch\n",
    "\n",
    "Memory Need pytorch_model.bin * 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5385f0cc-cf5c-4548-8208-d8fb54d52128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db749084-a457-4777-bc10-e5b85ab8843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IMDB_Reviews():\n",
    "    '''\n",
    "\n",
    "    (X_train,y_train),(X_test,y_test) = IMDB_Reviews()\n",
    "    word_index = imdb.get_word_index()\n",
    "    d = dict([(value,key) for (key,value) in word_index.items()])\n",
    "    \" \".join([d.get(i-3,\"?\") for i in X_train[6]])\n",
    "\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "\n",
    "        from tensorflow.keras.datasets import imdb\n",
    "        (X_train,y_train),(X_test,y_test) = imdb.load_data(num_words=10000)\n",
    "        return (X_train,y_train),(X_test,y_test)\n",
    "\n",
    "    except:\n",
    "        print(\"Do you have Tensor Flow Installed?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c024edca-2193-4cae-adc0-6b29f5a96c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSets import GenerateFakeMemberDF\n",
    "\n",
    "\n",
    "\n",
    "df = GenerateFakeMemberDF(100,20)\n",
    "\n",
    "\n",
    "branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']\n",
    "city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']\n",
    "lob = ['Retail','Corporate','Mid Market']\n",
    "duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']\n",
    "\n",
    "list_of_lists = [branches,city,lob,duration]\n",
    "\n",
    "df1 = pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])\n",
    "df1['BRANCHNAME'] = [np.random.choice(branches) for x in range(len(df1))]\n",
    "df1['CITY'] = [np.random.choice(city) for x in range(len(df1))]\n",
    "df1['LOB'] = [np.random.choice(lob) for x in range(len(df1))]\n",
    "df1['DURATION'] = [np.random.choice(duration) for x in range(len(df1))]\n",
    "\n",
    "final_df = df.merge(df1,on='MEMBERNBR',how='left')\n",
    "final_df['INTEREST_RATE'] = final_df['MONTH'].apply(lambda x:np.random.uniform(0.02, 0.07))\n",
    "final_df['MATURED_AMOUNT'] = final_df['LENDING'].apply(lambda x:x*np.random.uniform(0, 1)) \n",
    "final_df['RENEWED_AMOUNT'] = final_df['MATURED_AMOUNT'].apply(lambda x:x*.95) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57fd8e1-4116-4de6-872e-f76016e9472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\\n",
    "from itertools import product,permutations,combinations\n",
    "\n",
    "\n",
    "branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']\n",
    "city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']\n",
    "lob = ['Retail','Corporate','Mid Market']\n",
    "duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']\n",
    "list_of_lists = [branches,city,lob,duration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f0576-1d0c-484c-a3fb-313b04f102fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import difflib\n",
    "\n",
    "def compare_functions(func1, func2):\n",
    "    f1_lines = inspect.getsource(func1).splitlines()\n",
    "    f2_lines = inspect.getsource(func2).splitlines()\n",
    "    diff = difflib.unified_diff(f1_lines, f2_lines, lineterm='')\n",
    "    return '\\n'.join(diff)\n",
    "\n",
    "compare_functions(ColumnStatisticalReview,ColumnStatisticalReview1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35864245-c2f6-45e9-914c-15cce04a8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSets import GenerateFakeMemberDF\n",
    "from FeatureEngineering import CreateRandomDFColumn\n",
    "df = GenerateFakeMemberDF(10000,12)\n",
    "CreateRandomDFColumn(df,['Fenway','Yankee Stadium','Wrigly'],'Stadium')\n",
    "CreateRandomDFColumn(df,['John','Mark','Harry','Sally'],'Vendor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2fe730-5f75-4915-bb34-00d878c1c555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     text                     cleaned\n",
      "0  Transaction from BCCA001 and ONCA-fund  Transaction from and -fund\n",
      "1                  Nothing to remove here      Nothing to remove here\n",
      "2                    Another bcca999 item                Another item\n",
      "3                   Words withonca inside                Words inside\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'text': [\n",
    "        'Transaction from BCCA001 and ONCA-fund',\n",
    "        'Nothing to remove here',\n",
    "        'Another bcca999 item',\n",
    "        'Words withonca inside'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Regex to remove entire words containing 'bcca' or 'onca' (case-insensitive)\n",
    "pattern = r'\\b\\w*(bcca|onca)\\w*\\b'\n",
    "\n",
    "# Clean the text\n",
    "df['cleaned'] = df['text'].str.replace(pattern, '', regex=True, case=False).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05a0d3-ab6b-4c87-bb75-9b1b23efc5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "\n",
    "\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "plt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
    "[...]  # beautify the figure: add grid, legend, axis, labels, and circles\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n",
    "[...]  # beautify the figure: add labels, grid, legend, arrow, and text\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)\n",
    "plt.show()\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\n",
    "                                        normalize=\"true\", values_format=\".0%\")\n",
    "plt.show()\n",
    "\n",
    "sample_weight = (y_train_pred != y_train)\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\n",
    "                                        sample_weight=sample_weight,\n",
    "                                        normalize=\"true\", values_format=\".0%\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869b7ae-29ce-4244-8d59-12143afaa1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_digit(image_data):\n",
    "    image = image_data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "some_digit = X[0]\n",
    "plot_digit(some_digit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb2e90-c48e-4eb4-9fd2-5456c8a3547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE','RECORD_COUNT']\n",
    "index=['BRANCHNAME','CITY','LOB','DURATION']\n",
    "\n",
    "column='MONTH'\n",
    "\n",
    "final_df['RECORD_COUNT']=1\n",
    "\n",
    "def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):\n",
    "    '''\n",
    "    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through all Possible Metrics Selected.\n",
    "    for metric in metric_list:\n",
    "        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') \n",
    "        cols = list(all_df.columns)\n",
    "        all_df = all_df.reset_index(drop=True)\n",
    "        all_df['METRIC'] = metric\n",
    "        cols.insert(0,'METRIC')\n",
    "\n",
    "        for key in index:\n",
    "            cols.insert(0,key)\n",
    "            all_df[key] = 'All'\n",
    "\n",
    "        final_df = pd.concat([final_df,all_df[cols]])\n",
    "\n",
    "        # Iterate through all Index Items Individually\n",
    "        for key in index_list:\n",
    "            temp = CreatePivotTableFromTimeSeries(df,index=key,\n",
    "                                                  values=metric,\n",
    "                                                  columns=column).reset_index() \n",
    "            for missing in [x for x in index if x != key]:\n",
    "                temp[missing] = 'All'\n",
    "            temp['METRIC'] = metric\n",
    "            final_df = pd.concat([final_df,temp])\n",
    "        \n",
    "        # Add Value for Metric with Entire Index Combination\n",
    "        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()\n",
    "        temp['METRIC'] = metric\n",
    "        final_df = pd.concat([final_df,temp])\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],\n",
    "                                       index_list= index,\n",
    "                                       metric_list=metrics,\n",
    "                                       column= 'MONTH')\n",
    "\n",
    "\n",
    "def CreatePivotTableFromTimeSeries(df,\n",
    "                                   index,\n",
    "                                   columns,\n",
    "                                   values,\n",
    "                                   aggfunc='sum',\n",
    "                                   skipna=True):\n",
    "    \n",
    "    '''\n",
    "    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 1. Pivot\n",
    "    if index==None:\n",
    "        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)\n",
    "    else:\n",
    "        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)\n",
    "\n",
    "    # 2. Capture original month columns IMMEDIATELY after pivot\n",
    "    month_cols = df1.columns.tolist()\n",
    " \n",
    "    # 3. Add rolling window stats\n",
    "    if len(month_cols) >= 3:\n",
    "        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]\n",
    "        try:\n",
    "            df1['PERC_CHG_3M'] = df1['CHG_3M']/df1[month_cols[-3]]\n",
    "        except:\n",
    "            df1['PERC_CHG_3M'] = 0\n",
    "    \n",
    "    if len(month_cols) >= 6:\n",
    "        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]\n",
    "        try:\n",
    "            df1['PERC_CHG_6M'] = df1['CHG_6M']/df1[month_cols[-6]]\n",
    "        except:\n",
    "            df1['PERC_CHG_6M'] = 0\n",
    "            \n",
    "    if len(month_cols) >= 12:\n",
    "        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]\n",
    "        try:\n",
    "            df1['PERC_CHG_12M'] = df1['CHG_12M']/df1[month_cols[-12]]\n",
    "        except:\n",
    "            df1['PERC_CHG_12M'] = 0\n",
    "\n",
    "    df1['CHG_DF']  = df1[month_cols[-1]]-df1[month_cols[0]]\n",
    "    df1['AVG_DF'] = df1[month_cols[-1:]].mean(axis=1, skipna=skipna)\n",
    "    df1['PERC_CHG_DF'] = df1['AVG_DF']/df1[month_cols[-1]]\n",
    "\n",
    "    \n",
    "    # 4. Now calculate global stats **only using the original month columns**\n",
    "    stats = pd.DataFrame({\n",
    "        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),\n",
    "        'STD': df1[month_cols].std(axis=1, skipna=skipna),\n",
    "        'MAX': df1[month_cols].max(axis=1, skipna=skipna),\n",
    "        'MIN': df1[month_cols].min(axis=1, skipna=skipna),\n",
    "        'COUNT': df1[month_cols].count(axis=1)\n",
    "    })\n",
    "\n",
    "    # 5. Merge the stats\n",
    "    df1 = pd.concat([df1, stats], axis=1)\n",
    "    \n",
    "    return df1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b88962-6bae-4fea-a17c-d05757fe9fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "\n",
    "def long_format_summary(df, group_vars, agg_cols, agg_funcs):\n",
    "    \"\"\"\n",
    "    Summarizes df grouped by group_vars, with one row per (group, metric, function) and a single VALUE column.\n",
    "\n",
    "    Parameters:\n",
    "    - df: input DataFrame\n",
    "    - group_vars: list of columns to group by\n",
    "    - agg_cols: list of numeric columns to summarize\n",
    "    - agg_funcs: list of aggregation functions (e.g., ['sum', 'mean', 'min', 'max'])\n",
    "\n",
    "    Returns:\n",
    "    - long_df: tidy DataFrame with columns: group_vars + ['METRIC', 'FUNCTION', 'VALUE']\n",
    "    \"\"\"\n",
    "    # Multi-level aggregation\n",
    "    agg_df = df.groupby(group_vars)[agg_cols].agg(agg_funcs).reset_index()\n",
    "\n",
    "    # Flatten multi-index columns\n",
    "    agg_df.columns = group_vars + [f\"{col}_{func}\" for col in agg_cols for func in agg_funcs]\n",
    "\n",
    "    # Melt to long format\n",
    "    long_df = agg_df.melt(id_vars=group_vars, var_name='METRIC_FUNC', value_name='VALUE')\n",
    "\n",
    "    # Split \"DEPOSIT_sum\" → \"DEPOSIT\", \"sum\"\n",
    "    long_df[['METRIC', 'FUNCTION']] = long_df['METRIC_FUNC'].str.rsplit('_', n=1, expand=True)\n",
    "    long_df = long_df.drop(columns='METRIC_FUNC')\n",
    "\n",
    "    # Optional: sort for readability\n",
    "    return long_df.sort_values(by=group_vars + ['METRIC', 'FUNCTION']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf35fc-3dd1-4b1d-8657-c054e02f9f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateMLModels(df,\n",
    "                     Features,\n",
    "                     Target,\n",
    "                     model_list,\n",
    "                     scaler_list,\n",
    "                     balance_list,\n",
    "                     log_max_it=1000,\n",
    "                     rf_estimators=50,\n",
    "                     include_narrative=0):\n",
    "    \n",
    "    model_dict = {}\n",
    "    final_result_df = pd.DataFrame()\n",
    "    \n",
    "    param_list = list(product(model_list,scaler_list,balance_list))\n",
    "    \n",
    "    for count,params in enumerate(param_list):\n",
    "        print(count,params)\n",
    "        \n",
    "        model = params[0]\n",
    "        scaler = params[1]\n",
    "        if params[2]!=0:\n",
    "            balance_target_observations = [1,params[2]]\n",
    "        else:\n",
    "            balance_target_observations = [0,0]\n",
    "        \n",
    "        run_dict,result = MLManualPipeline(df=df,\n",
    "                                           Features=Features,\n",
    "                                           Target=Target,\n",
    "                                           model=model,\n",
    "                                           scaler_=scaler,\n",
    "                                           balance_target_observations=balance_target_observations,\n",
    "                                           include_narrative=include_narrative,\n",
    "                                           rf_estimators=rf_estimators,\n",
    "                                           SequenceNumber=count,\n",
    "                                           log_max_it=log_max_it)\n",
    "        \n",
    "        model_dict[count] = run_dict\n",
    "        final_result_df = pd.concat([final_result_df,result])\n",
    "        \n",
    "    return model_dict,final_result_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ModelPerformanceByCategory(df,\n",
    "                               primary_key='SequenceNumber',\n",
    "                               ml_parameters=['Model','Scaler','Training Set Balance Perc'],\n",
    "                               observations=20,\n",
    "                               metrics=['AUC','F1','Accuracy']):\n",
    "    \n",
    "    # Create List of Models Based on Original Input Dataframe\n",
    "    models = list(df['Model'].unique())\n",
    "    \n",
    "    # Create Values for Particular Metric under review\n",
    "    for metric in metrics:\n",
    "        # Sort Value based on Metric\n",
    "        metric_df = df.sort_values(metric,ascending=False)\n",
    "        # Iterate through Categories as defined by Function\n",
    "        temp = pd.DataFrame()\n",
    "        for ml_ in ml_parameters:\n",
    "            _ = pd.DataFrame(metric_df[ml_].fillna(\"\").head(observations).value_counts()).reset_index().rename(columns={'index':'Observed Value',ml_:metric})\n",
    "            _['Type'] = ml_\n",
    "            \n",
    "            temp = pd.concat([temp,_])\n",
    "            \n",
    "        # Generate Metric Summary Value \n",
    "        # Create Segments\n",
    "        brackets_v3(metric_df,metric,f\"{metric}_segment\",[0,.25,.5,.6,.75,.9])\n",
    "        perc = pd.DataFrame(metric_df[f\"{metric}_segment\"].value_counts()).reset_index().rename(columns={'index':\"Observed Value\",f\"{metric}_segment\":metric})\n",
    "        perc['Type'] = 'Model Performance'\n",
    "        \n",
    "        temp = pd.concat([temp,perc[perc[metric]!=0]])\n",
    "        \n",
    "        try:\n",
    "            final_df = final_df.merge(temp,on=['Observed Value','Type'],how='outer')\n",
    "        except:\n",
    "            final_df = temp.copy()\n",
    "            \n",
    "        test_dict = {}\n",
    "        perf_df = pd.DataFrame()\n",
    "        for model in models:\n",
    "            for metric in ['F1','AUC','Accuracy']:\n",
    "        \n",
    "                value_list = df[df['Model']==model].sort_values(metric,ascending=False)[metric].tolist()\n",
    "                test_dict.setdefault('Model Best Five',{})[metric] = np.mean(value_list[:4])\n",
    "                test_dict.setdefault('Model Best Performing',{})[metric] = value_list[0]\n",
    "                test_dict.setdefault('Model Mean Performance',{})[metric] = np.mean(value_list)\n",
    "\n",
    "                temp_df = pd.DataFrame(test_dict).T.reset_index().rename(columns={'index':'Type'})\n",
    "                temp_df['Observed Value'] = model\n",
    "            \n",
    "            perf_df = pd.concat([perf_df,temp_df])\n",
    "\n",
    "    return pd.concat([final_df,perf_df]).sort_values(['Type',metrics[0]],ascending=[True,False]).set_index(['Observed Value','Type']).fillna(0)\n",
    "    \n",
    "\n",
    "\n",
    "def MLManualPipeline(df,\n",
    "                     Features,\n",
    "                     Target,\n",
    "                     model,\n",
    "                     balance_target_observations=[0,.25],\n",
    "                     scaler_=None,\n",
    "                     test_size=.3,\n",
    "                     random_state=42,\n",
    "                     include_narrative=1,\n",
    "                     rf_estimators=50,\n",
    "                     log_max_it=100,\n",
    "                     SequenceNumber=0,\n",
    "                     time_model=1,\n",
    "                     xgb_objective='binary:logistic',\n",
    "                     xgb_eval_metric='logloss'):\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "        \n",
    "    generated_models = {}\n",
    "    \n",
    "    # Create Copy to ensure original data is not altered\n",
    "    df = df.copy()\n",
    "\n",
    "    if balance_target_observations[0]==1:\n",
    "        df = BalanceTargetDistribution(df=df,\n",
    "                                       Target=Target,\n",
    "                                       desired_percentage=balance_target_observations[1])\n",
    "        balance_target_observations = f'{balance_target_observations[1]}'\n",
    "    else:\n",
    "        balance_target_observations = '0'\n",
    "        \n",
    "    # If features not specified, entire Dataset to be processed (less Target), otherwise include only features\n",
    "    if len(Features) == 0:\n",
    "        Features = list(df.drop(Target,axis=1).columns)\n",
    "        X = np.array(df[Features].copy())\n",
    "        df_len = len(df)\n",
    "        feature_count = len(X[0])\n",
    "    else:\n",
    "        X = np.array(df[Features])\n",
    "        df_len = len(df)\n",
    "        feature_count = len(X[0])\n",
    "        \n",
    "    # Create Target Array - Target already provided as a List\n",
    "    y = np.array(df[Target].squeeze())\n",
    "    target_vc = df[Target].value_counts()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    if scaler_ in scaler_dict.keys():\n",
    "        scaler = scaler_dict[scaler_]\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        x_test = scaler.transform(X_test)\n",
    "    else:\n",
    "        scaler = None\n",
    "        \n",
    "    if include_narrative==1:\n",
    "        print(f\"Total Number of Records in Dataset: {df_len}\\ntotal features: {feature_count}\\nTarget Distribution:\\n{target_vc}\\n\")\n",
    "        print(f\"Training Data Set:\\nTraining Features:{len(X_train)}, Training Target:{len(y_train)}\\nTraining Target Distribution:\\n{pd.DataFrame(y_train).value_counts()}\\n\")\n",
    "        print(f\"Testing Data Set:\\nTest Features:{len(X_test)}, Test Target:{len(y_test)}Training Target Distribution:\\n{pd.DataFrame(y_test).value_counts()}\\n\")\n",
    "        \n",
    "    if model=='Logistic Regression':\n",
    "        \n",
    "        # Create Model\n",
    "        logreg=LogisticRegression(max_iter=log_max_it)\n",
    "        \n",
    "        # Train Model\n",
    "        logreg.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on Text\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        \n",
    "        # Generate Test DataFrame\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        \n",
    "        # Create Feature Importance DataFrame\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':np.abs(logreg.coef_[0]),}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        \n",
    "        # Generate Results and Dataframe\n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        \n",
    "        performance['RunTime'] = stop_time\n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':logreg,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}\n",
    "        \n",
    "        try:\n",
    "            # Generate a Balanced Variant of the Logistic Regression Where \n",
    "            perc_dist = len(df[df[Target]==0])/len(df)\n",
    "            if perc_dist>.6:\n",
    "                lr0=LogisticRegression(max_iter=log_max_it,class_weight='balanced')\n",
    "                lr0.fit(X_train, y_train)\n",
    "                y_pred_rf = lr0.predict(X_test)\n",
    "                result_df = pd.concat([pd.DataFrame(y_pred_rf,columns=['PREDICTION']),\n",
    "                                       pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "                feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                                   'IMPORTANCE':rf.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "                result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                               model='Logistic Regression - Balanced',\n",
    "                                                               scaler=scaler_,\n",
    "                                                               balance=balance_target_observations,\n",
    "                                                               sequence_num=SequenceNumber+1000000)\n",
    "                stop_time = timeit.default_timer() - start_time\n",
    "                performance['RunTime'] = stop_time\n",
    "                generated_models[SequenceNumber+1000000] = {'ModelType':'Logistic Regression - Balanced',\n",
    "                                                        'Model':lr0,\n",
    "                                                        'Scaler':scaler,\n",
    "                                                        'FeatureImportance':feature_importance,\n",
    "                                                        'BinaryResultDF':result_df,\n",
    "                                                        'ModelPerformance':performance}\n",
    "        except:\n",
    "            print('Attempted to Review Random Forrest - Balanced, could not')\n",
    "\n",
    "    elif model =='Decision Tree':    \n",
    "        ############################################ ESTIMATORS\n",
    "        dt = DecisionTreeClassifier(random_state=random_state)\n",
    "        dt.fit(X_train, y_train)\n",
    "        y_pred_dt = dt.predict(X_test)\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred_dt,columns=['PREDICTION']),pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':dt.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        performance['RunTime'] = stop_time\n",
    "        \n",
    "        \n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':dt,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}\n",
    "\n",
    "    elif model =='Random Forest':    \n",
    "        ############################################ ESTIMATORS\n",
    "        rf = RandomForestClassifier(random_state=random_state, n_estimators=rf_estimators)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred_rf = rf.predict(X_test)\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred_rf,columns=['PREDICTION']),\n",
    "                               pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':rf.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        performance['RunTime'] = stop_time\n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':rf,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}\n",
    "        \n",
    "        try:\n",
    "            # Generate a Balanced Variant of the Random Forest Where \n",
    "            perc_dist = len(df[df[Target]==0])/len(df)\n",
    "            if perc_dist>.6:\n",
    "                rf0 = RandomForestClassifier(random_state=random_state, n_estimators=rf_estimators,class_weight='balanced')\n",
    "                rf0.fit(X_train, y_train)\n",
    "                y_pred_rf = rf0.predict(X_test)\n",
    "                result_df = pd.concat([pd.DataFrame(y_pred_rf,columns=['PREDICTION']),\n",
    "                                       pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "                feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                                   'IMPORTANCE':rf.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "                result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                               model='Random Forest - Balanced',\n",
    "                                                               scaler=scaler_,\n",
    "                                                               balance=balance_target_observations,\n",
    "                                                               sequence_num=SequenceNumber+1000000)\n",
    "                stop_time = timeit.default_timer() - start_time\n",
    "                performance['RunTime'] = stop_time\n",
    "                generated_models[SequenceNumber+1000000] = {'ModelType':'Random Forest - Balanced',\n",
    "                                                        'Model':rf0,\n",
    "                                                        'Scaler':scaler,\n",
    "                                                        'FeatureImportance':feature_importance,\n",
    "                                                        'BinaryResultDF':result_df,\n",
    "                                                        'ModelPerformance':performance}\n",
    "        except:\n",
    "            print('Attempted to Review Random Forrest - Balanced, could not')\n",
    "                \n",
    "    elif model == 'Gradient Boosting':\n",
    "        gb = GradientBoostingClassifier()\n",
    "        gb.fit(X_train, y_train)\n",
    "        y_pred_gb = gb.predict(X_test)\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred_gb,columns=['PREDICTION']),pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':gb.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        \n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        performance['RunTime'] = stop_time\n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':gb,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}\n",
    "    elif model == 'Ada':\n",
    "        ada = AdaBoostClassifier()\n",
    "        ada.fit(X_train, y_train)\n",
    "        y_pred_ada = ada.predict(X_test)\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred_ada,columns=['PREDICTION']),pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':ada.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        \n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        performance['RunTime'] = stop_time\n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':ada,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}\n",
    "        \n",
    "    elif model == 'Extra Tree':\n",
    "        et = ExtraTreesClassifier()\n",
    "        et.fit(X_train, y_train)\n",
    "        y_pred_et = et.predict(X_test)\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred_et,columns=['PREDICTION']),pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':et.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        \n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        performance['RunTime'] = stop_time\n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':et,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}\n",
    "        \n",
    "    elif model == 'XGBoost':\n",
    "        xgb_ = xgb.XGBClassifier(objective=xgb_objective,\n",
    "                                eval_metric=xgb_eval_metric,\n",
    "                                use_label_encoder=False,\n",
    "                                random_state=random_state)\n",
    "        xgb_.fit(X_train, y_train)\n",
    "        y_pred_et = xgb_.predict(X_test)\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred_et,columns=['PREDICTION']),\n",
    "                               pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':xgb_.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        \n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        performance['RunTime'] = stop_time\n",
    "        \n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':xgb,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}    \n",
    "    else:\n",
    "        print('Model Not Defined, please update function')\n",
    "        \n",
    "    performance_metrics = pd.DataFrame()\n",
    "    for key in generated_models.keys():\n",
    "        performance_metrics = pd.concat([performance_metrics,generated_models[key]['ModelPerformance']])\n",
    "        \n",
    "    return generated_models,performance_metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d699a-ff75-4b3d-9dde-1a5cc95f5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trends(data, id_column, time_columns):\n",
    "    \"\"\"\n",
    "    Calculate trends (slopes) for entities over time using linear regression.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Input DataFrame containing entity IDs and time-based columns.\n",
    "        id_column (str): Column name for the unique entity identifier (e.g., member_id).\n",
    "        time_columns (list of str): List of column names representing the time periods (e.g., months).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing entity IDs, slopes, and trends.\n",
    "    \"\"\"\n",
    "    # Extract unique entity IDs and the time-series data\n",
    "    entity_ids = data[id_column].unique()\n",
    "    time_data = data[time_columns].values\n",
    "    \n",
    "    # Prepare the X (time) matrix\n",
    "    time_indices = np.arange(1, len(time_columns) + 1)  # Generate time indices\n",
    "    X = np.vstack([time_indices, np.ones(len(time_indices))]).T  # Add a bias term for the intercept\n",
    "\n",
    "    # Perform linear regression for all entities\n",
    "    slopes = np.linalg.lstsq(X, time_data.T, rcond=None)[0][0]  # Extract slopes\n",
    "    \n",
    "    # Prepare results DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        id_column: entity_ids,\n",
    "        \"slope\": slopes\n",
    "    })\n",
    "    \n",
    "    # Classify trends\n",
    "    results[\"trend\"] = np.where(\n",
    "        results[\"slope\"] > 0, \"increasing\",\n",
    "        np.where(results[\"slope\"] < 0, \"decreasing\", \"stable\")\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "test_dict = {}\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for model in models:\n",
    "    for metric in ['F1','AUC','Accuracy']:   \n",
    "        test_dict.setdefault('Top 5 Performing Models',{})[metric] = np.mean(value_list[:4])\n",
    "        test_dict.setdefault('Top Performing Model',{})[metric] = value_list[0]\n",
    "        test_dict.setdefault('All Models Mean',{})[metric] = np.mean(value_list)\n",
    "        \n",
    "        temp_df = pd.DataFrame(test_dict).T.reset_index().rename(columns={'index':'Type'})\n",
    "        temp_df['Observed Value'] = model\n",
    "    \n",
    "    final_df = pd.concat([final_df,temp_df])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190ff11-2d3f-449e-bde8-6ccfd6580935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(X,\n",
    "                y,\n",
    "                input_dim,\n",
    "                metrics,\n",
    "                hidden_layer_sizes,\n",
    "                activation, \n",
    "                optimizer,\n",
    "                learning_rate,\n",
    "                batch_size,\n",
    "                num_epochs,\n",
    "                validation_split,\n",
    "                verbose=0):\n",
    "                       \n",
    "\n",
    "    # Build the model.\n",
    "    model = build_binary_classification_model(input_dim=input_dim,\n",
    "                                              hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                              activation=activation, \n",
    "                                              optimizer=optimizer,\n",
    "                                              learning_rate=learning_rate,\n",
    "                                              metrics=metrics)\n",
    "    \n",
    "    print(model.summary())     \n",
    "                        \n",
    "    # Train the model.\n",
    "    history = model.fit(x=X,\n",
    "                        y=y,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=num_epochs,\n",
    "                        validation_split=validation_split,\n",
    "                        verbose=verbose)\n",
    "\n",
    "    # Retrieve the training metrics (after each train epoch) and the final test\n",
    "    # accuracy.\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(train_accuracy, label='train_accuracy')\n",
    "    plt.plot(val_accuracy, label='validation accuracy')\n",
    "    plt.xticks(range(num_epochs))\n",
    "    plt.xlabel('Train epochs')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    " \n",
    "    return history,model\n",
    "\n",
    "    \n",
    "def neural_network(X_df,\n",
    "                   y_df,\n",
    "                   input_dim, \n",
    "                   hidden_layer_sizes,\n",
    "                   activation,\n",
    "                   optimizer,\n",
    "                   learning_rate,\n",
    "                   metrics,\n",
    "                   verbose=0):\n",
    "\n",
    "    \"\"\"Build a binary classification model using Keras.\n",
    "\n",
    "      Args:\n",
    "        input_dim: Number of features in the input data.\n",
    "        hidden_layer_sizes: A list with the number of units in each hidden layer.\n",
    "        activation: The activation function to use for the hidden layers.\n",
    "        optimizer: The optimizer\n",
    "        learning_rate: The desired learning rate for the optimizer.\n",
    "\n",
    "      Returns:\n",
    "        model: A tf.keras model.\n",
    "    \"\"\"\n",
    "    # Instantiate Model\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # Add Input Layer\n",
    "    model.add(layers.InputLayer(input_shape=(input_dim,)))\n",
    "\n",
    "    # Add Hidden Layers\n",
    "    for nodes in hidden_layer_sizes:\n",
    "        model.add(layers.Dense(units=nodes, activation=activation))\n",
    "\n",
    "    # Add Output Layer\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Configure optimizer and compile the model\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5beec3-740b-41b0-975f-d9f037e1d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RecordElementCompare(df,\n",
    "                         column_name,\n",
    "                         column_name1,\n",
    "                         primary_key,\n",
    "                         summary_columns,\n",
    "                         bracketing=[-10000,-1000,-1,0,1,1000,10000],\n",
    "                         groupby_filter={'top':10,'minimum':5}):\n",
    "    '''\n",
    "    \n",
    "    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.\n",
    "    Initial Use Case Required Minimal Number Values, focused on TEXT\n",
    "        \n",
    "    Parameters:\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    \n",
    "    Example Usage:\n",
    "        \n",
    "        df= df[[START_BAL, START_BAL_, ACCTNBR]],\n",
    "        column_name='START_BAL'\n",
    "\n",
    "    '''\n",
    "    # Make a Copy.\n",
    "\n",
    "    start_cols = primary_key + summary_columns + [column_name] + [column_name1]\n",
    "    account_df = df[start_cols].copy()\n",
    "\n",
    "    if is_numeric_dtype(account_df[column_name]):\n",
    "        account_df[column_name] = np.where(account_df[column_name] == \"\", 0, account_df[column_name])\n",
    "        account_df[column_name1] = np.where(account_df[column_name1] == \"\", 0, account_df[column_name1])\n",
    "    else:\n",
    "        account_df[column_name] = np.where(account_df[column_name] == \"\", None, account_df[column_name])\n",
    "        account_df[column_name1] = np.where(account_df[column_name1] == \"\", None, account_df[column_name1])\n",
    "\n",
    "    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.\n",
    "    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()\n",
    "    \n",
    "    # Calculate DIfference\n",
    "    \n",
    "    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')\n",
    "    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)\n",
    "    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)\n",
    "    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)\n",
    "        \n",
    "    try:\n",
    "        account_df['RECORD_COUNT']\n",
    "    except:\n",
    "        account_df['RECORD_COUNT']=1\n",
    "    \n",
    "    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']\n",
    "\n",
    "    summary_df = pd.DataFrame(account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()).T\n",
    "    summary_df['COLUMN_NAME'] = column_name\n",
    "    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100\n",
    "    \n",
    "    total_columns_groupby = total_columns_summary + ['DF','DF1']\n",
    "    gb_columns = summary_columns + ['DF','DF1']\n",
    "    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values(\"RECORD_COUNT\",ascending=False)\n",
    "\n",
    "    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1\n",
    "\n",
    "    groupby_df = groupby_df[(groupby_df['CUM']<= groupby_filter['top'])&(groupby_df['RECORD_COUNT']>= groupby_filter['minimum'])]\n",
    "    groupby_df['COLUMN_NAME'] = column_name\n",
    "        \n",
    "    return account_df,summary_df,groupby_df\n",
    "\n",
    "account_df,summary_df,groupby_df = RecordElementCompare(df2,\n",
    "                                                        column_name='LENDING',\n",
    "                                                        column_name1='LENDING_',\n",
    "                                                        primary_key=['MEMBERNBR'],\n",
    "                                                        summary_columns=['CITY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4971b57-c3cf-40c9-91a2-c35879c76e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterDictionary(df,CombinationDictionaries,column_names):\n",
    "\n",
    "    dict_list = list(CombinationDictionaries.values())\n",
    "    keys = list(dict_list[0].keys())\n",
    "    print(keys)\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    if len(keys)==1:\n",
    "        col = keys[0]\n",
    "\n",
    "        for list_ in dict_list:\n",
    "            print(list_)\n",
    "            temp_df = df[(df[col]==list_[col])]\n",
    "            for column in column_names:\n",
    "                temp = ColumnStatisticalReview(temp_df,column).T.reset_index().rename(columns={'index':'CALCULATED_COLUMN'})\n",
    "                temp[col] = list_[col]\n",
    "                final_df = pd.concat([final_df,temp])\n",
    "        \n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    elif len(keys)==2:\n",
    "        col  = keys[0]\n",
    "        col1 = keys[1]\n",
    "\n",
    "        for list_ in dict_list:\n",
    "            print(list_)\n",
    "            temp_df = df[(df[col]==list_[col])&(df[col1]==list_[col1])]\n",
    "            for column in column_names:\n",
    "                temp = ColumnStatisticalReview(temp_df,column).T.reset_index().rename(columns={'index':column})\n",
    "                temp[col] = list_[col]\n",
    "                temp[col1] = list_[col1]\n",
    "                return temp\n",
    "    elif len(keys)==3:\n",
    "        \n",
    "        col  = keys[0]\n",
    "        col1 = keys[1]\n",
    "        col2 = keys[2]\n",
    "\n",
    "        for list_ in dict_list:\n",
    "            print(list_)\n",
    "            temp_df = df[(df[col]==list_[col])&(df[col1]==list_[col1])&(df[col2]==list_[col2])]\n",
    "\n",
    "            \n",
    "            return ColumnStatisticalReview(temp_df,'DEPOSIT')\n",
    "        \n",
    "v = FilterDictionary(df2,b,['DEPOSIT'])\n",
    "\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa04f56-0997-4116-a305-3257ed95e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def one_sample_statistical_test(values, popmean=0, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs a one-sample t-test on a list of numeric values.\n",
    "    \n",
    "    Args:\n",
    "        values (list or array): The numeric data to test\n",
    "        popmean (float): The population mean to test against (default is 0)\n",
    "        alpha (float): Significance level (default is 0.05)\n",
    "        \n",
    "    Returns:\n",
    "        dict: {\n",
    "            'mean': float,\n",
    "            'test': 'one-sample t-test',\n",
    "            't_statistic': float,\n",
    "            'p_value': float,\n",
    "            'significant': bool\n",
    "        }\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    t_stat, p_value = stats.ttest_1samp(values, popmean, nan_policy='omit')\n",
    "    \n",
    "    return {\n",
    "        'mean': np.nanmean(values),\n",
    "        'test': 'one-sample t-test',\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < alpha\n",
    "    }\n",
    "\n",
    "one_sample_statistical_test(v['MEAN'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5979061b-70bc-435f-a67c-837214cab0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Organization import D_Notes_Reader\n",
    "D_Notes_Reader('Machine Learning')\n",
    "\n",
    "from SharedFolder import ExtractPythonFiles\n",
    "\n",
    "d = ExtractPythonFiles(export_file=f\"//Users//derekdewald//Documents//Python//Github_Repo//d_py_functions//D_Python_Functions_{datetime.datetime.now().strftime('%d-%b-%y')}\")\n",
    "d.head()\n",
    "\n",
    "from difflib import unified_diff\n",
    "\n",
    "def diff_text(a: str, b: str, a_name=\"v1\", b_name=\"v2\") -> str:\n",
    "    return \"\\n\".join(\n",
    "        unified_diff(a.splitlines(), b.splitlines(),\n",
    "                     fromfile=a_name, tofile=b_name, lineterm=\"\")\n",
    "    )\n",
    "    \n",
    "from IPython.display import display, Markdown, Math\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def DailyTest(questions=20,updates=5):\n",
    "    \n",
    "    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vQq1-3cTas8DCWBa2NKYhVFXpl8kLaFDohg0zMfNTAU_Fiw6aIFLWfA5zRem4eSaGPa7UiQvkz05loW/pub?output=csv')\n",
    "    \n",
    "    updates = df[df['Definition'].isnull()].sample(updates)\n",
    "    print(updates['Word'].tolist())\n",
    "    \n",
    "    questions = df[df['Definition'].notnull()].sample(questions)\n",
    "    \n",
    "    for row in range(len(df)):\n",
    "        display_term_latex_dynamic(questions.iloc[row])\n",
    "\n",
    "def display_term_latex_dynamic(row):\n",
    "    '''\n",
    "    Function Created to Sample Data Dictionary Rows and present information in incremental Format\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "        Series\n",
    "        \n",
    "    Returns:\n",
    "        Nil\n",
    "    \n",
    "\n",
    "    '''\n",
    "    import time\n",
    "    \n",
    "    print(f\"\\n=== {row['Word']} ===\")\n",
    "    print(f\"Category: {row['Category']}\")\n",
    "    print(f\"Sub Category: {row['Sub Categorization']}\")\n",
    "    \n",
    "    print(f\"Definition: {row['Definition']}\\n\")\n",
    "    time.sleep(5)\n",
    "    if pd.notna(row['Markdown Equation']):\n",
    "        eq_text = row['Markdown Equation']\n",
    "        \n",
    "        # Extract equation\n",
    "        main_eq = re.search(r\"\\$\\$(.*?)\\$\\$\", eq_text, re.DOTALL)\n",
    "        if main_eq:\n",
    "            display(Markdown(\"**Equation:**\"))\n",
    "            display(Math(main_eq.group(1).strip()))\n",
    "        \n",
    "        # Extract \"where\" section\n",
    "        where_part = re.split(r\"\\bwhere:\\b\", eq_text, flags=re.IGNORECASE)\n",
    "        if len(where_part) > 1:\n",
    "            display(Markdown(\"**Where:**\"))\n",
    "            where_lines = where_part[1].strip().splitlines()\n",
    "            for line in where_lines:\n",
    "                cleaned = line.strip(\"-• \").strip()\n",
    "                if cleaned:\n",
    "                    display(Math(cleaned))\n",
    "    if pd.notna(row['Link']):\n",
    "        display(Markdown(f\"[More Info]({row['Link']})\"))\n",
    "\n",
    "\n",
    "DailyTest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
