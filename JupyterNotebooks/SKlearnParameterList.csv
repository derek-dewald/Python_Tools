Section,Name,Type,Default,Description,Model,Estimator
Parameters,max_iter,int,300,Maximum number of iterations.,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Parameters,tol,float,1e-3,Stop the algorithm if w has converged.,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Parameters,alpha_1,float,1e-6,Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter.,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Parameters,alpha_2,float,1e-6,Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter.,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Parameters,lambda_1,float,1e-6,Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter.,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Parameters,lambda_2,float,1e-6,Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter.,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Parameters,compute_score,bool,False,"If True, compute the objective function at each step of the model.",ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Parameters,threshold_lambda,float,10 000,Threshold for removing (pruning) weights with high precision from the computation.,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Parameters,copy_X,bool,True,"If True, X will be copied; else, it may be overwritten.",ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Attributes,verbose,bool,False,Verbose mode when fitting the model.,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Attributes,coef_,array-like of shape (n_features,,Coefficients of the regression model (mean of distribution),ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Attributes,alpha_,float,,,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Attributes,lambda_,array-like of shape (n_features,,,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Attributes,sigma_,array-like of shape (n_features,,estimated variance-covariance matrix of the weights,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Attributes,scores_,float,,"if computed, value of the objective function (to be maximized)",ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Attributes,n_iter_,int,,The actual number of iterations to reach the stopping criterion.,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Attributes,intercept_,float,,Independent term in decision function. Set to 0.0 if ``fit_intercept = False``.,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Attributes,X_offset_,float,,"If `fit_intercept=True`, offset subtracted for centering data to a zero mean. Set to np.zeros(n_features) otherwise.",ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Attributes,X_scale_,float,,Set to np.ones(n_features).,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,ARDRegression,<class 'sklearn.linear_model._bayes.ARDRegression'>
Parameters,estimator,object,None,"The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper ``classes_`` and ``n_classes_`` attributes. If ``None``, then the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier` initialized with `max_depth=1`. `base_estimator` was renamed to `estimator`.",AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Parameters,n_estimators,int,50,"The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. Values must be in the range `[1, inf)`.",AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Parameters,learning_rate,float,1.0,"Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. There is a trade-off between the `learning_rate` and `n_estimators` parameters. Values must be in the range `(0.0, inf)`.",AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Parameters,algorithm,{'SAMME'},'SAMME',Use the SAMME discrete boosting algorithm. `algorithm` is deprecated and will be removed in version 1.8. This estimator only implements the 'SAMME' algorithm.,AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Attributes,random_state,int,None,"Controls the random seed given at each `estimator` at each boosting iteration. Thus, it is only used when `estimator` exposes a `random_state`. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Attributes,estimator_,estimator,,The base estimator from which the ensemble is grown. `base_estimator_` was renamed to `estimator_`.,AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Attributes,estimators_,list of classifiers,,The collection of fitted sub-estimators.,AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,The classes labels.,AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Attributes,n_classes_,int,,The number of classes.,AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Attributes,estimator_weights_,ndarray of floats,,Weights for each estimator in the boosted ensemble.,AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Attributes,estimator_errors_,ndarray of floats,,Classification error for each estimator in the boosted ensemble.,AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Attributes,feature_importances_,ndarray of shape (n_features,,The impurity-based feature importances if supported by the ``estimator`` (when based on decision trees). Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.,AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,AdaBoostClassifier,<class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>
Parameters,estimator,object,None,"The base estimator from which the boosted ensemble is built. If ``None``, then the base estimator is :class:`~sklearn.tree.DecisionTreeRegressor` initialized with `max_depth=3`. `base_estimator` was renamed to `estimator`.",AdaBoostRegressor,<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>
Parameters,n_estimators,int,50,"The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. Values must be in the range `[1, inf)`.",AdaBoostRegressor,<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>
Parameters,learning_rate,float,1.0,"Weight applied to each regressor at each boosting iteration. A higher learning rate increases the contribution of each regressor. There is a trade-off between the `learning_rate` and `n_estimators` parameters. Values must be in the range `(0.0, inf)`.",AdaBoostRegressor,<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>
Parameters,loss,{'linear','linear',The loss function to use when updating the weights after each boosting iteration.,AdaBoostRegressor,<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>
Attributes,random_state,int,None,"Controls the random seed given at each `estimator` at each boosting iteration. Thus, it is only used when `estimator` exposes a `random_state`. In addition, it controls the bootstrap of the weights used to train the `estimator` at each boosting iteration. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",AdaBoostRegressor,<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>
Attributes,estimator_,estimator,,The base estimator from which the ensemble is grown. `base_estimator_` was renamed to `estimator_`.,AdaBoostRegressor,<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>
Attributes,estimators_,list of regressors,,The collection of fitted sub-estimators.,AdaBoostRegressor,<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>
Attributes,estimator_weights_,ndarray of floats,,Weights for each estimator in the boosted ensemble.,AdaBoostRegressor,<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>
Attributes,estimator_errors_,ndarray of floats,,Regression error for each estimator in the boosted ensemble.,AdaBoostRegressor,<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>
Attributes,feature_importances_,ndarray of shape (n_features,,The impurity-based feature importances if supported by the ``estimator`` (when based on decision trees). Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.,AdaBoostRegressor,<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,AdaBoostRegressor,<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,AdaBoostRegressor,<class 'sklearn.ensemble._weight_boosting.AdaBoostRegressor'>
Parameters,sample_steps,int,2,Gives the number of (complex) sampling points.,AdditiveChi2Sampler,<class 'sklearn.kernel_approximation.AdditiveChi2Sampler'>
Attributes,sample_interval,float,None,"Sampling interval. Must be specified when sample_steps not in {1,2,3}.",AdditiveChi2Sampler,<class 'sklearn.kernel_approximation.AdditiveChi2Sampler'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,AdditiveChi2Sampler,<class 'sklearn.kernel_approximation.AdditiveChi2Sampler'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,AdditiveChi2Sampler,<class 'sklearn.kernel_approximation.AdditiveChi2Sampler'>
Parameters,damping,float,0.5,"Damping factor in the range `[0.5, 1.0)` is the extent to which the current value is maintained relative to incoming values (weighted 1 - damping). This in order to avoid numerical oscillations when updating these values (messages).",AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Parameters,max_iter,int,200,Maximum number of iterations.,AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Parameters,convergence_iter,int,15,Number of iterations with no change in the number of estimated clusters that stops the convergence.,AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Parameters,copy,bool,True,Make a copy of input data.,AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Parameters,preference,array-like of shape (n_samples,None,"Preferences for each point - points with larger values of preferences are more likely to be chosen as exemplars. The number of exemplars, ie of clusters, is influenced by the input preferences value. If the preferences are not passed as arguments, they will be set to the median of the input similarities.",AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Parameters,affinity,{'euclidean','euclidean',Which affinity to use. At the moment 'precomputed' and ``euclidean`` are supported. 'euclidean' uses the negative squared euclidean distance between points.,AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Parameters,verbose,bool,False,Whether to be verbose.,AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Attributes,random_state,int,None,Pseudo-random number generator to control the starting state. Use an int for reproducible results across function calls. See the :term:`Glossary <random_state>`. this parameter was previously hardcoded as 0.,AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Attributes,cluster_centers_indices_,ndarray of shape (n_clusters,,Indices of cluster centers.,AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Attributes,cluster_centers_,ndarray of shape (n_clusters,,Cluster centers (if affinity != ``precomputed``).,AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Attributes,labels_,ndarray of shape (n_samples,,Labels of each point.,AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Attributes,affinity_matrix_,ndarray of shape (n_samples,,Stores the affinity matrix used in ``fit``.,AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Attributes,n_iter_,int,,Number of iterations taken to converge.,AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,AffinityPropagation,<class 'sklearn.cluster._affinity_propagation.AffinityPropagation'>
Parameters,n_clusters,int or None,2,The number of clusters to find. It must be ``None`` if ``distance_threshold`` is not ``None``.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,metric,str or callable,"""euclidean""","Metric used to compute the linkage. Can be ""euclidean"", ""l1"", ""l2"", ""manhattan"", ""cosine"", or ""precomputed"". If linkage is ""ward"", only ""euclidean"" is accepted. If ""precomputed"", a distance matrix is needed as input for the fit method. If connectivity is None, linkage is ""single"" and affinity is not ""precomputed"" any valid pairwise distance metric can be assigned.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,memory,str or object with the joblib.Memory interface,None,"Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,connectivity,array-like,None,"Connectivity matrix. Defines for each sample the neighboring samples following a given structure of the data. This can be a connectivity matrix itself or a callable that transforms the data into a connectivity matrix, such as derived from `kneighbors_graph`. Default is ``None``, i.e, the hierarchical clustering algorithm is unstructured. For an example of connectivity matrix using :class:`~sklearn.neighbors.kneighbors_graph`, see :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,compute_full_tree,'auto' or bool,'auto',"Stop early the construction of the tree at ``n_clusters``. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree. It must be ``True`` if ``distance_threshold`` is not ``None``. By default `compute_full_tree` is ""auto"", which is equivalent to `True` when `distance_threshold` is not `None` or that `n_clusters` is inferior to the maximum between 100 or `0.02 * n_samples`. Otherwise, ""auto"" is equivalent to `False`.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,linkage,{'ward','ward',"Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. - 'ward' minimizes the variance of the clusters being merged. - 'average' uses the average of the distances of each observation of the two sets. - 'complete' or 'maximum' linkage uses the maximum distances between all observations of the two sets. - 'single' uses the minimum of the distances between all observations of the two sets. Added the 'single' option For examples comparing different `linkage` criteria, see :ref:`sphx_glr_auto_examples_cluster_plot_linkage_comparison.py`.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,distance_threshold,float,None,"The linkage distance threshold at or above which clusters will not be merged. If not ``None``, ``n_clusters`` must be ``None`` and ``compute_full_tree`` must be ``True``.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,compute_distances,bool,False,"Computes distances between clusters even if `distance_threshold` is not used. This can be used to make dendrogram visualization, but introduces a computational and memory overhead. For an example of dendrogram visualization, see :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_dendrogram.py`.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,n_clusters_,int,,"The number of clusters found by the algorithm. If ``distance_threshold=None``, it will be equal to the given ``n_clusters``.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,labels_,ndarray of shape (n_samples),,Cluster labels for each point.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,n_leaves_,int,,Number of leaves in the hierarchical tree.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,n_connected_components_,int,,The estimated number of connected components in the graph. ``n_connected_components_`` was added to replace ``n_components_``.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,children_,array-like of shape (n_samples-1,,"The children of each non-leaf node. Values less than `n_samples` correspond to leaves of the tree which are the original samples. A node `i` greater than or equal to `n_samples` is a non-leaf node and has children `children_[i - n_samples]`. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node `n_samples + i`.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
,distances_,array-like of shape (n_nodes-1,,Distances between nodes in the corresponding place in `children_`. Only computed if `distance_threshold` is used or `compute_distances` is set to `True`.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,n_clusters,int or None,2,The number of clusters to find. It must be ``None`` if ``distance_threshold`` is not ``None``.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,metric,str or callable,"""euclidean""","Metric used to compute the linkage. Can be ""euclidean"", ""l1"", ""l2"", ""manhattan"", ""cosine"", or ""precomputed"". If linkage is ""ward"", only ""euclidean"" is accepted. If ""precomputed"", a distance matrix is needed as input for the fit method. If connectivity is None, linkage is ""single"" and affinity is not ""precomputed"" any valid pairwise distance metric can be assigned.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,memory,str or object with the joblib.Memory interface,None,"Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,connectivity,array-like,None,"Connectivity matrix. Defines for each sample the neighboring samples following a given structure of the data. This can be a connectivity matrix itself or a callable that transforms the data into a connectivity matrix, such as derived from `kneighbors_graph`. Default is ``None``, i.e, the hierarchical clustering algorithm is unstructured. For an example of connectivity matrix using :class:`~sklearn.neighbors.kneighbors_graph`, see :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,compute_full_tree,'auto' or bool,'auto',"Stop early the construction of the tree at ``n_clusters``. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree. It must be ``True`` if ``distance_threshold`` is not ``None``. By default `compute_full_tree` is ""auto"", which is equivalent to `True` when `distance_threshold` is not `None` or that `n_clusters` is inferior to the maximum between 100 or `0.02 * n_samples`. Otherwise, ""auto"" is equivalent to `False`.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,linkage,{'ward','ward',"Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. - 'ward' minimizes the variance of the clusters being merged. - 'average' uses the average of the distances of each observation of the two sets. - 'complete' or 'maximum' linkage uses the maximum distances between all observations of the two sets. - 'single' uses the minimum of the distances between all observations of the two sets. Added the 'single' option For examples comparing different `linkage` criteria, see :ref:`sphx_glr_auto_examples_cluster_plot_linkage_comparison.py`.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,distance_threshold,float,None,"The linkage distance threshold at or above which clusters will not be merged. If not ``None``, ``n_clusters`` must be ``None`` and ``compute_full_tree`` must be ``True``.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,compute_distances,bool,False,"Computes distances between clusters even if `distance_threshold` is not used. This can be used to make dendrogram visualization, but introduces a computational and memory overhead. For an example of dendrogram visualization, see :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_dendrogram.py`.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,n_clusters_,int,,"The number of clusters found by the algorithm. If ``distance_threshold=None``, it will be equal to the given ``n_clusters``.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,labels_,ndarray of shape (n_samples),,Cluster labels for each point.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,n_leaves_,int,,Number of leaves in the hierarchical tree.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,n_connected_components_,int,,The estimated number of connected components in the graph. ``n_connected_components_`` was added to replace ``n_components_``.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Attributes,children_,array-like of shape (n_samples-1,,"The children of each non-leaf node. Values less than `n_samples` correspond to leaves of the tree which are the original samples. A node `i` greater than or equal to `n_samples` is a non-leaf node and has children `children_[i - n_samples]`. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node `n_samples + i`.",AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
,distances_,array-like of shape (n_nodes-1,,Distances between nodes in the corresponding place in `children_`. Only computed if `distance_threshold` is used or `compute_distances` is set to `True`.,AgglomerativeClustering,<class 'sklearn.cluster._agglomerative.AgglomerativeClustering'>
Parameters,estimator,object,None,"The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a :class:`~sklearn.tree.DecisionTreeClassifier`. `base_estimator` was renamed to `estimator`.",BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Parameters,n_estimators,int,10,The number of base estimators in the ensemble.,BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Parameters,max_samples,int or float,1.0,"The number of samples to draw from X to train each base estimator (with replacement by default, see `bootstrap` for more details). - If int, then draw `max_samples` samples. - If float, then draw `max_samples * X.shape[0]` samples.",BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Parameters,max_features,int or float,1.0,"The number of features to draw from X to train each base estimator ( without replacement by default, see `bootstrap_features` for more details). - If int, then draw `max_features` features. - If float, then draw `max(1, int(max_features * n_features_in_))` features.",BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Parameters,bootstrap,bool,True,"Whether samples are drawn with replacement. If False, sampling without replacement is performed.",BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Parameters,bootstrap_features,bool,False,Whether features are drawn with replacement.,BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Parameters,oob_score,bool,False,Whether to use out-of-bag samples to estimate the generalization error. Only available if bootstrap=True.,BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Parameters,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble. See :term:`the Glossary <warm_start>`. *warm_start* constructor parameter.",BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Parameters,n_jobs,int,None,The number of jobs to run in parallel for both :meth:`fit` and :meth:`predict`. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Parameters,random_state,int,None,"Controls the random resampling of the original dataset (sample wise and feature wise). If the base estimator accepts a `random_state` attribute, a different seed is generated for each instance in the ensemble. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Attributes,verbose,int,0,Controls the verbosity when fitting and predicting.,BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Attributes,estimator_,estimator,,The base estimator from which the ensemble is grown. `base_estimator_` was renamed to `estimator_`.,BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Attributes,estimators_,list of estimators,,The collection of fitted base estimators.,BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Attributes,estimators_samples_,list of arrays,,"The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each subset is defined by an array of the indices selected.",BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Attributes,estimators_features_,list of arrays,,The subset of drawn features for each base estimator.,BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,The classes labels.,BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Attributes,n_classes_,int or list,,The number of classes.,BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Attributes,oob_score_,float,,Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when ``oob_score`` is True.,BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
,oob_decision_function_,ndarray of shape (n_samples,,"Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob_decision_function_` might contain NaN. This attribute exists only when ``oob_score`` is True.",BaggingClassifier,<class 'sklearn.ensemble._bagging.BaggingClassifier'>
Parameters,estimator,object,None,"The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a :class:`~sklearn.tree.DecisionTreeRegressor`. `base_estimator` was renamed to `estimator`.",BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Parameters,n_estimators,int,10,The number of base estimators in the ensemble.,BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Parameters,max_samples,int or float,1.0,"The number of samples to draw from X to train each base estimator (with replacement by default, see `bootstrap` for more details). - If int, then draw `max_samples` samples. - If float, then draw `max_samples * X.shape[0]` samples.",BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Parameters,max_features,int or float,1.0,"The number of features to draw from X to train each base estimator ( without replacement by default, see `bootstrap_features` for more details). - If int, then draw `max_features` features. - If float, then draw `max(1, int(max_features * n_features_in_))` features.",BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Parameters,bootstrap,bool,True,"Whether samples are drawn with replacement. If False, sampling without replacement is performed.",BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Parameters,bootstrap_features,bool,False,Whether features are drawn with replacement.,BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Parameters,oob_score,bool,False,Whether to use out-of-bag samples to estimate the generalization error. Only available if bootstrap=True.,BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Parameters,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble. See :term:`the Glossary <warm_start>`.",BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Parameters,n_jobs,int,None,The number of jobs to run in parallel for both :meth:`fit` and :meth:`predict`. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Parameters,random_state,int,None,"Controls the random resampling of the original dataset (sample wise and feature wise). If the base estimator accepts a `random_state` attribute, a different seed is generated for each instance in the ensemble. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Attributes,verbose,int,0,Controls the verbosity when fitting and predicting.,BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Attributes,estimator_,estimator,,The base estimator from which the ensemble is grown. `base_estimator_` was renamed to `estimator_`.,BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Attributes,estimators_,list of estimators,,The collection of fitted sub-estimators.,BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Attributes,estimators_samples_,list of arrays,,"The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each subset is defined by an array of the indices selected.",BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Attributes,estimators_features_,list of arrays,,The subset of drawn features for each base estimator.,BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Attributes,oob_score_,float,,Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when ``oob_score`` is True.,BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
,oob_prediction_,ndarray of shape (n_samples,,"Prediction computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob_prediction_` might contain NaN. This attribute exists only when ``oob_score`` is True.",BaggingRegressor,<class 'sklearn.ensemble._bagging.BaggingRegressor'>
Parameters,n_components,int,1,The number of mixture components. Depending on the data and the value of the `weight_concentration_prior` the model can decide to not use all the components by setting some component `weights_` to values very close to zero. The number of effective components is therefore smaller than n_components.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,covariance_type,{'full','full',"String describing the type of covariance parameters to use. Must be one of: - 'full' (each component has its own general covariance matrix), - 'tied' (all components share the same general covariance matrix), - 'diag' (each component has its own diagonal covariance matrix), - 'spherical' (each component has its own single variance).",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,tol,float,1e-3,The convergence threshold. EM iterations will stop when the lower bound average gain on the likelihood (of the training data with respect to the model) is below this threshold.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,reg_covar,float,1e-6,Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,max_iter,int,100,The number of EM iterations to perform.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,n_init,int,1,The number of initializations to perform. The result with the highest lower bound value on the likelihood is kept.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,init_params,{'kmeans','kmeans',"The method used to initialize the weights, the means and the covariances. String must be one of: - 'kmeans': responsibilities are initialized using kmeans. - 'k-means++': use the k-means++ method to initialize. - 'random': responsibilities are initialized randomly. - 'random_from_data': initial means are randomly selected data points. `init_params` now accepts 'random_from_data' and 'k-means++' as initialization methods.",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,weight_concentration_prior_type,{'dirichlet_process','dirichlet_process',String describing the type of the weight concentration prior.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,weight_concentration_prior,float or None,None,"The dirichlet concentration of each component on the weight distribution (Dirichlet). This is commonly called gamma in the literature. The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the mixture weights simplex. The value of the parameter must be greater than 0. If it is None, it's set to ``1. / n_components``.",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,mean_precision_prior,float or None,None,"The precision prior on the mean distribution (Gaussian). Controls the extent of where means can be placed. Larger values concentrate the cluster means around `mean_prior`. The value of the parameter must be greater than 0. If it is None, it is set to 1.",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,mean_prior,array-like,None,"The prior on the mean distribution (Gaussian). If it is None, it is set to the mean of X.",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,degrees_of_freedom_prior,float or None,None,"The prior of the number of degrees of freedom on the covariance distributions (Wishart). If it is None, it's set to `n_features`.",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,covariance_prior,float or array-like,None,"The prior on the covariance distribution (Wishart). If it is None, the emiprical covariance prior is initialized using the covariance of X. The shape depends on `covariance_type`:: (n_features, n_features) if 'full', (n_features, n_features) if 'tied', (n_features)             if 'diag', float                    if 'spherical'",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,random_state,int,None,"Controls the random seed given to the method chosen to initialize the parameters (see `init_params`). In addition, it controls the generation of random samples from the fitted distribution (see the method `sample`). Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,warm_start,bool,False,"If 'warm_start' is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. See :term:`the Glossary <warm_start>`.",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,verbose,int,0,Enable verbose output. If 1 then it prints the current initialization and each iteration step. If greater than 1 then it prints also the log probability and the time needed for each step.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,verbose_interval,int,10,Number of iteration done before the next print.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,weights_,array-like of shape (n_components,,The weights of each mixture components.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,means_,array-like of shape (n_components,,The mean of each mixture component.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,covariances_,array-like,,"The covariance of each mixture component. The shape depends on `covariance_type`:: (n_components,)                        if 'spherical', (n_features, n_features)               if 'tied', (n_components, n_features)             if 'diag', (n_components, n_features, n_features) if 'full'",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,precisions_,array-like,,"The precision matrices for each component in the mixture. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on ``covariance_type``:: (n_components,)                        if 'spherical', (n_features, n_features)               if 'tied', (n_components, n_features)             if 'diag', (n_components, n_features, n_features) if 'full'",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,precisions_cholesky_,array-like,,"The cholesky decomposition of the precision matrices of each mixture component. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on ``covariance_type``:: (n_components,)                        if 'spherical', (n_features, n_features)               if 'tied', (n_components, n_features)             if 'diag', (n_components, n_features, n_features) if 'full'",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,converged_,bool,,"True when convergence of the best fit of inference was reached, False otherwise.",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,n_iter_,int,,Number of step used by the best fit of inference to reach the convergence.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,lower_bound_,float,,Lower bound value on the model evidence (of the training data) of the best fit of inference.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,weight_concentration_prior_,tuple or float,,"The dirichlet concentration of each component on the weight distribution (Dirichlet). The type depends on ``weight_concentration_prior_type``:: (float, float) if 'dirichlet_process' (Beta parameters), float          if 'dirichlet_distribution' (Dirichlet parameters). The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the simplex.",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,weight_concentration_,array-like of shape (n_components,,The dirichlet concentration of each component on the weight distribution (Dirichlet).,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,mean_precision_prior_,float,,"The precision prior on the mean distribution (Gaussian). Controls the extent of where means can be placed. Larger values concentrate the cluster means around `mean_prior`. If mean_precision_prior is set to None, `mean_precision_prior_` is set to 1.",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,mean_precision_,array-like of shape (n_components,,The precision of each components on the mean distribution (Gaussian).,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,mean_prior_,array-like of shape (n_features,,The prior on the mean distribution (Gaussian).,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,degrees_of_freedom_prior_,float,,The prior of the number of degrees of freedom on the covariance distributions (Wishart).,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,degrees_of_freedom_,array-like of shape (n_components,,The number of degrees of freedom of each components in the model.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,covariance_prior_,float or array-like,,"The prior on the covariance distribution (Wishart). The shape depends on `covariance_type`:: (n_features, n_features) if 'full', (n_features, n_features) if 'tied', (n_features)             if 'diag', float                    if 'spherical'",BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,BayesianGaussianMixture,<class 'sklearn.mixture._bayesian_mixture.BayesianGaussianMixture'>
Parameters,max_iter,int,300,Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,tol,float,1e-3,Stop the algorithm if w has converged.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,alpha_1,float,1e-6,Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,alpha_2,float,1e-6,Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,lambda_1,float,1e-6,Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,lambda_2,float,1e-6,Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,alpha_init,float,None,"Initial value for alpha (precision of the noise). If not set, alpha_init is 1/Var(y).",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,lambda_init,float,None,"Initial value for lambda (precision of the weights). If not set, lambda_init is 1.",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,compute_score,bool,False,"If True, compute the log marginal likelihood at each iteration of the optimization.",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. The intercept is not treated as a probabilistic parameter and thus has no associated variance. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,copy_X,bool,True,"If True, X will be copied; else, it may be overwritten.",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,verbose,bool,False,Verbose mode when fitting the model.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,coef_,array-like of shape (n_features,,Coefficients of the regression model (mean of distribution),BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,intercept_,float,,Independent term in decision function. Set to 0.0 if `fit_intercept = False`.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,alpha_,float,,,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,lambda_,float,,,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,sigma_,array-like of shape (n_features,,Estimated variance-covariance matrix of the weights,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,scores_,array-like of shape (n_iter_+1,,"If computed_score is True, value of the log marginal likelihood (to be maximized) at each iteration of the optimization. The array starts with the value of the log marginal likelihood obtained for the initial values of alpha and lambda and ends with the value obtained for the estimated alpha and lambda.",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,n_iter_,int,,The actual number of iterations to reach the stopping criterion.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,X_offset_,ndarray of shape (n_features,,"If `fit_intercept=True`, offset subtracted for centering data to a zero mean. Set to np.zeros(n_features) otherwise.",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,X_scale_,ndarray of shape (n_features,,Set to np.ones(n_features).,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,max_iter,int,300,Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,tol,float,1e-3,Stop the algorithm if w has converged.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,alpha_1,float,1e-6,Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,alpha_2,float,1e-6,Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,lambda_1,float,1e-6,Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,lambda_2,float,1e-6,Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,alpha_init,float,None,"Initial value for alpha (precision of the noise). If not set, alpha_init is 1/Var(y).",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,lambda_init,float,None,"Initial value for lambda (precision of the weights). If not set, lambda_init is 1.",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,compute_score,bool,False,"If True, compute the log marginal likelihood at each iteration of the optimization.",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. The intercept is not treated as a probabilistic parameter and thus has no associated variance. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,copy_X,bool,True,"If True, X will be copied; else, it may be overwritten.",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,verbose,bool,False,Verbose mode when fitting the model.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,coef_,array-like of shape (n_features,,Coefficients of the regression model (mean of distribution),BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,intercept_,float,,Independent term in decision function. Set to 0.0 if `fit_intercept = False`.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,alpha_,float,,,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,lambda_,float,,,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,sigma_,array-like of shape (n_features,,Estimated variance-covariance matrix of the weights,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,scores_,array-like of shape (n_iter_+1,,"If computed_score is True, value of the log marginal likelihood (to be maximized) at each iteration of the optimization. The array starts with the value of the log marginal likelihood obtained for the initial values of alpha and lambda and ends with the value obtained for the estimated alpha and lambda.",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,n_iter_,int,,The actual number of iterations to reach the stopping criterion.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,X_offset_,ndarray of shape (n_features,,"If `fit_intercept=True`, offset subtracted for centering data to a zero mean. Set to np.zeros(n_features) otherwise.",BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,X_scale_,ndarray of shape (n_features,,Set to np.ones(n_features).,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,BayesianRidge,<class 'sklearn.linear_model._bayes.BayesianRidge'>
Parameters,alpha,float or array-like of shape (n_features,1.0,"Additive (Laplace/Lidstone) smoothing parameter (set alpha=0 and force_alpha=True, for no smoothing).",BernoulliNB,<class 'sklearn.naive_bayes.BernoulliNB'>
Parameters,force_alpha,bool,True,"If False and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged. This may cause numerical errors if alpha is too close to 0. The default value of `force_alpha` changed to `True`.",BernoulliNB,<class 'sklearn.naive_bayes.BernoulliNB'>
Parameters,binarize,float or None,0.0,"Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors.",BernoulliNB,<class 'sklearn.naive_bayes.BernoulliNB'>
Parameters,fit_prior,bool,True,"Whether to learn class prior probabilities or not. If false, a uniform prior will be used.",BernoulliNB,<class 'sklearn.naive_bayes.BernoulliNB'>
Attributes,class_prior,array-like of shape (n_classes,None,"Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.",BernoulliNB,<class 'sklearn.naive_bayes.BernoulliNB'>
Attributes,class_count_,ndarray of shape (n_classes,,Number of samples encountered for each class during fitting. This value is weighted by the sample weight when provided.,BernoulliNB,<class 'sklearn.naive_bayes.BernoulliNB'>
Attributes,class_log_prior_,ndarray of shape (n_classes,,Log probability of each class (smoothed).,BernoulliNB,<class 'sklearn.naive_bayes.BernoulliNB'>
Attributes,classes_,ndarray of shape (n_classes,,Class labels known to the classifier,BernoulliNB,<class 'sklearn.naive_bayes.BernoulliNB'>
Attributes,feature_count_,ndarray of shape (n_classes,,"Number of samples encountered for each (class, feature) during fitting. This value is weighted by the sample weight when provided.",BernoulliNB,<class 'sklearn.naive_bayes.BernoulliNB'>
Attributes,feature_log_prob_,ndarray of shape (n_classes,,"Empirical log probability of features given a class, P(x_i|y).",BernoulliNB,<class 'sklearn.naive_bayes.BernoulliNB'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,BernoulliNB,<class 'sklearn.naive_bayes.BernoulliNB'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,BernoulliNB,<class 'sklearn.naive_bayes.BernoulliNB'>
Parameters,n_components,int,256,Number of binary hidden units.,BernoulliRBM,<class 'sklearn.neural_network._rbm.BernoulliRBM'>
Parameters,learning_rate,float,0.1,"The learning rate for weight updates. It is *highly* recommended to tune this hyper-parameter. Reasonable values are in the 10**[0., -3.] range.",BernoulliRBM,<class 'sklearn.neural_network._rbm.BernoulliRBM'>
Parameters,batch_size,int,10,Number of examples per minibatch.,BernoulliRBM,<class 'sklearn.neural_network._rbm.BernoulliRBM'>
Parameters,n_iter,int,10,Number of iterations/sweeps over the training dataset to perform during training.,BernoulliRBM,<class 'sklearn.neural_network._rbm.BernoulliRBM'>
Parameters,verbose,int,0,"The verbosity level. The default, zero, means silent mode. Range of values is [0, inf].",BernoulliRBM,<class 'sklearn.neural_network._rbm.BernoulliRBM'>
Attributes,random_state,int,None,"Determines random number generation for: - Gibbs sampling from visible and hidden layers. - Initializing components, sampling from layers during fit. - Corrupting the data when scoring samples. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.",BernoulliRBM,<class 'sklearn.neural_network._rbm.BernoulliRBM'>
Attributes,intercept_hidden_,array-like of shape (n_components,,Biases of the hidden units.,BernoulliRBM,<class 'sklearn.neural_network._rbm.BernoulliRBM'>
Attributes,intercept_visible_,array-like of shape (n_features,,Biases of the visible units.,BernoulliRBM,<class 'sklearn.neural_network._rbm.BernoulliRBM'>
Attributes,components_,array-like of shape (n_components,,"Weight matrix, where `n_features` is the number of visible units and `n_components` is the number of hidden units.",BernoulliRBM,<class 'sklearn.neural_network._rbm.BernoulliRBM'>
Attributes,h_samples_,array-like of shape (batch_size,,"Hidden Activation sampled from the model distribution, where `batch_size` is the number of examples per minibatch and `n_components` is the number of hidden units.",BernoulliRBM,<class 'sklearn.neural_network._rbm.BernoulliRBM'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,BernoulliRBM,<class 'sklearn.neural_network._rbm.BernoulliRBM'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,BernoulliRBM,<class 'sklearn.neural_network._rbm.BernoulliRBM'>
Parameters,threshold,float,0.0,"Feature values below or equal to this are replaced by 0, above it by 1. Threshold may not be less than 0 for operations on sparse matrices.",Binarizer,<class 'sklearn.preprocessing._data.Binarizer'>
Attributes,copy,bool,True,Set to False to perform inplace binarization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix).,Binarizer,<class 'sklearn.preprocessing._data.Binarizer'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,Binarizer,<class 'sklearn.preprocessing._data.Binarizer'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,Binarizer,<class 'sklearn.preprocessing._data.Binarizer'>
Parameters,threshold,float,0.5,The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Otherwise a new subcluster is started. Setting this value to be very low promotes splitting and vice-versa.,Birch,<class 'sklearn.cluster._birch.Birch'>
Parameters,branching_factor,int,50,Maximum number of CF subclusters in each node. If a new samples enters such that the number of subclusters exceed the branching_factor then that node is split into two nodes with the subclusters redistributed in each. The parent subcluster of that node is removed and two new subclusters are added as parents of the 2 split nodes.,Birch,<class 'sklearn.cluster._birch.Birch'>
Parameters,n_clusters,int,3,"Number of clusters after the final clustering step, which treats the subclusters from the leaves as new samples. - `None` : the final clustering step is not performed and the subclusters are returned as they are. - :mod:`sklearn.cluster` Estimator : If a model is provided, the model is fit treating the subclusters as new samples and the initial data is mapped to the label of the closest subcluster. - `int` : the model fit is :class:`AgglomerativeClustering` with `n_clusters` set to be equal to the int.",Birch,<class 'sklearn.cluster._birch.Birch'>
Parameters,compute_labels,bool,True,Whether or not to compute labels for each fit.,Birch,<class 'sklearn.cluster._birch.Birch'>
Attributes,copy,bool,True,"Whether or not to make a copy of the given data. If set to False, the initial data will be overwritten. `copy` was deprecated in 1.6 and will be removed in 1.8. It has no effect as the estimator does not perform in-place operations on the input data.",Birch,<class 'sklearn.cluster._birch.Birch'>
Attributes,root_,_CFNode,,Root of the CFTree.,Birch,<class 'sklearn.cluster._birch.Birch'>
Attributes,dummy_leaf_,_CFNode,,Start pointer to all the leaves.,Birch,<class 'sklearn.cluster._birch.Birch'>
Attributes,subcluster_centers_,ndarray,,Centroids of all subclusters read directly from the leaves.,Birch,<class 'sklearn.cluster._birch.Birch'>
Attributes,subcluster_labels_,ndarray,,Labels assigned to the centroids of the subclusters after they are clustered globally.,Birch,<class 'sklearn.cluster._birch.Birch'>
Attributes,labels_,ndarray of shape (n_samples,,"Array of labels assigned to the input data. if partial_fit is used instead of fit, they are assigned to the last batch of data.",Birch,<class 'sklearn.cluster._birch.Birch'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,Birch,<class 'sklearn.cluster._birch.Birch'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,Birch,<class 'sklearn.cluster._birch.Birch'>
Parameters,n_clusters,int,8,The number of clusters to form as well as the number of centroids to generate.,BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Parameters,init,{'k-means++','random',"Method for initialization: 'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. 'random': choose `n_clusters` observations (rows) at random from data for the initial centroids. If a callable is passed, it should take arguments X, n_clusters and a random state and return an initialization.",BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Parameters,n_init,int,1,Number of time the inner k-means algorithm will be run with different centroid seeds in each bisection. That will result producing for each bisection best output of n_init consecutive runs in terms of inertia.,BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Parameters,random_state,int,None,Determines random number generation for centroid initialization in inner K-Means. Use an int to make the randomness deterministic. See :term:`Glossary <random_state>`.,BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Parameters,max_iter,int,300,Maximum number of iterations of the inner k-means algorithm at each bisection.,BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Parameters,verbose,int,0,Verbosity mode.,BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Parameters,tol,float,1e-4,Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations  to declare convergence. Used in inner k-means algorithm at each bisection to pick best possible clusters.,BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Parameters,copy_x,bool,True,"When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean. Note that if the original data is not C-contiguous, a copy will be made even if copy_x is False. If the original data is sparse, but not in CSR format, a copy will be made even if copy_x is False.",BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Parameters,algorithm,"{""lloyd""","""lloyd""","Inner K-means algorithm used in bisection. The classical EM-style algorithm is `""lloyd""`. The `""elkan""` variation can be more efficient on some datasets with well-defined clusters, by using the triangle inequality. However it's more memory intensive due to the allocation of an extra array of shape `(n_samples, n_clusters)`.",BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Attributes,bisecting_strategy,"{""biggest_inertia""","""biggest_inertia""","Defines how bisection should be performed: - ""biggest_inertia"" means that BisectingKMeans will always check all calculated cluster for cluster with biggest SSE (Sum of squared errors) and bisect it. This approach concentrates on precision, but may be costly in terms of execution time (especially for larger amount of data points). - ""largest_cluster"" - BisectingKMeans will always split cluster with largest amount of points assigned to it from all clusters previously calculated. That should work faster than picking by SSE ('biggest_inertia') and may produce similar results in most cases.",BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Attributes,cluster_centers_,ndarray of shape (n_clusters,,"Coordinates of cluster centers. If the algorithm stops before fully converging (see ``tol`` and ``max_iter``), these will not be consistent with ``labels_``.",BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Attributes,labels_,ndarray of shape (n_samples,,Labels of each point.,BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Attributes,inertia_,float,,"Sum of squared distances of samples to their closest cluster center, weighted by the sample weights if provided.",BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,BisectingKMeans,<class 'sklearn.cluster._bisect_k_means.BisectingKMeans'>
Parameters,n_components,int,2,"Number of components to keep. Should be in `[1, min(n_samples, n_features, n_targets)]`.",CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Parameters,scale,bool,True,Whether to scale `X` and `Y`.,CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Parameters,max_iter,int,500,The maximum number of iterations of the power method.,CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Parameters,tol,float,1e-06,"The tolerance used as convergence criteria in the power method: the algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less than `tol`, where `u` corresponds to the left singular vector.",CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Attributes,copy,bool,True,"Whether to copy `X` and `Y` in fit before applying centering, and potentially scaling. If False, these operations will be done inplace, modifying both arrays.",CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Attributes,x_weights_,ndarray of shape (n_features,,The left singular vectors of the cross-covariance matrices of each iteration.,CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Attributes,y_weights_,ndarray of shape (n_targets,,The right singular vectors of the cross-covariance matrices of each iteration.,CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Attributes,x_loadings_,ndarray of shape (n_features,,The loadings of `X`.,CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Attributes,y_loadings_,ndarray of shape (n_targets,,The loadings of `Y`.,CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Attributes,x_rotations_,ndarray of shape (n_features,,The projection matrix used to transform `X`.,CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Attributes,y_rotations_,ndarray of shape (n_targets,,The projection matrix used to transform `Y`.,CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Attributes,coef_,ndarray of shape (n_targets,,The coefficients of the linear model such that `Y` is approximated as `Y = X @ coef_.T + intercept_`.,CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Attributes,intercept_,ndarray of shape (n_targets,,The intercepts of the linear model such that `Y` is approximated as `Y = X @ coef_.T + intercept_`.,CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Attributes,n_iter_,list of shape (n_components,,"Number of iterations of the power method, for each component.",CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,CCA,<class 'sklearn.cross_decomposition._pls.CCA'>
Parameters,estimator,estimator instance,None,The classifier whose output need to be calibrated to provide more accurate `predict_proba` outputs. The default classifier is a :class:`~sklearn.svm.LinearSVC`.,CalibratedClassifierCV,<class 'sklearn.calibration.CalibratedClassifierCV'>
Parameters,method,{'sigmoid','sigmoid',The method to use for calibration. Can be 'sigmoid' which corresponds to Platt's method (i.e. a logistic regression model) or 'isotonic' which is a non-parametric approach. It is not advised to use isotonic calibration with too few calibration samples ``(<<1000)`` since it tends to overfit.,CalibratedClassifierCV,<class 'sklearn.calibration.CalibratedClassifierCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if ``y`` is binary or multiclass, :class:`~sklearn.model_selection.StratifiedKFold` is used. If ``y`` is neither binary nor multiclass, :class:`~sklearn.model_selection.KFold` is used. Refer to the :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ``cv`` default value if None changed from 3-fold to 5-fold. `""prefit""` is deprecated. Use :class:`~sklearn.frozen.FrozenEstimator` instead.",CalibratedClassifierCV,<class 'sklearn.calibration.CalibratedClassifierCV'>
Parameters,n_jobs,int,None,"Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. Base estimator clones are fitted in parallel across cross-validation iterations. Therefore parallelism happens only when `cv != ""prefit""`. See :term:`Glossary <n_jobs>` for more details.",CalibratedClassifierCV,<class 'sklearn.calibration.CalibratedClassifierCV'>
Attributes,ensemble,bool,"""auto""","Determines how the calibrator is fitted. ""auto"" will use `False` if the `estimator` is a :class:`~sklearn.frozen.FrozenEstimator`, and `True` otherwise. If `True`, the `estimator` is fitted using training data, and calibrated using testing data, for each `cv` fold. The final estimator is an ensemble of `n_cv` fitted classifier and calibrator pairs, where `n_cv` is the number of cross-validation folds. The output is the average predicted probabilities of all pairs. If `False`, `cv` is used to compute unbiased predictions, via :func:`~sklearn.model_selection.cross_val_predict`, which are then used for calibration. At prediction time, the classifier used is the `estimator` trained on all the data. Note that this method is also internally implemented  in :mod:`sklearn.svm` estimators with the `probabilities=True` parameter. `""auto""` option is added and is the default.",CalibratedClassifierCV,<class 'sklearn.calibration.CalibratedClassifierCV'>
Attributes,classes_,ndarray of shape (n_classes,,The class labels.,CalibratedClassifierCV,<class 'sklearn.calibration.CalibratedClassifierCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,CalibratedClassifierCV,<class 'sklearn.calibration.CalibratedClassifierCV'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,CalibratedClassifierCV,<class 'sklearn.calibration.CalibratedClassifierCV'>
,calibrated_classifiers_,list (len() equal to cv or 1 if `ensemble=False`),,"The list of classifier and calibrator pairs. - When `ensemble=True`, `n_cv` fitted `estimator` and calibrator pairs. `n_cv` is the number of cross-validation folds. - When `ensemble=False`, the `estimator`, fitted on all the data, and fitted calibrator. Single calibrated classifier case when `ensemble=False`.",CalibratedClassifierCV,<class 'sklearn.calibration.CalibratedClassifierCV'>
Parameters,alpha,float,1.0,"Additive (Laplace/Lidstone) smoothing parameter (set alpha=0 and force_alpha=True, for no smoothing).",CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>
Parameters,force_alpha,bool,True,"If False and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged. This may cause numerical errors if alpha is too close to 0. The default value of `force_alpha` changed to `True`.",CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>
Parameters,fit_prior,bool,True,"Whether to learn class prior probabilities or not. If false, a uniform prior will be used.",CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>
Parameters,class_prior,array-like of shape (n_classes,None,"Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.",CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>
Attributes,min_categories,int or array-like of shape (n_features,None,"Minimum number of categories per feature. - integer: Sets the minimum number of categories per feature to `n_categories` for each features. - array-like: shape (n_features,) where `n_categories[i]` holds the minimum number of categories for the ith column of the input. - None (default): Determines the number of categories automatically from the training data.",CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>
Attributes,category_count_,list of arrays of shape (n_features,,"Holds arrays of shape (n_classes, n_categories of respective feature) for each feature. Each array provides the number of samples encountered for each class and category of the specific feature.",CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>
Attributes,class_count_,ndarray of shape (n_classes,,Number of samples encountered for each class during fitting. This value is weighted by the sample weight when provided.,CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>
Attributes,class_log_prior_,ndarray of shape (n_classes,,Smoothed empirical log probability for each class.,CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>
Attributes,classes_,ndarray of shape (n_classes,,Class labels known to the classifier,CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>
Attributes,feature_log_prob_,list of arrays of shape (n_features,,"Holds arrays of shape (n_classes, n_categories of respective feature) for each feature. Each array provides the empirical log probability of categories given the respective feature and class, ``P(x_i|y)``.",CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>
,n_categories_,ndarray of shape (n_features,,Number of categories for each feature. This value is inferred from the data or set by the minimum number of categories.,CategoricalNB,<class 'sklearn.naive_bayes.CategoricalNB'>
Parameters,base_estimator,estimator,,The base estimator from which the classifier chain is built.,ClassifierChain,<class 'sklearn.multioutput.ClassifierChain'>
Parameters,order,array-like of shape (n_outputs,None,"If `None`, the order will be determined by the order of columns in the label matrix Y.:: order = [0, 1, 2, ..., Y.shape[1] - 1] The order of the chain can be explicitly set by providing a list of integers. For example, for a chain of length 5.:: order = [1, 3, 2, 4, 0] means that the first model in the chain will make predictions for column 1 in the Y matrix, the second model will make predictions for column 3, etc. If order is `random` a random ordering will be used.",ClassifierChain,<class 'sklearn.multioutput.ClassifierChain'>
Parameters,cv,int,None,"Determines whether to use cross validated predictions or true labels for the results of previous estimators in the chain. Possible inputs for cv are: - None, to use true labels when fitting, - integer, to specify the number of folds in a (Stratified)KFold, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.",ClassifierChain,<class 'sklearn.multioutput.ClassifierChain'>
Parameters,chain_method,{'predict','predict',"Prediction method to be used by estimators in the chain for the 'prediction' features of previous estimators in the chain. - if `str`, name of the method; - if a list of `str`, provides the method names in order of preference. The method used corresponds to the first method in the list that is implemented by `base_estimator`.",ClassifierChain,<class 'sklearn.multioutput.ClassifierChain'>
Parameters,random_state,int,,"If ``order='random'``, determines random number generation for the chain order. In addition, it controls the random seed given at each `base_estimator` at each chaining iteration. Thus, it is only used when `base_estimator` exposes a `random_state`. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",ClassifierChain,<class 'sklearn.multioutput.ClassifierChain'>
Attributes,verbose,bool,False,"If True, chain progress is output as each model is completed.",ClassifierChain,<class 'sklearn.multioutput.ClassifierChain'>
Attributes,classes_,list,,A list of arrays of length ``len(estimators_)`` containing the class labels for each estimator in the chain.,ClassifierChain,<class 'sklearn.multioutput.ClassifierChain'>
Attributes,estimators_,list,,A list of clones of base_estimator.,ClassifierChain,<class 'sklearn.multioutput.ClassifierChain'>
Attributes,order_,list,,The order of labels in the classifier chain.,ClassifierChain,<class 'sklearn.multioutput.ClassifierChain'>
Attributes,chain_method_,str,,Prediction method used by estimators in the chain for the prediction features.,ClassifierChain,<class 'sklearn.multioutput.ClassifierChain'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying `base_estimator` exposes such an attribute when fit.,ClassifierChain,<class 'sklearn.multioutput.ClassifierChain'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,ClassifierChain,<class 'sklearn.multioutput.ClassifierChain'>
Parameters,transformers,list of tuples,,"List of (name, transformer, columns) tuples specifying the transformer objects to be applied to subsets of the data. name : str Like in Pipeline and FeatureUnion, this allows the transformer and its parameters to be set using ``set_params`` and searched in grid search. transformer : {'drop', 'passthrough'} or estimator Estimator must support :term:`fit` and :term:`transform`. Special-cased strings 'drop' and 'passthrough' are accepted as well, to indicate to drop the columns or to pass them through untransformed, respectively. columns :  str, array-like of str, int, array-like of int,                 array-like of bool, slice or callable Indexes the data on its second axis. Integers are interpreted as positional columns, while strings can reference DataFrame columns by name.  A scalar string or int should be used where ``transformer`` expects X to be a 1d array-like (vector), otherwise a 2d array will be passed to the transformer. A callable is passed the input data `X` and can return any of the above. To select multiple columns by name or dtype, you can use :obj:`make_column_selector`.",ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
Parameters,remainder,{'drop','drop',"By default, only the specified columns in `transformers` are transformed and combined in the output, and the non-specified columns are dropped. (default of ``'drop'``). By specifying ``remainder='passthrough'``, all remaining columns that were not specified in `transformers`, but present in the data passed to `fit` will be automatically passed through. This subset of columns is concatenated with the output of the transformers. For dataframes, extra columns not seen during `fit` will be excluded from the output of `transform`. By setting ``remainder`` to be an estimator, the remaining non-specified columns will use the ``remainder`` estimator. The estimator must support :term:`fit` and :term:`transform`. Note that using this feature requires that the DataFrame columns input at :term:`fit` and :term:`transform` have identical order.",ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
Parameters,sparse_threshold,float,0.3,"If the output of the different transformers contains sparse matrices, these will be stacked as a sparse matrix if the overall density is lower than this value. Use ``sparse_threshold=0`` to always return dense.  When the transformed output consists of all dense data, the stacked result will be dense, and this keyword will be ignored.",ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
Parameters,n_jobs,int,None,Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
Parameters,transformer_weights,dict,None,"Multiplicative weights for features per transformer. The output of the transformer is multiplied by these weights. Keys are transformer names, values the weights.",ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
Parameters,verbose,bool,False,"If True, the time elapsed while fitting each transformer will be printed as it is completed.",ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
Parameters,verbose_feature_names_out,bool,True,"- If True, :meth:`ColumnTransformer.get_feature_names_out` will prefix all feature names with the name of the transformer that generated that feature. It is equivalent to setting `verbose_feature_names_out=""{transformer_name}__{feature_name}""`. - If False, :meth:`ColumnTransformer.get_feature_names_out` will not prefix any feature names and will error if feature names are not unique. - If ``Callable[[str, str], str]``, :meth:`ColumnTransformer.get_feature_names_out` will rename all the features using the name of the transformer. The first argument of the callable is the transformer name and the second argument is the feature name. The returned string will be the new feature name. - If ``str``, it must be a string ready for formatting. The given string will be formatted using two field names: ``transformer_name`` and ``feature_name``. e.g. ``""{feature_name}__{transformer_name}""``. See :meth:`str.format` method from the standard library for more info. `verbose_feature_names_out` can be a callable or a string to be formatted.",ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
Attributes,force_int_remainder_cols,bool,True,"Force the columns of the last entry of `transformers_`, which corresponds to the ""remainder"" transformer, to always be stored as indices (int) rather than column names (str). See description of the `transformers_` attribute for details. If you do not access the list of columns for the remainder columns in the `transformers_` fitted attribute, you do not need to set this parameter. The default value for `force_int_remainder_cols` will change from `True` to `False` in version 1.7.",ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
Attributes,transformers_,list,,"The collection of fitted transformers as tuples of (name, fitted_transformer, column). `fitted_transformer` can be an estimator, or `'drop'`; `'passthrough'` is replaced with an equivalent :class:`~sklearn.preprocessing.FunctionTransformer`. In case there were no columns selected, this will be the unfitted transformer. If there are remaining columns, the final element is a tuple of the form: ('remainder', transformer, remaining_columns) corresponding to the ``remainder`` parameter. If there are remaining columns, then ``len(transformers_)==len(transformers)+1``, otherwise ``len(transformers_)==len(transformers)``. If there are remaining columns and `force_int_remainder_cols` is True, the remaining columns are always represented by their positional indices in the input `X` (as in older versions). If `force_int_remainder_cols` is False, the format attempts to match that of the other transformers: if all columns were provided as column names (`str`), the remaining columns are stored as column names; if all columns were provided as mask arrays (`bool`), so are the remaining columns; in all other cases the remaining columns are stored as indices (`int`).",ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
Attributes,named_transformers_,:class:`~sklearn.utils.Bunch`,,Read-only attribute to access any transformer by given name. Keys are transformer names and values are the fitted transformer objects.,ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
Attributes,sparse_output_,bool,,"Boolean flag indicating whether the output of ``transform`` is a sparse matrix or a dense numpy array, which depends on the output of the individual transformers and the `sparse_threshold` keyword.",ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
Attributes,output_indices_,dict,,"A dictionary from each transformer name to a slice, where the slice corresponds to indices in the transformed output. This is useful to inspect which transformer is responsible for which transformed feature(s).",ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying transformers expose such an attribute when fit.,ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,ColumnTransformer,<class 'sklearn.compose._column_transformer.ColumnTransformer'>
Parameters,alpha,float or array-like of shape (n_features,1.0,"Additive (Laplace/Lidstone) smoothing parameter (set alpha=0 and force_alpha=True, for no smoothing).",ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>
Parameters,force_alpha,bool,True,"If False and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged. This may cause numerical errors if alpha is too close to 0. The default value of `force_alpha` changed to `True`.",ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>
Parameters,fit_prior,bool,True,Only used in edge case with a single class in the training set.,ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>
Parameters,class_prior,array-like of shape (n_classes,None,Prior probabilities of the classes. Not used.,ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>
Attributes,norm,bool,False,"Whether or not a second normalization of the weights is performed. The default behavior mirrors the implementations found in Mahout and Weka, which do not follow the full algorithm described in Table 9 of the paper.",ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>
Attributes,class_count_,ndarray of shape (n_classes,,Number of samples encountered for each class during fitting. This value is weighted by the sample weight when provided.,ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>
Attributes,class_log_prior_,ndarray of shape (n_classes,,Smoothed empirical log probability for each class. Only used in edge case with a single class in the training set.,ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>
Attributes,classes_,ndarray of shape (n_classes,,Class labels known to the classifier,ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>
Attributes,feature_all_,ndarray of shape (n_features,,Number of samples encountered for each feature during fitting. This value is weighted by the sample weight when provided.,ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>
Attributes,feature_count_,ndarray of shape (n_classes,,"Number of samples encountered for each (class, feature) during fitting. This value is weighted by the sample weight when provided.",ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>
Attributes,feature_log_prob_,ndarray of shape (n_classes,,Empirical weights for class complements.,ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,ComplementNB,<class 'sklearn.naive_bayes.ComplementNB'>
Parameters,input,{'filename','content',"- If `'filename'`, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze. - If `'file'`, the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory. - If `'content'`, the input is expected to be a sequence of items that can be of type string or byte.",CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,encoding,str,'utf-8',"If bytes or files are given to analyze, this encoding is used to decode.",CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,decode_error,{'strict','strict',"Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'.",CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,strip_accents,{'ascii',None,Remove accents and perform other character normalization during the preprocessing step. 'ascii' is a fast method that only works on characters that have a direct ASCII mapping. 'unicode' is a slightly slower method that works on any characters. None (default) means no character normalization is performed. Both 'ascii' and 'unicode' use NFKD normalization from :func:`unicodedata.normalize`.,CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,lowercase,bool,True,Convert all characters to lowercase before tokenizing.,CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,preprocessor,callable,None,Override the preprocessing (strip_accents and lowercase) stage while preserving the tokenizing and n-grams generation steps. Only applies if ``analyzer`` is not callable.,CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,tokenizer,callable,None,Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if ``analyzer == 'word'``.,CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,stop_words,{'english'},None,"If 'english', a built-in stop word list for English is used. There are several known issues with 'english' and you should consider an alternative (see :ref:`stop_words`). If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if ``analyzer == 'word'``. If None, no stop words will be used. In this case, setting `max_df` to a higher value, such as in the range (0.7, 1.0), can automatically detect and filter stop words based on intra corpus document frequency of terms.",CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,token_pattern,str or None,"r""(?u)\\b\\w\\w+\\b""","Regular expression denoting what constitutes a ""token"", only used if ``analyzer == 'word'``. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). If there is a capturing group in token_pattern then the captured group content, not the entire match, becomes the token. At most one capturing group is permitted.",CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,ngram_range,tuple (min_n,(1,"The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means only bigrams. Only applies if ``analyzer`` is not callable.",CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,analyzer,{'word','word',"Whether the feature should be made of word n-gram or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space. If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. Since v0.21, if ``input`` is ``filename`` or ``file``, the data is first read from the file and then passed to the given callable analyzer.",CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,max_df,float in range [0.0,1.0,"When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.",CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,min_df,float in range [0.0,1,"When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.",CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,max_features,int,None,"If not None, build a vocabulary that only consider the top `max_features` ordered by term frequency across the corpus. Otherwise, all features are used. This parameter is ignored if vocabulary is not None.",CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,vocabulary,Mapping or iterable,None,"Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. Indices in the mapping should not be repeated and should not have any gap between 0 and the largest index.",CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,binary,bool,False,"If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.",CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Attributes,dtype,dtype,np.int64,Type of the matrix returned by fit_transform() or transform().,CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Attributes,vocabulary_,dict,,A mapping of terms to feature indices.,CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
,fixed_vocabulary_,bool,,True if a fixed vocabulary of term to indices mapping is provided by the user.,CountVectorizer,<class 'sklearn.feature_extraction.text.CountVectorizer'>
Parameters,eps,float,0.5,The maximum distance between two samples for one to be considered as in the neighborhood of the other. This is not a maximum bound on the distances of points within a cluster. This is the most important DBSCAN parameter to choose appropriately for your data set and distance function.,DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>
Parameters,min_samples,int,5,"The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself. If `min_samples` is set to a higher value, DBSCAN will find denser clusters, whereas if it is set to a lower value, the found clusters will be more sparse.",DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>
Parameters,metric,str,'euclidean',"The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by :func:`sklearn.metrics.pairwise_distances` for its metric parameter. If metric is ""precomputed"", X is assumed to be a distance matrix and must be square. X may be a :term:`sparse graph`, in which case only ""nonzero"" elements may be considered neighbors for DBSCAN. metric *precomputed* to accept precomputed sparse matrix.",DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>
Parameters,metric_params,dict,None,Additional keyword arguments for the metric function.,DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>
Parameters,algorithm,{'auto','auto',The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details.,DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>
Parameters,leaf_size,int,30,"Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.",DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>
Parameters,p,float,None,"The power of the Minkowski metric to be used to calculate distance between points. If None, then ``p=2`` (equivalent to the Euclidean distance).",DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>
Attributes,n_jobs,int,None,The number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>
Attributes,core_sample_indices_,ndarray of shape (n_core_samples,,Indices of core samples.,DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>
Attributes,components_,ndarray of shape (n_core_samples,,Copy of each core sample found by training.,DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>
Attributes,labels_,ndarray of shape (n_samples),,Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1.,DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,DBSCAN,<class 'sklearn.cluster._dbscan.DBSCAN'>
Parameters,criterion,"{""gini""","""gini""","The function to measure the quality of a split. Supported criteria are ""gini"" for the Gini impurity and ""log_loss"" and ""entropy"" both for the Shannon information gain, see :ref:`tree_mathematical_formulation`.",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Parameters,splitter,"{""best""","""best""","The strategy used to choose the split at each node. Supported strategies are ""best"" to choose the best split and ""random"" to choose the best random split.",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Parameters,max_depth,int,None,"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. Added float values for fractions.",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. Added float values for fractions.",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Parameters,min_weight_fraction_leaf,float,0.0,The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.,DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Parameters,max_features,int,None,"The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `max(1, int(max_features * n_features_in_))` features are considered at each split. - If ""sqrt"", then `max_features=sqrt(n_features)`. - If ""log2"", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. The search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features.",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Parameters,random_state,int,None,"Controls the randomness of the estimator. The features are always randomly permuted at each split, even if ``splitter`` is set to ``""best""``. When ``max_features < n_features``, the algorithm will select ``max_features`` at random at each split before finding the best split among them. But the best found split may vary across different runs, even if ``max_features=n_features``. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, ``random_state`` has to be fixed to an integer. See :term:`Glossary <random_state>` for details.",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Parameters,max_leaf_nodes,int,None,Grow a tree with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.,DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Parameters,class_weight,dict,None,"Weights associated with classes in the form ``{class_label: weight}``. If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` For multi-output, the weights of each column of y will be multiplied. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Parameters,ccp_alpha,non-negative float,0.0,"Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. See :ref:`minimal_cost_complexity_pruning` for details. See :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py` for an example of such pruning.",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Attributes,monotonic_cst,array-like of int of shape (n_features),None,"Indicates the monotonicity constraint to enforce on each feature. - 1: monotonic increase - 0: no constraint - -1: monotonic decrease If monotonic_cst is None, no constraints are applied. Monotonicity constraints are not supported for: - multiclass classifications (i.e. when `n_classes > 2`), - multioutput classifications (i.e. when `n_outputs_ > 1`), - classifications trained on data with missing values. The constraints hold over the probability of the positive class. Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,"The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Attributes,feature_importances_,ndarray of shape (n_features,,"The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.  It is also known as the Gini importance [4]_. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Attributes,max_features_,int,,The inferred value of max_features.,DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Attributes,n_classes_,int or list of int,,"The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems).",DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Attributes,n_outputs_,int,,The number of outputs when ``fit`` is performed.,DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
,tree_,Tree instance,,The underlying Tree object. Please refer to ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py` for basic usage of these attributes.,DecisionTreeClassifier,<class 'sklearn.tree._classes.DecisionTreeClassifier'>
Parameters,criterion,"{""squared_error""","""squared_error""","The function to measure the quality of a split. Supported criteria are ""squared_error"" for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, ""friedman_mse"", which uses mean squared error with Friedman's improvement score for potential splits, ""absolute_error"" for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and ""poisson"" which uses reduction in the half mean Poisson deviance to find splits. Mean Absolute Error (MAE) criterion. Poisson deviance criterion.",DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Parameters,splitter,"{""best""","""best""","The strategy used to choose the split at each node. Supported strategies are ""best"" to choose the best split and ""random"" to choose the best random split.",DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Parameters,max_depth,int,None,"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. For an example of how ``max_depth`` influences the model, see :ref:`sphx_glr_auto_examples_tree_plot_tree_regression.py`.",DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. Added float values for fractions.",DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. Added float values for fractions.",DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Parameters,min_weight_fraction_leaf,float,0.0,The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.,DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Parameters,max_features,int,None,"The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `max(1, int(max_features * n_features_in_))` features are considered at each split. - If ""sqrt"", then `max_features=sqrt(n_features)`. - If ""log2"", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features.",DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Parameters,random_state,int,None,"Controls the randomness of the estimator. The features are always randomly permuted at each split, even if ``splitter`` is set to ``""best""``. When ``max_features < n_features``, the algorithm will select ``max_features`` at random at each split before finding the best split among them. But the best found split may vary across different runs, even if ``max_features=n_features``. That is the case, if the improvement of the criterion is identical for several splits and one split has to be selected at random. To obtain a deterministic behaviour during fitting, ``random_state`` has to be fixed to an integer. See :term:`Glossary <random_state>` for details.",DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Parameters,max_leaf_nodes,int,None,Grow a tree with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.,DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Parameters,ccp_alpha,non-negative float,0.0,"Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. See :ref:`minimal_cost_complexity_pruning` for details. See :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py` for an example of such pruning.",DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Attributes,monotonic_cst,array-like of int of shape (n_features),None,"Indicates the monotonicity constraint to enforce on each feature. - 1: monotonic increase - 0: no constraint - -1: monotonic decrease If monotonic_cst is None, no constraints are applied. Monotonicity constraints are not supported for: - multioutput regressions (i.e. when `n_outputs_ > 1`), - regressions trained on data with missing values. Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.",DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Attributes,feature_importances_,ndarray of shape (n_features,,"The feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance [4]_. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.",DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Attributes,max_features_,int,,The inferred value of max_features.,DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Attributes,n_outputs_,int,,The number of outputs when ``fit`` is performed.,DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
,tree_,Tree instance,,The underlying Tree object. Please refer to ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py` for basic usage of these attributes.,DecisionTreeRegressor,<class 'sklearn.tree._classes.DecisionTreeRegressor'>
Parameters,dtype,dtype,np.float64,The type of feature values. Passed to Numpy array/scipy.sparse matrix constructors as the dtype argument.,DictVectorizer,<class 'sklearn.feature_extraction._dict_vectorizer.DictVectorizer'>
Parameters,separator,str,"""",Separator string used when constructing new features for one-hot coding.,DictVectorizer,<class 'sklearn.feature_extraction._dict_vectorizer.DictVectorizer'>
Parameters,sparse,bool,True,Whether transform should produce scipy.sparse matrices.,DictVectorizer,<class 'sklearn.feature_extraction._dict_vectorizer.DictVectorizer'>
Attributes,sort,bool,True,Whether ``feature_names_`` and ``vocabulary_`` should be sorted when fitting.,DictVectorizer,<class 'sklearn.feature_extraction._dict_vectorizer.DictVectorizer'>
Attributes,vocabulary_,dict,,A dictionary mapping feature names to feature indices.,DictVectorizer,<class 'sklearn.feature_extraction._dict_vectorizer.DictVectorizer'>
,feature_names_,list,,"A list of length n_features containing the feature names (e.g., ""f=ham"" and ""f=spam"").",DictVectorizer,<class 'sklearn.feature_extraction._dict_vectorizer.DictVectorizer'>
Parameters,n_components,int,None,"Number of dictionary elements to extract. If None, then ``n_components`` is set to ``n_features``.",DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,alpha,float,1.0,Sparsity controlling parameter.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,max_iter,int,1000,Maximum number of iterations to perform.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,tol,float,1e-8,Tolerance for numerical error.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,fit_algorithm,{'lars','lars',* `'lars'`: uses the least angle regression method to solve the lasso problem (:func:`~sklearn.linear_model.lars_path`); * `'cd'`: uses the coordinate descent method to compute the Lasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will be faster if the estimated components are sparse. *cd* coordinate descent method to improve speed.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,transform_algorithm,{'lasso_lars','omp',Algorithm used to transform the data: - `'lars'`: uses the least angle regression method (:func:`~sklearn.linear_model.lars_path`); - `'lasso_lars'`: uses Lars to compute the Lasso solution. - `'lasso_cd'`: uses the coordinate descent method to compute the Lasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'` will be faster if the estimated components are sparse. - `'omp'`: uses orthogonal matching pursuit to estimate the sparse solution. - `'threshold'`: squashes to zero all coefficients less than alpha from the projection ``dictionary * X'``. *lasso_cd* coordinate descent method to improve speed.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,transform_n_nonzero_coefs,int,None,"Number of nonzero coefficients to target in each column of the solution. This is only used by `algorithm='lars'` and `algorithm='omp'`. If `None`, then `transform_n_nonzero_coefs=int(n_features / 10)`.",DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,transform_alpha,float,None,"If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the penalty applied to the L1 norm. If `algorithm='threshold'`, `alpha` is the absolute value of the threshold below which coefficients will be squashed to zero. If `None`, defaults to `alpha`. When None, default value changed from 1.0 to `alpha`.",DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,n_jobs,int or None,None,Number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,code_init,ndarray of shape (n_samples,None,"Initial value for the code, for warm restart. Only used if `code_init` and `dict_init` are not None.",DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,dict_init,ndarray of shape (n_components,None,"Initial values for the dictionary, for warm restart. Only used if `code_init` and `dict_init` are not None.",DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,callback,callable,None,Callable that gets invoked every five iterations.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,verbose,bool,False,To control the verbosity of the procedure.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,split_sign,bool,False,Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,random_state,int,None,"Used for initializing the dictionary when ``dict_init`` is not specified, randomly shuffling the data when ``shuffle`` is set to ``True``, and updating the dictionary. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.",DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,positive_code,bool,False,Whether to enforce positivity when finding the code.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,positive_dict,bool,False,Whether to enforce positivity when finding the dictionary.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Attributes,transform_max_iter,int,1000,Maximum number of iterations to perform if `algorithm='lasso_cd'` or `'lasso_lars'`.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Attributes,components_,ndarray of shape (n_components,,dictionary atoms extracted from the data,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Attributes,error_,array,,vector of errors at each iteration,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
,n_iter_,int,,Number of iterations run.,DictionaryLearning,<class 'sklearn.decomposition._dict_learning.DictionaryLearning'>
Parameters,strategy,"{""most_frequent""","""prior""","Strategy to use to generate predictions. * ""most_frequent"": the `predict` method always returns the most frequent class label in the observed `y` argument passed to `fit`. The `predict_proba` method returns the matching one-hot encoded vector. * ""prior"": the `predict` method always returns the most frequent class label in the observed `y` argument passed to `fit` (like ""most_frequent""). ``predict_proba`` always returns the empirical class distribution of `y` also known as the empirical class prior distribution. * ""stratified"": the `predict_proba` method randomly samples one-hot vectors from a multinomial distribution parametrized by the empirical class prior probabilities. The `predict` method returns the class label which got probability one in the one-hot vector of `predict_proba`. Each sampled row of both methods is therefore independent and identically distributed. * ""uniform"": generates predictions uniformly at random from the list of unique classes observed in `y`, i.e. each class has equal probability. * ""constant"": always predicts a constant label that is provided by the user. This is useful for metrics that evaluate a non-majority class. The default value of `strategy` has changed to ""prior"" in version 0.24.",DummyClassifier,<class 'sklearn.dummy.DummyClassifier'>
Parameters,random_state,int,None,Controls the randomness to generate the predictions when ``strategy='stratified'`` or ``strategy='uniform'``. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,DummyClassifier,<class 'sklearn.dummy.DummyClassifier'>
Attributes,constant,int or str or array-like of shape (n_outputs,None,"The explicit constant as predicted by the ""constant"" strategy. This parameter is useful only for the ""constant"" strategy.",DummyClassifier,<class 'sklearn.dummy.DummyClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,"Unique class labels observed in `y`. For multi-output classification problems, this attribute is a list of arrays as each output has an independent set of possible classes.",DummyClassifier,<class 'sklearn.dummy.DummyClassifier'>
Attributes,n_classes_,int or list of int,,Number of label for each output.,DummyClassifier,<class 'sklearn.dummy.DummyClassifier'>
Attributes,class_prior_,ndarray of shape (n_classes,,"Frequency of each class observed in `y`. For multioutput classification problems, this is computed independently for each output.",DummyClassifier,<class 'sklearn.dummy.DummyClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,DummyClassifier,<class 'sklearn.dummy.DummyClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,DummyClassifier,<class 'sklearn.dummy.DummyClassifier'>
Attributes,n_outputs_,int,,Number of outputs.,DummyClassifier,<class 'sklearn.dummy.DummyClassifier'>
,sparse_output_,bool,,True if the array returned from predict is to be in sparse CSC format. Is automatically set to True if the input `y` is passed in sparse format.,DummyClassifier,<class 'sklearn.dummy.DummyClassifier'>
Parameters,strategy,"{""mean""","""mean""","Strategy to use to generate predictions. * ""mean"": always predicts the mean of the training set * ""median"": always predicts the median of the training set * ""quantile"": always predicts a specified quantile of the training set, provided with the quantile parameter. * ""constant"": always predicts a constant value that is provided by the user.",DummyRegressor,<class 'sklearn.dummy.DummyRegressor'>
Parameters,constant,int or float or array-like of shape (n_outputs,None,"The explicit constant as predicted by the ""constant"" strategy. This parameter is useful only for the ""constant"" strategy.",DummyRegressor,<class 'sklearn.dummy.DummyRegressor'>
Attributes,quantile,float in [0.0,None,"The quantile to predict using the ""quantile"" strategy. A quantile of 0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the maximum.",DummyRegressor,<class 'sklearn.dummy.DummyRegressor'>
Attributes,constant_,ndarray of shape (1,,Mean or median or quantile of the training targets or constant value given by the user.,DummyRegressor,<class 'sklearn.dummy.DummyRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,DummyRegressor,<class 'sklearn.dummy.DummyRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,DummyRegressor,<class 'sklearn.dummy.DummyRegressor'>
,n_outputs_,int,,Number of outputs.,DummyRegressor,<class 'sklearn.dummy.DummyRegressor'>
Parameters,alpha,float,1.0,"Constant that multiplies the penalty terms. Defaults to 1.0. See the notes for the exact mathematical meaning of this parameter. ``alpha = 0`` is equivalent to an ordinary least square, solved by the :class:`LinearRegression` object. For numerical reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised. Given this, you should use the :class:`LinearRegression` object.",ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Parameters,l1_ratio,float,0.5,"The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2.",ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Parameters,fit_intercept,bool,True,"Whether the intercept should be estimated or not. If ``False``, the data is assumed to be already centered.",ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Parameters,precompute,bool or array-like of shape (n_features,False,Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix can also be passed as argument. For sparse input this option is always ``False`` to preserve sparsity. Check :ref:`an example on how to use a precomputed Gram Matrix in ElasticNet <sphx_glr_auto_examples_linear_model_plot_elastic_net_precomputed_gram_matrix_with_weighted_samples.py>` for details.,ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Parameters,max_iter,int,1000,The maximum number of iterations.,ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Parameters,copy_X,bool,True,"If ``True``, X will be copied; else, it may be overwritten.",ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Parameters,tol,float,1e-4,"The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``, see Notes below.",ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Parameters,positive,bool,False,"When set to ``True``, forces the coefficients to be positive.",ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Parameters,random_state,int,None,The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Attributes,selection,{'cyclic','cyclic',"If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.",ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Attributes,coef_,ndarray of shape (n_features,,Parameter vector (w in the cost function formula).,ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Attributes,sparse_coef_,sparse matrix of shape (n_features,,Sparse representation of the `coef_`.,ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Attributes,intercept_,float or ndarray of shape (n_targets,,Independent term in decision function.,ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Attributes,n_iter_,list of int,,Number of iterations run by the coordinate descent solver to reach the specified tolerance.,ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Attributes,dual_gap_,float or ndarray of shape (n_targets,,"Given param alpha, the dual gaps at the end of the optimization, same shape as each observation of y.",ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,ElasticNet,<class 'sklearn.linear_model._coordinate_descent.ElasticNet'>
Parameters,l1_ratio,float or list of float,0.5,"Float between 0 and 1 passed to ElasticNet (scaling between l1 and l2 penalties). For ``l1_ratio = 0`` the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2 This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7, .9, .95, .99, 1]``.",ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,eps,float,1e-3,Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,n_alphas,int,100,"Number of alphas along the regularization path, used for each l1_ratio.",ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,alphas,array-like,None,List of alphas where to compute the models. If None alphas are set automatically.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,precompute,'auto','auto',Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,max_iter,int,1000,The maximum number of iterations.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,tol,float,1e-4,"The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``.",ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - int, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For int/None inputs, :class:`~sklearn.model_selection.KFold` is used. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ``cv`` default value if None changed from 3-fold to 5-fold.",ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,copy_X,bool,True,"If ``True``, X will be copied; else, it may be overwritten.",ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,verbose,bool or int,0,Amount of verbosity.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,n_jobs,int,None,Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,positive,bool,False,"When set to ``True``, forces the coefficients to be positive.",ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,random_state,int,None,The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Attributes,selection,{'cyclic','cyclic',"If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.",ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Attributes,alpha_,float,,The amount of penalization chosen by cross validation.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Attributes,l1_ratio_,float,,The compromise between l1 and l2 penalization chosen by cross validation.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Attributes,coef_,ndarray of shape (n_features,,Parameter vector (w in the cost function formula).,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Attributes,intercept_,float or ndarray of shape (n_targets,,Independent term in the decision function.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Attributes,mse_path_,ndarray of shape (n_l1_ratio,,"Mean square error for the test set on each fold, varying l1_ratio and alpha.",ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Attributes,alphas_,ndarray of shape (n_alphas,,"The grid of alphas used for fitting, for each l1_ratio.",ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Attributes,dual_gap_,float,,The dual gaps at the end of the optimization for the optimal alpha.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Attributes,n_iter_,int,,Number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,ElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.ElasticNetCV'>
Parameters,store_precision,bool,True,Specify if the estimated precision is stored.,EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Parameters,assume_centered,bool,False,"If True, the support of robust location and covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.",EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Parameters,support_fraction,float,None,"The proportion of points to be included in the support of the raw MCD estimate. If None, the minimum value of support_fraction will be used within the algorithm: `(n_samples + n_features + 1) / 2 * n_samples`. Range is (0, 1).",EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Parameters,contamination,float,0.1,"The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Range is (0, 0.5].",EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Attributes,random_state,int,None,Determines the pseudo random number generator for shuffling the data. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Attributes,location_,ndarray of shape (n_features,,Estimated robust location.,EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Attributes,covariance_,ndarray of shape (n_features,,Estimated robust covariance matrix.,EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Attributes,precision_,ndarray of shape (n_features,,Estimated pseudo inverse matrix. (stored only if store_precision is True),EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Attributes,support_,ndarray of shape (n_samples,,A mask of the observations that have been used to compute the robust estimates of location and shape.,EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Attributes,offset_,float,,Offset used to define the decision function from the raw scores. We have the relation: ``decision_function = score_samples - offset_``. The offset depends on the contamination parameter and is defined in such a way we obtain the expected number of outliers (samples with decision function < 0) in training.,EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Attributes,raw_location_,ndarray of shape (n_features,,The raw robust estimated location before correction and re-weighting.,EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Attributes,raw_covariance_,ndarray of shape (n_features,,The raw robust estimated covariance before correction and re-weighting.,EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Attributes,raw_support_,ndarray of shape (n_samples,,"A mask of the observations that have been used to compute the raw robust estimates of location and shape, before correction and re-weighting.",EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Attributes,dist_,ndarray of shape (n_samples,,Mahalanobis distances of the training set (on which :meth:`fit` is called) observations.,EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,EllipticEnvelope,<class 'sklearn.covariance._elliptic_envelope.EllipticEnvelope'>
Parameters,store_precision,bool,True,Specifies if the estimated precision is stored.,EmpiricalCovariance,<class 'sklearn.covariance._empirical_covariance.EmpiricalCovariance'>
Attributes,assume_centered,bool,False,"If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data are centered before computation.",EmpiricalCovariance,<class 'sklearn.covariance._empirical_covariance.EmpiricalCovariance'>
Attributes,location_,ndarray of shape (n_features,,"Estimated location, i.e. the estimated mean.",EmpiricalCovariance,<class 'sklearn.covariance._empirical_covariance.EmpiricalCovariance'>
Attributes,covariance_,ndarray of shape (n_features,,Estimated covariance matrix,EmpiricalCovariance,<class 'sklearn.covariance._empirical_covariance.EmpiricalCovariance'>
Attributes,precision_,ndarray of shape (n_features,,Estimated pseudo-inverse matrix. (stored only if store_precision is True),EmpiricalCovariance,<class 'sklearn.covariance._empirical_covariance.EmpiricalCovariance'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,EmpiricalCovariance,<class 'sklearn.covariance._empirical_covariance.EmpiricalCovariance'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,EmpiricalCovariance,<class 'sklearn.covariance._empirical_covariance.EmpiricalCovariance'>
Parameters,criterion,"{""gini""","""gini""","The function to measure the quality of a split. Supported criteria are ""gini"" for the Gini impurity and ""log_loss"" and ""entropy"" both for the Shannon information gain, see :ref:`tree_mathematical_formulation`.",ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Parameters,splitter,"{""random""","""random""","The strategy used to choose the split at each node. Supported strategies are ""best"" to choose the best split and ""random"" to choose the best random split.",ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Parameters,max_depth,int,None,"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.",ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. Added float values for fractions.",ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. Added float values for fractions.",ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Parameters,min_weight_fraction_leaf,float,0.0,The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.,ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Parameters,max_features,int,"""sqrt""","The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `max(1, int(max_features * n_features_in_))` features are considered at each split. - If ""sqrt"", then `max_features=sqrt(n_features)`. - If ""log2"", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. The default of `max_features` changed from `""auto""` to `""sqrt""`. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features.",ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Parameters,random_state,int,None,Used to pick randomly the `max_features` used at each split. See :term:`Glossary <random_state>` for details.,ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Parameters,max_leaf_nodes,int,None,Grow a tree with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.,ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Parameters,class_weight,dict,None,"Weights associated with classes in the form ``{class_label: weight}``. If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` For multi-output, the weights of each column of y will be multiplied. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.",ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Parameters,ccp_alpha,non-negative float,0.0,"Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. See :ref:`minimal_cost_complexity_pruning` for details. See :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py` for an example of such pruning.",ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Attributes,monotonic_cst,array-like of int of shape (n_features),None,"Indicates the monotonicity constraint to enforce on each feature. - 1: monotonic increase - 0: no constraint - -1: monotonic decrease If monotonic_cst is None, no constraints are applied. Monotonicity constraints are not supported for: - multiclass classifications (i.e. when `n_classes > 2`), - multioutput classifications (i.e. when `n_outputs_ > 1`), - classifications trained on data with missing values. The constraints hold over the probability of the positive class. Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.",ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,"The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).",ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Attributes,max_features_,int,,The inferred value of max_features.,ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Attributes,n_classes_,int or list of int,,"The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems).",ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Attributes,feature_importances_,ndarray of shape (n_features,,"The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.  It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.",ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Attributes,n_outputs_,int,,The number of outputs when ``fit`` is performed.,ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
,tree_,Tree instance,,The underlying Tree object. Please refer to ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py` for basic usage of these attributes.,ExtraTreeClassifier,<class 'sklearn.tree._classes.ExtraTreeClassifier'>
Parameters,criterion,"{""squared_error""","""squared_error""","The function to measure the quality of a split. Supported criteria are ""squared_error"" for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, ""friedman_mse"", which uses mean squared error with Friedman's improvement score for potential splits, ""absolute_error"" for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and ""poisson"" which uses reduction in Poisson deviance to find splits. Mean Absolute Error (MAE) criterion. Poisson deviance criterion.",ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Parameters,splitter,"{""random""","""random""","The strategy used to choose the split at each node. Supported strategies are ""best"" to choose the best split and ""random"" to choose the best random split.",ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Parameters,max_depth,int,None,"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.",ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. Added float values for fractions.",ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. Added float values for fractions.",ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Parameters,min_weight_fraction_leaf,float,0.0,The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.,ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Parameters,max_features,int,1.0,"The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `max(1, int(max_features * n_features_in_))` features are considered at each split. - If ""sqrt"", then `max_features=sqrt(n_features)`. - If ""log2"", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. The default of `max_features` changed from `""auto""` to `1.0`. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features.",ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Parameters,random_state,int,None,Used to pick randomly the `max_features` used at each split. See :term:`Glossary <random_state>` for details.,ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Parameters,max_leaf_nodes,int,None,Grow a tree with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.,ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Parameters,ccp_alpha,non-negative float,0.0,"Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. See :ref:`minimal_cost_complexity_pruning` for details. See :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py` for an example of such pruning.",ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Attributes,monotonic_cst,array-like of int of shape (n_features),None,"Indicates the monotonicity constraint to enforce on each feature. - 1: monotonic increase - 0: no constraint - -1: monotonic decrease If monotonic_cst is None, no constraints are applied. Monotonicity constraints are not supported for: - multioutput regressions (i.e. when `n_outputs_ > 1`), - regressions trained on data with missing values. Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.",ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Attributes,max_features_,int,,The inferred value of max_features.,ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Attributes,feature_importances_,ndarray of shape (n_features,,"Return impurity-based feature importances (the higher, the more important the feature). Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.",ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Attributes,n_outputs_,int,,The number of outputs when ``fit`` is performed.,ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
,tree_,Tree instance,,The underlying Tree object. Please refer to ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py` for basic usage of these attributes.,ExtraTreeRegressor,<class 'sklearn.tree._classes.ExtraTreeRegressor'>
Parameters,n_estimators,int,100,The number of trees in the forest. The default value of ``n_estimators`` changed from 10 to 100 in 0.22.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,criterion,"{""gini""","""gini""","The function to measure the quality of a split. Supported criteria are ""gini"" for the Gini impurity and ""log_loss"" and ""entropy"" both for the Shannon information gain, see :ref:`tree_mathematical_formulation`. Note: This parameter is tree-specific.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,max_depth,int,None,"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. Added float values for fractions.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. Added float values for fractions.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,min_weight_fraction_leaf,float,0.0,The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,max_features,"{""sqrt""","""sqrt""","The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `max(1, int(max_features * n_features_in_))` features are considered at each split. - If ""sqrt"", then `max_features=sqrt(n_features)`. - If ""log2"", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. The default of `max_features` changed from `""auto""` to `""sqrt""`. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,max_leaf_nodes,int,None,Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,bootstrap,bool,False,"Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,oob_score,bool or callable,False,"Whether to use out-of-bag samples to estimate the generalization score. By default, :func:`~sklearn.metrics.accuracy_score` is used. Provide a callable with signature `metric(y_true, y_pred)` to use a custom metric. Only available if `bootstrap=True`.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,n_jobs,int,None,"The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`, :meth:`decision_path` and :meth:`apply` are all parallelized over the trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,random_state,int,None,Controls 3 sources of randomness: - the bootstrapping of the samples used when building trees (if ``bootstrap=True``) - the sampling of the features to consider when looking for the best split at each node (if ``max_features < n_features``) - the draw of the splits for each of the `max_features` See :term:`Glossary <random_state>` for details.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,verbose,int,0,Controls the verbosity when fitting and predicting.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See :term:`Glossary <warm_start>` and :ref:`tree_ensemble_warm_start` for details.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,class_weight,"{""balanced""",None,"Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` The ""balanced_subsample"" mode is the same as ""balanced"" except that weights are computed based on the bootstrap sample for every tree grown. For multi-output, the weights of each column of y will be multiplied. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,ccp_alpha,non-negative float,0.0,"Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. See :ref:`minimal_cost_complexity_pruning` for details. See :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py` for an example of such pruning.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,max_samples,int or float,None,"If bootstrap is True, the number of samples to draw from X to train each base estimator. - If None (default), then draw `X.shape[0]` samples. - If int, then draw `max_samples` samples. - If float, then draw `max_samples * X.shape[0]` samples. Thus, `max_samples` should be in the interval `(0.0, 1.0]`.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,monotonic_cst,array-like of int of shape (n_features),None,"Indicates the monotonicity constraint to enforce on each feature. - 1: monotonically increasing - 0: no constraint - -1: monotonically decreasing If monotonic_cst is None, no constraints are applied. Monotonicity constraints are not supported for: - multiclass classifications (i.e. when `n_classes > 2`), - multioutput classifications (i.e. when `n_outputs_ > 1`), - classifications trained on data with missing values. The constraints hold over the probability of the positive class. Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,estimator_,:class:`~sklearn.tree.ExtraTreeClassifier`,,The child estimator template used to create the collection of fitted sub-estimators. `base_estimator_` was renamed to `estimator_`.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,estimators_,list of DecisionTreeClassifier,,The collection of fitted sub-estimators.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,"The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,n_classes_,int or list,,"The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,feature_importances_,ndarray of shape (n_features,,"The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.  It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,n_outputs_,int,,The number of outputs when ``fit`` is performed.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,oob_score_,float,,Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when ``oob_score`` is True.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,oob_decision_function_,ndarray of shape (n_samples,,"Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob_decision_function_` might contain NaN. This attribute exists only when ``oob_score`` is True.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
,estimators_samples_,list of arrays,,"The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each subset is defined by an array of the indices selected.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,n_estimators,int,100,The number of trees in the forest. The default value of ``n_estimators`` changed from 10 to 100 in 0.22.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,criterion,"{""gini""","""gini""","The function to measure the quality of a split. Supported criteria are ""gini"" for the Gini impurity and ""log_loss"" and ""entropy"" both for the Shannon information gain, see :ref:`tree_mathematical_formulation`. Note: This parameter is tree-specific.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,max_depth,int,None,"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. Added float values for fractions.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. Added float values for fractions.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,min_weight_fraction_leaf,float,0.0,The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,max_features,"{""sqrt""","""sqrt""","The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `max(1, int(max_features * n_features_in_))` features are considered at each split. - If ""sqrt"", then `max_features=sqrt(n_features)`. - If ""log2"", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. The default of `max_features` changed from `""auto""` to `""sqrt""`. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,max_leaf_nodes,int,None,Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,bootstrap,bool,False,"Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,oob_score,bool or callable,False,"Whether to use out-of-bag samples to estimate the generalization score. By default, :func:`~sklearn.metrics.accuracy_score` is used. Provide a callable with signature `metric(y_true, y_pred)` to use a custom metric. Only available if `bootstrap=True`.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,n_jobs,int,None,"The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`, :meth:`decision_path` and :meth:`apply` are all parallelized over the trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,random_state,int,None,Controls 3 sources of randomness: - the bootstrapping of the samples used when building trees (if ``bootstrap=True``) - the sampling of the features to consider when looking for the best split at each node (if ``max_features < n_features``) - the draw of the splits for each of the `max_features` See :term:`Glossary <random_state>` for details.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,verbose,int,0,Controls the verbosity when fitting and predicting.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See :term:`Glossary <warm_start>` and :ref:`tree_ensemble_warm_start` for details.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,class_weight,"{""balanced""",None,"Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` The ""balanced_subsample"" mode is the same as ""balanced"" except that weights are computed based on the bootstrap sample for every tree grown. For multi-output, the weights of each column of y will be multiplied. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,ccp_alpha,non-negative float,0.0,"Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. See :ref:`minimal_cost_complexity_pruning` for details. See :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py` for an example of such pruning.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,max_samples,int or float,None,"If bootstrap is True, the number of samples to draw from X to train each base estimator. - If None (default), then draw `X.shape[0]` samples. - If int, then draw `max_samples` samples. - If float, then draw `max_samples * X.shape[0]` samples. Thus, `max_samples` should be in the interval `(0.0, 1.0]`.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,monotonic_cst,array-like of int of shape (n_features),None,"Indicates the monotonicity constraint to enforce on each feature. - 1: monotonically increasing - 0: no constraint - -1: monotonically decreasing If monotonic_cst is None, no constraints are applied. Monotonicity constraints are not supported for: - multiclass classifications (i.e. when `n_classes > 2`), - multioutput classifications (i.e. when `n_outputs_ > 1`), - classifications trained on data with missing values. The constraints hold over the probability of the positive class. Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,estimator_,:class:`~sklearn.tree.ExtraTreeClassifier`,,The child estimator template used to create the collection of fitted sub-estimators. `base_estimator_` was renamed to `estimator_`.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,estimators_,list of DecisionTreeClassifier,,The collection of fitted sub-estimators.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,"The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,n_classes_,int or list,,"The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,feature_importances_,ndarray of shape (n_features,,"The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.  It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,n_outputs_,int,,The number of outputs when ``fit`` is performed.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,oob_score_,float,,Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when ``oob_score`` is True.,ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Attributes,oob_decision_function_,ndarray of shape (n_samples,,"Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob_decision_function_` might contain NaN. This attribute exists only when ``oob_score`` is True.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
,estimators_samples_,list of arrays,,"The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each subset is defined by an array of the indices selected.",ExtraTreesClassifier,<class 'sklearn.ensemble._forest.ExtraTreesClassifier'>
Parameters,n_estimators,int,100,The number of trees in the forest. The default value of ``n_estimators`` changed from 10 to 100 in 0.22.,ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,criterion,"{""squared_error""","""squared_error""","The function to measure the quality of a split. Supported criteria are ""squared_error"" for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, ""friedman_mse"", which uses mean squared error with Friedman's improvement score for potential splits, ""absolute_error"" for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and ""poisson"" which uses reduction in Poisson deviance to find splits. Training using ""absolute_error"" is significantly slower than when using ""squared_error"". Mean Absolute Error (MAE) criterion.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,max_depth,int,None,"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. Added float values for fractions.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. Added float values for fractions.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,min_weight_fraction_leaf,float,0.0,The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.,ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,max_features,"{""sqrt""",1.0,"The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `max(1, int(max_features * n_features_in_))` features are considered at each split. - If ""sqrt"", then `max_features=sqrt(n_features)`. - If ""log2"", then `max_features=log2(n_features)`. - If None or 1.0, then `max_features=n_features`. The default of 1.0 is equivalent to bagged trees and more randomness can be achieved by setting smaller values, e.g. 0.3. The default of `max_features` changed from `""auto""` to 1.0. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,max_leaf_nodes,int,None,Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.,ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,bootstrap,bool,False,"Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,oob_score,bool or callable,False,"Whether to use out-of-bag samples to estimate the generalization score. By default, :func:`~sklearn.metrics.r2_score` is used. Provide a callable with signature `metric(y_true, y_pred)` to use a custom metric. Only available if `bootstrap=True`.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,n_jobs,int,None,"The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`, :meth:`decision_path` and :meth:`apply` are all parallelized over the trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,random_state,int,None,Controls 3 sources of randomness: - the bootstrapping of the samples used when building trees (if ``bootstrap=True``) - the sampling of the features to consider when looking for the best split at each node (if ``max_features < n_features``) - the draw of the splits for each of the `max_features` See :term:`Glossary <random_state>` for details.,ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,verbose,int,0,Controls the verbosity when fitting and predicting.,ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See :term:`Glossary <warm_start>` and :ref:`tree_ensemble_warm_start` for details.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,ccp_alpha,non-negative float,0.0,"Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. See :ref:`minimal_cost_complexity_pruning` for details. See :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py` for an example of such pruning.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,max_samples,int or float,None,"If bootstrap is True, the number of samples to draw from X to train each base estimator. - If None (default), then draw `X.shape[0]` samples. - If int, then draw `max_samples` samples. - If float, then draw `max_samples * X.shape[0]` samples. Thus, `max_samples` should be in the interval `(0.0, 1.0]`.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Attributes,monotonic_cst,array-like of int of shape (n_features),None,"Indicates the monotonicity constraint to enforce on each feature. - 1: monotonically increasing - 0: no constraint - -1: monotonically decreasing If monotonic_cst is None, no constraints are applied. Monotonicity constraints are not supported for: - multioutput regressions (i.e. when `n_outputs_ > 1`), - regressions trained on data with missing values. Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Attributes,estimator_,:class:`~sklearn.tree.ExtraTreeRegressor`,,The child estimator template used to create the collection of fitted sub-estimators. `base_estimator_` was renamed to `estimator_`.,ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Attributes,estimators_,list of DecisionTreeRegressor,,The collection of fitted sub-estimators.,ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Attributes,feature_importances_,ndarray of shape (n_features,,"The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.  It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Attributes,n_outputs_,int,,The number of outputs.,ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Attributes,oob_score_,float,,Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when ``oob_score`` is True.,ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Attributes,oob_prediction_,ndarray of shape (n_samples,,Prediction computed with out-of-bag estimate on the training set. This attribute exists only when ``oob_score`` is True.,ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
,estimators_samples_,list of arrays,,"The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each subset is defined by an array of the indices selected.",ExtraTreesRegressor,<class 'sklearn.ensemble._forest.ExtraTreesRegressor'>
Parameters,n_components,int,None,"Dimensionality of latent space, the number of components of ``X`` that are obtained after ``transform``. If None, n_components is set to the number of features.",FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Parameters,tol,float,1e-2,Stopping tolerance for log-likelihood increase.,FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Parameters,copy,bool,True,"Whether to make a copy of X. If ``False``, the input X gets overwritten during fitting.",FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Parameters,max_iter,int,1000,Maximum number of iterations.,FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Parameters,noise_variance_init,array-like of shape (n_features,None,"The initial guess of the noise variance for each feature. If None, it defaults to np.ones(n_features).",FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Parameters,svd_method,{'lapack','randomized',"Which SVD method to use. If 'lapack' use standard SVD from scipy.linalg, if 'randomized' use fast ``randomized_svd`` function. Defaults to 'randomized'. For most applications 'randomized' will be sufficiently precise while providing significant speed gains. Accuracy can also be improved by setting higher values for `iterated_power`. If this is not sufficient, for maximum precision you should choose 'lapack'.",FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Parameters,iterated_power,int,3,Number of iterations for the power method. 3 by default. Only used if ``svd_method`` equals 'randomized'.,FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Parameters,rotation,{'varimax',None,"If not None, apply the indicated rotation. Currently, varimax and quartimax are implemented. See `""The varimax criterion for analytic rotation in factor analysis"" <https://link.springer.com/article/10.1007%2FBF02289233>`_ H. F. Kaiser, 1958.",FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Attributes,random_state,int or RandomState instance,0,Only used when ``svd_method`` equals 'randomized'. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Attributes,components_,ndarray of shape (n_components,,Components with maximum variance.,FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Attributes,loglike_,list of shape (n_iterations,,The log likelihood at each iteration.,FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Attributes,noise_variance_,ndarray of shape (n_features,,The estimated noise variance for each feature.,FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Attributes,n_iter_,int,,Number of iterations run.,FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Attributes,mean_,ndarray of shape (n_features,,"Per-feature empirical mean, estimated from the training set.",FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,FactorAnalysis,<class 'sklearn.decomposition._factor_analysis.FactorAnalysis'>
Parameters,n_components,int,None,"Number of components to use. If None is passed, all are used.",FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Parameters,algorithm,{'parallel','parallel',Specify which algorithm to use for FastICA.,FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Parameters,whiten,str or bool,'unit-variance',"Specify the whitening strategy to use. - If 'arbitrary-variance', a whitening with variance arbitrary is used. - If 'unit-variance', the whitening matrix is rescaled to ensure that each recovered source has unit variance. - If False, the data is already considered to be whitened, and no whitening is performed. The default value of `whiten` changed to 'unit-variance' in 1.3.",FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Parameters,fun,{'logcosh','logcosh',"The functional form of the G function used in the approximation to neg-entropy. Could be either 'logcosh', 'exp', or 'cube'. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. The derivative should be averaged along its last dimension. Example:: def my_g(x): return x ** 3, (3 * x ** 2).mean(axis=-1)",FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Parameters,fun_args,dict,None,"Arguments to send to the functional form. If empty or None and if fun='logcosh', fun_args will take value {'alpha' : 1.0}.",FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Parameters,max_iter,int,200,Maximum number of iterations during fit.,FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Parameters,tol,float,1e-4,A positive scalar giving the tolerance at which the un-mixing matrix is considered to have converged.,FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Parameters,w_init,array-like of shape (n_components,None,"Initial un-mixing array. If `w_init=None`, then an array of values drawn from a normal distribution is used.",FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Parameters,whiten_solver,"{""eigh""","""svd""","The solver to use for whitening. - ""svd"" is more stable numerically if the problem is degenerate, and often faster when `n_samples <= n_features`. - ""eigh"" is generally more memory efficient when `n_samples >= n_features`, and can be faster when `n_samples >= 50 * n_features`.",FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Attributes,random_state,int,None,"Used to initialize ``w_init`` when not specified, with a normal distribution. Pass an int, for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.",FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Attributes,components_,ndarray of shape (n_components,,"The linear operator to apply to the data to get the independent sources. This is equal to the unmixing matrix when ``whiten`` is False, and equal to ``np.dot(unmixing_matrix, self.whitening_)`` when ``whiten`` is True.",FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Attributes,mixing_,ndarray of shape (n_features,,The pseudo-inverse of ``components_``. It is the linear operator that maps independent sources to the data.,FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Attributes,mean_,ndarray of shape(n_features,,The mean over features. Only set if `self.whiten` is True.,FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Attributes,n_iter_,int,,"If the algorithm is ""deflation"", n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge.",FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
,whitening_,ndarray of shape (n_components,,Only set if whiten is 'True'. This is the pre-whitening matrix that projects data onto the first `n_components` principal components.,FastICA,<class 'sklearn.decomposition._fastica.FastICA'>
Parameters,n_clusters,int or None,2,The number of clusters to find. It must be ``None`` if ``distance_threshold`` is not ``None``.,FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Parameters,metric,str or callable,"""euclidean""","Metric used to compute the linkage. Can be ""euclidean"", ""l1"", ""l2"", ""manhattan"", ""cosine"", or ""precomputed"". If linkage is ""ward"", only ""euclidean"" is accepted. If ""precomputed"", a distance matrix is needed as input for the fit method.",FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Parameters,memory,str or object with the joblib.Memory interface,None,"Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory.",FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Parameters,connectivity,array-like,None,"Connectivity matrix. Defines for each feature the neighboring features following a given structure of the data. This can be a connectivity matrix itself or a callable that transforms the data into a connectivity matrix, such as derived from `kneighbors_graph`. Default is `None`, i.e, the hierarchical clustering algorithm is unstructured.",FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Parameters,compute_full_tree,'auto' or bool,'auto',"Stop early the construction of the tree at `n_clusters`. This is useful to decrease computation time if the number of clusters is not small compared to the number of features. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree. It must be ``True`` if ``distance_threshold`` is not ``None``. By default `compute_full_tree` is ""auto"", which is equivalent to `True` when `distance_threshold` is not `None` or that `n_clusters` is inferior to the maximum between 100 or `0.02 * n_samples`. Otherwise, ""auto"" is equivalent to `False`.",FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Parameters,linkage,"{""ward""","""ward""","Which linkage criterion to use. The linkage criterion determines which distance to use between sets of features. The algorithm will merge the pairs of cluster that minimize this criterion. - ""ward"" minimizes the variance of the clusters being merged. - ""complete"" or maximum linkage uses the maximum distances between all features of the two sets. - ""average"" uses the average of the distances of each feature of the two sets. - ""single"" uses the minimum of the distances between all features of the two sets.",FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Parameters,pooling_func,callable,np.mean,"This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument `axis=1`, and reduce it to an array of size [M].",FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Parameters,distance_threshold,float,None,"The linkage distance threshold at or above which clusters will not be merged. If not ``None``, ``n_clusters`` must be ``None`` and ``compute_full_tree`` must be ``True``.",FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Attributes,compute_distances,bool,False,"Computes distances between clusters even if `distance_threshold` is not used. This can be used to make dendrogram visualization, but introduces a computational and memory overhead.",FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Attributes,n_clusters_,int,,"The number of clusters found by the algorithm. If ``distance_threshold=None``, it will be equal to the given ``n_clusters``.",FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Attributes,labels_,array-like of (n_features,,Cluster labels for each feature.,FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Attributes,n_leaves_,int,,Number of leaves in the hierarchical tree.,FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Attributes,n_connected_components_,int,,The estimated number of connected components in the graph. ``n_connected_components_`` was added to replace ``n_components_``.,FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Attributes,children_,array-like of shape (n_nodes-1,,"The children of each non-leaf node. Values less than `n_features` correspond to leaves of the tree which are the original samples. A node `i` greater than or equal to `n_features` is a non-leaf node and has children `children_[i - n_features]`. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node `n_features + i`.",FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
,distances_,array-like of shape (n_nodes-1,,Distances between nodes in the corresponding place in `children_`. Only computed if `distance_threshold` is used or `compute_distances` is set to `True`.,FeatureAgglomeration,<class 'sklearn.cluster._agglomerative.FeatureAgglomeration'>
Parameters,n_features,int,2**20,"The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.",FeatureHasher,<class 'sklearn.feature_extraction._hash.FeatureHasher'>
Parameters,input_type,str,'dict',"Choose a string from {'dict', 'pair', 'string'}. Either ""dict"" (the default) to accept dictionaries over (feature_name, value); ""pair"" to accept pairs of (feature_name, value); or ""string"" to accept single strings. feature_name should be a string, while value should be a number. In the case of ""string"", a value of 1 is implied. The feature_name is hashed to find the appropriate column for the feature. The value's sign might be flipped in the output (but see non_negative, below).",FeatureHasher,<class 'sklearn.feature_extraction._hash.FeatureHasher'>
Parameters,dtype,numpy dtype,np.float64,"The type of feature values. Passed to scipy.sparse matrix constructors as the dtype argument. Do not set this to bool, np.boolean or any unsigned integer type.",FeatureHasher,<class 'sklearn.feature_extraction._hash.FeatureHasher'>
,alternate_sign,bool,True,"When True, an alternating sign is added to the features as to approximately conserve the inner product in the hashed space even for small n_features. This approach is similar to sparse random projection. ``alternate_sign`` replaces the now deprecated ``non_negative`` parameter.",FeatureHasher,<class 'sklearn.feature_extraction._hash.FeatureHasher'>
Parameters,transformer_list,list of (str,,"List of transformer objects to be applied to the data. The first half of each tuple is the name of the transformer. The transformer can be 'drop' for it to be ignored or can be 'passthrough' for features to be passed unchanged. Added the option `""passthrough""`. Deprecated `None` as a transformer in favor of 'drop'.",FeatureUnion,<class 'sklearn.pipeline.FeatureUnion'>
Parameters,n_jobs,int,None,Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details. `n_jobs` default changed from 1 to None,FeatureUnion,<class 'sklearn.pipeline.FeatureUnion'>
Parameters,transformer_weights,dict,None,"Multiplicative weights for features per transformer. Keys are transformer names, values the weights. Raises ValueError if key not present in ``transformer_list``.",FeatureUnion,<class 'sklearn.pipeline.FeatureUnion'>
Parameters,verbose,bool,False,"If True, the time elapsed while fitting each transformer will be printed as it is completed.",FeatureUnion,<class 'sklearn.pipeline.FeatureUnion'>
Attributes,verbose_feature_names_out,bool,True,"If True, :meth:`get_feature_names_out` will prefix all feature names with the name of the transformer that generated that feature. If False, :meth:`get_feature_names_out` will not prefix any feature names and will error if feature names are not unique.",FeatureUnion,<class 'sklearn.pipeline.FeatureUnion'>
Attributes,named_transformers,:class:`~sklearn.utils.Bunch`,,"Dictionary-like object, with the following attributes. Read-only attribute to access any transformer parameter by user given name. Keys are transformer names and values are transformer parameters.",FeatureUnion,<class 'sklearn.pipeline.FeatureUnion'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying first transformer in `transformer_list` exposes such an attribute when fit.,FeatureUnion,<class 'sklearn.pipeline.FeatureUnion'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,FeatureUnion,<class 'sklearn.pipeline.FeatureUnion'>
Parameters,estimator,estimator instance,,"The binary classifier, fitted or not, for which we want to optimize the decision threshold used during `predict`.",FixedThresholdClassifier,<class 'sklearn.model_selection._classification_threshold.FixedThresholdClassifier'>
Parameters,threshold,"{""auto""} or float","""auto""","The decision threshold to use when converting posterior probability estimates (i.e. output of `predict_proba`) or decision scores (i.e. output of `decision_function`) into a class label. When `""auto""`, the threshold is set to 0.5 if `predict_proba` is used as `response_method`, otherwise it is set to 0 (i.e. the default threshold for `decision_function`).",FixedThresholdClassifier,<class 'sklearn.model_selection._classification_threshold.FixedThresholdClassifier'>
Parameters,pos_label,int,None,"The label of the positive class. Used to process the output of the `response_method` method. When `pos_label=None`, if `y_true` is in `{-1, 1}` or `{0, 1}`, `pos_label` is set to 1, otherwise an error will be raised.",FixedThresholdClassifier,<class 'sklearn.model_selection._classification_threshold.FixedThresholdClassifier'>
Attributes,response_method,"{""auto""","""auto""","Methods by the classifier `estimator` corresponding to the decision function for which we want to find a threshold. It can be: * if `""auto""`, it will try to invoke `""predict_proba""` or `""decision_function""` in that order. * otherwise, one of `""predict_proba""` or `""decision_function""`. If the method is not implemented by the classifier, it will raise an error.",FixedThresholdClassifier,<class 'sklearn.model_selection._classification_threshold.FixedThresholdClassifier'>
Attributes,estimator_,estimator instance,,The fitted classifier used when predicting.,FixedThresholdClassifier,<class 'sklearn.model_selection._classification_threshold.FixedThresholdClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,The class labels.,FixedThresholdClassifier,<class 'sklearn.model_selection._classification_threshold.FixedThresholdClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,FixedThresholdClassifier,<class 'sklearn.model_selection._classification_threshold.FixedThresholdClassifier'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,FixedThresholdClassifier,<class 'sklearn.model_selection._classification_threshold.FixedThresholdClassifier'>
,estimator,estimator,,The estimator which is to be kept frozen.,FrozenEstimator,<class 'sklearn.frozen._frozen.FrozenEstimator'>
Parameters,func,callable,None,"The callable to use for the transformation. This will be passed the same arguments as transform, with args and kwargs forwarded. If func is None, then func will be the identity function.",FunctionTransformer,<class 'sklearn.preprocessing._function_transformer.FunctionTransformer'>
Parameters,inverse_func,callable,None,"The callable to use for the inverse transformation. This will be passed the same arguments as inverse transform, with args and kwargs forwarded. If inverse_func is None, then inverse_func will be the identity function.",FunctionTransformer,<class 'sklearn.preprocessing._function_transformer.FunctionTransformer'>
Parameters,validate,bool,False,"Indicate that the input X array should be checked before calling ``func``. The possibilities are: - If False, there is no input validation. - If True, then X will be converted to a 2-dimensional NumPy array or sparse matrix. If the conversion is not possible an exception is raised. The default of ``validate`` changed from True to False.",FunctionTransformer,<class 'sklearn.preprocessing._function_transformer.FunctionTransformer'>
Parameters,accept_sparse,bool,False,"Indicate that func accepts a sparse matrix as input. If validate is False, this has no effect. Otherwise, if accept_sparse is false, sparse matrix inputs will cause an exception to be raised.",FunctionTransformer,<class 'sklearn.preprocessing._function_transformer.FunctionTransformer'>
Parameters,check_inverse,bool,True,,FunctionTransformer,<class 'sklearn.preprocessing._function_transformer.FunctionTransformer'>
Parameters,feature_names_out,callable,None,"Determines the list of feature names that will be returned by the `get_feature_names_out` method. If it is 'one-to-one', then the output feature names will be equal to the input feature names. If it is a callable, then it must take two positional arguments: this `FunctionTransformer` (`self`) and an array-like of input feature names (`input_features`). It must return an array-like of output feature names. The `get_feature_names_out` method is only defined if `feature_names_out` is not None. See ``get_feature_names_out`` for more details.",FunctionTransformer,<class 'sklearn.preprocessing._function_transformer.FunctionTransformer'>
Parameters,kw_args,dict,None,Dictionary of additional keyword arguments to pass to func.,FunctionTransformer,<class 'sklearn.preprocessing._function_transformer.FunctionTransformer'>
Attributes,inv_kw_args,dict,None,Dictionary of additional keyword arguments to pass to inverse_func.,FunctionTransformer,<class 'sklearn.preprocessing._function_transformer.FunctionTransformer'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,FunctionTransformer,<class 'sklearn.preprocessing._function_transformer.FunctionTransformer'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,FunctionTransformer,<class 'sklearn.preprocessing._function_transformer.FunctionTransformer'>
Parameters,alpha,float,1,"Constant that multiplies the L2 penalty term and determines the regularization strength. ``alpha = 0`` is equivalent to unpenalized GLMs. In this case, the design matrix `X` must have full column rank (no collinearities). Values of `alpha` must be in the range `[0.0, inf)`.",GammaRegressor,<class 'sklearn.linear_model._glm.glm.GammaRegressor'>
Parameters,fit_intercept,bool,True,Specifies if a constant (a.k.a. bias or intercept) should be added to the linear predictor `X @ coef_ + intercept_`.,GammaRegressor,<class 'sklearn.linear_model._glm.glm.GammaRegressor'>
Parameters,solver,{'lbfgs','lbfgs',"Algorithm to use in the optimization problem: 'lbfgs' Calls scipy's L-BFGS-B optimizer. 'newton-cholesky' Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to iterated reweighted least squares) with an inner Cholesky based solver. This solver is a good choice for `n_samples` >> `n_features`, especially with one-hot encoded categorical features with rare categories. Be aware that the memory usage of this solver has a quadratic dependency on `n_features` because it explicitly computes the Hessian matrix.",GammaRegressor,<class 'sklearn.linear_model._glm.glm.GammaRegressor'>
Parameters,max_iter,int,100,"The maximal number of iterations for the solver. Values must be in the range `[1, inf)`.",GammaRegressor,<class 'sklearn.linear_model._glm.glm.GammaRegressor'>
Parameters,tol,float,1e-4,"Stopping criterion. For the lbfgs solver, the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol`` where ``g_j`` is the j-th component of the gradient (derivative) of the objective function. Values must be in the range `(0.0, inf)`.",GammaRegressor,<class 'sklearn.linear_model._glm.glm.GammaRegressor'>
Parameters,warm_start,bool,False,"If set to ``True``, reuse the solution of the previous call to ``fit`` as initialization for `coef_` and `intercept_`.",GammaRegressor,<class 'sklearn.linear_model._glm.glm.GammaRegressor'>
Attributes,verbose,int,0,"For the lbfgs solver set verbose to any positive number for verbosity. Values must be in the range `[0, inf)`.",GammaRegressor,<class 'sklearn.linear_model._glm.glm.GammaRegressor'>
Attributes,coef_,array of shape (n_features,,Estimated coefficients for the linear predictor (`X @ coef_ + intercept_`) in the GLM.,GammaRegressor,<class 'sklearn.linear_model._glm.glm.GammaRegressor'>
Attributes,intercept_,float,,Intercept (a.k.a. bias) added to linear predictor.,GammaRegressor,<class 'sklearn.linear_model._glm.glm.GammaRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,GammaRegressor,<class 'sklearn.linear_model._glm.glm.GammaRegressor'>
Attributes,n_iter_,int,,Actual number of iterations used in the solver.,GammaRegressor,<class 'sklearn.linear_model._glm.glm.GammaRegressor'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,GammaRegressor,<class 'sklearn.linear_model._glm.glm.GammaRegressor'>
Parameters,n_components,int,1,The number of mixture components.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Parameters,covariance_type,{'full','full',String describing the type of covariance parameters to use. Must be one of: - 'full': each component has its own general covariance matrix. - 'tied': all components share the same general covariance matrix. - 'diag': each component has its own diagonal covariance matrix. - 'spherical': each component has its own single variance.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Parameters,tol,float,1e-3,The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Parameters,reg_covar,float,1e-6,Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Parameters,max_iter,int,100,The number of EM iterations to perform.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Parameters,n_init,int,1,The number of initializations to perform. The best results are kept.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Parameters,init_params,{'kmeans','kmeans',"The method used to initialize the weights, the means and the precisions. String must be one of: - 'kmeans' : responsibilities are initialized using kmeans. - 'k-means++' : use the k-means++ method to initialize. - 'random' : responsibilities are initialized randomly. - 'random_from_data' : initial means are randomly selected data points. `init_params` now accepts 'random_from_data' and 'k-means++' as initialization methods.",GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Parameters,weights_init,array-like of shape (n_components,None,"The user-provided initial weights. If it is None, weights are initialized using the `init_params` method.",GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Parameters,means_init,array-like of shape (n_components,None,"The user-provided initial means, If it is None, means are initialized using the `init_params` method.",GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Parameters,precisions_init,array-like,None,"The user-provided initial precisions (inverse of the covariance matrices). If it is None, precisions are initialized using the 'init_params' method. The shape depends on 'covariance_type':: (n_components,)                        if 'spherical', (n_features, n_features)               if 'tied', (n_components, n_features)             if 'diag', (n_components, n_features, n_features) if 'full'",GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Parameters,random_state,int,None,"Controls the random seed given to the method chosen to initialize the parameters (see `init_params`). In addition, it controls the generation of random samples from the fitted distribution (see the method `sample`). Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Parameters,warm_start,bool,False,"If 'warm_start' is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several times on similar problems. In that case, 'n_init' is ignored and only a single initialization occurs upon the first call. See :term:`the Glossary <warm_start>`.",GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Parameters,verbose,int,0,Enable verbose output. If 1 then it prints the current initialization and each iteration step. If greater than 1 then it prints also the log probability and the time needed for each step.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Attributes,verbose_interval,int,10,Number of iteration done before the next print.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Attributes,weights_,array-like of shape (n_components,,The weights of each mixture components.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Attributes,means_,array-like of shape (n_components,,The mean of each mixture component.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Attributes,covariances_,array-like,,"The covariance of each mixture component. The shape depends on `covariance_type`:: (n_components,)                        if 'spherical', (n_features, n_features)               if 'tied', (n_components, n_features)             if 'diag', (n_components, n_features, n_features) if 'full'",GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Attributes,precisions_,array-like,,"The precision matrices for each component in the mixture. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on `covariance_type`:: (n_components,)                        if 'spherical', (n_features, n_features)               if 'tied', (n_components, n_features)             if 'diag', (n_components, n_features, n_features) if 'full'",GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Attributes,precisions_cholesky_,array-like,,"The cholesky decomposition of the precision matrices of each mixture component. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on `covariance_type`:: (n_components,)                        if 'spherical', (n_features, n_features)               if 'tied', (n_components, n_features)             if 'diag', (n_components, n_features, n_features) if 'full'",GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Attributes,converged_,bool,,"True when convergence of the best fit of EM was reached, False otherwise.",GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Attributes,n_iter_,int,,Number of step used by the best fit of EM to reach the convergence.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Attributes,lower_bound_,float,,Lower bound value on the log-likelihood (of the training data with respect to the model) of the best fit of EM.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,GaussianMixture,<class 'sklearn.mixture._gaussian_mixture.GaussianMixture'>
Parameters,priors,array-like of shape (n_classes,None,"Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.",GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,var_smoothing,float,1e-9,Portion of the largest variance of all features that is added to variances for calculation stability.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,class_count_,ndarray of shape (n_classes,,number of training samples observed in each class.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,class_prior_,ndarray of shape (n_classes,,probability of each class.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,classes_,ndarray of shape (n_classes,,class labels known to the classifier.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,epsilon_,float,,absolute additive value to variances.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,var_,ndarray of shape (n_classes,,Variance of each feature per class.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
,theta_,ndarray of shape (n_classes,,mean of each feature per class.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Parameters,priors,array-like of shape (n_classes,None,"Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.",GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,var_smoothing,float,1e-9,Portion of the largest variance of all features that is added to variances for calculation stability.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,class_count_,ndarray of shape (n_classes,,number of training samples observed in each class.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,class_prior_,ndarray of shape (n_classes,,probability of each class.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,classes_,ndarray of shape (n_classes,,class labels known to the classifier.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,epsilon_,float,,absolute additive value to variances.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Attributes,var_,ndarray of shape (n_classes,,Variance of each feature per class.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
,theta_,ndarray of shape (n_classes,,mean of each feature per class.,GaussianNB,<class 'sklearn.naive_bayes.GaussianNB'>
Parameters,kernel,kernel instance,None,"The kernel specifying the covariance function of the GP. If None is passed, the kernel ""1.0 * RBF(1.0)"" is used as default. Note that the kernel's hyperparameters are optimized during fitting. Also kernel cannot be a `CompoundKernel`.",GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Parameters,optimizer,'fmin_l_bfgs_b','fmin_l_bfgs_b',"Can either be one of the internally supported optimizers for optimizing the kernel's parameters, specified by a string, or an externally defined optimizer passed as a callable. If a callable is passed, it must have the  signature:: def optimizer(obj_func, initial_theta, bounds): # * 'obj_func' is the objective function to be maximized, which #   takes the hyperparameters theta as parameter and an #   optional flag eval_gradient, which determines if the #   gradient is returned additionally to the function value # * 'initial_theta': the initial value for theta, which can be #   used by local optimizers # * 'bounds': the bounds on the values of theta # Returned are the best found hyperparameters theta and # the corresponding value of the target function. return theta_opt, func_min Per default, the 'L-BFGS-B' algorithm from scipy.optimize.minimize is used. If None is passed, the kernel's parameters are kept fixed. Available internal optimizers are:: 'fmin_l_bfgs_b'",GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Parameters,n_restarts_optimizer,int,0,"The number of restarts of the optimizer for finding the kernel's parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel's initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer=0 implies that one run is performed.",GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Parameters,max_iter_predict,int,100,The maximum number of iterations in Newton's method for approximating the posterior during predict. Smaller values will reduce computation time at the cost of worse results.,GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Parameters,warm_start,bool,False,"If warm-starts are enabled, the solution of the last Newton iteration on the Laplace approximation of the posterior mode is used as initialization for the next call of _posterior_mode(). This can speed up convergence when _posterior_mode is called several times on similar problems as in hyperparameter optimization. See :term:`the Glossary <warm_start>`.",GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Parameters,copy_X_train,bool,True,"If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally.",GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Parameters,random_state,int,None,Determines random number generation used to initialize the centers. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Parameters,multi_class,{'one_vs_rest','one_vs_rest',"Specifies how multi-class classification problems are handled. Supported are 'one_vs_rest' and 'one_vs_one'. In 'one_vs_rest', one binary Gaussian process classifier is fitted for each class, which is trained to separate this class from the rest. In 'one_vs_one', one binary Gaussian process classifier is fitted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors are combined into multi-class predictions. Note that 'one_vs_one' does not support predicting probability estimates.",GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Attributes,n_jobs,int,None,The number of jobs to use for the computation: the specified multiclass problems are computed in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Attributes,base_estimator_,``Estimator`` instance,,The estimator instance that defines the likelihood function using the observed data.,GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Attributes,kernel_,kernel instance,,"The kernel used for prediction. In case of binary classification, the structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters. In case of multi-class classification, a CompoundKernel is returned which consists of the different kernels used in the one-versus-rest classifiers.",GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Attributes,log_marginal_likelihood_value_,float,,The log-marginal-likelihood of ``self.kernel_.theta``,GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Attributes,classes_,array-like of shape (n_classes,,Unique class labels.,GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Attributes,n_classes_,int,,The number of classes in the training data,GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,GaussianProcessClassifier,<class 'sklearn.gaussian_process._gpc.GaussianProcessClassifier'>
Parameters,kernel,kernel instance,None,"The kernel specifying the covariance function of the GP. If None is passed, the kernel ``ConstantKernel(1.0, constant_value_bounds=""fixed"") * RBF(1.0, length_scale_bounds=""fixed"")`` is used as default. Note that the kernel hyperparameters are optimized during fitting unless the bounds are marked as ""fixed"".",GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Parameters,alpha,float or ndarray of shape (n_samples,1e-10,"Value added to the diagonal of the kernel matrix during fitting. This can prevent a potential numerical issue during fitting, by ensuring that the calculated values form a positive definite matrix. It can also be interpreted as the variance of additional Gaussian measurement noise on the training observations. Note that this is different from using a `WhiteKernel`. If an array is passed, it must have the same number of entries as the data used for fitting and is used as datapoint-dependent noise level. Allowing to specify the noise level directly as a parameter is mainly for convenience and for consistency with :class:`~sklearn.linear_model.Ridge`.",GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Parameters,optimizer,"""fmin_l_bfgs_b""","""fmin_l_bfgs_b""","Can either be one of the internally supported optimizers for optimizing the kernel's parameters, specified by a string, or an externally defined optimizer passed as a callable. If a callable is passed, it must have the signature:: def optimizer(obj_func, initial_theta, bounds): # * 'obj_func': the objective function to be minimized, which #   takes the hyperparameters theta as a parameter and an #   optional flag eval_gradient, which determines if the #   gradient is returned additionally to the function value # * 'initial_theta': the initial value for theta, which can be #   used by local optimizers # * 'bounds': the bounds on the values of theta # Returned are the best found hyperparameters theta and # the corresponding value of the target function. return theta_opt, func_min Per default, the L-BFGS-B algorithm from `scipy.optimize.minimize` is used. If None is passed, the kernel's parameters are kept fixed. Available internal optimizers are: `{'fmin_l_bfgs_b'}`.",GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Parameters,n_restarts_optimizer,int,0,"The number of restarts of the optimizer for finding the kernel's parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel's initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that `n_restarts_optimizer == 0` implies that one run is performed.",GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Parameters,normalize_y,bool,False,"Whether or not to normalize the target values `y` by removing the mean and scaling to unit-variance. This is recommended for cases where zero-mean, unit-variance priors are used. Note that, in this implementation, the normalisation is reversed before the GP predictions are reported.",GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Parameters,copy_X_train,bool,True,"If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally.",GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Parameters,n_targets,int,None,The number of dimensions of the target values. Used to decide the number of outputs when sampling from the prior distributions (i.e. calling :meth:`sample_y` before :meth:`fit`). This parameter is ignored once :meth:`fit` has been called.,GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Attributes,random_state,int,None,Determines random number generation used to initialize the centers. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Attributes,X_train_,array-like of shape (n_samples,,Feature vectors or other representations of training data (also required for prediction).,GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Attributes,y_train_,array-like of shape (n_samples,,Target values in training data (also required for prediction).,GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Attributes,kernel_,kernel instance,,The kernel used for prediction. The structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters.,GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Attributes,L_,array-like of shape (n_samples,,Lower-triangular Cholesky decomposition of the kernel in ``X_train_``.,GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Attributes,alpha_,array-like of shape (n_samples,,Dual coefficients of training data points in kernel space.,GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Attributes,log_marginal_likelihood_value_,float,,The log-marginal-likelihood of ``self.kernel_.theta``.,GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,GaussianProcessRegressor,<class 'sklearn.gaussian_process._gpr.GaussianProcessRegressor'>
Parameters,n_components,int or 'auto','auto',Dimensionality of the target projection space. n_components can be automatically adjusted according to the number of samples in the dataset and the bound given by the Johnson-Lindenstrauss lemma. In that case the quality of the embedding is controlled by the ``eps`` parameter. It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset.,GaussianRandomProjection,<class 'sklearn.random_projection.GaussianRandomProjection'>
Parameters,eps,float,0.1,Parameter to control the quality of the embedding according to the Johnson-Lindenstrauss lemma when `n_components` is set to 'auto'. The value should be strictly positive. Smaller values lead to better embedding and higher number of dimensions (n_components) in the target projection space.,GaussianRandomProjection,<class 'sklearn.random_projection.GaussianRandomProjection'>
Parameters,compute_inverse_components,bool,False,Learn the inverse transform by computing the pseudo-inverse of the components during fit. Note that computing the pseudo-inverse does not scale well to large matrices.,GaussianRandomProjection,<class 'sklearn.random_projection.GaussianRandomProjection'>
Attributes,random_state,int,None,Controls the pseudo random number generator used to generate the projection matrix at fit time. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,GaussianRandomProjection,<class 'sklearn.random_projection.GaussianRandomProjection'>
Attributes,n_components_,int,,"Concrete number of components computed when n_components=""auto"".",GaussianRandomProjection,<class 'sklearn.random_projection.GaussianRandomProjection'>
Attributes,components_,ndarray of shape (n_components,,Random matrix used for the projection.,GaussianRandomProjection,<class 'sklearn.random_projection.GaussianRandomProjection'>
Attributes,inverse_components_,ndarray of shape (n_features,,"Pseudo-inverse of the components, only computed if `compute_inverse_components` is True.",GaussianRandomProjection,<class 'sklearn.random_projection.GaussianRandomProjection'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,GaussianRandomProjection,<class 'sklearn.random_projection.GaussianRandomProjection'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,GaussianRandomProjection,<class 'sklearn.random_projection.GaussianRandomProjection'>
Parameters,score_func,callable,f_classif,"Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). For modes 'percentile' or 'kbest' it can return a single array scores.",GenericUnivariateSelect,<class 'sklearn.feature_selection._univariate_selection.GenericUnivariateSelect'>
Parameters,mode,{'percentile','percentile',Feature selection mode. Note that the `'percentile'` and `'kbest'` modes are supporting unsupervised feature selection (when `y` is `None`).,GenericUnivariateSelect,<class 'sklearn.feature_selection._univariate_selection.GenericUnivariateSelect'>
Attributes,param,"""all""",1e-5,Parameter of the corresponding mode.,GenericUnivariateSelect,<class 'sklearn.feature_selection._univariate_selection.GenericUnivariateSelect'>
Attributes,scores_,array-like of shape (n_features,,Scores of features.,GenericUnivariateSelect,<class 'sklearn.feature_selection._univariate_selection.GenericUnivariateSelect'>
Attributes,pvalues_,array-like of shape (n_features,,"p-values of feature scores, None if `score_func` returned scores only.",GenericUnivariateSelect,<class 'sklearn.feature_selection._univariate_selection.GenericUnivariateSelect'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,GenericUnivariateSelect,<class 'sklearn.feature_selection._univariate_selection.GenericUnivariateSelect'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,GenericUnivariateSelect,<class 'sklearn.feature_selection._univariate_selection.GenericUnivariateSelect'>
Parameters,loss,{'log_loss','log_loss',"The loss function to be optimized. 'log_loss' refers to binomial and multinomial deviance, the same as used in logistic regression. It is a good choice for classification with probabilistic outputs. For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,learning_rate,float,0.1,"Learning rate shrinks the contribution of each tree by `learning_rate`. There is a trade-off between learning_rate and n_estimators. Values must be in the range `[0.0, inf)`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,n_estimators,int,100,"The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Values must be in the range `[1, inf)`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,subsample,float,1.0,"The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias. Values must be in the range `(0.0, 1.0]`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,criterion,{'friedman_mse','friedman_mse',"The function to measure the quality of a split. Supported criteria are 'friedman_mse' for the mean squared error with improvement score by Friedman, 'squared_error' for mean squared error. The default value of 'friedman_mse' is generally the best as it can provide a better approximation in some cases.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, values must be in the range `[2, inf)`. - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split` will be `ceil(min_samples_split * n_samples)`. Added float values for fractions.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, values must be in the range `[1, inf)`. - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf` will be `ceil(min_samples_leaf * n_samples)`. Added float values for fractions.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,min_weight_fraction_leaf,float,0.0,"The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. Values must be in the range `[0.0, 0.5]`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,max_depth,int or None,3,"Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. If int, values must be in the range `[1, inf)`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Values must be in the range `[0.0, inf)`. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,init,estimator or 'zero',None,"An estimator object that is used to compute the initial predictions. ``init`` has to provide :term:`fit` and :term:`predict_proba`. If 'zero', the initial raw predictions are set to zero. By default, a ``DummyEstimator`` predicting the classes priors is used.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,random_state,int,None,"Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split (see Notes for more details). It also controls the random splitting of the training data to obtain a validation set if `n_iter_no_change` is not None. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,max_features,{'sqrt',None,"The number of features to consider when looking for the best split: - If int, values must be in the range `[1, inf)`. - If float, values must be in the range `(0.0, 1.0]` and the features considered at each split will be `max(1, int(max_features * n_features_in_))`. - If 'sqrt', then `max_features=sqrt(n_features)`. - If 'log2', then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Choosing `max_features < n_features` leads to a reduction of variance and an increase in bias. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,verbose,int,0,"Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. Values must be in the range `[0, inf)`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,max_leaf_nodes,int,None,"Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. Values must be in the range `[2, inf)`. If `None`, then unlimited number of leaf nodes.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,validation_fraction,float,0.1,"The proportion of training data to set aside as validation set for early stopping. Values must be in the range `(0.0, 1.0)`. Only used if ``n_iter_no_change`` is set to an integer.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,n_iter_no_change,int,None,"``n_iter_no_change`` is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to None to disable early stopping. If set to a number, it will set aside ``validation_fraction`` size of the training data as validation and terminate training when validation score is not improving in all of the previous ``n_iter_no_change`` numbers of iterations. The split is stratified. Values must be in the range `[1, inf)`. See :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,tol,float,1e-4,"Tolerance for the early stopping. When the loss is not improving by at least tol for ``n_iter_no_change`` iterations (if set to a number), the training stops. Values must be in the range `[0.0, inf)`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,ccp_alpha,non-negative float,0.0,"Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. Values must be in the range `[0.0, inf)`. See :ref:`minimal_cost_complexity_pruning` for details. See :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py` for an example of such pruning.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,n_estimators_,int,,The number of estimators as selected by early stopping (if ``n_iter_no_change`` is specified). Otherwise it is set to ``n_estimators``.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,n_trees_per_iteration_,int,,"The number of trees that are built at each iteration. For binary classifiers, this is always 1.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,feature_importances_,ndarray of shape (n_features,,"The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.  It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,oob_improvement_,ndarray of shape (n_estimators,,The improvement in loss on the out-of-bag samples relative to the previous iteration. ``oob_improvement_[0]`` is the improvement in loss of the first stage over the ``init`` estimator. Only available if ``subsample < 1.0``.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,oob_scores_,ndarray of shape (n_estimators,,The full history of the loss values on the out-of-bag samples. Only available if `subsample < 1.0`.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,oob_score_,float,,The last value of the loss on the out-of-bag samples. It is the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,train_score_,ndarray of shape (n_estimators,,The i-th score ``train_score_[i]`` is the loss of the model at iteration ``i`` on the in-bag sample. If ``subsample == 1`` this is the loss on the training data.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,init_,estimator,,The estimator that provides the initial predictions. Set via the ``init`` argument.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,estimators_,ndarray of DecisionTreeRegressor of             shape (n_estimators,,"The collection of fitted sub-estimators. ``n_trees_per_iteration_`` is 1 for binary classification, otherwise ``n_classes``.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,The classes labels.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,n_classes_,int,,The number of classes.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
,max_features_,int,,The inferred value of max_features.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,loss,{'log_loss','log_loss',"The loss function to be optimized. 'log_loss' refers to binomial and multinomial deviance, the same as used in logistic regression. It is a good choice for classification with probabilistic outputs. For loss 'exponential', gradient boosting recovers the AdaBoost algorithm.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,learning_rate,float,0.1,"Learning rate shrinks the contribution of each tree by `learning_rate`. There is a trade-off between learning_rate and n_estimators. Values must be in the range `[0.0, inf)`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,n_estimators,int,100,"The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Values must be in the range `[1, inf)`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,subsample,float,1.0,"The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias. Values must be in the range `(0.0, 1.0]`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,criterion,{'friedman_mse','friedman_mse',"The function to measure the quality of a split. Supported criteria are 'friedman_mse' for the mean squared error with improvement score by Friedman, 'squared_error' for mean squared error. The default value of 'friedman_mse' is generally the best as it can provide a better approximation in some cases.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, values must be in the range `[2, inf)`. - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split` will be `ceil(min_samples_split * n_samples)`. Added float values for fractions.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, values must be in the range `[1, inf)`. - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf` will be `ceil(min_samples_leaf * n_samples)`. Added float values for fractions.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,min_weight_fraction_leaf,float,0.0,"The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. Values must be in the range `[0.0, 0.5]`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,max_depth,int or None,3,"Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. If int, values must be in the range `[1, inf)`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Values must be in the range `[0.0, inf)`. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,init,estimator or 'zero',None,"An estimator object that is used to compute the initial predictions. ``init`` has to provide :term:`fit` and :term:`predict_proba`. If 'zero', the initial raw predictions are set to zero. By default, a ``DummyEstimator`` predicting the classes priors is used.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,random_state,int,None,"Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split (see Notes for more details). It also controls the random splitting of the training data to obtain a validation set if `n_iter_no_change` is not None. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,max_features,{'sqrt',None,"The number of features to consider when looking for the best split: - If int, values must be in the range `[1, inf)`. - If float, values must be in the range `(0.0, 1.0]` and the features considered at each split will be `max(1, int(max_features * n_features_in_))`. - If 'sqrt', then `max_features=sqrt(n_features)`. - If 'log2', then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Choosing `max_features < n_features` leads to a reduction of variance and an increase in bias. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,verbose,int,0,"Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. Values must be in the range `[0, inf)`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,max_leaf_nodes,int,None,"Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. Values must be in the range `[2, inf)`. If `None`, then unlimited number of leaf nodes.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,validation_fraction,float,0.1,"The proportion of training data to set aside as validation set for early stopping. Values must be in the range `(0.0, 1.0)`. Only used if ``n_iter_no_change`` is set to an integer.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,n_iter_no_change,int,None,"``n_iter_no_change`` is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to None to disable early stopping. If set to a number, it will set aside ``validation_fraction`` size of the training data as validation and terminate training when validation score is not improving in all of the previous ``n_iter_no_change`` numbers of iterations. The split is stratified. Values must be in the range `[1, inf)`. See :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,tol,float,1e-4,"Tolerance for the early stopping. When the loss is not improving by at least tol for ``n_iter_no_change`` iterations (if set to a number), the training stops. Values must be in the range `[0.0, inf)`.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,ccp_alpha,non-negative float,0.0,"Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. Values must be in the range `[0.0, inf)`. See :ref:`minimal_cost_complexity_pruning` for details. See :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py` for an example of such pruning.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,n_estimators_,int,,The number of estimators as selected by early stopping (if ``n_iter_no_change`` is specified). Otherwise it is set to ``n_estimators``.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,n_trees_per_iteration_,int,,"The number of trees that are built at each iteration. For binary classifiers, this is always 1.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,feature_importances_,ndarray of shape (n_features,,"The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.  It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,oob_improvement_,ndarray of shape (n_estimators,,The improvement in loss on the out-of-bag samples relative to the previous iteration. ``oob_improvement_[0]`` is the improvement in loss of the first stage over the ``init`` estimator. Only available if ``subsample < 1.0``.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,oob_scores_,ndarray of shape (n_estimators,,The full history of the loss values on the out-of-bag samples. Only available if `subsample < 1.0`.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,oob_score_,float,,The last value of the loss on the out-of-bag samples. It is the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,train_score_,ndarray of shape (n_estimators,,The i-th score ``train_score_[i]`` is the loss of the model at iteration ``i`` on the in-bag sample. If ``subsample == 1`` this is the loss on the training data.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,init_,estimator,,The estimator that provides the initial predictions. Set via the ``init`` argument.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,estimators_,ndarray of DecisionTreeRegressor of             shape (n_estimators,,"The collection of fitted sub-estimators. ``n_trees_per_iteration_`` is 1 for binary classification, otherwise ``n_classes``.",GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,The classes labels.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Attributes,n_classes_,int,,The number of classes.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
,max_features_,int,,The inferred value of max_features.,GradientBoostingClassifier,<class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
Parameters,loss,{'squared_error','squared_error',Loss function to be optimized. 'squared_error' refers to the squared error for regression. 'absolute_error' refers to the absolute error of regression and is a robust loss function. 'huber' is a combination of the two. 'quantile' allows quantile regression (use `alpha` to specify the quantile). See :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py` for an example that demonstrates quantile regression for creating prediction intervals with `loss='quantile'`.,GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,learning_rate,float,0.1,"Learning rate shrinks the contribution of each tree by `learning_rate`. There is a trade-off between learning_rate and n_estimators. Values must be in the range `[0.0, inf)`.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,n_estimators,int,100,"The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Values must be in the range `[1, inf)`.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,subsample,float,1.0,"The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias. Values must be in the range `(0.0, 1.0]`.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,criterion,{'friedman_mse','friedman_mse',"The function to measure the quality of a split. Supported criteria are ""friedman_mse"" for the mean squared error with improvement score by Friedman, ""squared_error"" for mean squared error. The default value of ""friedman_mse"" is generally the best as it can provide a better approximation in some cases.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, values must be in the range `[2, inf)`. - If float, values must be in the range `(0.0, 1.0]` and `min_samples_split` will be `ceil(min_samples_split * n_samples)`. Added float values for fractions.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, values must be in the range `[1, inf)`. - If float, values must be in the range `(0.0, 1.0)` and `min_samples_leaf` will be `ceil(min_samples_leaf * n_samples)`. Added float values for fractions.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,min_weight_fraction_leaf,float,0.0,"The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. Values must be in the range `[0.0, 0.5]`.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,max_depth,int or None,3,"Maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. If int, values must be in the range `[1, inf)`.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. Values must be in the range `[0.0, inf)`. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,init,estimator or 'zero',None,"An estimator object that is used to compute the initial predictions. ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the initial raw predictions are set to zero. By default a ``DummyEstimator`` is used, predicting either the average target value (for loss='squared_error'), or a quantile for the other losses.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,random_state,int,None,"Controls the random seed given to each Tree estimator at each boosting iteration. In addition, it controls the random permutation of the features at each split (see Notes for more details). It also controls the random splitting of the training data to obtain a validation set if `n_iter_no_change` is not None. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,max_features,{'sqrt',None,"The number of features to consider when looking for the best split: - If int, values must be in the range `[1, inf)`. - If float, values must be in the range `(0.0, 1.0]` and the features considered at each split will be `max(1, int(max_features * n_features_in_))`. - If ""sqrt"", then `max_features=sqrt(n_features)`. - If ""log2"", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. Choosing `max_features < n_features` leads to a reduction of variance and an increase in bias. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,alpha,float,0.9,"The alpha-quantile of the huber loss function and the quantile loss function. Only if ``loss='huber'`` or ``loss='quantile'``. Values must be in the range `(0.0, 1.0)`.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,verbose,int,0,"Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. Values must be in the range `[0, inf)`.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,max_leaf_nodes,int,None,"Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. Values must be in the range `[2, inf)`. If None, then unlimited number of leaf nodes.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,validation_fraction,float,0.1,"The proportion of training data to set aside as validation set for early stopping. Values must be in the range `(0.0, 1.0)`. Only used if ``n_iter_no_change`` is set to an integer.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,n_iter_no_change,int,None,"``n_iter_no_change`` is used to decide if early stopping will be used to terminate training when validation score is not improving. By default it is set to None to disable early stopping. If set to a number, it will set aside ``validation_fraction`` size of the training data as validation and terminate training when validation score is not improving in all of the previous ``n_iter_no_change`` numbers of iterations. Values must be in the range `[1, inf)`. See :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,tol,float,1e-4,"Tolerance for the early stopping. When the loss is not improving by at least tol for ``n_iter_no_change`` iterations (if set to a number), the training stops. Values must be in the range `[0.0, inf)`.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Attributes,ccp_alpha,non-negative float,0.0,"Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. Values must be in the range `[0.0, inf)`. See :ref:`minimal_cost_complexity_pruning` for details. See :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py` for an example of such pruning.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Attributes,n_estimators_,int,,The number of estimators as selected by early stopping (if ``n_iter_no_change`` is specified). Otherwise it is set to ``n_estimators``.,GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Attributes,n_trees_per_iteration_,int,,"The number of trees that are built at each iteration. For regressors, this is always 1.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Attributes,feature_importances_,ndarray of shape (n_features,,"The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.  It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.",GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Attributes,oob_improvement_,ndarray of shape (n_estimators,,The improvement in loss on the out-of-bag samples relative to the previous iteration. ``oob_improvement_[0]`` is the improvement in loss of the first stage over the ``init`` estimator. Only available if ``subsample < 1.0``.,GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Attributes,oob_scores_,ndarray of shape (n_estimators,,The full history of the loss values on the out-of-bag samples. Only available if `subsample < 1.0`.,GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Attributes,oob_score_,float,,The last value of the loss on the out-of-bag samples. It is the same as `oob_scores_[-1]`. Only available if `subsample < 1.0`.,GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Attributes,train_score_,ndarray of shape (n_estimators,,The i-th score ``train_score_[i]`` is the loss of the model at iteration ``i`` on the in-bag sample. If ``subsample == 1`` this is the loss on the training data.,GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Attributes,init_,estimator,,The estimator that provides the initial predictions. Set via the ``init`` argument.,GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Attributes,estimators_,ndarray of DecisionTreeRegressor of shape (n_estimators,,The collection of fitted sub-estimators.,GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
,max_features_,int,,The inferred value of max_features.,GradientBoostingRegressor,<class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
Parameters,alpha,float,0.01,"The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance. Range is (0, inf].",GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Parameters,mode,{'cd','cd',"The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p > n. Elsewhere prefer cd which is more numerically stable.",GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Parameters,covariance,"""precomputed""",None,"If covariance is ""precomputed"", the input data in `fit` is assumed to be the covariance matrix. If `None`, the empirical covariance is estimated from the data `X`.",GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Parameters,tol,float,1e-4,"The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. Range is (0, inf].",GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Parameters,enet_tol,float,1e-4,"The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode='cd'. Range is (0, inf].",GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Parameters,max_iter,int,100,The maximum number of iterations.,GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Parameters,verbose,bool,False,"If verbose is True, the objective function and dual gap are plotted at each iteration.",GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Parameters,eps,float,eps,The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Default is `np.finfo(np.float64).eps`.,GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Attributes,assume_centered,bool,False,"If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation.",GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Attributes,location_,ndarray of shape (n_features,,"Estimated location, i.e. the estimated mean.",GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Attributes,covariance_,ndarray of shape (n_features,,Estimated covariance matrix,GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Attributes,precision_,ndarray of shape (n_features,,Estimated pseudo inverse matrix.,GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Attributes,n_iter_,int,,Number of iterations run.,GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Attributes,costs_,list of (objective,,The list of values of the objective function and the dual gap at each iteration. Returned only if return_costs is True.,GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,GraphicalLasso,<class 'sklearn.covariance._graph_lasso.GraphicalLasso'>
Parameters,alphas,int or array-like of shape (n_alphas,4,"If an integer is given, it fixes the number of points on the grids of alpha to be used. If a list is given, it gives the grid to be used. See the notes in the class docstring for more details. Range is [1, inf) for an integer. Range is (0, inf] for an array-like of floats.",GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Parameters,n_refinements,int,4,"The number of times the grid is refined. Not used if explicit values of alphas are passed. Range is [1, inf).",GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs :class:`~sklearn.model_selection.KFold` is used. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ``cv`` default value if None changed from 3-fold to 5-fold.",GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Parameters,tol,float,1e-4,"The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. Range is (0, inf].",GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Parameters,enet_tol,float,1e-4,"The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode='cd'. Range is (0, inf].",GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Parameters,max_iter,int,100,Maximum number of iterations.,GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Parameters,mode,{'cd','cd',"The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where number of features is greater than number of samples. Elsewhere prefer cd which is more numerically stable.",GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Parameters,n_jobs,int,None,Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details. `n_jobs` default changed from 1 to None,GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Parameters,verbose,bool,False,"If verbose is True, the objective function and duality gap are printed at each iteration.",GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Parameters,eps,float,eps,The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Default is `np.finfo(np.float64).eps`.,GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Attributes,assume_centered,bool,False,"If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation.",GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Attributes,location_,ndarray of shape (n_features,,"Estimated location, i.e. the estimated mean.",GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Attributes,covariance_,ndarray of shape (n_features,,Estimated covariance matrix.,GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Attributes,precision_,ndarray of shape (n_features,,Estimated precision matrix (inverse covariance).,GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Attributes,costs_,list of (objective,,The list of values of the objective function and the dual gap at each iteration. Returned only if return_costs is True.,GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Attributes,alpha_,float,,Penalization parameter selected.,GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Attributes,cv_results_,dict of ndarrays,,"A dict with keys: alphas : ndarray of shape (n_alphas,) All penalization parameters explored. split(k)_test_score : ndarray of shape (n_alphas,) Log-likelihood score on left-out data across (k)th fold. mean_test_score : ndarray of shape (n_alphas,) Mean of scores over the folds. std_test_score : ndarray of shape (n_alphas,) Standard deviation of scores over the folds.",GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Attributes,n_iter_,int,,Number of iterations run for the optimal alpha.,GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,GraphicalLassoCV,<class 'sklearn.covariance._graph_lasso.GraphicalLassoCV'>
Parameters,estimator,estimator object,,"This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a ``score`` function, or ``scoring`` must be passed.",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Parameters,param_grid,dict or list of dictionaries,,"Dictionary with parameters names (`str`) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Parameters,scoring,str,None,"Strategy to evaluate the performance of the cross-validated model on the test set. If `scoring` represents a single score, one can use: - a single string (see :ref:`scoring_parameter`); - a callable (see :ref:`scoring_callable`) that returns a single value. If `scoring` represents multiple scores, one can use: - a list or tuple of unique strings; - a callable returning a dictionary where the keys are the metric names and the values are the metric scores; - a dictionary with metric names as keys and callables as values. See :ref:`multimetric_grid_search` for an example.",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Parameters,n_jobs,int,None,Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details. `n_jobs` default changed from 1 to None,GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Parameters,refit,bool,True,"Refit an estimator using the best found parameters on the whole dataset. For multiple metric evaluation, this needs to be a `str` denoting the scorer that would be used to find the best parameters for refitting the estimator at the end. Where there are considerations other than maximum score in choosing a best estimator, ``refit`` can be set to a function which returns the selected ``best_index_`` given ``cv_results_``. In that case, the ``best_estimator_`` and ``best_params_`` will be set according to the returned ``best_index_`` while the ``best_score_`` attribute will not be available. The refitted estimator is made available at the ``best_estimator_`` attribute and permits using ``predict`` directly on this ``GridSearchCV`` instance. Also for multiple metric evaluation, the attributes ``best_index_``, ``best_score_`` and ``best_params_`` will only be available if ``refit`` is set and all of them will be determined w.r.t this specific scorer. See ``scoring`` parameter to know more about multiple metric evaluation. See :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py` to see how to design a custom selection strategy using a callable via `refit`. Support for callable added.",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used. These splitters are instantiated with `shuffle=False` so the splits will be the same across calls. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ``cv`` default value if None changed from 3-fold to 5-fold.",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Parameters,verbose,int,,"Controls the verbosity: the higher, the more messages. - >1 : the computation time for each fold and parameter candidate is displayed; - >2 : the score is also displayed; - >3 : the fold and candidate parameter indexes are also displayed together with the starting time of the computation.",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Parameters,pre_dispatch,int,'2*n_jobs',"Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A str, giving an expression as a function of n_jobs, as in '2*n_jobs'",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Parameters,error_score,'raise' or numeric,np.nan,"Value to assign to the score if an error occurs in estimator fitting. If set to 'raise', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Attributes,return_train_score,bool,False,"If ``False``, the ``cv_results_`` attribute will not include training scores. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance. Default value was changed from ``True`` to ``False``",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Attributes,cv_results_,dict of numpy (masked) ndarrays,,"A dict with keys as column headers and values as columns, that can be imported into a pandas ``DataFrame``. For instance the below given table +------------+-----------+------------+-----------------+---+---------+ |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...| +============+===========+============+=================+===+=========+ |  'poly'    |     --    |      2     |       0.80      |...|    2    | +------------+-----------+------------+-----------------+---+---------+ |  'poly'    |     --    |      3     |       0.70      |...|    4    | +------------+-----------+------------+-----------------+---+---------+ |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    | +------------+-----------+------------+-----------------+---+---------+ |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    | +------------+-----------+------------+-----------------+---+---------+ will be represented by a ``cv_results_`` dict of:: { 'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'], mask = [False False False False]...) 'param_gamma': masked_array(data = [-- -- 0.1 0.2], mask = [ True  True False False]...), 'param_degree': masked_array(data = [2.0 3.0 -- --], mask = [False False  True  True]...), 'split0_test_score'  : [0.80, 0.70, 0.80, 0.93], 'split1_test_score'  : [0.82, 0.50, 0.70, 0.78], 'mean_test_score'    : [0.81, 0.60, 0.75, 0.85], 'std_test_score'     : [0.01, 0.10, 0.05, 0.08], 'rank_test_score'    : [2, 4, 3, 1], 'split0_train_score' : [0.80, 0.92, 0.70, 0.93], 'split1_train_score' : [0.82, 0.55, 0.70, 0.87], 'mean_train_score'   : [0.81, 0.74, 0.70, 0.90], 'std_train_score'    : [0.01, 0.19, 0.00, 0.03], 'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49], 'std_fit_time'       : [0.01, 0.02, 0.01, 0.01], 'mean_score_time'    : [0.01, 0.06, 0.04, 0.04], 'std_score_time'     : [0.00, 0.00, 0.00, 0.01], 'params'             : [{'kernel': 'poly', 'degree': 2}, ...], } NOTE The key ``'params'`` is used to store a list of parameter settings dicts for all the parameter candidates. The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and ``std_score_time`` are all in seconds. For multi-metric evaluation, the scores for all the scorers are available in the ``cv_results_`` dict at the keys ending with that scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown above. ('split0_test_precision', 'mean_train_precision' etc.)",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Attributes,best_estimator_,estimator,,"Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if ``refit=False``. See ``refit`` parameter for more information on allowed values.",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Attributes,best_score_,float,,"Mean cross-validated score of the best_estimator For multi-metric evaluation, this is present only if ``refit`` is specified. This attribute is not available if ``refit`` is a function.",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Attributes,best_params_,dict,,"Parameter setting that gave the best results on the hold out data. For multi-metric evaluation, this is present only if ``refit`` is specified.",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Attributes,best_index_,int,,"The index (of the ``cv_results_`` arrays) which corresponds to the best candidate parameter setting. The dict at ``search.cv_results_['params'][search.best_index_]`` gives the parameter setting for the best model, that gives the highest mean score (``search.best_score_``). For multi-metric evaluation, this is present only if ``refit`` is specified.",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Attributes,scorer_,function or a dict,,"Scorer function used on the held out data to choose the best parameters for the model. For multi-metric evaluation, this attribute holds the validated ``scoring`` dict which maps the scorer key to the scorer callable.",GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Attributes,n_splits_,int,,The number of cross-validation splits (folds/iterations).,GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Attributes,refit_time_,float,,Seconds used for refitting the best model on the whole dataset. This is present only if ``refit`` is not False.,GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Attributes,multimetric_,bool,,Whether or not the scorers compute several metrics.,GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Attributes,classes_,ndarray of shape (n_classes,,The classes labels. This is present only if ``refit`` is specified and the underlying estimator is a classifier.,GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if `best_estimator_` is defined (see the documentation for the `refit` parameter for more details) and that `best_estimator_` exposes `n_features_in_` when fit.,GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if `best_estimator_` is defined (see the documentation for the `refit` parameter for more details) and that `best_estimator_` exposes `feature_names_in_` when fit.,GridSearchCV,<class 'sklearn.model_selection._search.GridSearchCV'>
Parameters,min_cluster_size,int,5,The minimum number of samples in a group for that group to be considered a cluster; groupings smaller than this size will be left as noise.,HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Parameters,min_samples,int,None,"The parameter `k` used to calculate the distance between a point `x_p` and its k-th nearest neighbor. When `None`, defaults to `min_cluster_size`.",HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Parameters,cluster_selection_epsilon,float,0.0,A distance threshold. Clusters below this value will be merged. See [5]_ for more information.,HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Parameters,max_cluster_size,int,None,"A limit to the size of clusters returned by the `""eom""` cluster selection algorithm. There is no limit when `max_cluster_size=None`. Has no effect if `cluster_selection_method=""leaf""`.",HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Parameters,metric,str or callable,'euclidean',"The metric to use when calculating distance between instances in a feature array. - If metric is a string or callable, it must be one of the options allowed by :func:`~sklearn.metrics.pairwise_distances` for its metric parameter. - If metric is ""precomputed"", X is assumed to be a distance matrix and must be square.",HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Parameters,metric_params,dict,None,Arguments passed to the distance metric.,HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Parameters,alpha,float,1.0,A distance scaling parameter as used in robust single linkage. See [3]_ for more information.,HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Parameters,algorithm,"{""auto""","""auto""","Exactly which algorithm to use for computing core distances; By default this is set to `""auto""` which attempts to use a :class:`~sklearn.neighbors.KDTree` tree if possible, otherwise it uses a :class:`~sklearn.neighbors.BallTree` tree. Both `""kd_tree""` and `""ball_tree""` algorithms use the :class:`~sklearn.neighbors.NearestNeighbors` estimator. If the `X` passed during `fit` is sparse or `metric` is invalid for both :class:`~sklearn.neighbors.KDTree` and :class:`~sklearn.neighbors.BallTree`, then it resolves to use the `""brute""` algorithm.",HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Parameters,leaf_size,int,40,"Leaf size for trees responsible for fast nearest neighbour queries when a KDTree or a BallTree are used as core-distance algorithms. A large dataset size and small `leaf_size` may induce excessive memory usage. If you are running out of memory consider increasing the `leaf_size` parameter. Ignored for `algorithm=""brute""`.",HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Parameters,n_jobs,int,None,Number of jobs to run in parallel to calculate distances. `None` means 1 unless in a :obj:`joblib.parallel_backend` context. `-1` means using all processors. See :term:`Glossary <n_jobs>` for more details.,HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Parameters,cluster_selection_method,"{""eom""","""eom""","The method used to select clusters from the condensed tree. The standard approach for HDBSCAN* is to use an Excess of Mass (`""eom""`) algorithm to find the most persistent clusters. Alternatively you can instead select the clusters at the leaves of the tree -- this provides the most fine grained and homogeneous clusters.",HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Parameters,allow_single_cluster,bool,False,"By default HDBSCAN* will not produce a single cluster, setting this to True will override this and allow single cluster results in the case that you feel this is a valid result for your dataset.",HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Parameters,store_centers,str,None,"Which, if any, cluster centers to compute and store. The options are: - `None` which does not compute nor store any centers. - `""centroid""` which calculates the center by taking the weighted average of their positions. Note that the algorithm uses the euclidean metric and does not guarantee that the output will be an observed data point. - `""medoid""` which calculates the center by taking the point in the fitted data which minimizes the distance to all other points in the cluster. This is slower than ""centroid"" since it requires computing additional pairwise distances between points of the same cluster but guarantees the output is an observed data point. The medoid is also well-defined for arbitrary metrics, and does not depend on a euclidean metric. - `""both""` which computes and stores both forms of centers.",HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Attributes,copy,bool,False,"If `copy=True` then any time an in-place modifications would be made that would overwrite data passed to :term:`fit`, a copy will first be made, guaranteeing that the original data will be unchanged. Currently, it only applies when `metric=""precomputed""`, when passing a dense array or a CSR sparse matrix and when `algorithm=""brute""`.",HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Attributes,labels_,ndarray of shape (n_samples,,"Cluster labels for each point in the dataset given to :term:`fit`. Outliers are labeled as follows: - Noisy samples are given the label -1. - Samples with infinite elements (+/- np.inf) are given the label -2. - Samples with missing data are given the label -3, even if they also have infinite elements.",HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Attributes,probabilities_,ndarray of shape (n_samples,,The strength with which each sample is a member of its assigned cluster. - Clustered samples have probabilities proportional to the degree that they persist as part of the cluster. - Noisy samples have probability zero. - Samples with infinite elements (+/- np.inf) have probability 0. - Samples with missing data have probability `np.nan`.,HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Attributes,centroids_,ndarray of shape (n_clusters,,"A collection containing the centroid of each cluster calculated under the standard euclidean metric. The centroids may fall ""outside"" their respective clusters if the clusters themselves are non-convex. Note that `n_clusters` only counts non-outlier clusters. That is to say, the `-1, -2, -3` labels for the outlier clusters are excluded.",HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
,medoids_,ndarray of shape (n_clusters,,"A collection containing the medoid of each cluster calculated under the whichever metric was passed to the `metric` parameter. The medoids are points in the original cluster which minimize the average distance to all other points in that cluster under the chosen metric. These can be thought of as the result of projecting the `metric`-based centroid back onto the cluster. Note that `n_clusters` only counts non-outlier clusters. That is to say, the `-1, -2, -3` labels for the outlier clusters are excluded.",HDBSCAN,<class 'sklearn.cluster._hdbscan.hdbscan.HDBSCAN'>
Parameters,input,{'filename','content',"- If `'filename'`, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze. - If `'file'`, the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory. - If `'content'`, the input is expected to be a sequence of items that can be of type string or byte.",HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,encoding,str,'utf-8',"If bytes or files are given to analyze, this encoding is used to decode.",HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,decode_error,{'strict','strict',"Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'.",HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,strip_accents,{'ascii',None,Remove accents and perform other character normalization during the preprocessing step. 'ascii' is a fast method that only works on characters that have a direct ASCII mapping. 'unicode' is a slightly slower method that works on any character. None (default) means no character normalization is performed. Both 'ascii' and 'unicode' use NFKD normalization from :func:`unicodedata.normalize`.,HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,lowercase,bool,True,Convert all characters to lowercase before tokenizing.,HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,preprocessor,callable,None,Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if ``analyzer`` is not callable.,HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,tokenizer,callable,None,Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if ``analyzer == 'word'``.,HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,stop_words,{'english'},None,"If 'english', a built-in stop word list for English is used. There are several known issues with 'english' and you should consider an alternative (see :ref:`stop_words`). If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if ``analyzer == 'word'``.",HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,token_pattern,str or None,"r""(?u)\\b\\w\\w+\\b""","Regular expression denoting what constitutes a ""token"", only used if ``analyzer == 'word'``. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). If there is a capturing group in token_pattern then the captured group content, not the entire match, becomes the token. At most one capturing group is permitted.",HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,ngram_range,tuple (min_n,(1,"The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means only bigrams. Only applies if ``analyzer`` is not callable.",HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,analyzer,{'word','word',"Whether the feature should be made of word or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space. If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data is first read from the file and then passed to the given callable analyzer.",HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,n_features,int,(2 ** 20),"The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.",HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,binary,bool,False,"If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.",HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,norm,{'l1','l2',Norm used to normalize term vectors. None for no normalization.,HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,alternate_sign,bool,True,"When True, an alternating sign is added to the features as to approximately conserve the inner product in the hashed space even for small n_features. This approach is similar to sparse random projection.",HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
,dtype,type,np.float64,Type of the matrix returned by fit_transform() or transform().,HashingVectorizer,<class 'sklearn.feature_extraction.text.HashingVectorizer'>
Parameters,loss,{'log_loss'},'log_loss',"The loss function to use in the boosting process. For binary classification problems, 'log_loss' is also known as logistic loss, binomial deviance or binary crossentropy. Internally, the model fits one tree per boosting iteration and uses the logistic sigmoid function (expit) as inverse link function to compute the predicted positive class probability. For multiclass classification problems, 'log_loss' is also known as multinomial deviance or categorical crossentropy. Internally, the model fits one tree per boosting iteration and per class and uses the softmax function as inverse link function to compute the predicted probabilities of the classes.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,learning_rate,float,0.1,"The learning rate, also known as *shrinkage*. This is used as a multiplicative factor for the leaves values. Use ``1`` for no shrinkage.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,max_iter,int,100,"The maximum number of iterations of the boosting process, i.e. the maximum number of trees for binary classification. For multiclass classification, `n_classes` trees per iteration are built.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,max_leaf_nodes,int or None,31,"The maximum number of leaves for each tree. Must be strictly greater than 1. If None, there is no maximum limit.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,max_depth,int or None,None,The maximum depth of each tree. The depth of a tree is the number of edges to go from the root to the deepest leaf. Depth isn't constrained by default.,HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,min_samples_leaf,int,20,"The minimum number of samples per leaf. For small datasets with less than a few hundred samples, it is recommended to lower this value since only very shallow trees would be built.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,l2_regularization,float,0,The L2 regularization parameter penalizing leaves with small hessians. Use ``0`` for no regularization (default).,HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,max_features,float,1.0,"Proportion of randomly chosen features in each and every node split. This is a form of regularization, smaller values make the trees weaker learners and might prevent overfitting. If interaction constraints from `interaction_cst` are present, only allowed features are taken into account for the subsampling.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,max_bins,int,255,"The maximum number of bins to use for non-missing values. Before training, each feature of the input array `X` is binned into integer-valued bins, which allows for a much faster training stage. Features with a small number of unique values may use less than ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin is always reserved for missing values. Must be no larger than 255.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,categorical_features,array-like of {bool,None,"Indicates the categorical features. - None : no feature will be considered categorical. - boolean array-like : boolean mask indicating categorical features. - integer array-like : integer indices indicating categorical features. - str array-like: names of categorical features (assuming the training data has feature names). - `""from_dtype""`: dataframe columns with dtype ""category"" are considered to be categorical features. The input must be an object exposing a ``__dataframe__`` method such as pandas or polars DataFrames to use this feature. For each categorical feature, there must be at most `max_bins` unique categories. Negative values for categorical features encoded as numeric dtypes are treated as missing values. All categorical values are converted to floating point numbers. This means that categorical values of 1.0 and 1 are treated as the same category. Read more in the :ref:`User Guide <categorical_support_gbdt>`. Added support for feature names. Added `""from_dtype""` option. The default will changed from `None` to `""from_dtype""`.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,monotonic_cst,array-like of int of shape (n_features) or dict,None,"Monotonic constraint to enforce on each feature are specified using the following integer values: - 1: monotonic increase - 0: no constraint - -1: monotonic decrease If a dict with str keys, map feature to monotonic constraints by name. If an array, the features are mapped to constraints by position. See :ref:`monotonic_cst_features_names` for a usage example. The constraints are only valid for binary classifications and hold over the probability of the positive class. Read more in the :ref:`User Guide <monotonic_cst_gbdt>`. Accept dict of constraints with feature names as keys.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,interaction_cst,"{""pairwise""",None,"Specify interaction constraints, the sets of features which can interact with each other in child node splits. Each item specifies the set of feature indices that are allowed to interact with each other. If there are more features than specified in these constraints, they are treated as if they were specified as an additional set. The strings ""pairwise"" and ""no_interactions"" are shorthands for allowing only pairwise or no interactions, respectively. For instance, with 5 features in total, `interaction_cst=[{0, 1}]` is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`, and specifies that each branch of a tree will either only split on features 0 and 1 or only split on features 2, 3 and 4.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble. For results to be valid, the estimator should be re-trained on the same data only. See :term:`the Glossary <warm_start>`.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,early_stopping,'auto' or bool,'auto',"If 'auto', early stopping is enabled if the sample size is larger than 10000. If True, early stopping is enabled, otherwise early stopping is disabled.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,scoring,str or callable or None,'loss',"Scoring parameter to use for early stopping. It can be a single string (see :ref:`scoring_parameter`) or a callable (see :ref:`scoring_callable`). If None, the estimator's default scorer is used. If ``scoring='loss'``, early stopping is checked w.r.t the loss value. Only used if early stopping is performed.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,validation_fraction,int or float or None,0.1,"Proportion (or absolute size) of training data to set aside as validation data for early stopping. If None, early stopping is done on the training data. Only used if early stopping is performed.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,n_iter_no_change,int,10,"Used to determine when to ""early stop"". The fitting process is stopped when none of the last ``n_iter_no_change`` scores are better than the ``n_iter_no_change - 1`` -th-to-last one, up to some tolerance. Only used if early stopping is performed.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,tol,float,1e-7,"The absolute tolerance to use when comparing scores. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,verbose,int,0,"The verbosity level. If not zero, print some information about the fitting process. ``1`` prints only summary info, ``2`` prints info per iteration.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,random_state,int,None,"Pseudo-random number generator to control the subsampling in the binning process, and the train/validation data split if early stopping is enabled. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Attributes,class_weight,dict or 'balanced',None,"Weights associated with classes in the form `{class_label: weight}`. If not given, all classes are supposed to have weight one. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n_samples / (n_classes * np.bincount(y))`. Note that these weights will be multiplied with sample_weight (passed through the fit method) if `sample_weight` is specified.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Attributes,classes_,array,,Class labels.,HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Attributes,do_early_stopping_,bool,,Indicates whether early stopping is used during training.,HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Attributes,n_iter_,int,,"The number of iterations as selected by early stopping, depending on the `early_stopping` parameter. Otherwise it corresponds to max_iter.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Attributes,n_trees_per_iteration_,int,,"The number of tree that are built at each iteration. This is equal to 1 for binary classification, and to ``n_classes`` for multiclass classification.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Attributes,train_score_,ndarray,,"The scores at each iteration on the training data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the ``scoring`` parameter. If ``scoring`` is not 'loss', scores are computed on a subset of at most 10 000 samples. Empty if no early stopping.",HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Attributes,validation_score_,ndarray,,The scores at each iteration on the held-out validation data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the ``scoring`` parameter. Empty if no early stopping or if ``validation_fraction`` is None.,HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Attributes,is_categorical_,ndarray,,Boolean mask for the categorical features. ``None`` if there are no categorical features.,HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,HistGradientBoostingClassifier,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier'>
Parameters,loss,{'squared_error','squared_error',"The loss function to use in the boosting process. Note that the ""squared error"", ""gamma"" and ""poisson"" losses actually implement ""half least squares loss"", ""half gamma deviance"" and ""half poisson deviance"" to simplify the computation of the gradient. Furthermore, ""gamma"" and ""poisson"" losses internally use a log-link, ""gamma"" requires ``y > 0`` and ""poisson"" requires ``y >= 0``. ""quantile"" uses the pinball loss. Added option 'poisson'. Added option 'quantile'. Added option 'gamma'.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,quantile,float,None,"If loss is ""quantile"", this parameter specifies which quantile to be estimated and must be between 0 and 1.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,learning_rate,float,0.1,"The learning rate, also known as *shrinkage*. This is used as a multiplicative factor for the leaves values. Use ``1`` for no shrinkage.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,max_iter,int,100,"The maximum number of iterations of the boosting process, i.e. the maximum number of trees.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,max_leaf_nodes,int or None,31,"The maximum number of leaves for each tree. Must be strictly greater than 1. If None, there is no maximum limit.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,max_depth,int or None,None,The maximum depth of each tree. The depth of a tree is the number of edges to go from the root to the deepest leaf. Depth isn't constrained by default.,HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,min_samples_leaf,int,20,"The minimum number of samples per leaf. For small datasets with less than a few hundred samples, it is recommended to lower this value since only very shallow trees would be built.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,l2_regularization,float,0,The L2 regularization parameter penalizing leaves with small hessians. Use ``0`` for no regularization (default).,HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,max_features,float,1.0,"Proportion of randomly chosen features in each and every node split. This is a form of regularization, smaller values make the trees weaker learners and might prevent overfitting. If interaction constraints from `interaction_cst` are present, only allowed features are taken into account for the subsampling.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,max_bins,int,255,"The maximum number of bins to use for non-missing values. Before training, each feature of the input array `X` is binned into integer-valued bins, which allows for a much faster training stage. Features with a small number of unique values may use less than ``max_bins`` bins. In addition to the ``max_bins`` bins, one more bin is always reserved for missing values. Must be no larger than 255.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,categorical_features,array-like of {bool,None,"Indicates the categorical features. - None : no feature will be considered categorical. - boolean array-like : boolean mask indicating categorical features. - integer array-like : integer indices indicating categorical features. - str array-like: names of categorical features (assuming the training data has feature names). - `""from_dtype""`: dataframe columns with dtype ""category"" are considered to be categorical features. The input must be an object exposing a ``__dataframe__`` method such as pandas or polars DataFrames to use this feature. For each categorical feature, there must be at most `max_bins` unique categories. Negative values for categorical features encoded as numeric dtypes are treated as missing values. All categorical values are converted to floating point numbers. This means that categorical values of 1.0 and 1 are treated as the same category. Read more in the :ref:`User Guide <categorical_support_gbdt>`. Added support for feature names. Added `""from_dtype""` option. The default value changed from `None` to `""from_dtype""`.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,monotonic_cst,array-like of int of shape (n_features) or dict,None,"Monotonic constraint to enforce on each feature are specified using the following integer values: - 1: monotonic increase - 0: no constraint - -1: monotonic decrease If a dict with str keys, map feature to monotonic constraints by name. If an array, the features are mapped to constraints by position. See :ref:`monotonic_cst_features_names` for a usage example. Read more in the :ref:`User Guide <monotonic_cst_gbdt>`. Accept dict of constraints with feature names as keys.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,interaction_cst,"{""pairwise""",None,"Specify interaction constraints, the sets of features which can interact with each other in child node splits. Each item specifies the set of feature indices that are allowed to interact with each other. If there are more features than specified in these constraints, they are treated as if they were specified as an additional set. The strings ""pairwise"" and ""no_interactions"" are shorthands for allowing only pairwise or no interactions, respectively. For instance, with 5 features in total, `interaction_cst=[{0, 1}]` is equivalent to `interaction_cst=[{0, 1}, {2, 3, 4}]`, and specifies that each branch of a tree will either only split on features 0 and 1 or only split on features 2, 3 and 4.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble. For results to be valid, the estimator should be re-trained on the same data only. See :term:`the Glossary <warm_start>`.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,early_stopping,'auto' or bool,'auto',"If 'auto', early stopping is enabled if the sample size is larger than 10000. If True, early stopping is enabled, otherwise early stopping is disabled.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,scoring,str or callable or None,'loss',"Scoring parameter to use for early stopping. It can be a single string (see :ref:`scoring_parameter`) or a callable (see :ref:`scoring_callable`). If None, the estimator's default scorer is used. If ``scoring='loss'``, early stopping is checked w.r.t the loss value. Only used if early stopping is performed.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,validation_fraction,int or float or None,0.1,"Proportion (or absolute size) of training data to set aside as validation data for early stopping. If None, early stopping is done on the training data. Only used if early stopping is performed.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,n_iter_no_change,int,10,"Used to determine when to ""early stop"". The fitting process is stopped when none of the last ``n_iter_no_change`` scores are better than the ``n_iter_no_change - 1`` -th-to-last one, up to some tolerance. Only used if early stopping is performed.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,tol,float,1e-7,"The absolute tolerance to use when comparing scores during early stopping. The higher the tolerance, the more likely we are to early stop: higher tolerance means that it will be harder for subsequent iterations to be considered an improvement upon the reference score.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,verbose,int,0,"The verbosity level. If not zero, print some information about the fitting process. ``1`` prints only summary info, ``2`` prints info per iteration.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Attributes,random_state,int,None,"Pseudo-random number generator to control the subsampling in the binning process, and the train/validation data split if early stopping is enabled. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Attributes,do_early_stopping_,bool,,Indicates whether early stopping is used during training.,HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Attributes,n_iter_,int,,"The number of iterations as selected by early stopping, depending on the `early_stopping` parameter. Otherwise it corresponds to max_iter.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Attributes,n_trees_per_iteration_,int,,"The number of tree that are built at each iteration. For regressors, this is always 1.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Attributes,train_score_,ndarray,,"The scores at each iteration on the training data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the ``scoring`` parameter. If ``scoring`` is not 'loss', scores are computed on a subset of at most 10 000 samples. Empty if no early stopping.",HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Attributes,validation_score_,ndarray,,The scores at each iteration on the held-out validation data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the ``scoring`` parameter. Empty if no early stopping or if ``validation_fraction`` is None.,HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Attributes,is_categorical_,ndarray,,Boolean mask for the categorical features. ``None`` if there are no categorical features.,HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,HistGradientBoostingRegressor,<class 'sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingRegressor'>
Parameters,epsilon,float,1.35,"The parameter epsilon controls the number of samples that should be classified as outliers. The smaller the epsilon, the more robust it is to outliers. Epsilon must be in the range `[1, inf)`.",HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>
Parameters,max_iter,int,100,"Maximum number of iterations that ``scipy.optimize.minimize(method=""L-BFGS-B"")`` should run for.",HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>
Parameters,alpha,float,0.0001,"Strength of the squared L2 regularization. Note that the penalty is equal to ``alpha * ||w||^2``. Must be in the range `[0, inf)`.",HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>
Parameters,warm_start,bool,False,"This is useful if the stored attributes of a previously used model has to be reused. If set to False, then the coefficients will be rewritten for every call to fit. See :term:`the Glossary <warm_start>`.",HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>
Parameters,fit_intercept,bool,True,Whether or not to fit the intercept. This can be set to False if the data is already centered around the origin.,HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>
Attributes,tol,float,1e-05,"The iteration will stop when ``max{|proj g_i | i = 1, ..., n}`` <= ``tol`` where pg_i is the i-th component of the projected gradient.",HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>
Attributes,coef_,array,,Features got by optimizing the L2-regularized Huber loss.,HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>
Attributes,intercept_,float,,Bias.,HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>
Attributes,scale_,float,,The value by which ``|y - Xw - c|`` is scaled down.,HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>
Attributes,n_iter_,int,,"Number of iterations that ``scipy.optimize.minimize(method=""L-BFGS-B"")`` has run for. In SciPy <= 1.0.0 the number of lbfgs iterations may exceed ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.",HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>
,outliers_,array,,A boolean mask which is set to True where the samples are identified as outliers.,HuberRegressor,<class 'sklearn.linear_model._huber.HuberRegressor'>
Parameters,n_components,int,None,"Number of components to keep. If ``n_components`` is ``None``, then ``n_components`` is set to ``min(n_samples, n_features)``.",IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Parameters,whiten,bool,False,When True (False by default) the ``components_`` vectors are divided by ``n_samples`` times ``components_`` to ensure uncorrelated outputs with unit component-wise variances. Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometimes improve the predictive accuracy of the downstream estimators by making data respect some hard-wired assumptions.,IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Parameters,copy,bool,True,"If False, X will be overwritten. ``copy=False`` can be used to save memory but is unsafe for general use.",IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Attributes,batch_size,int,None,"The number of samples to use for each batch. Only used when calling ``fit``. If ``batch_size`` is ``None``, then ``batch_size`` is inferred from the data and set to ``5 * n_features``, to provide a balance between approximation accuracy and memory consumption.",IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Attributes,components_,ndarray of shape (n_components,,"Principal axes in feature space, representing the directions of maximum variance in the data. Equivalently, the right singular vectors of the centered input data, parallel to its eigenvectors. The components are sorted by decreasing ``explained_variance_``.",IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Attributes,explained_variance_,ndarray of shape (n_components,,Variance explained by each of the selected components.,IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Attributes,explained_variance_ratio_,ndarray of shape (n_components,,"Percentage of variance explained by each of the selected components. If all components are stored, the sum of explained variances is equal to 1.0.",IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Attributes,singular_values_,ndarray of shape (n_components,,The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the ``n_components`` variables in the lower-dimensional space.,IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Attributes,mean_,ndarray of shape (n_features,,"Per-feature empirical mean, aggregate over calls to ``partial_fit``.",IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Attributes,var_,ndarray of shape (n_features,,"Per-feature empirical variance, aggregate over calls to ``partial_fit``.",IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Attributes,noise_variance_,float,,"The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See ""Pattern Recognition and Machine Learning"" by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf.",IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Attributes,n_components_,int,,The estimated number of components. Relevant when ``n_components=None``.,IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Attributes,n_samples_seen_,int,,"The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across ``partial_fit`` calls.",IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Attributes,batch_size_,int,,Inferred batch size from ``batch_size``.,IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,IncrementalPCA,<class 'sklearn.decomposition._incremental_pca.IncrementalPCA'>
Parameters,n_estimators,int,100,The number of base estimators in the ensemble.,IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Parameters,max_samples,"""auto""","""auto""","The number of samples to draw from X to train each base estimator. - If int, then draw `max_samples` samples. - If float, then draw `max_samples * X.shape[0]` samples. - If ""auto"", then `max_samples=min(256, n_samples)`. If max_samples is larger than the number of samples provided, all samples will be used for all trees (no sampling).",IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Parameters,contamination,'auto' or float,'auto',"The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the scores of the samples. - If 'auto', the threshold is determined as in the original paper. - If float, the contamination should be in the range (0, 0.5]. The default value of ``contamination`` changed from 0.1 to ``'auto'``.",IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Parameters,max_features,int or float,1.0,"The number of features to draw from X to train each base estimator. - If int, then draw `max_features` features. - If float, then draw `max(1, int(max_features * n_features_in_))` features. Note: using a float number less than 1.0 or integer less than number of features will enable feature subsampling and leads to a longer runtime.",IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Parameters,bootstrap,bool,False,"If True, individual trees are fit on random subsets of the training data sampled with replacement. If False, sampling without replacement is performed.",IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Parameters,n_jobs,int,None,The number of jobs to run in parallel for :meth:`fit`. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Parameters,random_state,int,None,Controls the pseudo-randomness of the selection of the feature and split values for each branching step and each tree in the forest. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Parameters,verbose,int,0,Controls the verbosity of the tree building process.,IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Attributes,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See :term:`the Glossary <warm_start>`.",IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Attributes,estimator_,:class:`~sklearn.tree.ExtraTreeRegressor` instance,,The child estimator template used to create the collection of fitted sub-estimators. `base_estimator_` was renamed to `estimator_`.,IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Attributes,estimators_,list of ExtraTreeRegressor instances,,The collection of fitted sub-estimators.,IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Attributes,estimators_features_,list of ndarray,,The subset of drawn features for each base estimator.,IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Attributes,estimators_samples_,list of ndarray,,"The subset of drawn samples (i.e., the in-bag samples) for each base estimator.",IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Attributes,max_samples_,int,,The actual number of samples.,IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Attributes,offset_,float,,"Offset used to define the decision function from the raw scores. We have the relation: ``decision_function = score_samples - offset_``. ``offset_`` is defined as follows. When the contamination parameter is set to ""auto"", the offset is equal to -0.5 as the scores of inliers are close to 0 and the scores of outliers are close to -1. When a contamination parameter different than ""auto"" is provided, the offset is defined in such a way we obtain the expected number of outliers (samples with decision function < 0) in training.",IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,IsolationForest,<class 'sklearn.ensemble._iforest.IsolationForest'>
Parameters,n_neighbors,int or None,5,"Number of neighbors to consider for each point. If `n_neighbors` is an int, then `radius` must be `None`.",Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Parameters,radius,float or None,None,"Limiting distance of neighbors to return. If `radius` is a float, then `n_neighbors` must be set to `None`.",Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Parameters,n_components,int,2,Number of coordinates for the manifold.,Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Parameters,eigen_solver,{'auto','auto','auto' : Attempt to choose the most efficient solver for the given problem. 'arpack' : Use Arnoldi decomposition to find the eigenvalues and eigenvectors. 'dense' : Use a direct solver (i.e. LAPACK) for the eigenvalue decomposition.,Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Parameters,tol,float,0,Convergence tolerance passed to arpack or lobpcg. not used if eigen_solver == 'dense'.,Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Parameters,max_iter,int,None,Maximum number of iterations for the arpack solver. not used if eigen_solver == 'dense'.,Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Parameters,path_method,{'auto','auto',Method to use in finding shortest path. 'auto' : attempt to choose the best algorithm automatically. 'FW' : Floyd-Warshall algorithm. 'D' : Dijkstra's algorithm.,Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Parameters,neighbors_algorithm,{'auto','auto',"Algorithm to use for nearest neighbors search, passed to neighbors.NearestNeighbors instance.",Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Parameters,n_jobs,int or None,None,The number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Parameters,metric,str,"""minkowski""","The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by :func:`sklearn.metrics.pairwise_distances` for its metric parameter. If metric is ""precomputed"", X is assumed to be a distance matrix and must be square. X may be a :term:`Glossary <sparse graph>`.",Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Parameters,p,float,2,"Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Attributes,metric_params,dict,None,Additional keyword arguments for the metric function.,Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Attributes,embedding_,array-like,,Stores the embedding vectors.,Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Attributes,kernel_pca_,object,,:class:`~sklearn.decomposition.KernelPCA` object used to implement the embedding.,Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Attributes,nbrs_,sklearn.neighbors.NearestNeighbors instance,,"Stores nearest neighbors instance, including BallTree or KDtree if applicable.",Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Attributes,dist_matrix_,array-like,,Stores the geodesic distance matrix of training data.,Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,Isomap,<class 'sklearn.manifold._isomap.Isomap'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,Isomap,<class 'sklearn.manifold._isomap.Isomap'>
Parameters,y_min,float,None,"Lower bound on the lowest predicted value (the minimum value may still be higher). If not set, defaults to -inf.",IsotonicRegression,<class 'sklearn.isotonic.IsotonicRegression'>
Parameters,y_max,float,None,"Upper bound on the highest predicted value (the maximum may still be lower). If not set, defaults to +inf.",IsotonicRegression,<class 'sklearn.isotonic.IsotonicRegression'>
Parameters,increasing,bool or 'auto',True,Determines whether the predictions should be constrained to increase or decrease with `X`. 'auto' will decide based on the Spearman correlation estimate's sign.,IsotonicRegression,<class 'sklearn.isotonic.IsotonicRegression'>
Attributes,out_of_bounds,{'nan','nan',"Handles how `X` values outside of the training domain are handled during prediction. - 'nan', predictions will be NaN. - 'clip', predictions will be set to the value corresponding to the nearest train interval endpoint. - 'raise', a `ValueError` is raised.",IsotonicRegression,<class 'sklearn.isotonic.IsotonicRegression'>
Attributes,X_min_,float,,Minimum value of input array `X_` for left bound.,IsotonicRegression,<class 'sklearn.isotonic.IsotonicRegression'>
Attributes,X_max_,float,,Maximum value of input array `X_` for right bound.,IsotonicRegression,<class 'sklearn.isotonic.IsotonicRegression'>
Attributes,X_thresholds_,ndarray of shape (n_thresholds,,Unique ascending `X` values used to interpolate the y = f(X) monotonic function.,IsotonicRegression,<class 'sklearn.isotonic.IsotonicRegression'>
Attributes,y_thresholds_,ndarray of shape (n_thresholds,,De-duplicated `y` values suitable to interpolate the y = f(X) monotonic function.,IsotonicRegression,<class 'sklearn.isotonic.IsotonicRegression'>
Attributes,f_,function,,The stepwise interpolating function that covers the input domain ``X``.,IsotonicRegression,<class 'sklearn.isotonic.IsotonicRegression'>
,increasing_,bool,,Inferred value for ``increasing``.,IsotonicRegression,<class 'sklearn.isotonic.IsotonicRegression'>
Parameters,n_bins,int or array-like of shape (n_features,5,The number of bins to produce. Raises ValueError if ``n_bins < 2``.,KBinsDiscretizer,<class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>
Parameters,encode,{'onehot','onehot',Method used to encode the transformed result. - 'onehot': Encode the transformed result with one-hot encoding and return a sparse matrix. Ignored features are always stacked to the right. - 'onehot-dense': Encode the transformed result with one-hot encoding and return a dense array. Ignored features are always stacked to the right. - 'ordinal': Return the bin identifier encoded as an integer value.,KBinsDiscretizer,<class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>
Parameters,strategy,{'uniform','quantile',Strategy used to define the widths of the bins. - 'uniform': All bins in each feature have identical widths. - 'quantile': All bins in each feature have the same number of points. - 'kmeans': Values in each bin have the same nearest center of a 1D k-means cluster. For an example of the different strategies see: :ref:`sphx_glr_auto_examples_preprocessing_plot_discretization_strategies.py`.,KBinsDiscretizer,<class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>
Parameters,dtype,{np.float32,None,"The desired data-type for the output. If None, output dtype is consistent with input dtype. Only np.float32 and np.float64 are supported.",KBinsDiscretizer,<class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>
Parameters,subsample,int or None,200_000,"Maximum number of samples, used to fit the model, for computational efficiency. `subsample=None` means that all the training samples are used when computing the quantiles that determine the binning thresholds. Since quantile computation relies on sorting each column of `X` and that sorting has an `n log(n)` time complexity, it is recommended to use subsampling on datasets with a very large number of samples. The default value of `subsample` changed from `None` to `200_000` when `strategy=""quantile""`. The default value of `subsample` changed from `None` to `200_000` when `strategy=""uniform""` or `strategy=""kmeans""`.",KBinsDiscretizer,<class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>
Attributes,random_state,int,None,Determines random number generation for subsampling. Pass an int for reproducible results across multiple function calls. See the `subsample` parameter for more details. See :term:`Glossary <random_state>`.,KBinsDiscretizer,<class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>
Attributes,bin_edges_,ndarray of ndarray of shape (n_features,,"The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )`` Ignored features will have empty arrays.",KBinsDiscretizer,<class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>
Attributes,n_bins_,ndarray of shape (n_features,,"Number of bins per feature. Bins whose width are too small (i.e., <= 1e-8) are removed with a warning.",KBinsDiscretizer,<class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,KBinsDiscretizer,<class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,KBinsDiscretizer,<class 'sklearn.preprocessing._discretization.KBinsDiscretizer'>
Parameters,n_clusters,int,8,The number of clusters to form as well as the number of centroids to generate. For an example of how to choose an optimal value for `n_clusters` refer to :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.,KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Parameters,init,{'k-means++','k-means++',"Method for initialization: * 'k-means++' : selects initial cluster centroids using sampling             based on an empirical probability distribution of the points'             contribution to the overall inertia. This technique speeds up             convergence. The algorithm implemented is ""greedy k-means++"". It             differs from the vanilla k-means++ by making several trials at             each sampling step and choosing the best centroid among them. * 'random': choose `n_clusters` observations (rows) at random from         data for the initial centroids. * If an array is passed, it should be of shape (n_clusters, n_features)        and gives the initial centers. * If a callable is passed, it should take arguments X, n_clusters and a        random state and return an initialization. For an example of how to use the different `init` strategies, see :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`. For an evaluation of the impact of initialization, see the example :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_stability_low_dim_dense.py`.",KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Parameters,n_init,'auto' or int,'auto',"Number of times the k-means algorithm is run with different centroid seeds. The final results is the best output of `n_init` consecutive runs in terms of inertia. Several runs are recommended for sparse high-dimensional problems (see :ref:`kmeans_sparse_high_dim`). When `n_init='auto'`, the number of runs depends on the value of init: 10 if using `init='random'` or `init` is a callable; 1 if using `init='k-means++'` or `init` is an array-like. Added 'auto' option for `n_init`. Default value for `n_init` changed to `'auto'`.",KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Parameters,max_iter,int,300,Maximum number of iterations of the k-means algorithm for a single run.,KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Parameters,tol,float,1e-4,Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence.,KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Parameters,verbose,int,0,Verbosity mode.,KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Parameters,random_state,int,None,Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See :term:`Glossary <random_state>`.,KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Parameters,copy_x,bool,True,"When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean. Note that if the original data is not C-contiguous, a copy will be made even if copy_x is False. If the original data is sparse, but not in CSR format, a copy will be made even if copy_x is False.",KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Attributes,algorithm,"{""lloyd""","""lloyd""","K-means algorithm to use. The classical EM-style algorithm is `""lloyd""`. The `""elkan""` variation can be more efficient on some datasets with well-defined clusters, by using the triangle inequality. However it's more memory intensive due to the allocation of an extra array of shape `(n_samples, n_clusters)`. Added Elkan algorithm Renamed ""full"" to ""lloyd"", and deprecated ""auto"" and ""full"". Changed ""auto"" to use ""lloyd"" instead of ""elkan"".",KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Attributes,cluster_centers_,ndarray of shape (n_clusters,,"Coordinates of cluster centers. If the algorithm stops before fully converging (see ``tol`` and ``max_iter``), these will not be consistent with ``labels_``.",KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Attributes,labels_,ndarray of shape (n_samples,,Labels of each point,KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Attributes,inertia_,float,,"Sum of squared distances of samples to their closest cluster center, weighted by the sample weights if provided.",KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Attributes,n_iter_,int,,Number of iterations run.,KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,KMeans,<class 'sklearn.cluster._kmeans.KMeans'>
Parameters,missing_values,int,np.nan,"The placeholder for the missing values. All occurrences of `missing_values` will be imputed. For pandas' dataframes with nullable integer dtypes with missing values, `missing_values` should be set to np.nan, since `pd.NA` will be converted to np.nan.",KNNImputer,<class 'sklearn.impute._knn.KNNImputer'>
Parameters,n_neighbors,int,5,Number of neighboring samples to use for imputation.,KNNImputer,<class 'sklearn.impute._knn.KNNImputer'>
Parameters,weights,{'uniform','uniform',"Weight function used in prediction.  Possible values: - 'uniform' : uniform weights. All points in each neighborhood are weighted equally. - 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. - callable : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.",KNNImputer,<class 'sklearn.impute._knn.KNNImputer'>
Parameters,metric,{'nan_euclidean'} or callable,'nan_euclidean',"Distance metric for searching neighbors. Possible values: - 'nan_euclidean' - callable : a user-defined function which conforms to the definition of ``func_metric(x, y, *, missing_values=np.nan)``. `x` and `y` corresponds to a row (i.e. 1-D arrays) of `X` and `Y`, respectively. The callable should returns a scalar distance value.",KNNImputer,<class 'sklearn.impute._knn.KNNImputer'>
Parameters,copy,bool,True,"If True, a copy of X will be created. If False, imputation will be done in-place whenever possible.",KNNImputer,<class 'sklearn.impute._knn.KNNImputer'>
Parameters,add_indicator,bool,False,"If True, a :class:`MissingIndicator` transform will stack onto the output of the imputer's transform. This allows a predictive estimator to account for missingness despite imputation. If a feature has no missing values at fit/train time, the feature won't appear on the missing indicator even if there are missing values at transform/test time.",KNNImputer,<class 'sklearn.impute._knn.KNNImputer'>
Attributes,keep_empty_features,bool,False,"If True, features that consist exclusively of missing values when `fit` is called are returned in results when `transform` is called. The imputed value is always `0`.",KNNImputer,<class 'sklearn.impute._knn.KNNImputer'>
Attributes,indicator_,:class:`~sklearn.impute.MissingIndicator`,,Indicator used to add binary indicators for missing values. ``None`` if add_indicator is False.,KNNImputer,<class 'sklearn.impute._knn.KNNImputer'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,KNNImputer,<class 'sklearn.impute._knn.KNNImputer'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,KNNImputer,<class 'sklearn.impute._knn.KNNImputer'>
Parameters,n_neighbors,int,5,Number of neighbors to use by default for :meth:`kneighbors` queries.,KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Parameters,weights,{'uniform','uniform',"Weight function used in prediction.  Possible values: - 'uniform' : uniform weights.  All points in each neighborhood are weighted equally. - 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. - [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights. Refer to the example entitled :ref:`sphx_glr_auto_examples_neighbors_plot_classification.py` showing the impact of the `weights` parameter on the decision boundary.",KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Parameters,algorithm,{'auto','auto',"Algorithm used to compute the nearest neighbors: - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDTree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method. Note: fitting on sparse input will override the setting of this parameter, using brute force.",KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Parameters,leaf_size,int,30,"Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem.",KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Parameters,p,float,2,"Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected to be positive.",KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Parameters,metric,str or callable,'minkowski',"Metric to use for distance computation. Default is ""minkowski"", which results in the standard Euclidean distance when p = 2. See the documentation of `scipy.spatial.distance <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric values. If metric is ""precomputed"", X is assumed to be a distance matrix and must be square during fit. X may be a :term:`sparse graph`, in which case only ""nonzero"" elements may be considered neighbors. If metric is a callable function, it takes two arrays representing 1D vectors as inputs and must return one value indicating the distance between those vectors. This works for Scipy's metrics, but is less efficient than passing the metric name as a string.",KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Parameters,metric_params,dict,None,Additional keyword arguments for the metric function.,KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Attributes,n_jobs,int,None,The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details. Doesn't affect :meth:`fit` method.,KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Attributes,classes_,array of shape (n_classes,,Class labels known to the classifier,KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Attributes,effective_metric_,str or callble,,"The distance metric used. It will be same as the `metric` parameter or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to 'minkowski' and `p` parameter set to 2.",KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Attributes,effective_metric_params_,dict,,"Additional keyword arguments for the metric function. For most metrics will be same with `metric_params` parameter, but may also contain the `p` parameter value if the `effective_metric_` attribute is set to 'minkowski'.",KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Attributes,n_samples_fit_,int,,Number of samples in the fitted data.,KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
,outputs_2d_,bool,,"False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit otherwise True.",KNeighborsClassifier,<class 'sklearn.neighbors._classification.KNeighborsClassifier'>
Parameters,n_neighbors,int,5,Number of neighbors to use by default for :meth:`kneighbors` queries.,KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>
Parameters,weights,{'uniform','uniform',"Weight function used in prediction.  Possible values: - 'uniform' : uniform weights.  All points in each neighborhood are weighted equally. - 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. - [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights. Uniform weights are used by default. See the following example for a demonstration of the impact of different weighting schemes on predictions: :ref:`sphx_glr_auto_examples_neighbors_plot_regression.py`.",KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>
Parameters,algorithm,{'auto','auto',"Algorithm used to compute the nearest neighbors: - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDTree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method. Note: fitting on sparse input will override the setting of this parameter, using brute force.",KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>
Parameters,leaf_size,int,30,"Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem.",KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>
Parameters,p,float,2,"Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>
Parameters,metric,str,'minkowski',"Metric to use for distance computation. Default is ""minkowski"", which results in the standard Euclidean distance when p = 2. See the documentation of `scipy.spatial.distance <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric values. If metric is ""precomputed"", X is assumed to be a distance matrix and must be square during fit. X may be a :term:`sparse graph`, in which case only ""nonzero"" elements may be considered neighbors. If metric is a callable function, it takes two arrays representing 1D vectors as inputs and must return one value indicating the distance between those vectors. This works for Scipy's metrics, but is less efficient than passing the metric name as a string. If metric is a DistanceMetric object, it will be passed directly to the underlying computation routines.",KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>
Parameters,metric_params,dict,None,Additional keyword arguments for the metric function.,KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>
Attributes,n_jobs,int,None,The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details. Doesn't affect :meth:`fit` method.,KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>
Attributes,effective_metric_,str or callable,,"The distance metric to use. It will be same as the `metric` parameter or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to 'minkowski' and `p` parameter set to 2.",KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>
Attributes,effective_metric_params_,dict,,"Additional keyword arguments for the metric function. For most metrics will be same with `metric_params` parameter, but may also contain the `p` parameter value if the `effective_metric_` attribute is set to 'minkowski'.",KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>
,n_samples_fit_,int,,Number of samples in the fitted data.,KNeighborsRegressor,<class 'sklearn.neighbors._regression.KNeighborsRegressor'>
Parameters,mode,{'distance','distance',"Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, and 'distance' will return the distances between neighbors according to the given metric.",KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>
Parameters,n_neighbors,int,5,"Number of neighbors for each sample in the transformed sparse graph. For compatibility reasons, as each sample is considered as its own neighbor, one extra neighbor will be computed when mode == 'distance'. In this case, the sparse graph contains (n_neighbors + 1) neighbors.",KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>
Parameters,algorithm,{'auto','auto',"Algorithm used to compute the nearest neighbors: - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDTree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method. Note: fitting on sparse input will override the setting of this parameter, using brute force.",KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>
Parameters,leaf_size,int,30,"Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem.",KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>
Parameters,metric,str or callable,'minkowski',"Metric to use for distance computation. Default is ""minkowski"", which results in the standard Euclidean distance when p = 2. See the documentation of `scipy.spatial.distance <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric values. If metric is a callable function, it takes two arrays representing 1D vectors as inputs and must return one value indicating the distance between those vectors. This works for Scipy's metrics, but is less efficient than passing the metric name as a string. Distance matrices are not supported.",KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>
Parameters,p,float,2,"Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected to be positive.",KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>
Parameters,metric_params,dict,None,Additional keyword arguments for the metric function.,KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>
Attributes,n_jobs,int,None,"The number of parallel jobs to run for neighbors search. If ``-1``, then the number of jobs is set to the number of CPU cores.",KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>
Attributes,effective_metric_,str or callable,,"The distance metric used. It will be same as the `metric` parameter or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to 'minkowski' and `p` parameter set to 2.",KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>
Attributes,effective_metric_params_,dict,,"Additional keyword arguments for the metric function. For most metrics will be same with `metric_params` parameter, but may also contain the `p` parameter value if the `effective_metric_` attribute is set to 'minkowski'.",KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>
,n_samples_fit_,int,,Number of samples in the fitted data.,KNeighborsTransformer,<class 'sklearn.neighbors._graph.KNeighborsTransformer'>
Attributes,K_fit_rows_,ndarray of shape (n_samples,,Average of each column of kernel matrix.,KernelCenterer,<class 'sklearn.preprocessing._data.KernelCenterer'>
Attributes,K_fit_all_,float,,Average of kernel matrix.,KernelCenterer,<class 'sklearn.preprocessing._data.KernelCenterer'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,KernelCenterer,<class 'sklearn.preprocessing._data.KernelCenterer'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,KernelCenterer,<class 'sklearn.preprocessing._data.KernelCenterer'>
Parameters,bandwidth,"float or {""scott""",1.0,"The bandwidth of the kernel. If bandwidth is a float, it defines the bandwidth of the kernel. If bandwidth is a string, one of the estimation methods is implemented.",KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>
Parameters,algorithm,{'kd_tree','auto',The tree algorithm to use.,KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>
Parameters,kernel,{'gaussian','gaussian',The kernel to use.,KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>
Parameters,metric,str,'euclidean',Metric to use for distance computation. See the documentation of `scipy.spatial.distance <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric values. Not all metrics are valid with all algorithms: refer to the documentation of :class:`BallTree` and :class:`KDTree`. Note that the normalization of the density output is correct only for the Euclidean distance metric.,KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>
Parameters,atol,float,0,The desired absolute tolerance of the result.  A larger tolerance will generally lead to faster execution.,KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>
Parameters,rtol,float,0,The desired relative tolerance of the result.  A larger tolerance will generally lead to faster execution.,KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>
Parameters,breadth_first,bool,True,"If true (default), use a breadth-first approach to the problem. Otherwise use a depth-first approach.",KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>
Parameters,leaf_size,int,40,Specify the leaf size of the underlying tree.  See :class:`BallTree` or :class:`KDTree` for details.,KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>
Attributes,metric_params,dict,None,"Additional parameters to be passed to the tree for use with the metric.  For more information, see the documentation of :class:`BallTree` or :class:`KDTree`.",KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>
Attributes,tree_,``BinaryTree`` instance,,The tree algorithm for fast generalized N-point problems.,KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>
,bandwidth_,float,,"Value of the bandwidth, given directly by the bandwidth parameter or estimated using the 'scott' or 'silverman' method.",KernelDensity,<class 'sklearn.neighbors._kde.KernelDensity'>
Parameters,n_components,int,None,"Number of components. If None, all non-zero components are kept.",KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,kernel,{'linear','linear',Kernel used for PCA.,KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,gamma,float,None,"Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``.",KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,degree,float,3,Degree for poly kernels. Ignored by other kernels.,KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,coef0,float,1,Independent term in poly and sigmoid kernels. Ignored by other kernels.,KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,kernel_params,dict,None,Parameters (keyword arguments) and values for kernel passed as callable object. Ignored by other kernels.,KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,alpha,float,1.0,Hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True).,KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,fit_inverse_transform,bool,False,Learn the inverse transform for non-precomputed kernels (i.e. learn to find the pre-image of a point). This method is based on [2]_.,KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,eigen_solver,{'auto','auto',"Select eigensolver to use. If `n_components` is much less than the number of training samples, randomized (or arpack to a smaller extent) may be more efficient than the dense eigensolver. Randomized SVD is performed according to the method of Halko et al [3]_. auto : the solver is selected by a default policy based on n_samples (the number of training samples) and `n_components`: if the number of components to extract is less than 10 (strict) and the number of samples is more than 200 (strict), the 'arpack' method is enabled. Otherwise the exact full eigenvalue decomposition is computed and optionally truncated afterwards ('dense' method). dense : run exact full eigenvalue decomposition calling the standard LAPACK solver via `scipy.linalg.eigh`, and select the components by postprocessing arpack : run SVD truncated to n_components calling ARPACK solver using `scipy.sparse.linalg.eigsh`. It requires strictly 0 < n_components < n_samples randomized : run randomized SVD by the method of Halko et al. [3]_. The current implementation selects eigenvalues based on their module; therefore using this method can lead to unexpected results if the kernel is not positive semi-definite. See also [4]_. `'randomized'` was added.",KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,tol,float,0,"Convergence tolerance for arpack. If 0, optimal value will be chosen by arpack.",KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,max_iter,int,None,"Maximum number of iterations for arpack. If None, optimal value will be chosen by arpack.",KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,iterated_power,int >= 0,'auto',"Number of iterations for the power method computed by svd_solver == 'randomized'. When 'auto', it is set to 7 when `n_components < 0.1 * min(X.shape)`, other it is set to 4.",KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,remove_zero_eig,bool,False,"If True, then all components with zero eigenvalues are removed, so that the number of components in the output may be < n_components (and sometimes even zero due to numerical instability). When n_components is None, this parameter is ignored and components with zero eigenvalues are removed regardless.",KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,random_state,int,None,Used when ``eigen_solver`` == 'arpack' or 'randomized'. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,copy_X,bool,True,"If True, input X is copied and stored by the model in the `X_fit_` attribute. If no further changes will be done to X, setting `copy_X=False` saves memory by storing a reference.",KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Attributes,n_jobs,int,None,The number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Attributes,eigenvalues_,ndarray of shape (n_components,,"Eigenvalues of the centered kernel matrix in decreasing order. If `n_components` and `remove_zero_eig` are not set, then all values are stored.",KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Attributes,eigenvectors_,ndarray of shape (n_samples,,"Eigenvectors of the centered kernel matrix. If `n_components` and `remove_zero_eig` are not set, then all components are stored.",KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Attributes,dual_coef_,ndarray of shape (n_samples,,Inverse transform matrix. Only available when ``fit_inverse_transform`` is True.,KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Attributes,X_transformed_fit_,ndarray of shape (n_samples,,Projection of the fitted data on the kernel principal components. Only available when ``fit_inverse_transform`` is True.,KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Attributes,X_fit_,ndarray of shape (n_samples,,"The data used to fit the model. If `copy_X=False`, then `X_fit_` is a reference. This attribute is used for the calls to transform.",KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
,gamma_,float,,"Kernel coefficient for rbf, poly and sigmoid kernels. When `gamma` is explicitly provided, this is just the same as `gamma`. When `gamma` is `None`, this is the actual value of kernel coefficient.",KernelPCA,<class 'sklearn.decomposition._kernel_pca.KernelPCA'>
Parameters,alpha,float or array-like of shape (n_targets,1.0,"Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear models such as :class:`~sklearn.linear_model.LogisticRegression` or :class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number. See :ref:`ridge_regression` for formula.",KernelRidge,<class 'sklearn.kernel_ridge.KernelRidge'>
Parameters,kernel,str or callable,"""linear""","Kernel mapping used internally. This parameter is directly passed to :class:`~sklearn.metrics.pairwise.pairwise_kernels`. If `kernel` is a string, it must be one of the metrics in `pairwise.PAIRWISE_KERNEL_FUNCTIONS` or ""precomputed"". If `kernel` is ""precomputed"", X is assumed to be a kernel matrix. Alternatively, if `kernel` is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two rows from X as input and return the corresponding kernel value as a single number. This means that callables from :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on matrices, not single samples. Use the string identifying the kernel instead.",KernelRidge,<class 'sklearn.kernel_ridge.KernelRidge'>
Parameters,gamma,float,None,"Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels.",KernelRidge,<class 'sklearn.kernel_ridge.KernelRidge'>
Parameters,degree,float,3,Degree of the polynomial kernel. Ignored by other kernels.,KernelRidge,<class 'sklearn.kernel_ridge.KernelRidge'>
Parameters,coef0,float,1,Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.,KernelRidge,<class 'sklearn.kernel_ridge.KernelRidge'>
Attributes,kernel_params,dict,None,Additional parameters (keyword arguments) for kernel function passed as callable object.,KernelRidge,<class 'sklearn.kernel_ridge.KernelRidge'>
Attributes,dual_coef_,ndarray of shape (n_samples,,Representation of weight vector(s) in kernel space,KernelRidge,<class 'sklearn.kernel_ridge.KernelRidge'>
Attributes,X_fit_,{ndarray,,"Training data, which is also required for prediction. If kernel == ""precomputed"" this is instead the precomputed training matrix, of shape (n_samples, n_samples).",KernelRidge,<class 'sklearn.kernel_ridge.KernelRidge'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,KernelRidge,<class 'sklearn.kernel_ridge.KernelRidge'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,KernelRidge,<class 'sklearn.kernel_ridge.KernelRidge'>
Parameters,neg_label,int,0,Value with which negative labels must be encoded.,LabelBinarizer,<class 'sklearn.preprocessing._label.LabelBinarizer'>
Parameters,pos_label,int,1,Value with which positive labels must be encoded.,LabelBinarizer,<class 'sklearn.preprocessing._label.LabelBinarizer'>
Attributes,sparse_output,bool,False,True if the returned array from transform is desired to be in sparse CSR format.,LabelBinarizer,<class 'sklearn.preprocessing._label.LabelBinarizer'>
Attributes,classes_,ndarray of shape (n_classes,,Holds the label for each class.,LabelBinarizer,<class 'sklearn.preprocessing._label.LabelBinarizer'>
Attributes,y_type_,str,,"Represents the type of the target data as evaluated by :func:`~sklearn.utils.multiclass.type_of_target`. Possible type are 'continuous', 'continuous-multioutput', 'binary', 'multiclass', 'multiclass-multioutput', 'multilabel-indicator', and 'unknown'.",LabelBinarizer,<class 'sklearn.preprocessing._label.LabelBinarizer'>
,sparse_input_,bool,,"`True` if the input data to transform is given as a sparse matrix, `False` otherwise.",LabelBinarizer,<class 'sklearn.preprocessing._label.LabelBinarizer'>
,classes_,ndarray of shape (n_classes,,Holds the label for each class.,LabelEncoder,<class 'sklearn.preprocessing._label.LabelEncoder'>
Parameters,kernel,{'knn','rbf',"String identifier for kernel function to use or the kernel function itself. Only 'rbf' and 'knn' strings are valid inputs. The function passed should take two inputs, each of shape (n_samples, n_features), and return a (n_samples, n_samples) shaped weight matrix.",LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>
Parameters,gamma,float,20,Parameter for rbf kernel.,LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>
Parameters,n_neighbors,int,7,Parameter for knn kernel which need to be strictly positive.,LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>
Parameters,max_iter,int,1000,Change maximum number of iterations allowed.,LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>
Parameters,tol,float,1e-3,Convergence tolerance: threshold to consider the system at steady state.,LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>
Attributes,n_jobs,int,None,The number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>
Attributes,X_,{array-like,,Input array.,LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>
Attributes,classes_,ndarray of shape (n_classes,,The distinct labels used in classifying instances.,LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>
Attributes,label_distributions_,ndarray of shape (n_samples,,Categorical distribution for each item.,LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>
Attributes,transduction_,ndarray of shape (n_samples),,Label assigned to each item during :term:`fit`.,LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>
,n_iter_,int,,Number of iterations run.,LabelPropagation,<class 'sklearn.semi_supervised._label_propagation.LabelPropagation'>
Parameters,kernel,{'knn','rbf',"String identifier for kernel function to use or the kernel function itself. Only 'rbf' and 'knn' strings are valid inputs. The function passed should take two inputs, each of shape (n_samples, n_features), and return a (n_samples, n_samples) shaped weight matrix.",LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
Parameters,gamma,float,20,,LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
Parameters,n_neighbors,int,7,,LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
Parameters,alpha,float,0.2,,LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
Parameters,max_iter,int,30,,LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
Parameters,tol,float,1e-3,,LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
Attributes,n_jobs,int,None,The number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
Attributes,X_,ndarray of shape (n_samples,,Input array.,LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
Attributes,classes_,ndarray of shape (n_classes,,The distinct labels used in classifying instances.,LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
Attributes,label_distributions_,ndarray of shape (n_samples,,Categorical distribution for each item.,LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
Attributes,transduction_,ndarray of shape (n_samples,,Label assigned to each item during :term:`fit`.,LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
,n_iter_,int,,Number of iterations run.,LabelSpreading,<class 'sklearn.semi_supervised._label_propagation.LabelSpreading'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Parameters,verbose,bool or int,False,Sets the verbosity amount.,Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Parameters,precompute,bool,'auto',Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.,Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Parameters,n_nonzero_coefs,int,500,Target number of non-zero coefficients. Use ``np.inf`` for no limit.,Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Parameters,eps,float,np.finfo(float).eps,"The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.",Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Parameters,copy_X,bool,True,"If ``True``, X will be copied; else, it may be overwritten.",Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Parameters,fit_path,bool,True,"If True the full path is stored in the ``coef_path_`` attribute. If you compute the solution for a large problem or many targets, setting ``fit_path`` to ``False`` will lead to a speedup, especially with a small alpha.",Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Parameters,jitter,float,None,"Upper bound on a uniform noise parameter to be added to the `y` values, to satisfy the model's assumption of one-at-a-time computations. Might help with stability.",Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Attributes,random_state,int,None,Determines random number generation for jittering. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`. Ignored if `jitter` is None.,Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Attributes,alphas_,array-like of shape (n_alphas + 1,,"Maximum of covariances (in absolute value) at each iteration. ``n_alphas`` is either ``max_iter``, ``n_features`` or the number of nodes in the path with ``alpha >= alpha_min``, whichever is smaller. If this is a list of array-like, the length of the outer list is `n_targets`.",Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Attributes,active_,list of shape (n_alphas,,"Indices of active variables at the end of the path. If this is a list of list, the length of the outer list is `n_targets`.",Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Attributes,coef_path_,array-like of shape (n_features,,"The varying values of the coefficients along the path. It is not present if the ``fit_path`` parameter is ``False``. If this is a list of array-like, the length of the outer list is `n_targets`.",Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Attributes,coef_,array-like of shape (n_features,,Parameter vector (w in the formulation formula).,Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Attributes,intercept_,float or array-like of shape (n_targets,,Independent term in decision function.,Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Attributes,n_iter_,array-like or int,,The number of iterations taken by lars_path to find the grid of alphas for each target.,Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,Lars,<class 'sklearn.linear_model._least_angle.Lars'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,Lars,<class 'sklearn.linear_model._least_angle.Lars'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Parameters,verbose,bool or int,False,Sets the verbosity amount.,LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Parameters,max_iter,int,500,Maximum number of iterations to perform.,LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Parameters,precompute,bool,'auto',Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X.,LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ``cv`` default value if None changed from 3-fold to 5-fold.",LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Parameters,max_n_alphas,int,1000,The maximum number of points on the path used to compute the residuals in the cross-validation.,LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Parameters,n_jobs,int or None,None,Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Parameters,eps,float,np.finfo(float).eps,"The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.",LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Attributes,copy_X,bool,True,"If ``True``, X will be copied; else, it may be overwritten.",LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Attributes,active_,list of length n_alphas or list of such lists,,"Indices of active variables at the end of the path. If this is a list of lists, the outer list length is `n_targets`.",LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Attributes,coef_,array-like of shape (n_features,,parameter vector (w in the formulation formula),LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Attributes,intercept_,float,,independent term in decision function,LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Attributes,coef_path_,array-like of shape (n_features,,the varying values of the coefficients along the path,LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Attributes,alpha_,float,,the estimated regularization parameter alpha,LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Attributes,alphas_,array-like of shape (n_alphas,,the different values of alpha along the path,LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Attributes,cv_alphas_,array-like of shape (n_cv_alphas,,all the values of alpha along the path for the different folds,LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Attributes,mse_path_,array-like of shape (n_folds,,the mean square error on left-out for each fold along the path (alpha values given by ``cv_alphas``),LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Attributes,n_iter_,array-like or int,,the number of iterations run by Lars with the optimal alpha.,LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LarsCV,<class 'sklearn.linear_model._least_angle.LarsCV'>
Parameters,alpha,float,1.0,"Constant that multiplies the L1 term, controlling regularization strength. `alpha` must be a non-negative float i.e. in `[0, inf)`. When `alpha = 0`, the objective is equivalent to ordinary least squares, solved by the :class:`LinearRegression` object. For numerical reasons, using `alpha = 0` with the `Lasso` object is not advised. Instead, you should use the :class:`LinearRegression` object.",Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).",Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Parameters,precompute,bool or array-like of shape (n_features,False,Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix can also be passed as argument. For sparse input this option is always ``False`` to preserve sparsity.,Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Parameters,copy_X,bool,True,"If ``True``, X will be copied; else, it may be overwritten.",Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Parameters,max_iter,int,1000,The maximum number of iterations.,Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Parameters,tol,float,1e-4,"The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``, see Notes below.",Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Parameters,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Parameters,positive,bool,False,"When set to ``True``, forces the coefficients to be positive.",Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Parameters,random_state,int,None,The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Attributes,selection,{'cyclic','cyclic',"If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.",Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Attributes,coef_,ndarray of shape (n_features,,Parameter vector (w in the cost function formula).,Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Attributes,dual_gap_,float or ndarray of shape (n_targets,,"Given param alpha, the dual gaps at the end of the optimization, same shape as each observation of y.",Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Attributes,sparse_coef_,sparse matrix of shape (n_features,,Readonly property derived from ``coef_``.,Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Attributes,intercept_,float or ndarray of shape (n_targets,,Independent term in decision function.,Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Attributes,n_iter_,int or list of int,,Number of iterations run by the coordinate descent solver to reach the specified tolerance.,Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,Lasso,<class 'sklearn.linear_model._coordinate_descent.Lasso'>
Parameters,eps,float,1e-3,Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Parameters,n_alphas,int,100,Number of alphas along the regularization path.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Parameters,alphas,array-like,None,List of alphas where to compute the models. If ``None`` alphas are set automatically.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Parameters,precompute,'auto','auto',Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Parameters,max_iter,int,1000,The maximum number of iterations.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Parameters,tol,float,1e-4,"The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``.",LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Parameters,copy_X,bool,True,"If ``True``, X will be copied; else, it may be overwritten.",LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - int, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For int/None inputs, :class:`~sklearn.model_selection.KFold` is used. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ``cv`` default value if None changed from 3-fold to 5-fold.",LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Parameters,verbose,bool or int,False,Amount of verbosity.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Parameters,n_jobs,int,None,Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Parameters,positive,bool,False,"If positive, restrict regression coefficients to be positive.",LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Parameters,random_state,int,None,The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Attributes,selection,{'cyclic','cyclic',"If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.",LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Attributes,alpha_,float,,The amount of penalization chosen by cross validation.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Attributes,coef_,ndarray of shape (n_features,,Parameter vector (w in the cost function formula).,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Attributes,intercept_,float or ndarray of shape (n_targets,,Independent term in decision function.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Attributes,mse_path_,ndarray of shape (n_alphas,,"Mean square error for the test set on each fold, varying alpha.",LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Attributes,alphas_,ndarray of shape (n_alphas,,The grid of alphas used for fitting.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Attributes,dual_gap_,float or ndarray of shape (n_targets,,The dual gap at the end of the optimization for the optimal alpha (``alpha_``).,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Attributes,n_iter_,int,,Number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LassoCV,<class 'sklearn.linear_model._coordinate_descent.LassoCV'>
Parameters,alpha,float,1.0,"Constant that multiplies the penalty term. Defaults to 1.0. ``alpha = 0`` is equivalent to an ordinary least square, solved by :class:`LinearRegression`. For numerical reasons, using ``alpha = 0`` with the LassoLars object is not advised and you should prefer the LinearRegression object.",LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Parameters,verbose,bool or int,False,Sets the verbosity amount.,LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Parameters,precompute,bool,'auto',Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.,LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Parameters,max_iter,int,500,Maximum number of iterations to perform.,LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Parameters,eps,float,np.finfo(float).eps,"The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.",LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Parameters,copy_X,bool,True,"If True, X will be copied; else, it may be overwritten.",LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Parameters,fit_path,bool,True,"If ``True`` the full path is stored in the ``coef_path_`` attribute. If you compute the solution for a large problem or many targets, setting ``fit_path`` to ``False`` will lead to a speedup, especially with a small alpha.",LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Parameters,positive,bool,False,Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients will not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator.,LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Parameters,jitter,float,None,"Upper bound on a uniform noise parameter to be added to the `y` values, to satisfy the model's assumption of one-at-a-time computations. Might help with stability.",LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Attributes,random_state,int,None,Determines random number generation for jittering. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`. Ignored if `jitter` is None.,LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Attributes,alphas_,array-like of shape (n_alphas + 1,,"Maximum of covariances (in absolute value) at each iteration. ``n_alphas`` is either ``max_iter``, ``n_features`` or the number of nodes in the path with ``alpha >= alpha_min``, whichever is smaller. If this is a list of array-like, the length of the outer list is `n_targets`.",LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Attributes,active_,list of length n_alphas or list of such lists,,"Indices of active variables at the end of the path. If this is a list of list, the length of the outer list is `n_targets`.",LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Attributes,coef_path_,array-like of shape (n_features,,"If a list is passed it's expected to be one of n_targets such arrays. The varying values of the coefficients along the path. It is not present if the ``fit_path`` parameter is ``False``. If this is a list of array-like, the length of the outer list is `n_targets`.",LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Attributes,coef_,array-like of shape (n_features,,Parameter vector (w in the formulation formula).,LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Attributes,intercept_,float or array-like of shape (n_targets,,Independent term in decision function.,LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Attributes,n_iter_,array-like or int,,The number of iterations taken by lars_path to find the grid of alphas for each target.,LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LassoLars,<class 'sklearn.linear_model._least_angle.LassoLars'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Parameters,verbose,bool or int,False,Sets the verbosity amount.,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Parameters,max_iter,int,500,Maximum number of iterations to perform.,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Parameters,precompute,bool or 'auto','auto',Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X.,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ``cv`` default value if None changed from 3-fold to 5-fold.",LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Parameters,max_n_alphas,int,1000,The maximum number of points on the path used to compute the residuals in the cross-validation.,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Parameters,n_jobs,int or None,None,Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Parameters,eps,float,np.finfo(float).eps,"The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.",LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Parameters,copy_X,bool,True,"If True, X will be copied; else, it may be overwritten.",LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Attributes,positive,bool,False,Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients do not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator. As a consequence using LassoLarsCV only makes sense for problems where a sparse solution is expected and/or reached.,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Attributes,coef_,array-like of shape (n_features,,parameter vector (w in the formulation formula),LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Attributes,intercept_,float,,independent term in decision function.,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Attributes,coef_path_,array-like of shape (n_features,,the varying values of the coefficients along the path,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Attributes,alpha_,float,,the estimated regularization parameter alpha,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Attributes,alphas_,array-like of shape (n_alphas,,the different values of alpha along the path,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Attributes,cv_alphas_,array-like of shape (n_cv_alphas,,all the values of alpha along the path for the different folds,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Attributes,mse_path_,array-like of shape (n_folds,,the mean square error on left-out for each fold along the path (alpha values given by ``cv_alphas``),LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Attributes,n_iter_,array-like or int,,the number of iterations run by Lars with the optimal alpha.,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Attributes,active_,list of int,,Indices of active variables at the end of the path.,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LassoLarsCV,<class 'sklearn.linear_model._least_angle.LassoLarsCV'>
Parameters,criterion,{'aic','aic',The type of criterion to use.,LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Parameters,verbose,bool or int,False,Sets the verbosity amount.,LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Parameters,precompute,bool,'auto',Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.,LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Parameters,max_iter,int,500,Maximum number of iterations to perform. Can be used for early stopping.,LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Parameters,eps,float,np.finfo(float).eps,"The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.",LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Parameters,copy_X,bool,True,"If True, X will be copied; else, it may be overwritten.",LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Parameters,positive,bool,False,Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients do not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator. As a consequence using LassoLarsIC only makes sense for problems where a sparse solution is expected and/or reached.,LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Attributes,noise_variance,float,None,"The estimated noise variance of the data. If `None`, an unbiased estimate is computed by an OLS model. However, it is only possible in the case where `n_samples > n_features + fit_intercept`.",LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Attributes,coef_,array-like of shape (n_features,,parameter vector (w in the formulation formula),LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Attributes,intercept_,float,,independent term in decision function.,LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Attributes,alpha_,float,,the alpha parameter chosen by the information criterion,LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Attributes,alphas_,array-like of shape (n_alphas + 1,,"Maximum of covariances (in absolute value) at each iteration. ``n_alphas`` is either ``max_iter``, ``n_features`` or the number of nodes in the path with ``alpha >= alpha_min``, whichever is smaller. If a list, it will be of length `n_targets`.",LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Attributes,n_iter_,int,,number of iterations run by lars_path to find the grid of alphas.,LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Attributes,criterion_,array-like of shape (n_alphas,,"The value of the information criteria ('aic', 'bic') across all alphas. The alpha which has the smallest information criterion is chosen, as specified in [1]_.",LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Attributes,noise_variance_,float,,The estimated noise variance from the data used to compute the criterion.,LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LassoLarsIC,<class 'sklearn.linear_model._least_angle.LassoLarsIC'>
Parameters,n_components,int,10,Number of topics. ``n_topics`` was renamed to ``n_components``,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,doc_topic_prior,float,None,"Prior of document topic distribution `theta`. If the value is None, defaults to `1 / n_components`. In [1]_, this is called `alpha`.",LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,topic_word_prior,float,None,"Prior of topic word distribution `beta`. If the value is None, defaults to `1 / n_components`. In [1]_, this is called `eta`.",LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,learning_method,{'batch','batch',"Method used to update `_component`. Only used in :meth:`fit` method. In general, if the data size is large, the online update will be much faster than the batch update. Valid options: - 'batch': Batch variational Bayes method. Use all training data in each EM update. Old `components_` will be overwritten in each iteration. - 'online': Online variational Bayes method. In each EM update, use mini-batch of training data to update the ``components_`` variable incrementally. The learning rate is controlled by the ``learning_decay`` and the ``learning_offset`` parameters. The default learning method is now ``""batch""``.",LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,learning_decay,float,0.7,"It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value is 0.0 and batch_size is ``n_samples``, the update method is same as batch learning. In the literature, this is called kappa.",LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,learning_offset,float,10.0,"A (positive) parameter that downweights early iterations in online learning.  It should be greater than 1.0. In the literature, this is called tau_0.",LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,max_iter,int,10,"The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the :meth:`fit` method, and not the :meth:`partial_fit` method.",LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,batch_size,int,128,Number of documents to use in each EM iteration. Only used in online learning.,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,evaluate_every,int,-1,"How often to evaluate perplexity. Only used in `fit` method. set it to 0 or negative number to not evaluate perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold.",LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,total_samples,int,1e6,Total number of documents. Only used in the :meth:`partial_fit` method.,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,perp_tol,float,1e-1,Perplexity tolerance. Only used when ``evaluate_every`` is greater than 0.,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,mean_change_tol,float,1e-3,Stopping tolerance for updating document topic distribution in E-step.,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,max_doc_update_iter,int,100,Max number of iterations for updating document topic distribution in the E-step.,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,n_jobs,int,None,The number of jobs to use in the E-step. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,verbose,int,0,Verbosity level.,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Attributes,random_state,int,None,Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Attributes,components_,ndarray of shape (n_components,,"Variational parameters for topic word distribution. Since the complete conditional for topic word distribution is a Dirichlet, ``components_[i, j]`` can be viewed as pseudocount that represents the number of times word `j` was assigned to topic `i`. It can also be viewed as distribution over the words for each topic after normalization: ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``.",LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Attributes,exp_dirichlet_component_,ndarray of shape (n_components,,"Exponential value of expectation of log topic word distribution. In the literature, this is `exp(E[log(beta)])`.",LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Attributes,n_batch_iter_,int,,Number of iterations of the EM step.,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Attributes,n_iter_,int,,Number of passes over the dataset.,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Attributes,bound_,float,,Final perplexity score on training set.,LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Attributes,doc_topic_prior_,float,,"Prior of document topic distribution `theta`. If the value is None, it is `1 / n_components`.",LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Attributes,random_state_,RandomState instance,,"RandomState instance that is generated either from a seed, the random number generator or by `np.random`.",LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
,topic_word_prior_,float,,"Prior of topic word distribution `beta`. If the value is None, it is `1 / n_components`.",LatentDirichletAllocation,<class 'sklearn.decomposition._lda.LatentDirichletAllocation'>
Parameters,store_precision,bool,True,Specify if the estimated precision is stored.,LedoitWolf,<class 'sklearn.covariance._shrunk_covariance.LedoitWolf'>
Parameters,assume_centered,bool,False,"If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data will be centered before computation.",LedoitWolf,<class 'sklearn.covariance._shrunk_covariance.LedoitWolf'>
Attributes,block_size,int,1000,Size of blocks into which the covariance matrix will be split during its Ledoit-Wolf estimation. This is purely a memory optimization and does not affect results.,LedoitWolf,<class 'sklearn.covariance._shrunk_covariance.LedoitWolf'>
Attributes,covariance_,ndarray of shape (n_features,,Estimated covariance matrix.,LedoitWolf,<class 'sklearn.covariance._shrunk_covariance.LedoitWolf'>
Attributes,location_,ndarray of shape (n_features,,"Estimated location, i.e. the estimated mean.",LedoitWolf,<class 'sklearn.covariance._shrunk_covariance.LedoitWolf'>
Attributes,precision_,ndarray of shape (n_features,,Estimated pseudo inverse matrix. (stored only if store_precision is True),LedoitWolf,<class 'sklearn.covariance._shrunk_covariance.LedoitWolf'>
Attributes,shrinkage_,float,,"Coefficient in the convex combination used for the computation of the shrunk estimate. Range is [0, 1].",LedoitWolf,<class 'sklearn.covariance._shrunk_covariance.LedoitWolf'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LedoitWolf,<class 'sklearn.covariance._shrunk_covariance.LedoitWolf'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LedoitWolf,<class 'sklearn.covariance._shrunk_covariance.LedoitWolf'>
Parameters,solver,{'svd','svd',"Solver to use, possible values: - 'svd': Singular value decomposition (default). Does not compute the covariance matrix, therefore this solver is recommended for data with a large number of features. - 'lsqr': Least squares solution. Can be combined with shrinkage or custom covariance estimator. - 'eigen': Eigenvalue decomposition. Can be combined with shrinkage or custom covariance estimator. `solver=""svd""` now has experimental Array API support. See the :ref:`Array API User Guide <array_api>` for more details.",LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Parameters,shrinkage,'auto' or float,None,"Shrinkage parameter, possible values: - None: no shrinkage (default). - 'auto': automatic shrinkage using the Ledoit-Wolf lemma. - float between 0 and 1: fixed shrinkage parameter. This should be left to None if `covariance_estimator` is used. Note that shrinkage works only with 'lsqr' and 'eigen' solvers. For a usage example, see :ref:`sphx_glr_auto_examples_classification_plot_lda.py`.",LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Parameters,priors,array-like of shape (n_classes,None,"The class prior probabilities. By default, the class proportions are inferred from the training data.",LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Parameters,n_components,int,None,"Number of components (<= min(n_classes - 1, n_features)) for dimensionality reduction. If None, will be set to min(n_classes - 1, n_features). This parameter only affects the `transform` method. For a usage example, see :ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`.",LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Parameters,store_covariance,bool,False,"If True, explicitly compute the weighted within-class covariance matrix when solver is 'svd'. The matrix is always computed and stored for the other solvers.",LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Parameters,tol,float,1.0e-4,"Absolute threshold for a singular value of X to be considered significant, used to estimate the rank of X. Dimensions whose singular values are non-significant are discarded. Only used if solver is 'svd'.",LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Attributes,covariance_estimator,covariance estimator,None,"If not None, `covariance_estimator` is used to estimate the covariance matrices instead of relying on the empirical covariance estimator (with potential shrinkage). The object should have a fit method and a ``covariance_`` attribute like the estimators in :mod:`sklearn.covariance`. if None the shrinkage parameter drives the estimate. This should be left to None if `shrinkage` is used. Note that `covariance_estimator` works only with 'lsqr' and 'eigen' solvers.",LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Attributes,coef_,ndarray of shape (n_features,,Weight vector(s).,LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Attributes,intercept_,ndarray of shape (n_classes,,Intercept term.,LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Attributes,covariance_,array-like of shape (n_features,,"Weighted within-class covariance matrix. It corresponds to `sum_k prior_k * C_k` where `C_k` is the covariance matrix of the samples in class `k`. The `C_k` are estimated using the (potentially shrunk) biased estimator of covariance. If solver is 'svd', only exists when `store_covariance` is True.",LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Attributes,explained_variance_ratio_,ndarray of shape (n_components,,Percentage of variance explained by each of the selected components. If ``n_components`` is not set then all components are stored and the sum of explained variances is equal to 1.0. Only available when eigen or svd solver is used.,LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Attributes,means_,array-like of shape (n_classes,,Class-wise means.,LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Attributes,priors_,array-like of shape (n_classes,,Class priors (sum to 1).,LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Attributes,scalings_,array-like of shape (rank,,Scaling of the features in the space spanned by the class centroids. Only available for 'svd' and 'eigen' solvers.,LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Attributes,xbar_,array-like of shape (n_features,,Overall mean. Only present if solver is 'svd'.,LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Attributes,classes_,array-like of shape (n_classes,,Unique class labels.,LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LinearDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).",LinearRegression,<class 'sklearn.linear_model._base.LinearRegression'>
Parameters,copy_X,bool,True,"If True, X will be copied; else, it may be overwritten.",LinearRegression,<class 'sklearn.linear_model._base.LinearRegression'>
Parameters,n_jobs,int,None,"The number of jobs to use for the computation. This will only provide speedup in case of sufficiently large problems, that is if firstly `n_targets > 1` and secondly `X` is sparse or if `positive` is set to `True`. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",LinearRegression,<class 'sklearn.linear_model._base.LinearRegression'>
Attributes,positive,bool,False,"When set to ``True``, forces the coefficients to be positive. This option is only supported for dense arrays.",LinearRegression,<class 'sklearn.linear_model._base.LinearRegression'>
Attributes,coef_,array of shape (n_features,,"Estimated coefficients for the linear regression problem. If multiple targets are passed during the fit (y 2D), this is a 2D array of shape (n_targets, n_features), while if only one target is passed, this is a 1D array of length n_features.",LinearRegression,<class 'sklearn.linear_model._base.LinearRegression'>
Attributes,rank_,int,,Rank of matrix `X`. Only available when `X` is dense.,LinearRegression,<class 'sklearn.linear_model._base.LinearRegression'>
Attributes,singular_,array of shape (min(X,,Singular values of `X`. Only available when `X` is dense.,LinearRegression,<class 'sklearn.linear_model._base.LinearRegression'>
Attributes,intercept_,float or array of shape (n_targets,,Independent term in the linear model. Set to 0.0 if `fit_intercept = False`.,LinearRegression,<class 'sklearn.linear_model._base.LinearRegression'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LinearRegression,<class 'sklearn.linear_model._base.LinearRegression'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LinearRegression,<class 'sklearn.linear_model._base.LinearRegression'>
Parameters,penalty,{'l1','l2',Specifies the norm used in the penalization. The 'l2' penalty is the standard used in SVC. The 'l1' leads to ``coef_`` vectors that are sparse.,LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Parameters,loss,{'hinge','squared_hinge',Specifies the loss function. 'hinge' is the standard SVM loss (used e.g. by the SVC class) while 'squared_hinge' is the square of the hinge loss. The combination of ``penalty='l1'`` and ``loss='hinge'`` is not supported.,LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Parameters,dual,"""auto"" or bool","""auto""","Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features. `dual=""auto""` will choose the value of the parameter automatically, based on the values of `n_samples`, `n_features`, `loss`, `multi_class` and `penalty`. If `n_samples` < `n_features` and optimizer supports chosen `loss`, `multi_class` and `penalty`, then dual will be set to True, otherwise it will be set to False. The `""auto""` option is added in version 1.3 and will be the default in version 1.5.",LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Parameters,tol,float,1e-4,Tolerance for stopping criteria.,LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Parameters,C,float,1.0,"Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. For an intuitive visualization of the effects of scaling the regularization parameter C, see :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.",LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Parameters,multi_class,{'ovr','ovr',"Determines the multi-class strategy if `y` contains more than two classes. ``""ovr""`` trains n_classes one-vs-rest classifiers, while ``""crammer_singer""`` optimizes a joint objective over all classes. While `crammer_singer` is interesting from a theoretical perspective as it is consistent, it is seldom used in practice as it rarely leads to better accuracy and is more expensive to compute. If ``""crammer_singer""`` is chosen, the options loss, penalty and dual will be ignored.",LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Parameters,fit_intercept,bool,True,"Whether or not to fit an intercept. If set to True, the feature vector is extended to include an intercept term: `[x_1, ..., x_n, 1]`, where 1 corresponds to the intercept. If set to False, no intercept will be used in calculations (i.e. data is expected to be already centered).",LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Parameters,intercept_scaling,float,1.0,"When `fit_intercept` is True, the instance vector x becomes ``[x_1, constant value equal to `intercept_scaling` is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight. Note that liblinear internally penalizes the intercept, treating it like any other term in the feature vector. To reduce the impact of the regularization on the intercept, the `intercept_scaling` parameter can be set to a value greater than 1; the higher the value of `intercept_scaling`, the lower the impact of regularization on it. Then, the weights become `[w_x_1, ..., w_x_n, w_intercept*intercept_scaling]`, where `w_x_1, ..., w_x_n` represent the feature weights and the intercept weight is scaled by `intercept_scaling`. This scaling allows the intercept term to have a different regularization behavior compared to the other features.",LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Parameters,class_weight,dict or 'balanced',None,"Set the parameter C of class i to ``class_weight[i]*C`` for SVC. If not given, all classes are supposed to have weight one. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.",LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Parameters,verbose,int,0,"Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context.",LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Parameters,random_state,int,None,Controls the pseudo random number generation for shuffling the data for the dual coordinate descent (if ``dual=True``). When ``dual=False`` the underlying implementation of :class:`LinearSVC` is not random and ``random_state`` has no effect on the results. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Attributes,max_iter,int,1000,The maximum number of iterations to be run.,LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Attributes,coef_,ndarray of shape (1,,Weights assigned to the features (coefficients in the primal problem). ``coef_`` is a readonly property derived from ``raw_coef_`` that follows the internal memory layout of liblinear.,LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Attributes,intercept_,ndarray of shape (1,,Constants in decision function.,LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Attributes,classes_,ndarray of shape (n_classes,,The unique classes labels.,LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
,n_iter_,int,,Maximum number of iterations run across all classes.,LinearSVC,<class 'sklearn.svm._classes.LinearSVC'>
Parameters,epsilon,float,0.0,"Epsilon parameter in the epsilon-insensitive loss function. Note that the value of this parameter depends on the scale of the target variable y. If unsure, set ``epsilon=0``.",LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Parameters,tol,float,1e-4,Tolerance for stopping criteria.,LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Parameters,C,float,1.0,Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive.,LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Parameters,loss,{'epsilon_insensitive','epsilon_insensitive',"Specifies the loss function. The epsilon-insensitive loss (standard SVR) is the L1 loss, while the squared epsilon-insensitive loss ('squared_epsilon_insensitive') is the L2 loss.",LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Parameters,fit_intercept,bool,True,"Whether or not to fit an intercept. If set to True, the feature vector is extended to include an intercept term: `[x_1, ..., x_n, 1]`, where 1 corresponds to the intercept. If set to False, no intercept will be used in calculations (i.e. data is expected to be already centered).",LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Parameters,intercept_scaling,float,1.0,"When `fit_intercept` is True, the instance vector x becomes `[x_1, ..., x_n, intercept_scaling]`, i.e. a ""synthetic"" feature with a constant value equal to `intercept_scaling` is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight. Note that liblinear internally penalizes the intercept, treating it like any other term in the feature vector. To reduce the impact of the regularization on the intercept, the `intercept_scaling` parameter can be set to a value greater than 1; the higher the value of `intercept_scaling`, the lower the impact of regularization on it. Then, the weights become `[w_x_1, ..., w_x_n, w_intercept*intercept_scaling]`, where `w_x_1, ..., w_x_n` represent the feature weights and the intercept weight is scaled by `intercept_scaling`. This scaling allows the intercept term to have a different regularization behavior compared to the other features.",LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Parameters,dual,"""auto"" or bool","""auto""","Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features. `dual=""auto""` will choose the value of the parameter automatically, based on the values of `n_samples`, `n_features` and `loss`. If `n_samples` < `n_features` and optimizer supports chosen `loss`, then dual will be set to True, otherwise it will be set to False. The `""auto""` option is added in version 1.3 and will be the default in version 1.5.",LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Parameters,verbose,int,0,"Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context.",LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Parameters,random_state,int,None,Controls the pseudo random number generation for shuffling the data. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Attributes,max_iter,int,1000,The maximum number of iterations to be run.,LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Attributes,coef_,ndarray of shape (n_features) if n_classes == 2             else (n_classes,,Weights assigned to the features (coefficients in the primal problem). `coef_` is a readonly property derived from `raw_coef_` that follows the internal memory layout of liblinear.,LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Attributes,intercept_,ndarray of shape (1) if n_classes == 2 else (n_classes),,Constants in decision function.,LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
,n_iter_,int,,Maximum number of iterations run across all classes.,LinearSVR,<class 'sklearn.svm._classes.LinearSVR'>
Parameters,n_neighbors,int,20,"Number of neighbors to use by default for :meth:`kneighbors` queries. If n_neighbors is larger than the number of samples provided, all samples will be used.",LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Parameters,algorithm,{'auto','auto',"Algorithm used to compute the nearest neighbors: - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDTree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method. Note: fitting on sparse input will override the setting of this parameter, using brute force.",LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Parameters,leaf_size,int,30,"Leaf is size passed to :class:`BallTree` or :class:`KDTree`. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.",LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Parameters,metric,str or callable,'minkowski',"Metric to use for distance computation. Default is ""minkowski"", which results in the standard Euclidean distance when p = 2. See the documentation of `scipy.spatial.distance <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric values. If metric is ""precomputed"", X is assumed to be a distance matrix and must be square during fit. X may be a :term:`sparse graph`, in which case only ""nonzero"" elements may be considered neighbors. If metric is a callable function, it takes two arrays representing 1D vectors as inputs and must return one value indicating the distance between those vectors. This works for Scipy's metrics, but is less efficient than passing the metric name as a string.",LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Parameters,p,float,2,"Parameter for the Minkowski metric from :func:`sklearn.metrics.pairwise_distances`. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Parameters,metric_params,dict,None,Additional keyword arguments for the metric function.,LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Parameters,contamination,'auto' or float,'auto',"The amount of contamination of the data set, i.e. the proportion of outliers in the data set. When fitting this is used to define the threshold on the scores of the samples. - if 'auto', the threshold is determined as in the original paper, - if a float, the contamination should be in the range (0, 0.5]. The default value of ``contamination`` changed from 0.1 to ``'auto'``.",LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Parameters,novelty,bool,False,"By default, LocalOutlierFactor is only meant to be used for outlier detection (novelty=False). Set novelty to True if you want to use LocalOutlierFactor for novelty detection. In this case be aware that you should only use predict, decision_function and score_samples on new unseen data and not on the training set; and note that the results obtained this way may differ from the standard LOF results.",LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Attributes,n_jobs,int,None,The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Attributes,negative_outlier_factor_,ndarray of shape (n_samples,,"The opposite LOF of the training samples. The higher, the more normal. Inliers tend to have a LOF score close to 1 (``negative_outlier_factor_`` close to -1), while outliers tend to have a larger LOF score. The local outlier factor (LOF) of a sample captures its supposed 'degree of abnormality'. It is the average of the ratio of the local reachability density of a sample and those of its k-nearest neighbors.",LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Attributes,n_neighbors_,int,,The actual number of neighbors used for :meth:`kneighbors` queries.,LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Attributes,offset_,float,,"Offset used to obtain binary labels from the raw scores. Observations having a negative_outlier_factor smaller than `offset_` are detected as abnormal. The offset is set to -1.5 (inliers score around -1), except when a contamination parameter different than ""auto"" is provided. In that case, the offset is defined in such a way we obtain the expected number of outliers in training.",LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Attributes,effective_metric_,str,,The effective metric used for the distance computation.,LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Attributes,effective_metric_params_,dict,,The effective additional keyword arguments for the metric function.,LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
,n_samples_fit_,int,,It is the number of samples in the fitted data.,LocalOutlierFactor,<class 'sklearn.neighbors._lof.LocalOutlierFactor'>
Parameters,n_neighbors,int,5,Number of neighbors to consider for each point.,LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Parameters,n_components,int,2,Number of coordinates for the manifold.,LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Parameters,reg,float,1e-3,"Regularization constant, multiplies the trace of the local covariance matrix of the distances.",LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Parameters,eigen_solver,{'auto','auto',"The solver used to compute the eigenvectors. The available options are: - `'auto'` : algorithm will attempt to choose the best method for input data. - `'arpack'` : use arnoldi iteration in shift-invert mode. For this method, M may be a dense matrix, sparse matrix, or general linear operator. - `'dense'`  : use standard dense matrix operations for the eigenvalue decomposition. For this method, M must be an array or matrix type. This method should be avoided for large problems. ARPACK can be unstable for some problems.  It is best to try several random seeds in order to check results.",LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Parameters,tol,float,1e-6,Tolerance for 'arpack' method Not used if eigen_solver=='dense'.,LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Parameters,max_iter,int,100,Maximum number of iterations for the arpack solver. Not used if eigen_solver=='dense'.,LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Parameters,method,{'standard','standard',- `standard`: use the standard locally linear embedding algorithm. see reference [1]_ - `hessian`: use the Hessian eigenmap method. This method requires ``n_neighbors > n_components * (1 + (n_components + 1) / 2``. see reference [2]_ - `modified`: use the modified locally linear embedding algorithm. see reference [3]_ - `ltsa`: use local tangent space alignment algorithm. see reference [4]_,LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Parameters,hessian_tol,float,1e-4,Tolerance for Hessian eigenmapping method. Only used if ``method == 'hessian'``.,LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Parameters,modified_tol,float,1e-12,Tolerance for modified LLE method. Only used if ``method == 'modified'``.,LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Parameters,neighbors_algorithm,{'auto','auto',"Algorithm to use for nearest neighbors search, passed to :class:`~sklearn.neighbors.NearestNeighbors` instance.",LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Parameters,random_state,int,None,Determines the random number generator when ``eigen_solver`` == 'arpack'. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Attributes,n_jobs,int or None,None,The number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Attributes,embedding_,array-like,,Stores the embedding vectors,LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Attributes,reconstruction_error_,float,,Reconstruction error associated with `embedding_`,LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
,nbrs_,NearestNeighbors object,,"Stores nearest neighbors instance, including BallTree or KDtree if applicable.",LocallyLinearEmbedding,<class 'sklearn.manifold._locally_linear.LocallyLinearEmbedding'>
Parameters,penalty,{'l1','l2',"Specify the norm of the penalty: - `None`: no penalty is added; - `'l2'`: add a L2 penalty term and it is the default choice; - `'l1'`: add a L1 penalty term; - `'elasticnet'`: both L1 and L2 penalty terms are added. Some penalties may not work with some solvers. See the parameter `solver` below, to know the compatibility between the penalty and solver. l1 penalty with SAGA solver (allowing 'multinomial' + L1)",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,dual,bool,False,"Dual (constrained) or primal (regularized, see also :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,tol,float,1e-4,Tolerance for stopping criteria.,LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,C,float,1.0,"Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,fit_intercept,bool,True,Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.,LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,intercept_scaling,float,1,"Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a ""synthetic"" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes ``intercept_scaling * synthetic_feature_weight``. Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,class_weight,dict or 'balanced',None,"Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. *class_weight='balanced'*",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,random_state,int,None,"Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the data. See :term:`Glossary <random_state>` for details.",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,solver,{'lbfgs','lbfgs',"Algorithm to use in the optimization problem. Default is 'lbfgs'. To choose a solver, you might want to consider the following aspects: - For small datasets, 'liblinear' is a good choice, whereas 'sag' and 'saga' are faster for large ones; - For :term:`multiclass` problems, all solvers except 'liblinear' minimize the full multinomial loss; - 'liblinear' can only handle binary classification by default. To apply a one-versus-rest scheme for the multiclass setting one can wrap it with the :class:`~sklearn.multiclass.OneVsRestClassifier`. - 'newton-cholesky' is a good choice for `n_samples` >> `n_features * n_classes`, especially with one-hot encoded categorical features with rare categories. Be aware that the memory usage of this solver has a quadratic dependency on `n_features * n_classes` because it explicitly computes the full Hessian matrix. The choice of the algorithm depends on the penalty chosen and on (multinomial) multiclass support: ================= ============================== ====================== solver            penalty                        multinomial multiclass ================= ============================== ====================== 'lbfgs'           'l2', None                     yes 'liblinear'       'l1', 'l2'                     no 'newton-cg'       'l2', None                     yes 'newton-cholesky' 'l2', None                     no 'sag'             'l2', None                     yes 'saga'            'elasticnet', 'l1', 'l2', None yes ================= ============================== ====================== 'sag' and 'saga' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from :mod:`sklearn.preprocessing`. Refer to the :ref:`User Guide <Logistic_regression>` for more information regarding :class:`LogisticRegression` and more specifically the :ref:`Table <logistic_regression_solvers>` summarizing solver/penalty supports. Stochastic Average Gradient descent solver. SAGA solver. The default solver changed from 'liblinear' to 'lbfgs' in 0.22. newton-cholesky solver.",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,max_iter,int,100,Maximum number of iterations taken for the solvers to converge.,LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,multi_class,{'auto','auto',"If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, *even when the data is binary*. 'multinomial' is unavailable when solver='liblinear'. 'auto' selects 'ovr' if the data is binary, or if solver='liblinear', and otherwise selects 'multinomial'. Stochastic Average Gradient descent solver for 'multinomial' case. Default changed from 'ovr' to 'auto' in 0.22. ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7. From then on, the recommended 'multinomial' will always be used for `n_classes >= 3`. Solvers that do not support 'multinomial' will raise an error. Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you still want to use OvR.",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,verbose,int,0,For the liblinear and lbfgs solvers set verbose to any positive number for verbosity.,LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver. See :term:`the Glossary <warm_start>`. *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,n_jobs,int,None,"Number of CPU cores used when parallelizing over classes if multi_class='ovr'"". This parameter is ignored when the ``solver`` is set to 'liblinear' regardless of whether 'multi_class' is specified or not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Attributes,l1_ratio,float,None,"The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination of L1 and L2.",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Attributes,classes_,ndarray of shape (n_classes,,A list of class labels known to the classifier.,LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Attributes,coef_,ndarray of shape (1,,"Coefficient of the features in the decision function. `coef_` is of shape (1, n_features) when the given problem is binary. In particular, when `multi_class='multinomial'`, `coef_` corresponds to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Attributes,intercept_,ndarray of shape (1,,"Intercept (a.k.a. bias) added to the decision function. If `fit_intercept` is set to False, the intercept is set to zero. `intercept_` is of shape (1,) when the given problem is binary. In particular, when `multi_class='multinomial'`, `intercept_` corresponds to outcome 1 (True) and `-intercept_` corresponds to outcome 0 (False).",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
,n_iter_,ndarray of shape (n_classes,,"Actual number of iterations for all classes. If binary or multinomial, it returns only 1 element. For liblinear solver, only the maximum number of iteration across all classes is given. In SciPy <= 1.0.0 the number of lbfgs iterations may exceed ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.",LogisticRegression,<class 'sklearn.linear_model._logistic.LogisticRegression'>
Parameters,Cs,int or list of floats,10,"Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. Like in support vector machines, smaller values specify stronger regularization.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,fit_intercept,bool,True,Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function.,LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,cv,int or cross-validation generator,None,"The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module :mod:`sklearn.model_selection` module for the list of possible cross-validation objects. ``cv`` default value if None changed from 3-fold to 5-fold.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,dual,bool,False,"Dual (constrained) or primal (regularized, see also :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,penalty,{'l1','l2',"Specify the norm of the penalty: - `'l2'`: add a L2 penalty term (used by default); - `'l1'`: add a L1 penalty term; - `'elasticnet'`: both L1 and L2 penalty terms are added. Some penalties may not work with some solvers. See the parameter `solver` below, to know the compatibility between the penalty and solver.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,scoring,str or callable,None,"A string (see :ref:`scoring_parameter`) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. For a list of scoring functions that can be used, look at :mod:`sklearn.metrics`. The default scoring option used is 'accuracy'.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,solver,{'lbfgs','lbfgs',"Algorithm to use in the optimization problem. Default is 'lbfgs'. To choose a solver, you might want to consider the following aspects: - For small datasets, 'liblinear' is a good choice, whereas 'sag' and 'saga' are faster for large ones; - For multiclass problems, all solvers except 'liblinear' minimize the full multinomial loss; - 'liblinear' might be slower in :class:`LogisticRegressionCV` because it does not handle warm-starting. - 'liblinear' can only handle binary classification by default. To apply a one-versus-rest scheme for the multiclass setting one can wrap it with the :class:`~sklearn.multiclass.OneVsRestClassifier`. - 'newton-cholesky' is a good choice for `n_samples` >> `n_features * n_classes`, especially with one-hot encoded categorical features with rare categories. Be aware that the memory usage of this solver has a quadratic dependency on `n_features * n_classes` because it explicitly computes the full Hessian matrix. The choice of the algorithm depends on the penalty chosen and on (multinomial) multiclass support: ================= ============================== ====================== solver            penalty                        multinomial multiclass ================= ============================== ====================== 'lbfgs'           'l2'                           yes 'liblinear'       'l1', 'l2'                     no 'newton-cg'       'l2'                           yes 'newton-cholesky' 'l2',                          no 'sag'             'l2',                          yes 'saga'            'elasticnet', 'l1', 'l2'       yes ================= ============================== ====================== 'sag' and 'saga' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from :mod:`sklearn.preprocessing`. Stochastic Average Gradient descent solver. SAGA solver. newton-cholesky solver.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,tol,float,1e-4,Tolerance for stopping criteria.,LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,max_iter,int,100,Maximum number of iterations of the optimization algorithm.,LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,class_weight,dict or 'balanced',None,"Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. class_weight == 'balanced'",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,n_jobs,int,None,Number of CPU cores used during the cross-validation loop. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,verbose,int,0,"For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any positive number for verbosity.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,refit,bool,True,"If set to True, the scores are averaged across all folds, and the coefs and the C that corresponds to the best score is taken, and a final refit is done using these parameters. Otherwise the coefs, intercepts and C that correspond to the best scores across folds are averaged.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,intercept_scaling,float,1,"Useful only when the solver 'liblinear' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a ""synthetic"" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes ``intercept_scaling * synthetic_feature_weight``. Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,multi_class,{'auto,'auto',"If the option chosen is 'ovr', then a binary problem is fit for each label. For 'multinomial' the loss minimised is the multinomial loss fit across the entire probability distribution, *even when the data is binary*. 'multinomial' is unavailable when solver='liblinear'. 'auto' selects 'ovr' if the data is binary, or if solver='liblinear', and otherwise selects 'multinomial'. Stochastic Average Gradient descent solver for 'multinomial' case. Default changed from 'ovr' to 'auto' in 0.22. ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7. From then on, the recommended 'multinomial' will always be used for `n_classes >= 3`. Solvers that do not support 'multinomial' will raise an error. Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegressionCV())` if you still want to use OvR.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,random_state,int,None,"Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data. Note that this only applies to the solver and not the cross-validation generator. See :term:`Glossary <random_state>` for details.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Attributes,l1_ratios,list of float,None,"The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to using ``penalty='l2'``, while 1 is equivalent to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination of L1 and L2.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Attributes,classes_,ndarray of shape (n_classes,,A list of class labels known to the classifier.,LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Attributes,coef_,ndarray of shape (1,,"Coefficient of the features in the decision function. `coef_` is of shape (1, n_features) when the given problem is binary.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Attributes,intercept_,ndarray of shape (1,,"Intercept (a.k.a. bias) added to the decision function. If `fit_intercept` is set to False, the intercept is set to zero. `intercept_` is of shape(1,) when the problem is binary.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Attributes,Cs_,ndarray of shape (n_cs),,Array of C i.e. inverse of regularization parameter values used for cross-validation.,LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Attributes,l1_ratios_,ndarray of shape (n_l1_ratios),,"Array of l1_ratios used for cross-validation. If no l1_ratio is used (i.e. penalty is not 'elasticnet'), this is set to ``[None]``",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Attributes,coefs_paths_,ndarray of shape (n_folds,,"dict with classes as the keys, and the path of coefficients obtained during cross-validating across each fold and then across each Cs after doing an OvR for the corresponding class as values. If the 'multi_class' option is set to 'multinomial', then the coefs_paths are the coefficients corresponding to each class. Each dict value has shape ``(n_folds, n_cs, n_features)`` or ``(n_folds, n_cs, n_features + 1)`` depending on whether the intercept is fit or not. If ``penalty='elasticnet'``, the shape is ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Attributes,scores_,dict,,"dict with classes as the keys, and the values as the grid of scores obtained during cross-validating each fold, after doing an OvR for the corresponding class. If the 'multi_class' option given is 'multinomial' then the same scores are repeated across all classes, since this is the multinomial class. Each dict value has shape ``(n_folds, n_cs)`` or ``(n_folds, n_cs, n_l1_ratios)`` if ``penalty='elasticnet'``.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Attributes,C_,ndarray of shape (n_classes,,"Array of C that maps to the best scores across every class. If refit is set to False, then for each class, the best C is the average of the C's that correspond to the best scores for each fold. `C_` is of shape(n_classes,) when the problem is binary.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Attributes,l1_ratio_,ndarray of shape (n_classes,,"Array of l1_ratio that maps to the best scores across every class. If refit is set to False, then for each class, the best l1_ratio is the average of the l1_ratio's that correspond to the best scores for each fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Attributes,n_iter_,ndarray of shape (n_classes,,"Actual number of iterations for all classes, folds and Cs. In the binary or multinomial cases, the first dimension is equal to 1. If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds, n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.",LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,LogisticRegressionCV,<class 'sklearn.linear_model._logistic.LogisticRegressionCV'>
Parameters,n_components,int,2,Number of dimensions in which to immerse the dissimilarities.,MDS,<class 'sklearn.manifold._mds.MDS'>
Parameters,metric,bool,True,"If ``True``, perform metric MDS; otherwise, perform nonmetric MDS. When ``False`` (i.e. non-metric MDS), dissimilarities with 0 are considered as missing values.",MDS,<class 'sklearn.manifold._mds.MDS'>
Parameters,n_init,int,4,"Number of times the SMACOF algorithm will be run with different initializations. The final results will be the best output of the runs, determined by the run with the smallest final stress.",MDS,<class 'sklearn.manifold._mds.MDS'>
Parameters,max_iter,int,300,Maximum number of iterations of the SMACOF algorithm for a single run.,MDS,<class 'sklearn.manifold._mds.MDS'>
Parameters,verbose,int,0,Level of verbosity.,MDS,<class 'sklearn.manifold._mds.MDS'>
Parameters,eps,float,1e-3,Relative tolerance with respect to stress at which to declare convergence. The value of `eps` should be tuned separately depending on whether or not `normalized_stress` is being used.,MDS,<class 'sklearn.manifold._mds.MDS'>
Parameters,n_jobs,int,None,"The number of jobs to use for the computation. If multiple initializations are used (``n_init``), each run of the algorithm is computed in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",MDS,<class 'sklearn.manifold._mds.MDS'>
Parameters,random_state,int,None,Determines the random number generator used to initialize the centers. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,MDS,<class 'sklearn.manifold._mds.MDS'>
Parameters,dissimilarity,{'euclidean','euclidean',Dissimilarity measure to use: - 'euclidean': Pairwise Euclidean distances between points in the dataset. - 'precomputed': Pre-computed dissimilarities are passed directly to ``fit`` and ``fit_transform``.,MDS,<class 'sklearn.manifold._mds.MDS'>
Attributes,normalized_stress,"bool or ""auto"" default=""auto""",,"Whether use and return normed stress value (Stress-1) instead of raw stress calculated by default. Only supported in non-metric MDS. The default value changed from `False` to `""auto""` in version 1.4.",MDS,<class 'sklearn.manifold._mds.MDS'>
Attributes,embedding_,ndarray of shape (n_samples,,Stores the position of the dataset in the embedding space.,MDS,<class 'sklearn.manifold._mds.MDS'>
Attributes,stress_,float,,"The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points). If `normalized_stress=True`, and `metric=False` returns Stress-1. A value of 0 indicates ""perfect"" fit, 0.025 excellent, 0.05 good, 0.1 fair, and 0.2 poor [1]_.",MDS,<class 'sklearn.manifold._mds.MDS'>
Attributes,dissimilarity_matrix_,ndarray of shape (n_samples,,Pairwise dissimilarities between the points. Symmetric matrix that: - either uses a custom dissimilarity matrix by setting `dissimilarity` to 'precomputed'; - or constructs a dissimilarity matrix from data using Euclidean distances.,MDS,<class 'sklearn.manifold._mds.MDS'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MDS,<class 'sklearn.manifold._mds.MDS'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MDS,<class 'sklearn.manifold._mds.MDS'>
,n_iter_,int,,The number of iterations corresponding to the best stress.,MDS,<class 'sklearn.manifold._mds.MDS'>
Parameters,hidden_layer_sizes,array-like of shape(n_layers - 2,(100,The ith element represents the number of neurons in the ith hidden layer.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,activation,{'identity','relu',"Activation function for the hidden layer. - 'identity', no-op activation, useful to implement linear bottleneck, returns f(x) = x - 'logistic', the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)). - 'tanh', the hyperbolic tan function, returns f(x) = tanh(x). - 'relu', the rectified linear unit function, returns f(x) = max(0, x)",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,solver,{'lbfgs','adam',"The solver for weight optimization. - 'lbfgs' is an optimizer in the family of quasi-Newton methods. - 'sgd' refers to stochastic gradient descent. - 'adam' refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba For a comparison between Adam optimizer and SGD, see :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_training_curves.py`. Note: The default solver 'adam' works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, 'lbfgs' can converge faster and perform better.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,alpha,float,0.0001,"Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss. For an example usage and visualization of varying regularization, see :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_alpha.py`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,batch_size,int,'auto',"Size of minibatches for stochastic optimizers. If the solver is 'lbfgs', the classifier will not use minibatch. When set to ""auto"", `batch_size=min(200, n_samples)`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,learning_rate,{'constant','constant',"Learning rate schedule for weight updates. - 'constant' is a constant learning rate given by 'learning_rate_init'. - 'invscaling' gradually decreases the learning rate at each time step 't' using an inverse scaling exponent of 'power_t'. effective_learning_rate = learning_rate_init / pow(t, power_t) - 'adaptive' keeps the learning rate constant to 'learning_rate_init' as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if 'early_stopping' is on, the current learning rate is divided by 5. Only used when ``solver='sgd'``.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,learning_rate_init,float,0.001,The initial learning rate used. It controls the step-size in updating the weights. Only used when solver='sgd' or 'adam'.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,power_t,float,0.5,The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to 'invscaling'. Only used when solver='sgd'.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,max_iter,int,200,"Maximum number of iterations. The solver iterates until convergence (determined by 'tol') or this number of iterations. For stochastic solvers ('sgd', 'adam'), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,shuffle,bool,True,Whether to shuffle samples in each iteration. Only used when solver='sgd' or 'adam'.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,random_state,int,None,"Determines random number generation for weights and bias initialization, train-test split if early stopping is used, and batch sampling when solver='sgd' or 'adam'. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,tol,float,1e-4,"Tolerance for the optimization. When the loss or score is not improving by at least ``tol`` for ``n_iter_no_change`` consecutive iterations, unless ``learning_rate`` is set to 'adaptive', convergence is considered to be reached and training stops.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,verbose,bool,False,Whether to print progress messages to stdout.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,momentum,float,0.9,Momentum for gradient descent update. Should be between 0 and 1. Only used when solver='sgd'.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,nesterovs_momentum,bool,True,Whether to use Nesterov's momentum. Only used when solver='sgd' and momentum > 0.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,early_stopping,bool,False,"Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least ``tol`` for ``n_iter_no_change`` consecutive epochs. The split is stratified, except in a multilabel setting. If early stopping is False, then the training stops when the training loss does not improve by more than tol for n_iter_no_change consecutive passes over the training set. Only effective when solver='sgd' or 'adam'.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,validation_fraction,float,0.1,The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,beta_1,float,0.9,"Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver='adam'.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,beta_2,float,0.999,"Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver='adam'.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,epsilon,float,1e-8,Value for numerical stability in adam. Only used when solver='adam'.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,n_iter_no_change,int,10,Maximum number of epochs to not meet ``tol`` improvement. Only effective when solver='sgd' or 'adam'.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,max_fun,int,15000,"Only used when solver='lbfgs'. Maximum number of loss function calls. The solver iterates until convergence (determined by 'tol'), number of iterations reaches max_iter, or this number of loss function calls. Note that number of loss function calls will be greater than or equal to the number of iterations for the `MLPClassifier`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,classes_,ndarray or list of ndarray of shape (n_classes,,Class labels for each output.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,loss_,float,,The current loss computed with the loss function.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,best_loss_,float or None,,"The minimum loss reached by the solver throughout fitting. If `early_stopping=True`, this attribute is set to `None`. Refer to the `best_validation_score_` fitted attribute instead.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,loss_curve_,list of shape (`n_iter_`,,The ith element in the list represents the loss at the ith iteration.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,validation_scores_,list of shape (`n_iter_`,,"The score at each iteration on a held-out validation set. The score reported is the accuracy score. Only available if `early_stopping=True`, otherwise the attribute is set to `None`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,best_validation_score_,float or None,,"The best validation score (i.e. accuracy score) that triggered the early stopping. Only available if `early_stopping=True`, otherwise the attribute is set to `None`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,t_,int,,The number of training samples seen by the solver during fitting.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,coefs_,list of shape (n_layers - 1,,The ith element in the list represents the weight matrix corresponding to layer i.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,intercepts_,list of shape (n_layers - 1,,The ith element in the list represents the bias vector corresponding to layer i + 1.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,n_iter_,int,,The number of iterations the solver has run.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,n_layers_,int,,Number of layers.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,n_outputs_,int,,Number of outputs.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
,out_activation_,str,,Name of the output activation function.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,hidden_layer_sizes,array-like of shape(n_layers - 2,(100,The ith element represents the number of neurons in the ith hidden layer.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,activation,{'identity','relu',"Activation function for the hidden layer. - 'identity', no-op activation, useful to implement linear bottleneck, returns f(x) = x - 'logistic', the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)). - 'tanh', the hyperbolic tan function, returns f(x) = tanh(x). - 'relu', the rectified linear unit function, returns f(x) = max(0, x)",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,solver,{'lbfgs','adam',"The solver for weight optimization. - 'lbfgs' is an optimizer in the family of quasi-Newton methods. - 'sgd' refers to stochastic gradient descent. - 'adam' refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba For a comparison between Adam optimizer and SGD, see :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_training_curves.py`. Note: The default solver 'adam' works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, 'lbfgs' can converge faster and perform better.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,alpha,float,0.0001,"Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss. For an example usage and visualization of varying regularization, see :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_alpha.py`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,batch_size,int,'auto',"Size of minibatches for stochastic optimizers. If the solver is 'lbfgs', the classifier will not use minibatch. When set to ""auto"", `batch_size=min(200, n_samples)`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,learning_rate,{'constant','constant',"Learning rate schedule for weight updates. - 'constant' is a constant learning rate given by 'learning_rate_init'. - 'invscaling' gradually decreases the learning rate at each time step 't' using an inverse scaling exponent of 'power_t'. effective_learning_rate = learning_rate_init / pow(t, power_t) - 'adaptive' keeps the learning rate constant to 'learning_rate_init' as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if 'early_stopping' is on, the current learning rate is divided by 5. Only used when ``solver='sgd'``.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,learning_rate_init,float,0.001,The initial learning rate used. It controls the step-size in updating the weights. Only used when solver='sgd' or 'adam'.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,power_t,float,0.5,The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to 'invscaling'. Only used when solver='sgd'.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,max_iter,int,200,"Maximum number of iterations. The solver iterates until convergence (determined by 'tol') or this number of iterations. For stochastic solvers ('sgd', 'adam'), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,shuffle,bool,True,Whether to shuffle samples in each iteration. Only used when solver='sgd' or 'adam'.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,random_state,int,None,"Determines random number generation for weights and bias initialization, train-test split if early stopping is used, and batch sampling when solver='sgd' or 'adam'. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,tol,float,1e-4,"Tolerance for the optimization. When the loss or score is not improving by at least ``tol`` for ``n_iter_no_change`` consecutive iterations, unless ``learning_rate`` is set to 'adaptive', convergence is considered to be reached and training stops.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,verbose,bool,False,Whether to print progress messages to stdout.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,momentum,float,0.9,Momentum for gradient descent update. Should be between 0 and 1. Only used when solver='sgd'.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,nesterovs_momentum,bool,True,Whether to use Nesterov's momentum. Only used when solver='sgd' and momentum > 0.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,early_stopping,bool,False,"Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least ``tol`` for ``n_iter_no_change`` consecutive epochs. The split is stratified, except in a multilabel setting. If early stopping is False, then the training stops when the training loss does not improve by more than tol for n_iter_no_change consecutive passes over the training set. Only effective when solver='sgd' or 'adam'.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,validation_fraction,float,0.1,The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,beta_1,float,0.9,"Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver='adam'.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,beta_2,float,0.999,"Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver='adam'.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,epsilon,float,1e-8,Value for numerical stability in adam. Only used when solver='adam'.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,n_iter_no_change,int,10,Maximum number of epochs to not meet ``tol`` improvement. Only effective when solver='sgd' or 'adam'.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,max_fun,int,15000,"Only used when solver='lbfgs'. Maximum number of loss function calls. The solver iterates until convergence (determined by 'tol'), number of iterations reaches max_iter, or this number of loss function calls. Note that number of loss function calls will be greater than or equal to the number of iterations for the `MLPClassifier`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,classes_,ndarray or list of ndarray of shape (n_classes,,Class labels for each output.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,loss_,float,,The current loss computed with the loss function.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,best_loss_,float or None,,"The minimum loss reached by the solver throughout fitting. If `early_stopping=True`, this attribute is set to `None`. Refer to the `best_validation_score_` fitted attribute instead.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,loss_curve_,list of shape (`n_iter_`,,The ith element in the list represents the loss at the ith iteration.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,validation_scores_,list of shape (`n_iter_`,,"The score at each iteration on a held-out validation set. The score reported is the accuracy score. Only available if `early_stopping=True`, otherwise the attribute is set to `None`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,best_validation_score_,float or None,,"The best validation score (i.e. accuracy score) that triggered the early stopping. Only available if `early_stopping=True`, otherwise the attribute is set to `None`.",MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,t_,int,,The number of training samples seen by the solver during fitting.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,coefs_,list of shape (n_layers - 1,,The ith element in the list represents the weight matrix corresponding to layer i.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,intercepts_,list of shape (n_layers - 1,,The ith element in the list represents the bias vector corresponding to layer i + 1.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,n_iter_,int,,The number of iterations the solver has run.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,n_layers_,int,,Number of layers.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Attributes,n_outputs_,int,,Number of outputs.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
,out_activation_,str,,Name of the output activation function.,MLPClassifier,<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>
Parameters,hidden_layer_sizes,array-like of shape(n_layers - 2,(100,The ith element represents the number of neurons in the ith hidden layer.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,activation,{'identity','relu',"Activation function for the hidden layer. - 'identity', no-op activation, useful to implement linear bottleneck, returns f(x) = x - 'logistic', the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)). - 'tanh', the hyperbolic tan function, returns f(x) = tanh(x). - 'relu', the rectified linear unit function, returns f(x) = max(0, x)",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,solver,{'lbfgs','adam',"The solver for weight optimization. - 'lbfgs' is an optimizer in the family of quasi-Newton methods. - 'sgd' refers to stochastic gradient descent. - 'adam' refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba For a comparison between Adam optimizer and SGD, see :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_training_curves.py`. Note: The default solver 'adam' works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, 'lbfgs' can converge faster and perform better.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,alpha,float,0.0001,Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,batch_size,int,'auto',"Size of minibatches for stochastic optimizers. If the solver is 'lbfgs', the regressor will not use minibatch. When set to ""auto"", `batch_size=min(200, n_samples)`.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,learning_rate,{'constant','constant',"Learning rate schedule for weight updates. - 'constant' is a constant learning rate given by 'learning_rate_init'. - 'invscaling' gradually decreases the learning rate ``learning_rate_`` at each time step 't' using an inverse scaling exponent of 'power_t'. effective_learning_rate = learning_rate_init / pow(t, power_t) - 'adaptive' keeps the learning rate constant to 'learning_rate_init' as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if 'early_stopping' is on, the current learning rate is divided by 5. Only used when solver='sgd'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,learning_rate_init,float,0.001,The initial learning rate used. It controls the step-size in updating the weights. Only used when solver='sgd' or 'adam'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,power_t,float,0.5,The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to 'invscaling'. Only used when solver='sgd'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,max_iter,int,200,"Maximum number of iterations. The solver iterates until convergence (determined by 'tol') or this number of iterations. For stochastic solvers ('sgd', 'adam'), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,shuffle,bool,True,Whether to shuffle samples in each iteration. Only used when solver='sgd' or 'adam'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,random_state,int,None,"Determines random number generation for weights and bias initialization, train-test split if early stopping is used, and batch sampling when solver='sgd' or 'adam'. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,tol,float,1e-4,"Tolerance for the optimization. When the loss or score is not improving by at least ``tol`` for ``n_iter_no_change`` consecutive iterations, unless ``learning_rate`` is set to 'adaptive', convergence is considered to be reached and training stops.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,verbose,bool,False,Whether to print progress messages to stdout.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,momentum,float,0.9,Momentum for gradient descent update. Should be between 0 and 1. Only used when solver='sgd'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,nesterovs_momentum,bool,True,Whether to use Nesterov's momentum. Only used when solver='sgd' and momentum > 0.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,early_stopping,bool,False,"Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside ``validation_fraction`` of training data as validation and terminate training when validation score is not improving by at least ``tol`` for ``n_iter_no_change`` consecutive epochs. Only effective when solver='sgd' or 'adam'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,validation_fraction,float,0.1,The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,beta_1,float,0.9,"Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver='adam'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,beta_2,float,0.999,"Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver='adam'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,epsilon,float,1e-8,Value for numerical stability in adam. Only used when solver='adam'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,n_iter_no_change,int,10,Maximum number of epochs to not meet ``tol`` improvement. Only effective when solver='sgd' or 'adam'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,max_fun,int,15000,"Only used when solver='lbfgs'. Maximum number of function calls. The solver iterates until convergence (determined by ``tol``), number of iterations reaches max_iter, or this number of function calls. Note that number of function calls will be greater than or equal to the number of iterations for the MLPRegressor.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,loss_,float,,The current loss computed with the loss function.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,best_loss_,float,,"The minimum loss reached by the solver throughout fitting. If `early_stopping=True`, this attribute is set to `None`. Refer to the `best_validation_score_` fitted attribute instead. Only accessible when solver='sgd' or 'adam'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,loss_curve_,list of shape (`n_iter_`,,Loss value evaluated at the end of each training step. The ith element in the list represents the loss at the ith iteration. Only accessible when solver='sgd' or 'adam'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,validation_scores_,list of shape (`n_iter_`,,"The score at each iteration on a held-out validation set. The score reported is the R2 score. Only available if `early_stopping=True`, otherwise the attribute is set to `None`. Only accessible when solver='sgd' or 'adam'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,best_validation_score_,float or None,,"The best validation score (i.e. R2 score) that triggered the early stopping. Only available if `early_stopping=True`, otherwise the attribute is set to `None`. Only accessible when solver='sgd' or 'adam'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,t_,int,,"The number of training samples seen by the solver during fitting. Mathematically equals `n_iters * X.shape[0]`, it means `time_step` and it is used by optimizer's learning rate scheduler.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,coefs_,list of shape (n_layers - 1,,The ith element in the list represents the weight matrix corresponding to layer i.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,intercepts_,list of shape (n_layers - 1,,The ith element in the list represents the bias vector corresponding to layer i + 1.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,n_iter_,int,,The number of iterations the solver has run.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,n_layers_,int,,Number of layers.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,n_outputs_,int,,Number of outputs.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
,out_activation_,str,,Name of the output activation function.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,hidden_layer_sizes,array-like of shape(n_layers - 2,(100,The ith element represents the number of neurons in the ith hidden layer.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,activation,{'identity','relu',"Activation function for the hidden layer. - 'identity', no-op activation, useful to implement linear bottleneck, returns f(x) = x - 'logistic', the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)). - 'tanh', the hyperbolic tan function, returns f(x) = tanh(x). - 'relu', the rectified linear unit function, returns f(x) = max(0, x)",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,solver,{'lbfgs','adam',"The solver for weight optimization. - 'lbfgs' is an optimizer in the family of quasi-Newton methods. - 'sgd' refers to stochastic gradient descent. - 'adam' refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba For a comparison between Adam optimizer and SGD, see :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_training_curves.py`. Note: The default solver 'adam' works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, 'lbfgs' can converge faster and perform better.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,alpha,float,0.0001,Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,batch_size,int,'auto',"Size of minibatches for stochastic optimizers. If the solver is 'lbfgs', the regressor will not use minibatch. When set to ""auto"", `batch_size=min(200, n_samples)`.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,learning_rate,{'constant','constant',"Learning rate schedule for weight updates. - 'constant' is a constant learning rate given by 'learning_rate_init'. - 'invscaling' gradually decreases the learning rate ``learning_rate_`` at each time step 't' using an inverse scaling exponent of 'power_t'. effective_learning_rate = learning_rate_init / pow(t, power_t) - 'adaptive' keeps the learning rate constant to 'learning_rate_init' as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if 'early_stopping' is on, the current learning rate is divided by 5. Only used when solver='sgd'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,learning_rate_init,float,0.001,The initial learning rate used. It controls the step-size in updating the weights. Only used when solver='sgd' or 'adam'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,power_t,float,0.5,The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to 'invscaling'. Only used when solver='sgd'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,max_iter,int,200,"Maximum number of iterations. The solver iterates until convergence (determined by 'tol') or this number of iterations. For stochastic solvers ('sgd', 'adam'), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,shuffle,bool,True,Whether to shuffle samples in each iteration. Only used when solver='sgd' or 'adam'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,random_state,int,None,"Determines random number generation for weights and bias initialization, train-test split if early stopping is used, and batch sampling when solver='sgd' or 'adam'. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,tol,float,1e-4,"Tolerance for the optimization. When the loss or score is not improving by at least ``tol`` for ``n_iter_no_change`` consecutive iterations, unless ``learning_rate`` is set to 'adaptive', convergence is considered to be reached and training stops.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,verbose,bool,False,Whether to print progress messages to stdout.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,momentum,float,0.9,Momentum for gradient descent update. Should be between 0 and 1. Only used when solver='sgd'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,nesterovs_momentum,bool,True,Whether to use Nesterov's momentum. Only used when solver='sgd' and momentum > 0.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,early_stopping,bool,False,"Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside ``validation_fraction`` of training data as validation and terminate training when validation score is not improving by at least ``tol`` for ``n_iter_no_change`` consecutive epochs. Only effective when solver='sgd' or 'adam'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,validation_fraction,float,0.1,The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,beta_1,float,0.9,"Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver='adam'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,beta_2,float,0.999,"Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver='adam'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,epsilon,float,1e-8,Value for numerical stability in adam. Only used when solver='adam'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Parameters,n_iter_no_change,int,10,Maximum number of epochs to not meet ``tol`` improvement. Only effective when solver='sgd' or 'adam'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,max_fun,int,15000,"Only used when solver='lbfgs'. Maximum number of function calls. The solver iterates until convergence (determined by ``tol``), number of iterations reaches max_iter, or this number of function calls. Note that number of function calls will be greater than or equal to the number of iterations for the MLPRegressor.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,loss_,float,,The current loss computed with the loss function.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,best_loss_,float,,"The minimum loss reached by the solver throughout fitting. If `early_stopping=True`, this attribute is set to `None`. Refer to the `best_validation_score_` fitted attribute instead. Only accessible when solver='sgd' or 'adam'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,loss_curve_,list of shape (`n_iter_`,,Loss value evaluated at the end of each training step. The ith element in the list represents the loss at the ith iteration. Only accessible when solver='sgd' or 'adam'.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,validation_scores_,list of shape (`n_iter_`,,"The score at each iteration on a held-out validation set. The score reported is the R2 score. Only available if `early_stopping=True`, otherwise the attribute is set to `None`. Only accessible when solver='sgd' or 'adam'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,best_validation_score_,float or None,,"The best validation score (i.e. R2 score) that triggered the early stopping. Only available if `early_stopping=True`, otherwise the attribute is set to `None`. Only accessible when solver='sgd' or 'adam'.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,t_,int,,"The number of training samples seen by the solver during fitting. Mathematically equals `n_iters * X.shape[0]`, it means `time_step` and it is used by optimizer's learning rate scheduler.",MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,coefs_,list of shape (n_layers - 1,,The ith element in the list represents the weight matrix corresponding to layer i.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,intercepts_,list of shape (n_layers - 1,,The ith element in the list represents the bias vector corresponding to layer i + 1.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,n_iter_,int,,The number of iterations the solver has run.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,n_layers_,int,,Number of layers.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,n_outputs_,int,,Number of outputs.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
,out_activation_,str,,Name of the output activation function.,MLPRegressor,<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>
Attributes,copy,bool,True,Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy array).,MaxAbsScaler,<class 'sklearn.preprocessing._data.MaxAbsScaler'>
Attributes,scale_,ndarray of shape (n_features,,Per feature relative scaling of the data. *scale_* attribute.,MaxAbsScaler,<class 'sklearn.preprocessing._data.MaxAbsScaler'>
Attributes,max_abs_,ndarray of shape (n_features,,Per feature maximum absolute value.,MaxAbsScaler,<class 'sklearn.preprocessing._data.MaxAbsScaler'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MaxAbsScaler,<class 'sklearn.preprocessing._data.MaxAbsScaler'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MaxAbsScaler,<class 'sklearn.preprocessing._data.MaxAbsScaler'>
,n_samples_seen_,int,,"The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across ``partial_fit`` calls.",MaxAbsScaler,<class 'sklearn.preprocessing._data.MaxAbsScaler'>
Parameters,bandwidth,float,None,"Bandwidth used in the flat kernel. If not given, the bandwidth is estimated using sklearn.cluster.estimate_bandwidth; see the documentation for that function for hints on scalability (see also the Notes, below).",MeanShift,<class 'sklearn.cluster._mean_shift.MeanShift'>
Parameters,seeds,array-like of shape (n_samples,None,"Seeds used to initialize kernels. If not set, the seeds are calculated by clustering.get_bin_seeds with bandwidth as the grid size and default values for other parameters.",MeanShift,<class 'sklearn.cluster._mean_shift.MeanShift'>
Parameters,bin_seeding,bool,False,"If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. The default value is False. Ignored if seeds argument is not None.",MeanShift,<class 'sklearn.cluster._mean_shift.MeanShift'>
Parameters,min_bin_freq,int,1,,MeanShift,<class 'sklearn.cluster._mean_shift.MeanShift'>
Parameters,cluster_all,bool,True,"If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1.",MeanShift,<class 'sklearn.cluster._mean_shift.MeanShift'>
Parameters,n_jobs,int,None,The number of jobs to use for the computation. The following tasks benefit from the parallelization: - The search of nearest neighbors for bandwidth estimation and label assignments. See the details in the docstring of the ``NearestNeighbors`` class. - Hill-climbing optimization for all seeds. See :term:`Glossary <n_jobs>` for more details. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,MeanShift,<class 'sklearn.cluster._mean_shift.MeanShift'>
Attributes,max_iter,int,300,"Maximum number of iterations, per seed point before the clustering operation terminates (for that seed point), if has not converged yet.",MeanShift,<class 'sklearn.cluster._mean_shift.MeanShift'>
Attributes,cluster_centers_,ndarray of shape (n_clusters,,Coordinates of cluster centers.,MeanShift,<class 'sklearn.cluster._mean_shift.MeanShift'>
Attributes,labels_,ndarray of shape (n_samples,,Labels of each point.,MeanShift,<class 'sklearn.cluster._mean_shift.MeanShift'>
Attributes,n_iter_,int,,Maximum number of iterations performed on each seed.,MeanShift,<class 'sklearn.cluster._mean_shift.MeanShift'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MeanShift,<class 'sklearn.cluster._mean_shift.MeanShift'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MeanShift,<class 'sklearn.cluster._mean_shift.MeanShift'>
Parameters,store_precision,bool,True,Specify if the estimated precision is stored.,MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
Parameters,assume_centered,bool,False,"If True, the support of the robust location and the covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment.",MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
Parameters,support_fraction,float,None,"The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: `(n_samples + n_features + 1) / 2 * n_samples`. The parameter must be in the range (0, 1].",MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
Attributes,random_state,int,None,Determines the pseudo random number generator for shuffling the data. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
Attributes,raw_location_,ndarray of shape (n_features,,The raw robust estimated location before correction and re-weighting.,MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
Attributes,raw_covariance_,ndarray of shape (n_features,,The raw robust estimated covariance before correction and re-weighting.,MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
Attributes,raw_support_,ndarray of shape (n_samples,,"A mask of the observations that have been used to compute the raw robust estimates of location and shape, before correction and re-weighting.",MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
Attributes,location_,ndarray of shape (n_features,,Estimated robust location.,MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
Attributes,covariance_,ndarray of shape (n_features,,Estimated robust covariance matrix.,MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
Attributes,precision_,ndarray of shape (n_features,,Estimated pseudo inverse matrix. (stored only if store_precision is True),MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
Attributes,support_,ndarray of shape (n_samples,,A mask of the observations that have been used to compute the robust estimates of location and shape.,MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
Attributes,dist_,ndarray of shape (n_samples,,Mahalanobis distances of the training set (on which :meth:`fit` is called) observations.,MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MinCovDet,<class 'sklearn.covariance._robust_covariance.MinCovDet'>
Parameters,feature_range,tuple (min,(0,Desired range of transformed data.,MinMaxScaler,<class 'sklearn.preprocessing._data.MinMaxScaler'>
Parameters,copy,bool,True,Set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array).,MinMaxScaler,<class 'sklearn.preprocessing._data.MinMaxScaler'>
Attributes,clip,bool,False,Set to True to clip transformed values of held-out data to provided `feature range`.,MinMaxScaler,<class 'sklearn.preprocessing._data.MinMaxScaler'>
Attributes,min_,ndarray of shape (n_features,,Per feature adjustment for minimum. Equivalent to ``min - X.min(axis=0) * self.scale_``,MinMaxScaler,<class 'sklearn.preprocessing._data.MinMaxScaler'>
Attributes,scale_,ndarray of shape (n_features,,Per feature relative scaling of the data. Equivalent to ``(max - min) / (X.max(axis=0) - X.min(axis=0))`` *scale_* attribute.,MinMaxScaler,<class 'sklearn.preprocessing._data.MinMaxScaler'>
Attributes,data_min_,ndarray of shape (n_features,,Per feature minimum seen in the data *data_min_*,MinMaxScaler,<class 'sklearn.preprocessing._data.MinMaxScaler'>
Attributes,data_max_,ndarray of shape (n_features,,Per feature maximum seen in the data *data_max_*,MinMaxScaler,<class 'sklearn.preprocessing._data.MinMaxScaler'>
Attributes,data_range_,ndarray of shape (n_features,,Per feature range ``(data_max_ - data_min_)`` seen in the data *data_range_*,MinMaxScaler,<class 'sklearn.preprocessing._data.MinMaxScaler'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MinMaxScaler,<class 'sklearn.preprocessing._data.MinMaxScaler'>
Attributes,n_samples_seen_,int,,"The number of samples processed by the estimator. It will be reset on new calls to fit, but increments across ``partial_fit`` calls.",MinMaxScaler,<class 'sklearn.preprocessing._data.MinMaxScaler'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MinMaxScaler,<class 'sklearn.preprocessing._data.MinMaxScaler'>
Parameters,n_components,int,None,Number of dictionary elements to extract.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,alpha,float,1,Sparsity controlling parameter.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,max_iter,int,1_000,Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,fit_algorithm,{'lars','lars',The algorithm used: - `'lars'`: uses the least angle regression method to solve the lasso problem (`linear_model.lars_path`) - `'cd'`: uses the coordinate descent method to compute the Lasso solution (`linear_model.Lasso`). Lars will be faster if the estimated components are sparse.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,n_jobs,int,None,Number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,batch_size,int,256,Number of samples in each mini-batch. The default value of `batch_size` changed from 3 to 256 in version 1.3.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,shuffle,bool,True,Whether to shuffle the samples before forming batches.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,dict_init,ndarray of shape (n_components,None,Initial value of the dictionary for warm restart scenarios.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,transform_algorithm,{'lasso_lars','omp',Algorithm used to transform the data: - `'lars'`: uses the least angle regression method (`linear_model.lars_path`); - `'lasso_lars'`: uses Lars to compute the Lasso solution. - `'lasso_cd'`: uses the coordinate descent method to compute the Lasso solution (`linear_model.Lasso`). `'lasso_lars'` will be faster if the estimated components are sparse. - `'omp'`: uses orthogonal matching pursuit to estimate the sparse solution. - `'threshold'`: squashes to zero all coefficients less than alpha from the projection ``dictionary * X'``.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,transform_n_nonzero_coefs,int,None,"Number of nonzero coefficients to target in each column of the solution. This is only used by `algorithm='lars'` and `algorithm='omp'`. If `None`, then `transform_n_nonzero_coefs=int(n_features / 10)`.",MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,transform_alpha,float,None,"If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the penalty applied to the L1 norm. If `algorithm='threshold'`, `alpha` is the absolute value of the threshold below which coefficients will be squashed to zero. If `None`, defaults to `alpha`. When None, default value changed from 1.0 to `alpha`.",MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,verbose,bool or int,False,To control the verbosity of the procedure.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,split_sign,bool,False,Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,random_state,int,None,"Used for initializing the dictionary when ``dict_init`` is not specified, randomly shuffling the data when ``shuffle`` is set to ``True``, and updating the dictionary. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.",MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,positive_code,bool,False,Whether to enforce positivity when finding the code.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,positive_dict,bool,False,Whether to enforce positivity when finding the dictionary.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,transform_max_iter,int,1000,Maximum number of iterations to perform if `algorithm='lasso_cd'` or `'lasso_lars'`.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,callback,callable,None,A callable that gets invoked at the end of each iteration.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,tol,float,1e-3,"Control early stopping based on the norm of the differences in the dictionary between 2 steps. To disable early stopping based on changes in the dictionary, set `tol` to 0.0.",MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Attributes,max_no_improvement,int,10,"Control early stopping based on the consecutive number of mini batches that does not yield an improvement on the smoothed cost function. To disable convergence detection based on cost function, set `max_no_improvement` to None.",MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Attributes,components_,ndarray of shape (n_components,,Components extracted from the data.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Attributes,n_iter_,int,,Number of iterations over the full dataset.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
,n_steps_,int,,Number of mini-batches processed.,MiniBatchDictionaryLearning,<class 'sklearn.decomposition._dict_learning.MiniBatchDictionaryLearning'>
Parameters,n_clusters,int,8,The number of clusters to form as well as the number of centroids to generate.,MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Parameters,init,{'k-means++','k-means++',"Method for initialization: 'k-means++' : selects initial cluster centroids using sampling based on an empirical probability distribution of the points' contribution to the overall inertia. This technique speeds up convergence. The algorithm implemented is ""greedy k-means++"". It differs from the vanilla k-means++ by making several trials at each sampling step and choosing the best centroid among them. 'random': choose `n_clusters` observations (rows) at random from data for the initial centroids. If an array is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. If a callable is passed, it should take arguments X, n_clusters and a random state and return an initialization. For an evaluation of the impact of initialization, see the example :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_stability_low_dim_dense.py`.",MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Parameters,max_iter,int,100,Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics.,MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Parameters,batch_size,int,1024,"Size of the mini batches. For faster computations, you can set the ``batch_size`` greater than 256 * number of cores to enable parallelism on all cores. `batch_size` default changed from 100 to 1024.",MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Parameters,verbose,int,0,Verbosity mode.,MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Parameters,compute_labels,bool,True,Compute label assignment and inertia for the complete dataset once the minibatch optimization has converged in fit.,MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Parameters,random_state,int,None,Determines random number generation for centroid initialization and random reassignment. Use an int to make the randomness deterministic. See :term:`Glossary <random_state>`.,MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Parameters,tol,float,0.0,"Control early stopping based on the relative center changes as measured by a smoothed, variance-normalized of the mean center squared position changes. This early stopping heuristics is closer to the one used for the batch variant of the algorithms but induces a slight computational and memory overhead over the inertia heuristic. To disable convergence detection based on normalized center change, set tol to 0.0 (default).",MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Parameters,max_no_improvement,int,10,"Control early stopping based on the consecutive number of mini batches that does not yield an improvement on the smoothed inertia. To disable convergence detection based on inertia, set max_no_improvement to None.",MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Parameters,init_size,int,None,"Number of samples to randomly sample for speeding up the initialization (sometimes at the expense of accuracy): the only algorithm is initialized by running a batch KMeans on a random subset of the data. This needs to be larger than n_clusters. If `None`, the heuristic is `init_size = 3 * batch_size` if `3 * batch_size < n_clusters`, else `init_size = 3 * n_clusters`.",MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Parameters,n_init,'auto' or int,"""auto""","Number of random initializations that are tried. In contrast to KMeans, the algorithm is only run once, using the best of the `n_init` initializations as measured by inertia. Several runs are recommended for sparse high-dimensional problems (see :ref:`kmeans_sparse_high_dim`). When `n_init='auto'`, the number of runs depends on the value of init: 3 if using `init='random'` or `init` is a callable; 1 if using `init='k-means++'` or `init` is an array-like. Added 'auto' option for `n_init`. Default value for `n_init` changed to `'auto'` in version.",MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Attributes,reassignment_ratio,float,0.01,"Control the fraction of the maximum number of counts for a center to be reassigned. A higher value means that low count centers are more easily reassigned, which means that the model will take longer to converge, but should converge in a better clustering. However, too high a value may cause convergence issues, especially with a small batch size.",MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Attributes,cluster_centers_,ndarray of shape (n_clusters,,Coordinates of cluster centers.,MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Attributes,labels_,ndarray of shape (n_samples,,Labels of each point (if compute_labels is set to True).,MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Attributes,inertia_,float,,"The value of the inertia criterion associated with the chosen partition if compute_labels is set to True. If compute_labels is set to False, it's an approximation of the inertia based on an exponentially weighted average of the batch inertiae. The inertia is defined as the sum of square distances of samples to their cluster center, weighted by the sample weights if provided.",MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Attributes,n_iter_,int,,Number of iterations over the full dataset.,MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Attributes,n_steps_,int,,Number of minibatches processed.,MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MiniBatchKMeans,<class 'sklearn.cluster._kmeans.MiniBatchKMeans'>
Parameters,n_components,int or {'auto'} or None,'auto',"Number of components. If `None`, all features are kept. If `n_components='auto'`, the number of components is automatically inferred from W or H shapes. Added `'auto'` value. Default value changed from `None` to `'auto'`.",MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,init,{'random',None,"Method used to initialize the procedure. Valid options: - `None`: 'nndsvda' if `n_components <= min(n_samples, n_features)`, otherwise random. - `'random'`: non-negative random matrices, scaled with: `sqrt(X.mean() / n_components)` - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD) initialization (better for sparseness). - `'nndsvda'`: NNDSVD with zeros filled with the average of X (better when sparsity is not desired). - `'nndsvdar'` NNDSVD with zeros filled with small random values (generally faster, less accurate alternative to NNDSVDa for when sparsity is not desired). - `'custom'`: Use custom matrices `W` and `H` which must both be provided.",MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,batch_size,int,1024,Number of samples in each mini-batch. Large batch sizes give better long-term convergence at the cost of a slower start.,MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,beta_loss,float or {'frobenius','frobenius',"Beta divergence to be minimized, measuring the distance between `X` and the dot product `WH`. Note that values different from 'frobenius' (or 2) and 'kullback-leibler' (or 1) lead to significantly slower fits. Note that for `beta_loss <= 0` (or 'itakura-saito'), the input matrix `X` cannot contain zeros.",MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,tol,float,1e-4,"Control early stopping based on the norm of the differences in `H` between 2 steps. To disable early stopping based on changes in `H`, set `tol` to 0.0.",MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,max_no_improvement,int,10,"Control early stopping based on the consecutive number of mini batches that does not yield an improvement on the smoothed cost function. To disable convergence detection based on cost function, set `max_no_improvement` to None.",MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,max_iter,int,200,Maximum number of iterations over the complete dataset before timing out.,MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,alpha_W,float,0.0,Constant that multiplies the regularization terms of `W`. Set it to zero (default) to have no regularization on `W`.,MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,alpha_H,"float or ""same""","""same""","Constant that multiplies the regularization terms of `H`. Set it to zero to have no regularization on `H`. If ""same"" (default), it takes the same value as `alpha_W`.",MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,l1_ratio,float,0.0,"The regularization mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.",MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,forget_factor,float,0.7,Amount of rescaling of past information. Its value could be 1 with finite datasets. Choosing values < 1 is recommended with online learning as more recent batches will weight more than past batches.,MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,fresh_restarts,bool,False,Whether to completely solve for W at each step. Doing fresh restarts will likely lead to a better solution for a same number of iterations but it is much slower.,MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,fresh_restarts_max_iter,int,30,Maximum number of iterations when solving for W at each step. Only used when doing fresh restarts. These iterations may be stopped early based on a small change of W controlled by `tol`.,MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,transform_max_iter,int,None,"Maximum number of iterations when solving for W at transform time. If None, it defaults to `max_iter`.",MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,random_state,int,None,"Used for initialisation (when ``init`` == 'nndsvdar' or 'random'), and in Coordinate Descent. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.",MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Attributes,verbose,bool,False,Whether to be verbose.,MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Attributes,components_,ndarray of shape (n_components,,"Factorization matrix, sometimes called 'dictionary'.",MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Attributes,n_components_,int,,"The number of components. It is same as the `n_components` parameter if it was given. Otherwise, it will be same as the number of features.",MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Attributes,reconstruction_err_,float,,"Frobenius norm of the matrix difference, or beta-divergence, between the training data `X` and the reconstructed data `WH` from the fitted model.",MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Attributes,n_iter_,int,,Actual number of started iterations over the whole dataset.,MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Attributes,n_steps_,int,,Number of mini-batches processed.,MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MiniBatchNMF,<class 'sklearn.decomposition._nmf.MiniBatchNMF'>
Parameters,n_components,int,None,"Number of sparse atoms to extract. If None, then ``n_components`` is set to ``n_features``.",MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Parameters,alpha,int,1,Sparsity controlling parameter. Higher values lead to sparser components.,MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Parameters,ridge_alpha,float,0.01,Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method.,MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Parameters,max_iter,int,1_000,Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics.,MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Parameters,callback,callable,None,Callable that gets invoked every five iterations.,MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Parameters,batch_size,int,3,The number of features to take in each mini batch.,MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Parameters,verbose,int or bool,False,"Controls the verbosity; the higher, the more messages. Defaults to 0.",MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Parameters,shuffle,bool,True,Whether to shuffle the data before splitting it in batches.,MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Parameters,n_jobs,int,None,Number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Parameters,method,{'lars','lars',Method to be used for optimization. lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.,MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Parameters,random_state,int,None,"Used for random shuffling when ``shuffle`` is set to ``True``, during online dictionary learning. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.",MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Parameters,tol,float,1e-3,"Control early stopping based on the norm of the differences in the dictionary between 2 steps. To disable early stopping based on changes in the dictionary, set `tol` to 0.0.",MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Attributes,max_no_improvement,int or None,10,"Control early stopping based on the consecutive number of mini batches that does not yield an improvement on the smoothed cost function. To disable convergence detection based on cost function, set `max_no_improvement` to `None`.",MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Attributes,components_,ndarray of shape (n_components,,Sparse components extracted from the data.,MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Attributes,n_components_,int,,Estimated number of components.,MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Attributes,n_iter_,int,,Number of iterations run.,MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Attributes,mean_,ndarray of shape (n_features,,"Per-feature empirical mean, estimated from the training set. Equal to ``X.mean(axis=0)``.",MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MiniBatchSparsePCA,<class 'sklearn.decomposition._sparse_pca.MiniBatchSparsePCA'>
Parameters,missing_values,int,np.nan,"The placeholder for the missing values. All occurrences of `missing_values` will be imputed. For pandas' dataframes with nullable integer dtypes with missing values, `missing_values` should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.",MissingIndicator,<class 'sklearn.impute._base.MissingIndicator'>
Parameters,features,{'missing-only','missing-only',"Whether the imputer mask should represent all or a subset of features. - If `'missing-only'` (default), the imputer mask will only represent features containing missing values during fit time. - If `'all'`, the imputer mask will represent all features.",MissingIndicator,<class 'sklearn.impute._base.MissingIndicator'>
Parameters,sparse,bool or 'auto','auto',"Whether the imputer mask format should be sparse or dense. - If `'auto'` (default), the imputer mask will be of same type as input. - If `True`, the imputer mask will be a sparse matrix. - If `False`, the imputer mask will be a numpy array.",MissingIndicator,<class 'sklearn.impute._base.MissingIndicator'>
Attributes,error_on_new,bool,True,"If `True`, :meth:`transform` will raise an error when there are features with missing values that have no missing values in :meth:`fit`. This is applicable only when `features='missing-only'`.",MissingIndicator,<class 'sklearn.impute._base.MissingIndicator'>
Attributes,features_,ndarray of shape (n_missing_features,,"The features indices which will be returned when calling :meth:`transform`. They are computed during :meth:`fit`. If `features='all'`, `features_` is equal to `range(n_features)`.",MissingIndicator,<class 'sklearn.impute._base.MissingIndicator'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MissingIndicator,<class 'sklearn.impute._base.MissingIndicator'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MissingIndicator,<class 'sklearn.impute._base.MissingIndicator'>
Parameters,classes,array-like of shape (n_classes,None,Indicates an ordering for the class labels. All entries should be unique (cannot contain duplicate classes).,MultiLabelBinarizer,<class 'sklearn.preprocessing._label.MultiLabelBinarizer'>
Attributes,sparse_output,bool,False,Set to True if output binary array is desired in CSR sparse format.,MultiLabelBinarizer,<class 'sklearn.preprocessing._label.MultiLabelBinarizer'>
,classes_,ndarray of shape (n_classes,,A copy of the `classes` parameter when provided. Otherwise it corresponds to the sorted set of classes found when fitting.,MultiLabelBinarizer,<class 'sklearn.preprocessing._label.MultiLabelBinarizer'>
Parameters,estimator,estimator object,,An estimator object implementing :term:`fit` and :term:`predict`. A :term:`predict_proba` method will be exposed only if `estimator` implements it.,MultiOutputClassifier,<class 'sklearn.multioutput.MultiOutputClassifier'>
Attributes,n_jobs,int or None,,"The number of jobs to run in parallel. :meth:`fit`, :meth:`predict` and :meth:`partial_fit` (if supported by the passed estimator) will be parallelized for each target. When individual estimators are fast to train or predict, using ``n_jobs > 1`` can result in slower performance due to the parallelism overhead. ``None`` means `1` unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all available processes / threads. See :term:`Glossary <n_jobs>` for more details. `n_jobs` default changed from `1` to `None`.",MultiOutputClassifier,<class 'sklearn.multioutput.MultiOutputClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,Class labels.,MultiOutputClassifier,<class 'sklearn.multioutput.MultiOutputClassifier'>
Attributes,estimators_,list of ``n_output`` estimators,,Estimators used for predictions.,MultiOutputClassifier,<class 'sklearn.multioutput.MultiOutputClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying `estimator` exposes such an attribute when fit.,MultiOutputClassifier,<class 'sklearn.multioutput.MultiOutputClassifier'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if the underlying estimators expose such an attribute when fit.,MultiOutputClassifier,<class 'sklearn.multioutput.MultiOutputClassifier'>
Parameters,estimator,estimator object,,An estimator object implementing :term:`fit` and :term:`predict`.,MultiOutputRegressor,<class 'sklearn.multioutput.MultiOutputRegressor'>
Attributes,n_jobs,int or None,,"The number of jobs to run in parallel. :meth:`fit`, :meth:`predict` and :meth:`partial_fit` (if supported by the passed estimator) will be parallelized for each target. When individual estimators are fast to train or predict, using ``n_jobs > 1`` can result in slower performance due to the parallelism overhead. ``None`` means `1` unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all available processes / threads. See :term:`Glossary <n_jobs>` for more details. `n_jobs` default changed from `1` to `None`.",MultiOutputRegressor,<class 'sklearn.multioutput.MultiOutputRegressor'>
Attributes,estimators_,list of ``n_output`` estimators,,Estimators used for predictions.,MultiOutputRegressor,<class 'sklearn.multioutput.MultiOutputRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying `estimator` exposes such an attribute when fit.,MultiOutputRegressor,<class 'sklearn.multioutput.MultiOutputRegressor'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if the underlying estimators expose such an attribute when fit.,MultiOutputRegressor,<class 'sklearn.multioutput.MultiOutputRegressor'>
Parameters,alpha,float,1.0,Constant that multiplies the L1/L2 term. Defaults to 1.0.,MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Parameters,l1_ratio,float,0.5,"The ElasticNet mixing parameter, with 0 < l1_ratio <= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.",MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Parameters,copy_X,bool,True,"If ``True``, X will be copied; else, it may be overwritten.",MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Parameters,max_iter,int,1000,The maximum number of iterations.,MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Parameters,tol,float,1e-4,"The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``.",MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Parameters,random_state,int,None,The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Attributes,selection,{'cyclic','cyclic',"If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.",MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Attributes,intercept_,ndarray of shape (n_targets,,Independent term in decision function.,MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Attributes,coef_,ndarray of shape (n_targets,,"Parameter vector (W in the cost function formula). If a 1D y is passed in at fit (non multi-task usage), ``coef_`` is then a 1D array. Note that ``coef_`` stores the transpose of ``W``, ``W.T``.",MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Attributes,n_iter_,int,,Number of iterations run by the coordinate descent solver to reach the specified tolerance.,MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Attributes,dual_gap_,float,,The dual gaps at the end of the optimization.,MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Attributes,eps_,float,,The tolerance scaled scaled by the variance of the target `y`.,MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Attributes,sparse_coef_,sparse matrix of shape (n_features,,Sparse representation of the `coef_`.,MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MultiTaskElasticNet,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNet'>
Parameters,l1_ratio,float or list of float,0.5,"The ElasticNet mixing parameter, with 0 < l1_ratio <= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2. This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7, .9, .95, .99, 1]``.",MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Parameters,eps,float,1e-3,Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``.,MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Parameters,n_alphas,int,100,Number of alphas along the regularization path.,MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Parameters,alphas,array-like,None,"List of alphas where to compute the models. If not provided, set automatically.",MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Parameters,max_iter,int,1000,The maximum number of iterations.,MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Parameters,tol,float,1e-4,"The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``.",MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - int, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For int/None inputs, :class:`~sklearn.model_selection.KFold` is used. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ``cv`` default value if None changed from 3-fold to 5-fold.",MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Parameters,copy_X,bool,True,"If ``True``, X will be copied; else, it may be overwritten.",MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Parameters,verbose,bool or int,0,Amount of verbosity.,MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Parameters,n_jobs,int,None,Number of CPUs to use during the cross validation. Note that this is used only if multiple values for l1_ratio are given. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Parameters,random_state,int,None,The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Attributes,selection,{'cyclic','cyclic',"If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.",MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Attributes,intercept_,ndarray of shape (n_targets,,Independent term in decision function.,MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Attributes,coef_,ndarray of shape (n_targets,,"Parameter vector (W in the cost function formula). Note that ``coef_`` stores the transpose of ``W``, ``W.T``.",MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Attributes,alpha_,float,,The amount of penalization chosen by cross validation.,MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Attributes,mse_path_,ndarray of shape (n_alphas,,"Mean square error for the test set on each fold, varying alpha.",MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Attributes,alphas_,ndarray of shape (n_alphas,,"The grid of alphas used for fitting, for each l1_ratio.",MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Attributes,l1_ratio_,float,,Best l1_ratio obtained by cross-validation.,MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Attributes,n_iter_,int,,Number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha.,MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Attributes,dual_gap_,float,,The dual gap at the end of the optimization for the optimal alpha.,MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MultiTaskElasticNetCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskElasticNetCV'>
Parameters,alpha,float,1.0,Constant that multiplies the L1/L2 term. Defaults to 1.0.,MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Parameters,copy_X,bool,True,"If ``True``, X will be copied; else, it may be overwritten.",MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Parameters,max_iter,int,1000,The maximum number of iterations.,MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Parameters,tol,float,1e-4,"The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``.",MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Parameters,random_state,int,None,The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Attributes,selection,{'cyclic','cyclic',"If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.",MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Attributes,coef_,ndarray of shape (n_targets,,"Parameter vector (W in the cost function formula). Note that ``coef_`` stores the transpose of ``W``, ``W.T``.",MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Attributes,intercept_,ndarray of shape (n_targets,,Independent term in decision function.,MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Attributes,n_iter_,int,,Number of iterations run by the coordinate descent solver to reach the specified tolerance.,MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Attributes,dual_gap_,ndarray of shape (n_alphas,,The dual gaps at the end of the optimization for each alpha.,MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Attributes,eps_,float,,The tolerance scaled scaled by the variance of the target `y`.,MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Attributes,sparse_coef_,sparse matrix of shape (n_features,,Sparse representation of the `coef_`.,MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MultiTaskLasso,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
Parameters,eps,float,1e-3,Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``.,MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Parameters,n_alphas,int,100,Number of alphas along the regularization path.,MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Parameters,alphas,array-like,None,"List of alphas where to compute the models. If not provided, set automatically.",MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Parameters,max_iter,int,1000,The maximum number of iterations.,MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Parameters,tol,float,1e-4,"The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``.",MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Parameters,copy_X,bool,True,"If ``True``, X will be copied; else, it may be overwritten.",MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - int, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For int/None inputs, :class:`~sklearn.model_selection.KFold` is used. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ``cv`` default value if None changed from 3-fold to 5-fold.",MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Parameters,verbose,bool or int,False,Amount of verbosity.,MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Parameters,n_jobs,int,None,Number of CPUs to use during the cross validation. Note that this is used only if multiple values for l1_ratio are given. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Parameters,random_state,int,None,The seed of the pseudo random number generator that selects a random feature to update. Used when ``selection`` == 'random'. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Attributes,selection,{'cyclic','cyclic',"If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4.",MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Attributes,intercept_,ndarray of shape (n_targets,,Independent term in decision function.,MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Attributes,coef_,ndarray of shape (n_targets,,"Parameter vector (W in the cost function formula). Note that ``coef_`` stores the transpose of ``W``, ``W.T``.",MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Attributes,alpha_,float,,The amount of penalization chosen by cross validation.,MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Attributes,mse_path_,ndarray of shape (n_alphas,,"Mean square error for the test set on each fold, varying alpha.",MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Attributes,alphas_,ndarray of shape (n_alphas,,The grid of alphas used for fitting.,MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Attributes,n_iter_,int,,Number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha.,MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Attributes,dual_gap_,float,,The dual gap at the end of the optimization for the optimal alpha.,MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MultiTaskLassoCV,<class 'sklearn.linear_model._coordinate_descent.MultiTaskLassoCV'>
Parameters,alpha,float or array-like of shape (n_features,1.0,"Additive (Laplace/Lidstone) smoothing parameter (set alpha=0 and force_alpha=True, for no smoothing).",MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Parameters,force_alpha,bool,True,"If False and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged. This may cause numerical errors if alpha is too close to 0. The default value of `force_alpha` changed to `True`.",MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Parameters,fit_prior,bool,True,"Whether to learn class prior probabilities or not. If false, a uniform prior will be used.",MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,class_prior,array-like of shape (n_classes,None,"Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.",MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,class_count_,ndarray of shape (n_classes,,Number of samples encountered for each class during fitting. This value is weighted by the sample weight when provided.,MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,class_log_prior_,ndarray of shape (n_classes,,Smoothed empirical log probability for each class.,MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,classes_,ndarray of shape (n_classes,,Class labels known to the classifier,MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,feature_count_,ndarray of shape (n_classes,,"Number of samples encountered for each (class, feature) during fitting. This value is weighted by the sample weight when provided.",MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,feature_log_prob_,ndarray of shape (n_classes,,"Empirical log probability of features given a class, ``P(x_i|y)``.",MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Parameters,alpha,float or array-like of shape (n_features,1.0,"Additive (Laplace/Lidstone) smoothing parameter (set alpha=0 and force_alpha=True, for no smoothing).",MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Parameters,force_alpha,bool,True,"If False and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged. This may cause numerical errors if alpha is too close to 0. The default value of `force_alpha` changed to `True`.",MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Parameters,fit_prior,bool,True,"Whether to learn class prior probabilities or not. If false, a uniform prior will be used.",MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,class_prior,array-like of shape (n_classes,None,"Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.",MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,class_count_,ndarray of shape (n_classes,,Number of samples encountered for each class during fitting. This value is weighted by the sample weight when provided.,MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,class_log_prior_,ndarray of shape (n_classes,,Smoothed empirical log probability for each class.,MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,classes_,ndarray of shape (n_classes,,Class labels known to the classifier,MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,feature_count_,ndarray of shape (n_classes,,"Number of samples encountered for each (class, feature) during fitting. This value is weighted by the sample weight when provided.",MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,feature_log_prob_,ndarray of shape (n_classes,,"Empirical log probability of features given a class, ``P(x_i|y)``.",MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,MultinomialNB,<class 'sklearn.naive_bayes.MultinomialNB'>
Parameters,n_components,int or {'auto'} or None,'auto',"Number of components. If `None`, all features are kept. If `n_components='auto'`, the number of components is automatically inferred from W or H shapes. Added `'auto'` value. Default value changed from `None` to `'auto'`.",NMF,<class 'sklearn.decomposition._nmf.NMF'>
Parameters,init,{'random',None,"Method used to initialize the procedure. Valid options: - `None`: 'nndsvda' if n_components <= min(n_samples, n_features), otherwise random. - `'random'`: non-negative random matrices, scaled with: `sqrt(X.mean() / n_components)` - `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD) initialization (better for sparseness) - `'nndsvda'`: NNDSVD with zeros filled with the average of X (better when sparsity is not desired) - `'nndsvdar'` NNDSVD with zeros filled with small random values (generally faster, less accurate alternative to NNDSVDa for when sparsity is not desired) - `'custom'`: Use custom matrices `W` and `H` which must both be provided. When `init=None` and n_components is less than n_samples and n_features defaults to `nndsvda` instead of `nndsvd`.",NMF,<class 'sklearn.decomposition._nmf.NMF'>
Parameters,solver,{'cd','cd',Numerical solver to use: - 'cd' is a Coordinate Descent solver. - 'mu' is a Multiplicative Update solver. Coordinate Descent solver. Multiplicative Update solver.,NMF,<class 'sklearn.decomposition._nmf.NMF'>
Parameters,beta_loss,float or {'frobenius','frobenius',"Beta divergence to be minimized, measuring the distance between X and the dot product WH. Note that values different from 'frobenius' (or 2) and 'kullback-leibler' (or 1) lead to significantly slower fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input matrix X cannot contain zeros. Used only in 'mu' solver.",NMF,<class 'sklearn.decomposition._nmf.NMF'>
Parameters,tol,float,1e-4,Tolerance of the stopping condition.,NMF,<class 'sklearn.decomposition._nmf.NMF'>
Parameters,max_iter,int,200,Maximum number of iterations before timing out.,NMF,<class 'sklearn.decomposition._nmf.NMF'>
Parameters,random_state,int,None,"Used for initialisation (when ``init`` == 'nndsvdar' or 'random'), and in Coordinate Descent. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.",NMF,<class 'sklearn.decomposition._nmf.NMF'>
Parameters,alpha_W,float,0.0,Constant that multiplies the regularization terms of `W`. Set it to zero (default) to have no regularization on `W`.,NMF,<class 'sklearn.decomposition._nmf.NMF'>
Parameters,alpha_H,"float or ""same""","""same""","Constant that multiplies the regularization terms of `H`. Set it to zero to have no regularization on `H`. If ""same"" (default), it takes the same value as `alpha_W`.",NMF,<class 'sklearn.decomposition._nmf.NMF'>
Parameters,l1_ratio,float,0.0,"The regularization mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2. Regularization parameter *l1_ratio* used in the Coordinate Descent solver.",NMF,<class 'sklearn.decomposition._nmf.NMF'>
Parameters,verbose,int,0,Whether to be verbose.,NMF,<class 'sklearn.decomposition._nmf.NMF'>
Attributes,shuffle,bool,False,"If true, randomize the order of coordinates in the CD solver. *shuffle* parameter used in the Coordinate Descent solver.",NMF,<class 'sklearn.decomposition._nmf.NMF'>
Attributes,components_,ndarray of shape (n_components,,"Factorization matrix, sometimes called 'dictionary'.",NMF,<class 'sklearn.decomposition._nmf.NMF'>
Attributes,n_components_,int,,"The number of components. It is same as the `n_components` parameter if it was given. Otherwise, it will be same as the number of features.",NMF,<class 'sklearn.decomposition._nmf.NMF'>
Attributes,reconstruction_err_,float,,"Frobenius norm of the matrix difference, or beta-divergence, between the training data ``X`` and the reconstructed data ``WH`` from the fitted model.",NMF,<class 'sklearn.decomposition._nmf.NMF'>
Attributes,n_iter_,int,,Actual number of iterations.,NMF,<class 'sklearn.decomposition._nmf.NMF'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,NMF,<class 'sklearn.decomposition._nmf.NMF'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,NMF,<class 'sklearn.decomposition._nmf.NMF'>
Parameters,metric,"{""euclidean""","""euclidean""","Metric to use for distance computation. If `metric=""euclidean""`, the centroid for the samples corresponding to each class is the arithmetic mean, which minimizes the sum of squared L1 distances. If `metric=""manhattan""`, the centroid is the feature-wise median, which minimizes the sum of L1 distances. All metrics but `""euclidean""` and `""manhattan""` were deprecated and now raise an error. `metric='precomputed'` was deprecated and now raises an error",NearestCentroid,<class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>
Parameters,shrink_threshold,float,None,Threshold for shrinking centroids to remove features.,NearestCentroid,<class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>
Attributes,priors,"{""uniform""","""uniform""","The class prior probabilities. By default, the class proportions are inferred from the training data.",NearestCentroid,<class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>
Attributes,centroids_,array-like of shape (n_classes,,Centroid of each class.,NearestCentroid,<class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>
Attributes,classes_,array of shape (n_classes,,The unique classes labels.,NearestCentroid,<class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,NearestCentroid,<class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,NearestCentroid,<class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>
Attributes,deviations_,ndarray of shape (n_classes,,"Deviations (or shrinkages) of the centroids of each class from the overall centroid. Equal to eq. (18.4) if `shrink_threshold=None`, else (18.5) p. 653 of [2]. Can be used to identify features used for classification.",NearestCentroid,<class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>
Attributes,within_class_std_dev_,ndarray of shape (n_features,,Pooled or within-class standard deviation of input data.,NearestCentroid,<class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>
,class_prior_,ndarray of shape (n_classes,,The class prior probabilities.,NearestCentroid,<class 'sklearn.neighbors._nearest_centroid.NearestCentroid'>
Parameters,n_neighbors,int,5,Number of neighbors to use by default for :meth:`kneighbors` queries.,NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>
Parameters,radius,float,1.0,Range of parameter space to use by default for :meth:`radius_neighbors` queries.,NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>
Parameters,algorithm,{'auto','auto',"Algorithm used to compute the nearest neighbors: - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDTree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method. Note: fitting on sparse input will override the setting of this parameter, using brute force.",NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>
Parameters,leaf_size,int,30,"Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem.",NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>
Parameters,metric,str or callable,'minkowski',"Metric to use for distance computation. Default is ""minkowski"", which results in the standard Euclidean distance when p = 2. See the documentation of `scipy.spatial.distance <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric values. If metric is ""precomputed"", X is assumed to be a distance matrix and must be square during fit. X may be a :term:`sparse graph`, in which case only ""nonzero"" elements may be considered neighbors. If metric is a callable function, it takes two arrays representing 1D vectors as inputs and must return one value indicating the distance between those vectors. This works for Scipy's metrics, but is less efficient than passing the metric name as a string.",NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>
Parameters,p,float (positive),2,"Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>
Parameters,metric_params,dict,None,Additional keyword arguments for the metric function.,NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>
Attributes,n_jobs,int,None,The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>
Attributes,effective_metric_,str,,Metric used to compute distances to neighbors.,NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>
Attributes,effective_metric_params_,dict,,Parameters for the metric used to compute distances to neighbors.,NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>
,n_samples_fit_,int,,Number of samples in the fitted data.,NearestNeighbors,<class 'sklearn.neighbors._unsupervised.NearestNeighbors'>
Parameters,n_components,int,None,Preferred dimensionality of the projected space. If None it will be set to `n_features`.,NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>
Parameters,init,{'auto','auto',"Initialization of the linear transformation. Possible options are `'auto'`, `'pca'`, `'lda'`, `'identity'`, `'random'`, and a numpy array of shape `(n_features_a, n_features_b)`. - `'auto'` Depending on `n_components`, the most reasonable initialization is chosen. If `n_components <= min(n_features, n_classes - 1)` we use `'lda'`, as it uses labels information. If not, but `n_components < min(n_features, n_samples)`, we use `'pca'`, as it projects data in meaningful directions (those of higher variance). Otherwise, we just use `'identity'`. - `'pca'` `n_components` principal components of the inputs passed to :meth:`fit` will be used to initialize the transformation. (See :class:`~sklearn.decomposition.PCA`) - `'lda'` `min(n_components, n_classes)` most discriminative components of the inputs passed to :meth:`fit` will be used to initialize the transformation. (If `n_components > n_classes`, the rest of the components will be zero.) (See :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`) - `'identity'` If `n_components` is strictly smaller than the dimensionality of the inputs passed to :meth:`fit`, the identity matrix will be truncated to the first `n_components` rows. - `'random'` The initial transformation will be a random array of shape `(n_components, n_features)`. Each value is sampled from the standard normal distribution. - numpy array `n_features_b` must match the dimensionality of the inputs passed to :meth:`fit` and n_features_a must be less than or equal to that. If `n_components` is not `None`, `n_features_a` must match it.",NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>
Parameters,warm_start,bool,False,"If `True` and :meth:`fit` has been called before, the solution of the previous call to :meth:`fit` is used as the initial linear transformation (`n_components` and `init` will be ignored).",NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>
Parameters,max_iter,int,50,Maximum number of iterations in the optimization.,NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>
Parameters,tol,float,1e-5,Convergence tolerance for the optimization.,NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>
Parameters,callback,callable,None,"If not `None`, this function is called after every iteration of the optimizer, taking as arguments the current solution (flattened transformation matrix) and the number of iterations. This might be useful in case one wants to examine or store the transformation found after each iteration.",NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>
Parameters,verbose,int,0,"If 0, no progress messages will be printed. If 1, progress messages will be printed to stdout. If > 1, progress messages will be printed and the `disp` parameter of :func:`scipy.optimize.minimize` will be set to `verbose - 2`.",NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>
Attributes,random_state,int or numpy.RandomState,None,"A pseudo random number generator object or a seed for it if int. If `init='random'`, `random_state` is used to initialize the random transformation. If `init='pca'`, `random_state` is passed as an argument to PCA when initializing the transformation. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.",NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>
Attributes,components_,ndarray of shape (n_components,,The linear transformation learned during fitting.,NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>
Attributes,n_iter_,int,,Counts the number of iterations performed by the optimizer.,NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>
Attributes,random_state_,numpy.RandomState,,Pseudo random number generator object used during initialization.,NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,NeighborhoodComponentsAnalysis,<class 'sklearn.neighbors._nca.NeighborhoodComponentsAnalysis'>
Parameters,norm,{'l1','l2',"The norm to use to normalize each non zero sample. If norm='max' is used, values will be rescaled by the maximum of the absolute values.",Normalizer,<class 'sklearn.preprocessing._data.Normalizer'>
Attributes,copy,bool,True,Set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix).,Normalizer,<class 'sklearn.preprocessing._data.Normalizer'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,Normalizer,<class 'sklearn.preprocessing._data.Normalizer'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,Normalizer,<class 'sklearn.preprocessing._data.Normalizer'>
Parameters,nu,float,0.5,"An upper bound on the fraction of margin errors (see :ref:`User Guide <nu_svc>`) and a lower bound of the fraction of support vectors. Should be in the interval (0, 1].",NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,kernel,{'linear','rbf',"Specifies the kernel type to be used in the algorithm. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix. For an intuitive visualization of different kernel types see :ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`.",NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,degree,int,3,Degree of the polynomial kernel function ('poly'). Must be non-negative. Ignored by all other kernels.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,gamma,{'scale','scale',"Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features - if float, must be non-negative. The default value of ``gamma`` changed from 'auto' to 'scale'.",NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,coef0,float,0.0,Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,shrinking,bool,True,Whether to use the shrinking heuristic. See the :ref:`User Guide <shrinking_svm>`.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,probability,bool,False,"Whether to enable probability estimates. This must be enabled prior to calling `fit`, will slow down that method as it internally uses 5-fold cross-validation, and `predict_proba` may be inconsistent with `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.",NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,tol,float,1e-3,Tolerance for stopping criterion.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,cache_size,float,200,Specify the size of the kernel cache (in MB).,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,class_weight,{dict,None,"Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies as ``n_samples / (n_classes * np.bincount(y))``.",NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,verbose,bool,False,"Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.",NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,max_iter,int,-1,"Hard limit on iterations within solver, or -1 for no limit.",NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,decision_function_shape,{'ovo','ovr',"Whether to return a one-vs-rest ('ovr') decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one ('ovo') decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one ('ovo') is always used as multi-class strategy. The parameter is ignored for binary classification. decision_function_shape is 'ovr' by default. *decision_function_shape='ovr'* is recommended. Deprecated *decision_function_shape='ovo' and None*.",NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,break_ties,bool,False,"If true, ``decision_function_shape='ovr'``, and number of classes > 2, :term:`predict` will break ties according to the confidence values of :term:`decision_function`; otherwise the first class among the tied classes is returned. Please note that breaking ties comes at a relatively high computational cost compared to a simple predict. See :ref:`sphx_glr_auto_examples_svm_plot_svm_tie_breaking.py` for an example of its usage with ``decision_function_shape='ovr'``.",NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,random_state,int,None,Controls the pseudo random number generation for shuffling the data for probability estimates. Ignored when `probability` is False. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,class_weight_,ndarray of shape (n_classes,,Multipliers of parameter C of each class. Computed based on the ``class_weight`` parameter.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,classes_,ndarray of shape (n_classes,,The unique classes labels.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,coef_,ndarray of shape (n_classes * (n_classes -1) / 2,,Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. `coef_` is readonly property derived from `dual_coef_` and `support_vectors_`.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,dual_coef_,ndarray of shape (n_classes - 1,,"Dual coefficients of the support vector in the decision function (see :ref:`sgd_mathematical_formulation`), multiplied by their targets. For multiclass, coefficient for all 1-vs-1 classifiers. The layout of the coefficients in the multiclass case is somewhat non-trivial. See the :ref:`multi-class section of the User Guide <svm_multi_class>` for details.",NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,fit_status_,int,,"0 if correctly fitted, 1 if the algorithm did not converge.",NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,intercept_,ndarray of shape (n_classes * (n_classes - 1) / 2,,Constants in decision function.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,n_iter_,ndarray of shape (n_classes * (n_classes - 1) // 2,,Number of iterations run by the optimization routine to fit the model. The shape of this attribute depends on the number of models optimized which in turn depends on the number of classes.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,support_,ndarray of shape (n_SV,,Indices of support vectors.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,support_vectors_,ndarray of shape (n_SV,,Support vectors.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,n_support_,ndarray of shape (n_classes,,Number of support vectors for each class.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,fit_status_,int,,"0 if correctly fitted, 1 if the algorithm did not converge.",NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,probA_,ndarray of shape (n_classes * (n_classes - 1) / 2,,,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Attributes,probB_,ndarray of shape (n_classes * (n_classes - 1) / 2,,"If `probability=True`, it corresponds to the parameters learned in Platt scaling to produce probability estimates from decision values. If `probability=False`, it's an empty array. Platt scaling uses the logistic function ``1 / (1 + exp(decision_value * probA_ + probB_))`` where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For more information on the multiclass case and training procedure see section 8 of [1]_.",NuSVC,<class 'sklearn.svm._classes.NuSVC'>
,shape_fit_,tuple of int of shape (n_dimensions_of_X,,Array dimensions of training vector ``X``.,NuSVC,<class 'sklearn.svm._classes.NuSVC'>
Parameters,nu,float,0.5,"An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1].  By default 0.5 will be taken.",NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Parameters,C,float,1.0,"Penalty parameter C of the error term. For an intuitive visualization of the effects of scaling the regularization parameter C, see :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.",NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Parameters,kernel,{'linear','rbf',"Specifies the kernel type to be used in the algorithm. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix. For an intuitive visualization of different kernel types see See :ref:`sphx_glr_auto_examples_svm_plot_svm_regression.py`",NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Parameters,degree,int,3,Degree of the polynomial kernel function ('poly'). Must be non-negative. Ignored by all other kernels.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Parameters,gamma,{'scale','scale',"Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features - if float, must be non-negative. The default value of ``gamma`` changed from 'auto' to 'scale'.",NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Parameters,coef0,float,0.0,Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Parameters,shrinking,bool,True,Whether to use the shrinking heuristic. See the :ref:`User Guide <shrinking_svm>`.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Parameters,tol,float,1e-3,Tolerance for stopping criterion.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Parameters,cache_size,float,200,Specify the size of the kernel cache (in MB).,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Parameters,verbose,bool,False,"Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.",NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Attributes,max_iter,int,-1,"Hard limit on iterations within solver, or -1 for no limit.",NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Attributes,coef_,ndarray of shape (1,,Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. `coef_` is readonly property derived from `dual_coef_` and `support_vectors_`.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Attributes,dual_coef_,ndarray of shape (1,,Coefficients of the support vector in the decision function.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Attributes,fit_status_,int,,"0 if correctly fitted, 1 otherwise (will raise warning)",NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Attributes,intercept_,ndarray of shape (1,,Constants in decision function.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Attributes,n_iter_,int,,Number of iterations run by the optimization routine to fit the model.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Attributes,n_support_,ndarray of shape (1,,Number of support vectors.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Attributes,shape_fit_,tuple of int of shape (n_dimensions_of_X,,Array dimensions of training vector ``X``.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Attributes,support_,ndarray of shape (n_SV,,Indices of support vectors.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
,support_vectors_,ndarray of shape (n_SV,,Support vectors.,NuSVR,<class 'sklearn.svm._classes.NuSVR'>
Parameters,kernel,str or callable,'rbf',"Kernel map to be approximated. A callable should accept two arguments and the keyword arguments passed to this object as `kernel_params`, and should return a floating point number.",Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>
Parameters,gamma,float,None,"Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels.",Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>
Parameters,coef0,float,None,Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.,Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>
Parameters,degree,float,None,Degree of the polynomial kernel. Ignored by other kernels.,Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>
Parameters,kernel_params,dict,None,Additional parameters (keyword arguments) for kernel function passed as callable object.,Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>
Parameters,n_components,int,100,Number of features to construct. How many data points will be used to construct the mapping.,Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>
Parameters,random_state,int,None,Pseudo-random number generator to control the uniform sampling without replacement of `n_components` of the training data to construct the basis kernel. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>
Attributes,n_jobs,int,None,The number of jobs to use for the computation. This works by breaking down the kernel matrix into `n_jobs` even slices and computing them in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>
Attributes,components_,ndarray of shape (n_components,,Subset of training points used to construct the feature map.,Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>
Attributes,component_indices_,ndarray of shape (n_components),,Indices of ``components_`` in the training set.,Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>
Attributes,normalization_,ndarray of shape (n_components,,Normalization matrix needed for embedding. Square root of the kernel matrix on ``components_``.,Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,Nystroem,<class 'sklearn.kernel_approximation.Nystroem'>
Parameters,store_precision,bool,True,Specify if the estimated precision is stored.,OAS,<class 'sklearn.covariance._shrunk_covariance.OAS'>
Attributes,assume_centered,bool,False,"If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data will be centered before computation.",OAS,<class 'sklearn.covariance._shrunk_covariance.OAS'>
Attributes,covariance_,ndarray of shape (n_features,,Estimated covariance matrix.,OAS,<class 'sklearn.covariance._shrunk_covariance.OAS'>
Attributes,location_,ndarray of shape (n_features,,"Estimated location, i.e. the estimated mean.",OAS,<class 'sklearn.covariance._shrunk_covariance.OAS'>
Attributes,precision_,ndarray of shape (n_features,,Estimated pseudo inverse matrix. (stored only if store_precision is True),OAS,<class 'sklearn.covariance._shrunk_covariance.OAS'>
Attributes,shrinkage_,float,,,OAS,<class 'sklearn.covariance._shrunk_covariance.OAS'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,OAS,<class 'sklearn.covariance._shrunk_covariance.OAS'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,OAS,<class 'sklearn.covariance._shrunk_covariance.OAS'>
Parameters,min_samples,int > 1 or float between 0 and 1,5,"The number of samples in a neighborhood for a point to be considered as a core point. Also, up and down steep regions can't have more than ``min_samples`` consecutive non-steep points. Expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2).",OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Parameters,max_eps,float,np.inf,The maximum distance between two samples for one to be considered as in the neighborhood of the other. Default value of ``np.inf`` will identify clusters across all scales; reducing ``max_eps`` will result in shorter run times.,OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Parameters,metric,str or callable,'minkowski',"Metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used. If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy's metrics, but is less efficient than passing the metric name as a string. If metric is ""precomputed"", `X` is assumed to be a distance matrix and must be square. Valid values for metric are: - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan'] - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'] Sparse matrices are only supported by scikit-learn metrics. See the documentation for scipy.spatial.distance for details on these metrics. `'kulsinski'` is deprecated from SciPy 1.9 and will be removed in SciPy 1.11.",OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Parameters,p,float,2,"Parameter for the Minkowski metric from :class:`~sklearn.metrics.pairwise_distances`. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Parameters,metric_params,dict,None,Additional keyword arguments for the metric function.,OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Parameters,cluster_method,str,'xi',"The extraction method used to extract clusters using the calculated reachability and ordering. Possible values are ""xi"" and ""dbscan"".",OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Parameters,eps,float,None,The maximum distance between two samples for one to be considered as in the neighborhood of the other. By default it assumes the same value as ``max_eps``. Used only when ``cluster_method='dbscan'``.,OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Parameters,xi,float between 0 and 1,0.05,"Determines the minimum steepness on the reachability plot that constitutes a cluster boundary. For example, an upwards point in the reachability plot is defined by the ratio from one point to its successor being at most 1-xi. Used only when ``cluster_method='xi'``.",OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Parameters,predecessor_correction,bool,True,Correct clusters according to the predecessors calculated by OPTICS [2]_. This parameter has minimal effect on most datasets. Used only when ``cluster_method='xi'``.,OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Parameters,min_cluster_size,int > 1 or float between 0 and 1,None,"Minimum number of samples in an OPTICS cluster, expressed as an absolute number or a fraction of the number of samples (rounded to be at least 2). If ``None``, the value of ``min_samples`` is used instead. Used only when ``cluster_method='xi'``.",OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Parameters,algorithm,{'auto','auto',"Algorithm used to compute the nearest neighbors: - 'ball_tree' will use :class:`~sklearn.neighbors.BallTree`. - 'kd_tree' will use :class:`~sklearn.neighbors.KDTree`. - 'brute' will use a brute-force search. - 'auto' (default) will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method. Note: fitting on sparse input will override the setting of this parameter, using brute force.",OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Parameters,leaf_size,int,30,"Leaf size passed to :class:`~sklearn.neighbors.BallTree` or :class:`~sklearn.neighbors.KDTree`. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.",OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Parameters,memory,str or object with the joblib.Memory interface,None,"Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory.",OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Attributes,n_jobs,int,None,The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Attributes,labels_,ndarray of shape (n_samples,,Cluster labels for each point in the dataset given to fit(). Noisy samples and points which are not included in a leaf cluster of ``cluster_hierarchy_`` are labeled as -1.,OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Attributes,reachability_,ndarray of shape (n_samples,,"Reachability distances per sample, indexed by object order. Use ``clust.reachability_[clust.ordering_]`` to access in cluster order.",OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Attributes,ordering_,ndarray of shape (n_samples,,The cluster ordered list of sample indices.,OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Attributes,core_distances_,ndarray of shape (n_samples,,"Distance at which each sample becomes a core point, indexed by object order. Points which will never be core have a distance of inf. Use ``clust.core_distances_[clust.ordering_]`` to access in cluster order.",OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Attributes,predecessor_,ndarray of shape (n_samples,,"Point that a sample was reached from, indexed by object order. Seed points have a predecessor of -1.",OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Attributes,cluster_hierarchy_,ndarray of shape (n_clusters,,"The list of clusters in the form of ``[start, end]`` in each row, with all indices inclusive. The clusters are ordered according to ``(end, -start)`` (ascending) so that larger clusters encompassing smaller clusters come after those smaller ones. Since ``labels_`` does not reflect the hierarchy, usually ``len(cluster_hierarchy_) > np.unique(optics.labels_)``. Please also note that these indices are of the ``ordering_``, i.e. ``X[ordering_][start:end + 1]`` form a cluster. Only available when ``cluster_method='xi'``.",OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,OPTICS,<class 'sklearn.cluster._optics.OPTICS'>
Parameters,kernel,{'linear','rbf',"Specifies the kernel type to be used in the algorithm. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix.",OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Parameters,degree,int,3,Degree of the polynomial kernel function ('poly'). Must be non-negative. Ignored by all other kernels.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Parameters,gamma,{'scale','scale',"Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features - if float, must be non-negative. The default value of ``gamma`` changed from 'auto' to 'scale'.",OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Parameters,coef0,float,0.0,Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Parameters,tol,float,1e-3,Tolerance for stopping criterion.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Parameters,nu,float,0.5,"An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]. By default 0.5 will be taken.",OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Parameters,shrinking,bool,True,Whether to use the shrinking heuristic. See the :ref:`User Guide <shrinking_svm>`.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Parameters,cache_size,float,200,Specify the size of the kernel cache (in MB).,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Parameters,verbose,bool,False,"Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.",OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Attributes,max_iter,int,-1,"Hard limit on iterations within solver, or -1 for no limit.",OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Attributes,coef_,ndarray of shape (1,,Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. `coef_` is readonly property derived from `dual_coef_` and `support_vectors_`.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Attributes,dual_coef_,ndarray of shape (1,,Coefficients of the support vectors in the decision function.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Attributes,fit_status_,int,,"0 if correctly fitted, 1 otherwise (will raise warning)",OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Attributes,intercept_,ndarray of shape (1,,Constant in the decision function.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Attributes,n_iter_,int,,Number of iterations run by the optimization routine to fit the model.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Attributes,n_support_,ndarray of shape (n_classes,,Number of support vectors for each class.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Attributes,offset_,float,,Offset used to define the decision function from the raw scores. We have the relation: decision_function = score_samples - `offset_`. The offset is the opposite of `intercept_` and is provided for consistency with other outlier detection algorithms.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Attributes,shape_fit_,tuple of int of shape (n_dimensions_of_X,,Array dimensions of training vector ``X``.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Attributes,support_,ndarray of shape (n_SV,,Indices of support vectors.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
,support_vectors_,ndarray of shape (n_SV,,Support vectors.,OneClassSVM,<class 'sklearn.svm._classes.OneClassSVM'>
Parameters,categories,'auto' or a list of array-like,'auto',"Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : ``categories[i]`` holds the categories expected in the ith column. The passed categories should not mix strings and numeric values within a single feature, and should be sorted in case of numeric values. The used categories can be found in the ``categories_`` attribute.",OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
Parameters,drop,{'first',None,"Specifies a methodology to use to drop one of the categories per feature. This is useful in situations where perfectly collinear features cause problems, such as when feeding the resulting data into an unregularized linear regression model. However, dropping one category breaks the symmetry of the original representation and can therefore induce a bias in downstream models, for instance for penalized linear classification or regression models. - None : retain all features (the default). - 'first' : drop the first category in each feature. If only one category is present, the feature will be dropped entirely. - 'if_binary' : drop the first category in each feature with two categories. Features with 1 or more than 2 categories are left intact. - array : ``drop[i]`` is the category in feature ``X[:, i]`` that should be dropped. When `max_categories` or `min_frequency` is configured to group infrequent categories, the dropping behavior is handled after the grouping. The parameter `drop` was added in 0.21. The option `drop='if_binary'` was added in 0.23. Support for dropping infrequent categories.",OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
Parameters,sparse_output,bool,True,"When ``True``, it returns a :class:`scipy.sparse.csr_matrix`, i.e. a sparse matrix in ""Compressed Sparse Row"" (CSR) format. `sparse` was renamed to `sparse_output`",OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
Parameters,dtype,number type,np.float64,Desired dtype of output.,OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
Parameters,handle_unknown,{'error','error',"Specifies the way unknown categories are handled during :meth:`transform`. - 'error' : Raise an error if an unknown category is present during transform. - 'ignore' : When an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. In the inverse transform, an unknown category will be denoted as None. - 'infrequent_if_exist' : When an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will map to the infrequent category if it exists. The infrequent category will be mapped to the last position in the encoding. During inverse transform, an unknown category will be mapped to the category denoted `'infrequent'` if it exists. If the `'infrequent'` category does not exist, then :meth:`transform` and :meth:`inverse_transform` will handle an unknown category as with `handle_unknown='ignore'`. Infrequent categories exist based on `min_frequency` and `max_categories`. Read more in the :ref:`User Guide <encoder_infrequent_categories>`. - 'warn' : When an unknown category is encountered during transform a warning is issued, and the encoding then proceeds as described for `handle_unknown=""infrequent_if_exist""`. `'infrequent_if_exist'` was added to automatically handle unknown categories and infrequent categories. The option `""warn""` was added in 1.6.",OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
Parameters,min_frequency,int or float,None,"Specifies the minimum frequency below which a category will be considered infrequent. - If `int`, categories with a smaller cardinality will be considered infrequent. - If `float`, categories with a smaller cardinality than `min_frequency * n_samples`  will be considered infrequent. Read more in the :ref:`User Guide <encoder_infrequent_categories>`.",OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
Parameters,max_categories,int,None,"Specifies an upper limit to the number of output features for each input feature when considering infrequent categories. If there are infrequent categories, `max_categories` includes the category representing the infrequent categories along with the frequent categories. If `None`, there is no limit to the number of output features. Read more in the :ref:`User Guide <encoder_infrequent_categories>`.",OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
Attributes,feature_name_combiner,"""concat"" or callable","""concat""","Callable with signature `def callable(input_feature, category)` that returns a string. This is used to create feature names to be returned by :meth:`get_feature_names_out`. `""concat""` concatenates encoded feature name and category with `feature + ""_"" + str(category)`.E.g. feature X with values 1, 6, 7 create feature names `X_1, X_6, X_7`.",OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
Attributes,categories_,list of arrays,,The categories of each feature determined during fitting (in order of the features in X and corresponding with the output of ``transform``). This includes the category specified in ``drop`` (if any).,OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
Attributes,drop_idx_,array of shape (n_features,,"- ``drop_idx_[i]`` is the index in ``categories_[i]`` of the category to be dropped for each feature. - ``drop_idx_[i] = None`` if no category is to be dropped from the feature with index ``i``, e.g. when `drop='if_binary'` and the feature isn't binary. - ``drop_idx_ = None`` if all the transformed features will be retained. If infrequent categories are enabled by setting `min_frequency` or `max_categories` to a non-default value and `drop_idx[i]` corresponds to a infrequent category, then the entire infrequent category is dropped. Added the possibility to contain `None` values.",OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
Attributes,infrequent_categories_,list of ndarray,,Defined only if infrequent categories are enabled by setting `min_frequency` or `max_categories` to a non-default value. `infrequent_categories_[i]` are the infrequent categories for feature `i`. If the feature `i` has no infrequent categories `infrequent_categories_[i]` is None.,OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
,feature_name_combiner,callable or None,,"Callable with signature `def callable(input_feature, category)` that returns a string. This is used to create feature names to be returned by :meth:`get_feature_names_out`.",OneHotEncoder,<class 'sklearn.preprocessing._encoders.OneHotEncoder'>
Parameters,estimator,estimator object,,"A regressor or a classifier that implements :term:`fit`. When a classifier is passed, :term:`decision_function` will be used in priority and it will fallback to :term:`predict_proba` if it is not available. When a regressor is passed, :term:`predict` is used.",OneVsOneClassifier,<class 'sklearn.multiclass.OneVsOneClassifier'>
Attributes,n_jobs,int,None,The number of jobs to use for the computation: the `n_classes * ( n_classes - 1) / 2` OVO problems are computed in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,OneVsOneClassifier,<class 'sklearn.multiclass.OneVsOneClassifier'>
Attributes,estimators_,list of ``n_classes * (n_classes - 1) / 2`` estimators,,Estimators used for predictions.,OneVsOneClassifier,<class 'sklearn.multiclass.OneVsOneClassifier'>
Attributes,classes_,numpy array of shape [n_classes],,Array containing labels.,OneVsOneClassifier,<class 'sklearn.multiclass.OneVsOneClassifier'>
Attributes,n_classes_,int,,Number of classes.,OneVsOneClassifier,<class 'sklearn.multiclass.OneVsOneClassifier'>
Attributes,pairwise_indices_,list,,Indices of samples used when training the estimators. ``None`` when ``estimator``'s `pairwise` tag is False.,OneVsOneClassifier,<class 'sklearn.multiclass.OneVsOneClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,OneVsOneClassifier,<class 'sklearn.multiclass.OneVsOneClassifier'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,OneVsOneClassifier,<class 'sklearn.multiclass.OneVsOneClassifier'>
Parameters,estimator,estimator object,,"A regressor or a classifier that implements :term:`fit`. When a classifier is passed, :term:`decision_function` will be used in priority and it will fallback to :term:`predict_proba` if it is not available. When a regressor is passed, :term:`predict` is used.",OneVsRestClassifier,<class 'sklearn.multiclass.OneVsRestClassifier'>
Parameters,n_jobs,int,None,The number of jobs to use for the computation: the `n_classes` one-vs-rest problems are computed in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details. `n_jobs` default changed from 1 to None,OneVsRestClassifier,<class 'sklearn.multiclass.OneVsRestClassifier'>
Attributes,verbose,int,0,"The verbosity level, if non zero, progress messages are printed. Below 50, the output is sent to stderr. Otherwise, the output is sent to stdout. The frequency of the messages increases with the verbosity level, reporting all iterations at 10. See :class:`joblib.Parallel` for more details.",OneVsRestClassifier,<class 'sklearn.multiclass.OneVsRestClassifier'>
Attributes,estimators_,list of `n_classes` estimators,,Estimators used for predictions.,OneVsRestClassifier,<class 'sklearn.multiclass.OneVsRestClassifier'>
Attributes,classes_,array,,Class labels.,OneVsRestClassifier,<class 'sklearn.multiclass.OneVsRestClassifier'>
Attributes,n_classes_,int,,Number of classes.,OneVsRestClassifier,<class 'sklearn.multiclass.OneVsRestClassifier'>
Attributes,label_binarizer_,LabelBinarizer object,,Object used to transform multiclass labels to binary labels and vice-versa.,OneVsRestClassifier,<class 'sklearn.multiclass.OneVsRestClassifier'>
Attributes,multilabel_,boolean,,Whether a OneVsRestClassifier is a multilabel classifier.,OneVsRestClassifier,<class 'sklearn.multiclass.OneVsRestClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,OneVsRestClassifier,<class 'sklearn.multiclass.OneVsRestClassifier'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,OneVsRestClassifier,<class 'sklearn.multiclass.OneVsRestClassifier'>
Parameters,categories,'auto' or a list of array-like,'auto',"Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : ``categories[i]`` holds the categories expected in the ith column. The passed categories should not mix strings and numeric values, and should be sorted in case of numeric values. The used categories can be found in the ``categories_`` attribute.",OrdinalEncoder,<class 'sklearn.preprocessing._encoders.OrdinalEncoder'>
Parameters,dtype,number type,np.float64,Desired dtype of output.,OrdinalEncoder,<class 'sklearn.preprocessing._encoders.OrdinalEncoder'>
Parameters,handle_unknown,{'error','error',"When set to 'error' an error will be raised in case an unknown categorical feature is present during transform. When set to 'use_encoded_value', the encoded value of unknown categories will be set to the value given for the parameter `unknown_value`. In :meth:`inverse_transform`, an unknown category will be denoted as None.",OrdinalEncoder,<class 'sklearn.preprocessing._encoders.OrdinalEncoder'>
Parameters,unknown_value,int or np.nan,None,"When the parameter handle_unknown is set to 'use_encoded_value', this parameter is required and will set the encoded value of unknown categories. It has to be distinct from the values used to encode any of the categories in `fit`. If set to np.nan, the `dtype` parameter must be a float dtype.",OrdinalEncoder,<class 'sklearn.preprocessing._encoders.OrdinalEncoder'>
Parameters,encoded_missing_value,int or np.nan,np.nan,"Encoded value of missing categories. If set to `np.nan`, then the `dtype` parameter must be a float dtype.",OrdinalEncoder,<class 'sklearn.preprocessing._encoders.OrdinalEncoder'>
Parameters,min_frequency,int or float,None,"Specifies the minimum frequency below which a category will be considered infrequent. - If `int`, categories with a smaller cardinality will be considered infrequent. - If `float`, categories with a smaller cardinality than `min_frequency * n_samples`  will be considered infrequent. Read more in the :ref:`User Guide <encoder_infrequent_categories>`.",OrdinalEncoder,<class 'sklearn.preprocessing._encoders.OrdinalEncoder'>
Attributes,max_categories,int,None,"Specifies an upper limit to the number of output categories for each input feature when considering infrequent categories. If there are infrequent categories, `max_categories` includes the category representing the infrequent categories along with the frequent categories. If `None`, there is no limit to the number of output features. `max_categories` do **not** take into account missing or unknown categories. Setting `unknown_value` or `encoded_missing_value` to an integer will increase the number of unique integer codes by one each. This can result in up to `max_categories + 2` integer codes. Read more in the :ref:`User Guide <encoder_infrequent_categories>`.",OrdinalEncoder,<class 'sklearn.preprocessing._encoders.OrdinalEncoder'>
Attributes,categories_,list of arrays,,The categories of each feature determined during ``fit`` (in order of the features in X and corresponding with the output of ``transform``). This does not include categories that weren't seen during ``fit``.,OrdinalEncoder,<class 'sklearn.preprocessing._encoders.OrdinalEncoder'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,OrdinalEncoder,<class 'sklearn.preprocessing._encoders.OrdinalEncoder'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,OrdinalEncoder,<class 'sklearn.preprocessing._encoders.OrdinalEncoder'>
,infrequent_categories_,list of ndarray,,Defined only if infrequent categories are enabled by setting `min_frequency` or `max_categories` to a non-default value. `infrequent_categories_[i]` are the infrequent categories for feature `i`. If the feature `i` has no infrequent categories `infrequent_categories_[i]` is None.,OrdinalEncoder,<class 'sklearn.preprocessing._encoders.OrdinalEncoder'>
Parameters,n_nonzero_coefs,int,None,"Desired number of non-zero entries in the solution. Ignored if `tol` is set. When `None` and `tol` is also `None`, this value is either set to 10% of `n_features` or 1, whichever is greater.",OrthogonalMatchingPursuit,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuit'>
Parameters,tol,float,None,"Maximum squared norm of the residual. If not None, overrides n_nonzero_coefs.",OrthogonalMatchingPursuit,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuit'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",OrthogonalMatchingPursuit,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuit'>
Attributes,precompute,'auto' or bool,'auto',"Whether to use a precomputed Gram and Xy matrix to speed up calculations. Improves performance when :term:`n_targets` or :term:`n_samples` is very large. Note that if you already have such matrices, you can pass them directly to the fit method.",OrthogonalMatchingPursuit,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuit'>
Attributes,coef_,ndarray of shape (n_features,,Parameter vector (w in the formula).,OrthogonalMatchingPursuit,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuit'>
Attributes,intercept_,float or ndarray of shape (n_targets,,Independent term in decision function.,OrthogonalMatchingPursuit,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuit'>
Attributes,n_iter_,int or array-like,,Number of active features across every target.,OrthogonalMatchingPursuit,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuit'>
Attributes,n_nonzero_coefs_,int or None,,"The number of non-zero coefficients in the solution or `None` when `tol` is set. If `n_nonzero_coefs` is None and `tol` is None this value is either set to 10% of `n_features` or 1, whichever is greater.",OrthogonalMatchingPursuit,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuit'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,OrthogonalMatchingPursuit,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuit'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,OrthogonalMatchingPursuit,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuit'>
Parameters,copy,bool,True,"Whether the design matrix X must be copied by the algorithm. A false value is only helpful if X is already Fortran-ordered, otherwise a copy is made anyway.",OrthogonalMatchingPursuitCV,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuitCV'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",OrthogonalMatchingPursuitCV,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuitCV'>
Parameters,max_iter,int,None,"Maximum numbers of iterations to perform, therefore maximum features to include. 10% of ``n_features`` but at least 5 if available.",OrthogonalMatchingPursuitCV,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuitCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, :class:`~sklearn.model_selection.KFold` is used. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ``cv`` default value if None changed from 3-fold to 5-fold.",OrthogonalMatchingPursuitCV,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuitCV'>
Parameters,n_jobs,int,None,Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,OrthogonalMatchingPursuitCV,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuitCV'>
Attributes,verbose,bool or int,False,Sets the verbosity amount.,OrthogonalMatchingPursuitCV,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuitCV'>
Attributes,intercept_,float or ndarray of shape (n_targets,,Independent term in decision function.,OrthogonalMatchingPursuitCV,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuitCV'>
Attributes,coef_,ndarray of shape (n_features,,Parameter vector (w in the problem formulation).,OrthogonalMatchingPursuitCV,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuitCV'>
Attributes,n_nonzero_coefs_,int,,Estimated number of non-zero coefficients giving the best mean squared error over the cross-validation folds.,OrthogonalMatchingPursuitCV,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuitCV'>
Attributes,n_iter_,int or array-like,,Number of active features across every target for the model refit with the best hyperparameters got by cross-validating across all folds.,OrthogonalMatchingPursuitCV,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuitCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,OrthogonalMatchingPursuitCV,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuitCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,OrthogonalMatchingPursuitCV,<class 'sklearn.linear_model._omp.OrthogonalMatchingPursuitCV'>
Parameters,estimator,estimator object,,An estimator object implementing :term:`fit` and one of :term:`decision_function` or :term:`predict_proba`.,OutputCodeClassifier,<class 'sklearn.multiclass.OutputCodeClassifier'>
Parameters,code_size,float,1.5,Percentage of the number of classes to be used to create the code book. A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. A number greater than 1 will require more classifiers than one-vs-the-rest.,OutputCodeClassifier,<class 'sklearn.multiclass.OutputCodeClassifier'>
Parameters,random_state,int,None,The generator used to initialize the codebook. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,OutputCodeClassifier,<class 'sklearn.multiclass.OutputCodeClassifier'>
Attributes,n_jobs,int,None,The number of jobs to use for the computation: the multiclass problems are computed in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,OutputCodeClassifier,<class 'sklearn.multiclass.OutputCodeClassifier'>
Attributes,estimators_,list of `int(n_classes * code_size)` estimators,,Estimators used for predictions.,OutputCodeClassifier,<class 'sklearn.multiclass.OutputCodeClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,Array containing labels.,OutputCodeClassifier,<class 'sklearn.multiclass.OutputCodeClassifier'>
Attributes,code_book_,ndarray of shape (n_classes,,Binary array containing the code of each class.,OutputCodeClassifier,<class 'sklearn.multiclass.OutputCodeClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,OutputCodeClassifier,<class 'sklearn.multiclass.OutputCodeClassifier'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,OutputCodeClassifier,<class 'sklearn.multiclass.OutputCodeClassifier'>
Parameters,n_components,int,None,"Number of components to keep. if n_components is not set all components are kept:: n_components == min(n_samples, n_features) If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's MLE is used to guess the dimension. Use of ``n_components == 'mle'`` will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``. If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components. If ``svd_solver == 'arpack'``, the number of components must be strictly less than the minimum of n_features and n_samples. Hence, the None case results in:: n_components == min(n_samples, n_features) - 1",PCA,<class 'sklearn.decomposition._pca.PCA'>
Parameters,copy,bool,True,"If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead.",PCA,<class 'sklearn.decomposition._pca.PCA'>
Parameters,whiten,bool,False,When True (False by default) the `components_` vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances. Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions.,PCA,<class 'sklearn.decomposition._pca.PCA'>
Parameters,svd_solver,{'auto','auto',"""auto"" : The solver is selected by a default 'auto' policy is based on `X.shape` and `n_components`: if the input data has fewer than 1000 features and more than 10 times as many samples, then the ""covariance_eigh"" solver is used. Otherwise, if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient ""randomized"" method is selected. Otherwise the exact ""full"" SVD is computed and optionally truncated afterwards. ""full"" : Run exact full SVD calling the standard LAPACK solver via `scipy.linalg.svd` and select the components by postprocessing ""covariance_eigh"" : Precompute the covariance matrix (on centered data), run a classical eigenvalue decomposition on the covariance matrix typically using LAPACK and select the components by postprocessing. This solver is very efficient for n_samples >> n_features and small n_features. It is, however, not tractable otherwise for large n_features (large memory footprint required to materialize the covariance matrix). Also note that compared to the ""full"" solver, this solver effectively doubles the condition number and is therefore less numerical stable (e.g. on input data with a large range of singular values). ""arpack"" : Run SVD truncated to `n_components` calling ARPACK solver via `scipy.sparse.linalg.svds`. It requires strictly `0 < n_components < min(X.shape)` ""randomized"" : Run randomized SVD by the method of Halko et al. Added the 'covariance_eigh' solver.",PCA,<class 'sklearn.decomposition._pca.PCA'>
Parameters,tol,float,0.0,"Tolerance for singular values computed by svd_solver == 'arpack'. Must be of range [0.0, infinity).",PCA,<class 'sklearn.decomposition._pca.PCA'>
Parameters,iterated_power,int or 'auto','auto',"Number of iterations for the power method computed by svd_solver == 'randomized'. Must be of range [0, infinity).",PCA,<class 'sklearn.decomposition._pca.PCA'>
Parameters,n_oversamples,int,10,"This parameter is only relevant when `svd_solver=""randomized""`. It corresponds to the additional number of random vectors to sample the range of `X` so as to ensure proper conditioning. See :func:`~sklearn.utils.extmath.randomized_svd` for more details.",PCA,<class 'sklearn.decomposition._pca.PCA'>
Parameters,power_iteration_normalizer,{'auto','auto',Power iteration normalizer for randomized SVD solver. Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd` for more details.,PCA,<class 'sklearn.decomposition._pca.PCA'>
Attributes,random_state,int,None,Used when the 'arpack' or 'randomized' solvers are used. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,PCA,<class 'sklearn.decomposition._pca.PCA'>
Attributes,components_,ndarray of shape (n_components,,"Principal axes in feature space, representing the directions of maximum variance in the data. Equivalently, the right singular vectors of the centered input data, parallel to its eigenvectors. The components are sorted by decreasing ``explained_variance_``.",PCA,<class 'sklearn.decomposition._pca.PCA'>
Attributes,explained_variance_,ndarray of shape (n_components,,The amount of variance explained by each of the selected components. The variance estimation uses `n_samples - 1` degrees of freedom. Equal to n_components largest eigenvalues of the covariance matrix of X.,PCA,<class 'sklearn.decomposition._pca.PCA'>
Attributes,explained_variance_ratio_,ndarray of shape (n_components,,Percentage of variance explained by each of the selected components. If ``n_components`` is not set then all components are stored and the sum of the ratios is equal to 1.0.,PCA,<class 'sklearn.decomposition._pca.PCA'>
Attributes,singular_values_,ndarray of shape (n_components,,The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the ``n_components`` variables in the lower-dimensional space.,PCA,<class 'sklearn.decomposition._pca.PCA'>
Attributes,mean_,ndarray of shape (n_features,,"Per-feature empirical mean, estimated from the training set. Equal to `X.mean(axis=0)`.",PCA,<class 'sklearn.decomposition._pca.PCA'>
Attributes,n_components_,int,,"The estimated number of components. When n_components is set to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None.",PCA,<class 'sklearn.decomposition._pca.PCA'>
Attributes,n_samples_,int,,Number of samples in the training data.,PCA,<class 'sklearn.decomposition._pca.PCA'>
Attributes,noise_variance_,float,,"The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See ""Pattern Recognition and Machine Learning"" by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf. It is required to compute the estimated data covariance and score samples. Equal to the average of (min(n_features, n_samples) - n_components) smallest eigenvalues of the covariance matrix of X.",PCA,<class 'sklearn.decomposition._pca.PCA'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,PCA,<class 'sklearn.decomposition._pca.PCA'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,PCA,<class 'sklearn.decomposition._pca.PCA'>
Parameters,n_components,int,2,"Number of components to keep. Should be in `[1, min(n_samples, n_features, n_targets)]`.",PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Parameters,scale,bool,True,Whether to scale `X` and `Y`.,PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Parameters,algorithm,{'nipals','nipals',The algorithm used to estimate the first singular vectors of the cross-covariance matrix. 'nipals' uses the power method while 'svd' will compute the whole SVD.,PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Parameters,max_iter,int,500,The maximum number of iterations of the power method when `algorithm='nipals'`. Ignored otherwise.,PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Parameters,tol,float,1e-06,"The tolerance used as convergence criteria in the power method: the algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less than `tol`, where `u` corresponds to the left singular vector.",PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Attributes,copy,bool,True,"Whether to copy `X` and `Y` in fit before applying centering, and potentially scaling. If False, these operations will be done inplace, modifying both arrays.",PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Attributes,x_weights_,ndarray of shape (n_features,,The left singular vectors of the cross-covariance matrices of each iteration.,PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Attributes,y_weights_,ndarray of shape (n_targets,,The right singular vectors of the cross-covariance matrices of each iteration.,PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Attributes,x_loadings_,ndarray of shape (n_features,,The loadings of `X`.,PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Attributes,y_loadings_,ndarray of shape (n_targets,,The loadings of `Y`.,PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Attributes,x_rotations_,ndarray of shape (n_features,,The projection matrix used to transform `X`.,PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Attributes,y_rotations_,ndarray of shape (n_targets,,The projection matrix used to transform `Y`.,PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Attributes,coef_,ndarray of shape (n_targets,,The coefficients of the linear model such that `Y` is approximated as `Y = X @ coef_.T + intercept_`.,PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Attributes,intercept_,ndarray of shape (n_targets,,The intercepts of the linear model such that `Y` is approximated as `Y = X @ coef_.T + intercept_`.,PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Attributes,n_iter_,list of shape (n_components,,"Number of iterations of the power method, for each component. Empty if `algorithm='svd'`.",PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,PLSCanonical,<class 'sklearn.cross_decomposition._pls.PLSCanonical'>
Parameters,n_components,int,2,"Number of components to keep. Should be in `[1, n_features]`.",PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Parameters,scale,bool,True,Whether to scale `X` and `Y`.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Parameters,max_iter,int,500,The maximum number of iterations of the power method when `algorithm='nipals'`. Ignored otherwise.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Parameters,tol,float,1e-06,"The tolerance used as convergence criteria in the power method: the algorithm stops whenever the squared norm of `u_i - u_{i-1}` is less than `tol`, where `u` corresponds to the left singular vector.",PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Attributes,copy,bool,True,"Whether to copy `X` and `Y` in :term:`fit` before applying centering, and potentially scaling. If `False`, these operations will be done inplace, modifying both arrays.",PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Attributes,x_weights_,ndarray of shape (n_features,,The left singular vectors of the cross-covariance matrices of each iteration.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Attributes,y_weights_,ndarray of shape (n_targets,,The right singular vectors of the cross-covariance matrices of each iteration.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Attributes,x_loadings_,ndarray of shape (n_features,,The loadings of `X`.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Attributes,y_loadings_,ndarray of shape (n_targets,,The loadings of `Y`.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Attributes,x_scores_,ndarray of shape (n_samples,,The transformed training samples.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Attributes,y_scores_,ndarray of shape (n_samples,,The transformed training targets.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Attributes,x_rotations_,ndarray of shape (n_features,,The projection matrix used to transform `X`.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Attributes,y_rotations_,ndarray of shape (n_targets,,The projection matrix used to transform `Y`.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Attributes,coef_,ndarray of shape (n_target,,The coefficients of the linear model such that `Y` is approximated as `Y = X @ coef_.T + intercept_`.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Attributes,intercept_,ndarray of shape (n_targets,,The intercepts of the linear model such that `Y` is approximated as `Y = X @ coef_.T + intercept_`.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Attributes,n_iter_,list of shape (n_components,,"Number of iterations of the power method, for each component.",PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,PLSRegression,<class 'sklearn.cross_decomposition._pls.PLSRegression'>
Parameters,n_components,int,2,"The number of components to keep. Should be in `[1, min(n_samples, n_features, n_targets)]`.",PLSSVD,<class 'sklearn.cross_decomposition._pls.PLSSVD'>
Parameters,scale,bool,True,Whether to scale `X` and `Y`.,PLSSVD,<class 'sklearn.cross_decomposition._pls.PLSSVD'>
Attributes,copy,bool,True,"Whether to copy `X` and `Y` in fit before applying centering, and potentially scaling. If `False`, these operations will be done inplace, modifying both arrays.",PLSSVD,<class 'sklearn.cross_decomposition._pls.PLSSVD'>
Attributes,x_weights_,ndarray of shape (n_features,,The left singular vectors of the SVD of the cross-covariance matrix. Used to project `X` in :meth:`transform`.,PLSSVD,<class 'sklearn.cross_decomposition._pls.PLSSVD'>
Attributes,y_weights_,ndarray of (n_targets,,The right singular vectors of the SVD of the cross-covariance matrix. Used to project `X` in :meth:`transform`.,PLSSVD,<class 'sklearn.cross_decomposition._pls.PLSSVD'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,PLSSVD,<class 'sklearn.cross_decomposition._pls.PLSSVD'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,PLSSVD,<class 'sklearn.cross_decomposition._pls.PLSSVD'>
Parameters,C,float,1.0,Maximum step size (regularization). Defaults to 1.0.,PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,fit_intercept,bool,True,"Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.",PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,max_iter,int,1000,"The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`~sklearn.linear_model.PassiveAggressiveClassifier.partial_fit` method.",PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,tol,float or None,1e-3,"The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).",PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,early_stopping,bool,False,"Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score is not improving by at least `tol` for `n_iter_no_change` consecutive epochs.",PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,validation_fraction,float,0.1,The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.,PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,n_iter_no_change,int,5,Number of iterations with no improvement to wait before early stopping.,PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,shuffle,bool,True,Whether or not the training data should be shuffled after each epoch.,PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,verbose,int,0,The verbosity level.,PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,loss,str,"""hinge""",The loss function to be used: hinge: equivalent to PA-I in the reference paper. squared_hinge: equivalent to PA-II in the reference paper.,PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,n_jobs,int or None,None,"The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,random_state,int,None,"Used to shuffle the training data, when ``shuffle`` is set to ``True``. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`. Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled.",PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,class_weight,dict,None,"Preset for the class_weight fit parameter. Weights associated with classes. If not given, all classes are supposed to have weight one. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``. parameter *class_weight* to automatically weight samples.",PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Attributes,average,bool or int,False,"When set to True, computes the averaged SGD weights and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples. parameter *average* to use weights averaging in SGD.",PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Attributes,coef_,ndarray of shape (1,,Weights assigned to the features.,PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Attributes,intercept_,ndarray of shape (1,,Constants in decision function.,PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Attributes,n_iter_,int,,"The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit.",PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,The unique classes labels.,PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
,t_,int,,Number of weight updates performed during training. Same as ``(n_iter_ * n_samples + 1)``.,PassiveAggressiveClassifier,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveClassifier'>
Parameters,C,float,1.0,Maximum step size (regularization). Defaults to 1.0.,PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Parameters,fit_intercept,bool,True,"Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True.",PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Parameters,max_iter,int,1000,"The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`~sklearn.linear_model.PassiveAggressiveRegressor.partial_fit` method.",PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Parameters,tol,float or None,1e-3,"The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).",PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Parameters,early_stopping,bool,False,"Whether to use early stopping to terminate training when validation. score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.",PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Parameters,validation_fraction,float,0.1,The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.,PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Parameters,n_iter_no_change,int,5,Number of iterations with no improvement to wait before early stopping.,PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Parameters,shuffle,bool,True,Whether or not the training data should be shuffled after each epoch.,PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Parameters,verbose,int,0,The verbosity level.,PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Parameters,loss,str,"""epsilon_insensitive""",The loss function to be used: epsilon_insensitive: equivalent to PA-I in the reference paper. squared_epsilon_insensitive: equivalent to PA-II in the reference paper.,PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Parameters,epsilon,float,0.1,"If the difference between the current prediction and the correct label is below this threshold, the model is not updated.",PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Parameters,random_state,int,None,"Used to shuffle the training data, when ``shuffle`` is set to ``True``. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Parameters,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`. Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled.",PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Attributes,average,bool or int,False,"When set to True, computes the averaged SGD weights and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples. parameter *average* to use weights averaging in SGD.",PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Attributes,coef_,array,,Weights assigned to the features.,PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Attributes,intercept_,array,,Constants in decision function.,PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Attributes,n_iter_,int,,The actual number of iterations to reach the stopping criterion.,PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
,t_,int,,Number of weight updates performed during training. Same as ``(n_iter_ * n_samples + 1)``.,PassiveAggressiveRegressor,<class 'sklearn.linear_model._passive_aggressive.PassiveAggressiveRegressor'>
Parameters,patch_size,tuple of int (patch_height,None,"The dimensions of one patch. If set to None, the patch size will be automatically set to `(img_height // 10, img_width // 10)`, where `img_height` and `img_width` are the dimensions of the input images.",PatchExtractor,<class 'sklearn.feature_extraction.image.PatchExtractor'>
Parameters,max_patches,int or float,None,"The maximum number of patches per image to extract. If `max_patches` is a float in (0, 1), it is taken to mean a proportion of the total number of patches. If set to None, extract all possible patches.",PatchExtractor,<class 'sklearn.feature_extraction.image.PatchExtractor'>
,random_state,int,None,Determines the random number generator used for random sampling when `max_patches is not None`. Use an int to make the randomness deterministic. See :term:`Glossary <random_state>`.,PatchExtractor,<class 'sklearn.feature_extraction.image.PatchExtractor'>
Parameters,penalty,{'l2',None,The penalty (aka regularization term) to be used.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,alpha,float,0.0001,Constant that multiplies the regularization term if regularization is used.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,l1_ratio,float,0.15,"The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`. `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1. Only used if `penalty='elasticnet'`.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,fit_intercept,bool,True,"Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,max_iter,int,1000,"The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,tol,float or None,1e-3,"The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,shuffle,bool,True,Whether or not the training data should be shuffled after each epoch.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,verbose,int,0,The verbosity level.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,eta0,float,1,Constant by which the updates are multiplied.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,n_jobs,int,None,"The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,random_state,int,0,"Used to shuffle the training data, when ``shuffle`` is set to ``True``. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,early_stopping,bool,False,"Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score is not improving by at least `tol` for `n_iter_no_change` consecutive epochs.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,validation_fraction,float,0.1,The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,n_iter_no_change,int,5,Number of iterations with no improvement to wait before early stopping.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,class_weight,dict,None,"Preset for the class_weight fit parameter. Weights associated with classes. If not given, all classes are supposed to have weight one. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,classes_,ndarray of shape (n_classes,,The unique classes labels.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,coef_,ndarray of shape (1,,Weights assigned to the features.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,intercept_,ndarray of shape (1,,Constants in decision function.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,n_iter_,int,,"The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
,t_,int,,Number of weight updates performed during training. Same as ``(n_iter_ * n_samples + 1)``.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,penalty,{'l2',None,The penalty (aka regularization term) to be used.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,alpha,float,0.0001,Constant that multiplies the regularization term if regularization is used.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,l1_ratio,float,0.15,"The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`. `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1. Only used if `penalty='elasticnet'`.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,fit_intercept,bool,True,"Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,max_iter,int,1000,"The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,tol,float or None,1e-3,"The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol).",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,shuffle,bool,True,Whether or not the training data should be shuffled after each epoch.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,verbose,int,0,The verbosity level.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,eta0,float,1,Constant by which the updates are multiplied.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,n_jobs,int,None,"The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,random_state,int,0,"Used to shuffle the training data, when ``shuffle`` is set to ``True``. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,early_stopping,bool,False,"Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score is not improving by at least `tol` for `n_iter_no_change` consecutive epochs.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,validation_fraction,float,0.1,The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,n_iter_no_change,int,5,Number of iterations with no improvement to wait before early stopping.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,class_weight,dict,None,"Preset for the class_weight fit parameter. Weights associated with classes. If not given, all classes are supposed to have weight one. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,classes_,ndarray of shape (n_classes,,The unique classes labels.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,coef_,ndarray of shape (1,,Weights assigned to the features.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,intercept_,ndarray of shape (1,,Constants in decision function.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Attributes,n_iter_,int,,"The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit.",Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
,t_,int,,Number of weight updates performed during training. Same as ``(n_iter_ * n_samples + 1)``.,Perceptron,<class 'sklearn.linear_model._perceptron.Perceptron'>
Parameters,steps,list of tuples,,"List of (name of step, estimator) tuples that are to be chained in sequential order. To be compatible with the scikit-learn API, all steps must define `fit`. All non-last steps must also define `transform`. See :ref:`Combining Estimators <combining_estimators>` for more details.",Pipeline,<class 'sklearn.pipeline.Pipeline'>
Parameters,transform_input,list of str,None,"The names of the :term:`metadata` parameters that should be transformed by the pipeline before passing it to the step consuming it. This enables transforming some input arguments to ``fit`` (other than ``X``) to be transformed by the steps of the pipeline up to the step which requires them. Requirement is defined via :ref:`metadata routing <metadata_routing>`. For instance, this can be used to pass a validation set through the pipeline. You can only set this if metadata routing is enabled, which you can enable using ``sklearn.set_config(enable_metadata_routing=True)``.",Pipeline,<class 'sklearn.pipeline.Pipeline'>
Parameters,memory,str or object with the joblib.Memory interface,None,"Used to cache the fitted transformers of the pipeline. The last step will never be cached, even if it is a transformer. By default, no caching is performed. If a string is given, it is the path to the caching directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. Use the attribute ``named_steps`` or ``steps`` to inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time consuming. See :ref:`sphx_glr_auto_examples_neighbors_plot_caching_nearest_neighbors.py` for an example on how to enable caching.",Pipeline,<class 'sklearn.pipeline.Pipeline'>
Attributes,verbose,bool,False,"If True, the time elapsed while fitting each step will be printed as it is completed.",Pipeline,<class 'sklearn.pipeline.Pipeline'>
Attributes,named_steps,:class:`~sklearn.utils.Bunch`,,"Dictionary-like object, with the following attributes. Read-only attribute to access any step parameter by user given name. Keys are step names and values are steps parameters.",Pipeline,<class 'sklearn.pipeline.Pipeline'>
Attributes,classes_,ndarray of shape (n_classes,,The classes labels. Only exist if the last step of the pipeline is a classifier.,Pipeline,<class 'sklearn.pipeline.Pipeline'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying first estimator in `steps` exposes such an attribute when fit.,Pipeline,<class 'sklearn.pipeline.Pipeline'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,Pipeline,<class 'sklearn.pipeline.Pipeline'>
Parameters,alpha,float,1,"Constant that multiplies the L2 penalty term and determines the regularization strength. ``alpha = 0`` is equivalent to unpenalized GLMs. In this case, the design matrix `X` must have full column rank (no collinearities). Values of `alpha` must be in the range `[0.0, inf)`.",PoissonRegressor,<class 'sklearn.linear_model._glm.glm.PoissonRegressor'>
Parameters,fit_intercept,bool,True,Specifies if a constant (a.k.a. bias or intercept) should be added to the linear predictor (`X @ coef + intercept`).,PoissonRegressor,<class 'sklearn.linear_model._glm.glm.PoissonRegressor'>
Parameters,solver,{'lbfgs','lbfgs',"Algorithm to use in the optimization problem: 'lbfgs' Calls scipy's L-BFGS-B optimizer. 'newton-cholesky' Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to iterated reweighted least squares) with an inner Cholesky based solver. This solver is a good choice for `n_samples` >> `n_features`, especially with one-hot encoded categorical features with rare categories. Be aware that the memory usage of this solver has a quadratic dependency on `n_features` because it explicitly computes the Hessian matrix.",PoissonRegressor,<class 'sklearn.linear_model._glm.glm.PoissonRegressor'>
Parameters,max_iter,int,100,"The maximal number of iterations for the solver. Values must be in the range `[1, inf)`.",PoissonRegressor,<class 'sklearn.linear_model._glm.glm.PoissonRegressor'>
Parameters,tol,float,1e-4,"Stopping criterion. For the lbfgs solver, the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol`` where ``g_j`` is the j-th component of the gradient (derivative) of the objective function. Values must be in the range `(0.0, inf)`.",PoissonRegressor,<class 'sklearn.linear_model._glm.glm.PoissonRegressor'>
Parameters,warm_start,bool,False,"If set to ``True``, reuse the solution of the previous call to ``fit`` as initialization for ``coef_`` and ``intercept_`` .",PoissonRegressor,<class 'sklearn.linear_model._glm.glm.PoissonRegressor'>
Attributes,verbose,int,0,"For the lbfgs solver set verbose to any positive number for verbosity. Values must be in the range `[0, inf)`.",PoissonRegressor,<class 'sklearn.linear_model._glm.glm.PoissonRegressor'>
Attributes,coef_,array of shape (n_features,,Estimated coefficients for the linear predictor (`X @ coef_ + intercept_`) in the GLM.,PoissonRegressor,<class 'sklearn.linear_model._glm.glm.PoissonRegressor'>
Attributes,intercept_,float,,Intercept (a.k.a. bias) added to linear predictor.,PoissonRegressor,<class 'sklearn.linear_model._glm.glm.PoissonRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,PoissonRegressor,<class 'sklearn.linear_model._glm.glm.PoissonRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,PoissonRegressor,<class 'sklearn.linear_model._glm.glm.PoissonRegressor'>
,n_iter_,int,,Actual number of iterations used in the solver.,PoissonRegressor,<class 'sklearn.linear_model._glm.glm.PoissonRegressor'>
Parameters,gamma,float,1.0,Parameter of the polynomial kernel whose feature map will be approximated.,PolynomialCountSketch,<class 'sklearn.kernel_approximation.PolynomialCountSketch'>
Parameters,degree,int,2,Degree of the polynomial kernel whose feature map will be approximated.,PolynomialCountSketch,<class 'sklearn.kernel_approximation.PolynomialCountSketch'>
Parameters,coef0,int,0,Constant term of the polynomial kernel whose feature map will be approximated.,PolynomialCountSketch,<class 'sklearn.kernel_approximation.PolynomialCountSketch'>
Parameters,n_components,int,100,"Dimensionality of the output feature space. Usually, `n_components` should be greater than the number of features in input samples in order to achieve good performance. The optimal score / run time balance is typically achieved around `n_components` = 10 * `n_features`, but this depends on the specific dataset being used.",PolynomialCountSketch,<class 'sklearn.kernel_approximation.PolynomialCountSketch'>
Attributes,random_state,int,None,Determines random number generation for indexHash and bitHash initialization. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,PolynomialCountSketch,<class 'sklearn.kernel_approximation.PolynomialCountSketch'>
Attributes,indexHash_,ndarray of shape (degree,,"Array of indexes in range [0, n_components) used to represent the 2-wise independent hash functions for Count Sketch computation.",PolynomialCountSketch,<class 'sklearn.kernel_approximation.PolynomialCountSketch'>
Attributes,bitHash_,ndarray of shape (degree,,"Array with random entries in {+1, -1}, used to represent the 2-wise independent hash functions for Count Sketch computation.",PolynomialCountSketch,<class 'sklearn.kernel_approximation.PolynomialCountSketch'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,PolynomialCountSketch,<class 'sklearn.kernel_approximation.PolynomialCountSketch'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,PolynomialCountSketch,<class 'sklearn.kernel_approximation.PolynomialCountSketch'>
Parameters,degree,int or tuple (min_degree,2,"If a single int is given, it specifies the maximal degree of the polynomial features. If a tuple `(min_degree, max_degree)` is passed, then `min_degree` is the minimum and `max_degree` is the maximum polynomial degree of the generated features. Note that `min_degree=0` and `min_degree=1` are equivalent as outputting the degree zero term is determined by `include_bias`.",PolynomialFeatures,<class 'sklearn.preprocessing._polynomial.PolynomialFeatures'>
Parameters,interaction_only,bool,False,"If `True`, only interaction features are produced: features that are products of at most `degree` *distinct* input features, i.e. terms with power of 2 or higher of the same input feature are excluded: - included: `x[0]`, `x[1]`, `x[0] * x[1]`, etc. - excluded: `x[0] ** 2`, `x[0] ** 2 * x[1]`, etc.",PolynomialFeatures,<class 'sklearn.preprocessing._polynomial.PolynomialFeatures'>
Parameters,include_bias,bool,True,"If `True` (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model).",PolynomialFeatures,<class 'sklearn.preprocessing._polynomial.PolynomialFeatures'>
Attributes,order,{'C','C',"Order of output array in the dense case. `'F'` order is faster to compute, but may slow down subsequent estimators.",PolynomialFeatures,<class 'sklearn.preprocessing._polynomial.PolynomialFeatures'>
Attributes,powers_,ndarray of shape (`n_output_features_`,,"`powers_[i, j]` is the exponent of the jth input in the ith output.",PolynomialFeatures,<class 'sklearn.preprocessing._polynomial.PolynomialFeatures'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,PolynomialFeatures,<class 'sklearn.preprocessing._polynomial.PolynomialFeatures'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,PolynomialFeatures,<class 'sklearn.preprocessing._polynomial.PolynomialFeatures'>
,n_output_features_,int,,The total number of polynomial output features. The number of output features is computed by iterating over all suitably sized combinations of input features.,PolynomialFeatures,<class 'sklearn.preprocessing._polynomial.PolynomialFeatures'>
Parameters,method,{'yeo-johnson','yeo-johnson',"The power transform method. Available methods are: - 'yeo-johnson' [1]_, works with positive and negative values - 'box-cox' [2]_, only works with strictly positive values",PowerTransformer,<class 'sklearn.preprocessing._data.PowerTransformer'>
Parameters,standardize,bool,True,"Set to True to apply zero-mean, unit-variance normalization to the transformed output.",PowerTransformer,<class 'sklearn.preprocessing._data.PowerTransformer'>
Attributes,copy,bool,True,Set to False to perform inplace computation during transformation.,PowerTransformer,<class 'sklearn.preprocessing._data.PowerTransformer'>
Attributes,lambdas_,ndarray of float of shape (n_features,,The parameters of the power transformation for the selected features.,PowerTransformer,<class 'sklearn.preprocessing._data.PowerTransformer'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,PowerTransformer,<class 'sklearn.preprocessing._data.PowerTransformer'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,PowerTransformer,<class 'sklearn.preprocessing._data.PowerTransformer'>
Parameters,priors,array-like of shape (n_classes,None,"Class priors. By default, the class proportions are inferred from the training data.",QuadraticDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>
Parameters,reg_param,float,0.0,"Regularizes the per-class covariance estimates by transforming S2 as ``S2 = (1 - reg_param) * S2 + reg_param * np.eye(n_features)``, where S2 corresponds to the `scaling_` attribute of a given class.",QuadraticDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>
Parameters,store_covariance,bool,False,"If True, the class covariance matrices are explicitly computed and stored in the `self.covariance_` attribute.",QuadraticDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>
Attributes,tol,float,1.0e-4,Absolute threshold for the covariance matrix to be considered rank deficient after applying some regularization (see `reg_param`) to each `Sk` where `Sk` represents covariance matrix for k-th class. This parameter does not affect the predictions. It controls when a warning is raised if the covariance matrix is not full rank.,QuadraticDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>
Attributes,covariance_,list of len n_classes of ndarray             of shape (n_features,,"For each class, gives the covariance matrix estimated using the samples of that class. The estimations are unbiased. Only present if `store_covariance` is True.",QuadraticDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>
Attributes,means_,array-like of shape (n_classes,,Class-wise means.,QuadraticDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>
Attributes,priors_,array-like of shape (n_classes,,Class priors (sum to 1).,QuadraticDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>
Attributes,rotations_,list of len n_classes of ndarray of shape (n_features,,"For each class k an array of shape (n_features, n_k), where ``n_k = min(n_features, number of elements in class k)`` It is the rotation of the Gaussian distribution, i.e. its principal axis. It corresponds to `V`, the matrix of eigenvectors coming from the SVD of `Xk = U S Vt` where `Xk` is the centered matrix of samples from class k.",QuadraticDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>
Attributes,scalings_,list of len n_classes of ndarray of shape (n_k,,"For each class, contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance in the rotated coordinate system. It corresponds to `S^2 / (n_samples - 1)`, where `S` is the diagonal matrix of singular values from the SVD of `Xk`, where `Xk` is the centered matrix of samples from class k.",QuadraticDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>
Attributes,classes_,ndarray of shape (n_classes,,Unique class labels.,QuadraticDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,QuadraticDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,QuadraticDiscriminantAnalysis,<class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>
Parameters,quantile,float,0.5,"The quantile that the model tries to predict. It must be strictly between 0 and 1. If 0.5 (default), the model predicts the 50% quantile, i.e. the median.",QuantileRegressor,<class 'sklearn.linear_model._quantile.QuantileRegressor'>
Parameters,alpha,float,1.0,Regularization constant that multiplies the L1 penalty term.,QuantileRegressor,<class 'sklearn.linear_model._quantile.QuantileRegressor'>
Parameters,fit_intercept,bool,True,Whether or not to fit the intercept.,QuantileRegressor,<class 'sklearn.linear_model._quantile.QuantileRegressor'>
Parameters,solver,{'highs-ds','highs',"Method used by :func:`scipy.optimize.linprog` to solve the linear programming formulation. It is recommended to use the highs methods because they are the fastest ones. Solvers ""highs-ds"", ""highs-ipm"" and ""highs"" support sparse input data and, in fact, always convert to sparse csc. From `scipy>=1.11.0`, ""interior-point"" is not available anymore. The default of `solver` changed to `""highs""` in version 1.4.",QuantileRegressor,<class 'sklearn.linear_model._quantile.QuantileRegressor'>
Attributes,solver_options,dict,None,"Additional parameters passed to :func:`scipy.optimize.linprog` as options. If `None` and if `solver='interior-point'`, then `{""lstsq"": True}` is passed to :func:`scipy.optimize.linprog` for the sake of stability.",QuantileRegressor,<class 'sklearn.linear_model._quantile.QuantileRegressor'>
Attributes,coef_,array of shape (n_features,,Estimated coefficients for the features.,QuantileRegressor,<class 'sklearn.linear_model._quantile.QuantileRegressor'>
Attributes,intercept_,float,,"The intercept of the model, aka bias term.",QuantileRegressor,<class 'sklearn.linear_model._quantile.QuantileRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,QuantileRegressor,<class 'sklearn.linear_model._quantile.QuantileRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,QuantileRegressor,<class 'sklearn.linear_model._quantile.QuantileRegressor'>
,n_iter_,int,,The actual number of iterations performed by the solver.,QuantileRegressor,<class 'sklearn.linear_model._quantile.QuantileRegressor'>
Parameters,n_quantiles,int,1000 or n_samples,"Number of quantiles to be computed. It corresponds to the number of landmarks used to discretize the cumulative distribution function. If n_quantiles is larger than the number of samples, n_quantiles is set to the number of samples as a larger number of quantiles does not give a better approximation of the cumulative distribution function estimator.",QuantileTransformer,<class 'sklearn.preprocessing._data.QuantileTransformer'>
Parameters,output_distribution,{'uniform','uniform',Marginal distribution for the transformed data. The choices are 'uniform' (default) or 'normal'.,QuantileTransformer,<class 'sklearn.preprocessing._data.QuantileTransformer'>
Parameters,ignore_implicit_zeros,bool,False,"Only applies to sparse matrices. If True, the sparse entries of the matrix are discarded to compute the quantile statistics. If False, these entries are treated as zeros.",QuantileTransformer,<class 'sklearn.preprocessing._data.QuantileTransformer'>
Parameters,subsample,int or None,10_000,Maximum number of samples used to estimate the quantiles for computational efficiency. Note that the subsampling procedure may differ for value-identical sparse and dense matrices. Disable subsampling by setting `subsample=None`. The option `None` to disable subsampling was added.,QuantileTransformer,<class 'sklearn.preprocessing._data.QuantileTransformer'>
Parameters,random_state,int,None,Determines random number generation for subsampling and smoothing noise. Please see ``subsample`` for more details. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,QuantileTransformer,<class 'sklearn.preprocessing._data.QuantileTransformer'>
Attributes,copy,bool,True,Set to False to perform inplace transformation and avoid a copy (if the input is already a numpy array).,QuantileTransformer,<class 'sklearn.preprocessing._data.QuantileTransformer'>
Attributes,n_quantiles_,int,,The actual number of quantiles used to discretize the cumulative distribution function.,QuantileTransformer,<class 'sklearn.preprocessing._data.QuantileTransformer'>
Attributes,quantiles_,ndarray of shape (n_quantiles,,The values corresponding the quantiles of reference.,QuantileTransformer,<class 'sklearn.preprocessing._data.QuantileTransformer'>
Attributes,references_,ndarray of shape (n_quantiles,,Quantiles of references.,QuantileTransformer,<class 'sklearn.preprocessing._data.QuantileTransformer'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,QuantileTransformer,<class 'sklearn.preprocessing._data.QuantileTransformer'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,QuantileTransformer,<class 'sklearn.preprocessing._data.QuantileTransformer'>
Parameters,estimator,object,None,"Base estimator object which implements the following methods: * `fit(X, y)`: Fit model to given training data and target values. * `score(X, y)`: Returns the mean accuracy on the given test data, which is used for the stop criterion defined by `stop_score`. Additionally, the score is used to decide which of two equally large consensus sets is chosen as the better one. * `predict(X)`: Returns predicted values using the linear model, which is used to compute residual error using loss function. If `estimator` is None, then :class:`~sklearn.linear_model.LinearRegression` is used for target values of dtype float. Note that the current implementation only supports regression estimators.",RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Parameters,min_samples,int (>= 1) or float ([0,None,"Minimum number of samples chosen randomly from original data. Treated as an absolute number of samples for `min_samples >= 1`, treated as a relative number `ceil(min_samples * X.shape[0])` for `min_samples < 1`. This is typically chosen as the minimal number of samples necessary to estimate the given `estimator`. By default a :class:`~sklearn.linear_model.LinearRegression` estimator is assumed and `min_samples` is chosen as ``X.shape[1] + 1``. This parameter is highly dependent upon the model, so if a `estimator` other than :class:`~sklearn.linear_model.LinearRegression` is used, the user must provide a value.",RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Parameters,residual_threshold,float,None,Maximum residual for a data sample to be classified as an inlier. By default the threshold is chosen as the MAD (median absolute deviation) of the target values `y`. Points whose residuals are strictly equal to the threshold are considered as inliers.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Parameters,is_data_valid,callable,None,"This function is called with the randomly selected data before the model is fitted to it: `is_data_valid(X, y)`. If its return value is False the current randomly chosen sub-sample is skipped.",RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Parameters,is_model_valid,callable,None,"This function is called with the estimated model and the randomly selected data: `is_model_valid(model, X, y)`. If its return value is False the current randomly chosen sub-sample is skipped. Rejecting samples with this function is computationally costlier than with `is_data_valid`. `is_model_valid` should therefore only be used if the estimated model is needed for making the rejection decision.",RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Parameters,max_trials,int,100,Maximum number of iterations for random sample selection.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Parameters,max_skips,int,np.inf,Maximum number of iterations that can be skipped due to finding zero inliers or invalid data defined by ``is_data_valid`` or invalid models defined by ``is_model_valid``.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Parameters,stop_n_inliers,int,np.inf,Stop iteration if at least this number of inliers are found.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Parameters,stop_score,float,np.inf,Stop iteration if score is greater equal than this threshold.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Parameters,stop_probability,float in range [0,0.99,RANSAC iteration stops if at least one outlier-free set of the training data is sampled in RANSAC. This requires to generate at least N samples (iterations):: N >= log(1 - probability) / log(1 - e**m) where the probability (confidence) is typically set to high value such as 0.99 (the default) and e is the current fraction of inliers w.r.t. the total number of samples.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Parameters,loss,str,'absolute_error',"String inputs, 'absolute_error' and 'squared_error' are supported which find the absolute error and squared error per sample respectively. If ``loss`` is a callable, then it should be a function that takes two arrays as inputs, the true and predicted value and returns a 1-D array with the i-th value of the array corresponding to the loss on ``X[i]``. If the loss on a sample is greater than the ``residual_threshold``, then this sample is classified as an outlier.",RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Attributes,random_state,int,None,The generator used to initialize the centers. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Attributes,estimator_,object,,"Final model fitted on the inliers predicted by the ""best"" model found during RANSAC sampling (copy of the `estimator` object).",RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Attributes,n_trials_,int,,Number of random selection trials until one of the stop criteria is met. It is always ``<= max_trials``.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Attributes,inlier_mask_,bool array of shape [n_samples],,Boolean mask of inliers classified as ``True``.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Attributes,n_skips_no_inliers_,int,,Number of iterations skipped due to finding zero inliers.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Attributes,n_skips_invalid_data_,int,,Number of iterations skipped due to invalid data defined by ``is_data_valid``.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Attributes,n_skips_invalid_model_,int,,Number of iterations skipped due to an invalid model defined by ``is_model_valid``.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RANSACRegressor,<class 'sklearn.linear_model._ransac.RANSACRegressor'>
Parameters,gamma,'scale' or float,1.0,"Parameter of RBF kernel: exp(-gamma * x^2). If ``gamma='scale'`` is passed then it uses 1 / (n_features * X.var()) as value of gamma. The option `""scale""` was added in 1.2.",RBFSampler,<class 'sklearn.kernel_approximation.RBFSampler'>
Parameters,n_components,int,100,Number of Monte Carlo samples per original feature. Equals the dimensionality of the computed feature space.,RBFSampler,<class 'sklearn.kernel_approximation.RBFSampler'>
Attributes,random_state,int,None,Pseudo-random number generator to control the generation of the random weights and random offset when fitting the training data. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,RBFSampler,<class 'sklearn.kernel_approximation.RBFSampler'>
Attributes,random_offset_,ndarray of shape (n_components,,Random offset used to compute the projection in the `n_components` dimensions of the feature space.,RBFSampler,<class 'sklearn.kernel_approximation.RBFSampler'>
Attributes,random_weights_,ndarray of shape (n_features,,Random projection directions drawn from the Fourier transform of the RBF kernel.,RBFSampler,<class 'sklearn.kernel_approximation.RBFSampler'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,RBFSampler,<class 'sklearn.kernel_approximation.RBFSampler'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RBFSampler,<class 'sklearn.kernel_approximation.RBFSampler'>
Parameters,estimator,``Estimator`` instance,,"A supervised learning estimator with a ``fit`` method that provides information about feature importance (e.g. `coef_`, `feature_importances_`).",RFE,<class 'sklearn.feature_selection._rfe.RFE'>
Parameters,n_features_to_select,int or float,None,"The number of features to select. If `None`, half of the features are selected. If integer, the parameter is the absolute number of features to select. If float between 0 and 1, it is the fraction of features to select. Added float values for fractions.",RFE,<class 'sklearn.feature_selection._rfe.RFE'>
Parameters,step,int or float,1,"If greater than or equal to 1, then ``step`` corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then ``step`` corresponds to the percentage (rounded down) of features to remove at each iteration.",RFE,<class 'sklearn.feature_selection._rfe.RFE'>
Parameters,verbose,int,0,Controls verbosity of output.,RFE,<class 'sklearn.feature_selection._rfe.RFE'>
Attributes,importance_getter,str or callable,'auto',"If 'auto', uses the feature importance either through a `coef_` or `feature_importances_` attributes of estimator. Also accepts a string that specifies an attribute name/path for extracting feature importance (implemented with `attrgetter`). For example, give `regressor_.coef_` in case of :class:`~sklearn.compose.TransformedTargetRegressor`  or `named_steps.clf.feature_importances_` in case of class:`~sklearn.pipeline.Pipeline` with its last step named `clf`. If `callable`, overrides the default feature importance getter. The callable is passed with the fitted estimator and it should return importance for each feature.",RFE,<class 'sklearn.feature_selection._rfe.RFE'>
Attributes,classes_,ndarray of shape (n_classes,,The classes labels. Only available when `estimator` is a classifier.,RFE,<class 'sklearn.feature_selection._rfe.RFE'>
Attributes,estimator_,``Estimator`` instance,,The fitted estimator used to select features.,RFE,<class 'sklearn.feature_selection._rfe.RFE'>
Attributes,n_features_,int,,The number of selected features.,RFE,<class 'sklearn.feature_selection._rfe.RFE'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,RFE,<class 'sklearn.feature_selection._rfe.RFE'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RFE,<class 'sklearn.feature_selection._rfe.RFE'>
Attributes,ranking_,ndarray of shape (n_features,,"The feature ranking, such that ``ranking_[i]`` corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.",RFE,<class 'sklearn.feature_selection._rfe.RFE'>
,support_,ndarray of shape (n_features,,The mask of selected features.,RFE,<class 'sklearn.feature_selection._rfe.RFE'>
Parameters,estimator,``Estimator`` instance,,A supervised learning estimator with a ``fit`` method that provides information about feature importance either through a ``coef_`` attribute or through a ``feature_importances_`` attribute.,RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Parameters,step,int or float,1,"If greater than or equal to 1, then ``step`` corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then ``step`` corresponds to the percentage (rounded down) of features to remove at each iteration. Note that the last iteration may remove fewer than ``step`` features in order to reach ``min_features_to_select``.",RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Parameters,min_features_to_select,int,1,"The minimum number of features to be selected. This number of features will always be scored, even if the difference between the original feature count and ``min_features_to_select`` isn't divisible by ``step``.",RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross-validation, - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if ``y`` is binary or multiclass, :class:`~sklearn.model_selection.StratifiedKFold` is used. If the estimator is not a classifier or if ``y`` is neither binary nor multiclass, :class:`~sklearn.model_selection.KFold` is used. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ``cv`` default value of None changed from 3-fold to 5-fold.",RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Parameters,scoring,str,None,"A string (see :ref:`scoring_parameter`) or a scorer callable object / function with signature ``scorer(estimator, X, y)``.",RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Parameters,verbose,int,0,Controls verbosity of output.,RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Parameters,n_jobs,int or None,None,Number of cores to run in parallel while fitting across folds. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Attributes,importance_getter,str or callable,'auto',"If 'auto', uses the feature importance either through a `coef_` or `feature_importances_` attributes of estimator. Also accepts a string that specifies an attribute name/path for extracting feature importance. For example, give `regressor_.coef_` in case of :class:`~sklearn.compose.TransformedTargetRegressor`  or `named_steps.clf.feature_importances_` in case of :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`. If `callable`, overrides the default feature importance getter. The callable is passed with the fitted estimator and it should return importance for each feature.",RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Attributes,classes_,ndarray of shape (n_classes,,The classes labels. Only available when `estimator` is a classifier.,RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Attributes,estimator_,``Estimator`` instance,,The fitted estimator used to select features.,RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Attributes,cv_results_,dict of ndarrays,,"All arrays (values of the dictionary) are sorted in ascending order by the number of features used (i.e., the first element of the array represents the models that used the least number of features, while the last element represents the models that used all available features). This dictionary contains the following keys: split(k)_test_score : ndarray of shape (n_subsets_of_features,) The cross-validation scores across (k)th fold. mean_test_score : ndarray of shape (n_subsets_of_features,) Mean of scores over the folds. std_test_score : ndarray of shape (n_subsets_of_features,) Standard deviation of scores over the folds. n_features : ndarray of shape (n_subsets_of_features,) Number of features used at each step.",RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Attributes,n_features_,int,,The number of selected features with cross-validation.,RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Attributes,ranking_,narray of shape (n_features,,"The feature ranking, such that `ranking_[i]` corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.",RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
,support_,ndarray of shape (n_features,,The mask of selected features.,RFECV,<class 'sklearn.feature_selection._rfe.RFECV'>
Parameters,radius,float,1.0,Range of parameter space to use by default for :meth:`radius_neighbors` queries.,RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Parameters,weights,{'uniform','uniform',"Weight function used in prediction.  Possible values: - 'uniform' : uniform weights.  All points in each neighborhood are weighted equally. - 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. - [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights. Uniform weights are used by default.",RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Parameters,algorithm,{'auto','auto',"Algorithm used to compute the nearest neighbors: - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDTree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method. Note: fitting on sparse input will override the setting of this parameter, using brute force.",RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Parameters,leaf_size,int,30,"Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem.",RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Parameters,p,float,2,"Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected to be positive.",RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Parameters,metric,str or callable,'minkowski',"Metric to use for distance computation. Default is ""minkowski"", which results in the standard Euclidean distance when p = 2. See the documentation of `scipy.spatial.distance <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric values. If metric is ""precomputed"", X is assumed to be a distance matrix and must be square during fit. X may be a :term:`sparse graph`, in which case only ""nonzero"" elements may be considered neighbors. If metric is a callable function, it takes two arrays representing 1D vectors as inputs and must return one value indicating the distance between those vectors. This works for Scipy's metrics, but is less efficient than passing the metric name as a string.",RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Parameters,outlier_label,{manual label,None,"Label for outlier samples (samples with no neighbors in given radius). - manual label: str or int label (should be the same type as y) or list of manual labels if multi-output is used. - 'most_frequent' : assign the most frequent label of y to outliers. - None : when any outlier is detected, ValueError will be raised. The outlier label should be selected from among the unique 'Y' labels. If it is specified with a different value a warning will be raised and all class probabilities of outliers will be assigned to be 0.",RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Parameters,metric_params,dict,None,Additional keyword arguments for the metric function.,RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Attributes,n_jobs,int,None,The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,Class labels known to the classifier.,RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Attributes,effective_metric_,str or callable,,"The distance metric used. It will be same as the `metric` parameter or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to 'minkowski' and `p` parameter set to 2.",RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Attributes,effective_metric_params_,dict,,"Additional keyword arguments for the metric function. For most metrics will be same with `metric_params` parameter, but may also contain the `p` parameter value if the `effective_metric_` attribute is set to 'minkowski'.",RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Attributes,n_samples_fit_,int,,Number of samples in the fitted data.,RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Attributes,outlier_label_,int or array-like of shape (n_class,,Label which is given for outlier samples (samples with no neighbors on given radius).,RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
,outputs_2d_,bool,,"False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit otherwise True.",RadiusNeighborsClassifier,<class 'sklearn.neighbors._classification.RadiusNeighborsClassifier'>
Parameters,radius,float,1.0,Range of parameter space to use by default for :meth:`radius_neighbors` queries.,RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>
Parameters,weights,{'uniform','uniform',"Weight function used in prediction.  Possible values: - 'uniform' : uniform weights.  All points in each neighborhood are weighted equally. - 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. - [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights. Uniform weights are used by default.",RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>
Parameters,algorithm,{'auto','auto',"Algorithm used to compute the nearest neighbors: - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDTree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method. Note: fitting on sparse input will override the setting of this parameter, using brute force.",RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>
Parameters,leaf_size,int,30,"Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem.",RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>
Parameters,p,float,2,"Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>
Parameters,metric,str or callable,'minkowski',"Metric to use for distance computation. Default is ""minkowski"", which results in the standard Euclidean distance when p = 2. See the documentation of `scipy.spatial.distance <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric values. If metric is ""precomputed"", X is assumed to be a distance matrix and must be square during fit. X may be a :term:`sparse graph`, in which case only ""nonzero"" elements may be considered neighbors. If metric is a callable function, it takes two arrays representing 1D vectors as inputs and must return one value indicating the distance between those vectors. This works for Scipy's metrics, but is less efficient than passing the metric name as a string.",RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>
Parameters,metric_params,dict,None,Additional keyword arguments for the metric function.,RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>
Attributes,n_jobs,int,None,The number of parallel jobs to run for neighbors search. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>
Attributes,effective_metric_,str or callable,,"The distance metric to use. It will be same as the `metric` parameter or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to 'minkowski' and `p` parameter set to 2.",RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>
Attributes,effective_metric_params_,dict,,"Additional keyword arguments for the metric function. For most metrics will be same with `metric_params` parameter, but may also contain the `p` parameter value if the `effective_metric_` attribute is set to 'minkowski'.",RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>
,n_samples_fit_,int,,Number of samples in the fitted data.,RadiusNeighborsRegressor,<class 'sklearn.neighbors._regression.RadiusNeighborsRegressor'>
Parameters,mode,{'distance','distance',"Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, and 'distance' will return the distances between neighbors according to the given metric.",RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>
Parameters,radius,float,1.0,Radius of neighborhood in the transformed sparse graph.,RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>
Parameters,algorithm,{'auto','auto',"Algorithm used to compute the nearest neighbors: - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDTree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method. Note: fitting on sparse input will override the setting of this parameter, using brute force.",RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>
Parameters,leaf_size,int,30,"Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem.",RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>
Parameters,metric,str or callable,'minkowski',"Metric to use for distance computation. Default is ""minkowski"", which results in the standard Euclidean distance when p = 2. See the documentation of `scipy.spatial.distance <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric values. If metric is a callable function, it takes two arrays representing 1D vectors as inputs and must return one value indicating the distance between those vectors. This works for Scipy's metrics, but is less efficient than passing the metric name as a string. Distance matrices are not supported.",RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>
Parameters,p,float,2,"Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected to be positive.",RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>
Parameters,metric_params,dict,None,Additional keyword arguments for the metric function.,RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>
Attributes,n_jobs,int,None,"The number of parallel jobs to run for neighbors search. If ``-1``, then the number of jobs is set to the number of CPU cores.",RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>
Attributes,effective_metric_,str or callable,,"The distance metric used. It will be same as the `metric` parameter or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to 'minkowski' and `p` parameter set to 2.",RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>
Attributes,effective_metric_params_,dict,,"Additional keyword arguments for the metric function. For most metrics will be same with `metric_params` parameter, but may also contain the `p` parameter value if the `effective_metric_` attribute is set to 'minkowski'.",RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>
,n_samples_fit_,int,,Number of samples in the fitted data.,RadiusNeighborsTransformer,<class 'sklearn.neighbors._graph.RadiusNeighborsTransformer'>
Parameters,n_estimators,int,100,The number of trees in the forest. The default value of ``n_estimators`` changed from 10 to 100 in 0.22.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,criterion,"{""gini""","""gini""","The function to measure the quality of a split. Supported criteria are ""gini"" for the Gini impurity and ""log_loss"" and ""entropy"" both for the Shannon information gain, see :ref:`tree_mathematical_formulation`. Note: This parameter is tree-specific.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,max_depth,int,None,"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. Added float values for fractions.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. Added float values for fractions.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,min_weight_fraction_leaf,float,0.0,The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,max_features,"{""sqrt""","""sqrt""","The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `max(1, int(max_features * n_features_in_))` features are considered at each split. - If ""sqrt"", then `max_features=sqrt(n_features)`. - If ""log2"", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. The default of `max_features` changed from `""auto""` to `""sqrt""`. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,max_leaf_nodes,int,None,Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,bootstrap,bool,True,"Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,oob_score,bool or callable,False,"Whether to use out-of-bag samples to estimate the generalization score. By default, :func:`~sklearn.metrics.accuracy_score` is used. Provide a callable with signature `metric(y_true, y_pred)` to use a custom metric. Only available if `bootstrap=True`.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,n_jobs,int,None,"The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`, :meth:`decision_path` and :meth:`apply` are all parallelized over the trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,random_state,int,None,Controls both the randomness of the bootstrapping of the samples used when building trees (if ``bootstrap=True``) and the sampling of the features to consider when looking for the best split at each node (if ``max_features < n_features``). See :term:`Glossary <random_state>` for details.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,verbose,int,0,Controls the verbosity when fitting and predicting.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See :term:`Glossary <warm_start>` and :ref:`tree_ensemble_warm_start` for details.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,class_weight,"{""balanced""",None,"Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` The ""balanced_subsample"" mode is the same as ""balanced"" except that weights are computed based on the bootstrap sample for every tree grown. For multi-output, the weights of each column of y will be multiplied. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,ccp_alpha,non-negative float,0.0,"Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. See :ref:`minimal_cost_complexity_pruning` for details. See :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py` for an example of such pruning.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,max_samples,int or float,None,"If bootstrap is True, the number of samples to draw from X to train each base estimator. - If None (default), then draw `X.shape[0]` samples. - If int, then draw `max_samples` samples. - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus, `max_samples` should be in the interval `(0.0, 1.0]`.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,monotonic_cst,array-like of int of shape (n_features),None,"Indicates the monotonicity constraint to enforce on each feature. - 1: monotonic increase - 0: no constraint - -1: monotonic decrease If monotonic_cst is None, no constraints are applied. Monotonicity constraints are not supported for: - multiclass classifications (i.e. when `n_classes > 2`), - multioutput classifications (i.e. when `n_outputs_ > 1`), - classifications trained on data with missing values. The constraints hold over the probability of the positive class. Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,estimator_,:class:`~sklearn.tree.DecisionTreeClassifier`,,The child estimator template used to create the collection of fitted sub-estimators. `base_estimator_` was renamed to `estimator_`.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,estimators_,list of DecisionTreeClassifier,,The collection of fitted sub-estimators.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,"The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,n_classes_,int or list,,"The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,n_outputs_,int,,The number of outputs when ``fit`` is performed.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,feature_importances_,ndarray of shape (n_features,,"The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.  It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,oob_score_,float,,Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when ``oob_score`` is True.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,oob_decision_function_,ndarray of shape (n_samples,,"Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob_decision_function_` might contain NaN. This attribute exists only when ``oob_score`` is True.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
,estimators_samples_,list of arrays,,"The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each subset is defined by an array of the indices selected.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,n_estimators,int,100,The number of trees in the forest. The default value of ``n_estimators`` changed from 10 to 100 in 0.22.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,criterion,"{""gini""","""gini""","The function to measure the quality of a split. Supported criteria are ""gini"" for the Gini impurity and ""log_loss"" and ""entropy"" both for the Shannon information gain, see :ref:`tree_mathematical_formulation`. Note: This parameter is tree-specific.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,max_depth,int,None,"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. Added float values for fractions.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. Added float values for fractions.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,min_weight_fraction_leaf,float,0.0,The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,max_features,"{""sqrt""","""sqrt""","The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `max(1, int(max_features * n_features_in_))` features are considered at each split. - If ""sqrt"", then `max_features=sqrt(n_features)`. - If ""log2"", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`. The default of `max_features` changed from `""auto""` to `""sqrt""`. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,max_leaf_nodes,int,None,Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,bootstrap,bool,True,"Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,oob_score,bool or callable,False,"Whether to use out-of-bag samples to estimate the generalization score. By default, :func:`~sklearn.metrics.accuracy_score` is used. Provide a callable with signature `metric(y_true, y_pred)` to use a custom metric. Only available if `bootstrap=True`.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,n_jobs,int,None,"The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`, :meth:`decision_path` and :meth:`apply` are all parallelized over the trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,random_state,int,None,Controls both the randomness of the bootstrapping of the samples used when building trees (if ``bootstrap=True``) and the sampling of the features to consider when looking for the best split at each node (if ``max_features < n_features``). See :term:`Glossary <random_state>` for details.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,verbose,int,0,Controls the verbosity when fitting and predicting.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See :term:`Glossary <warm_start>` and :ref:`tree_ensemble_warm_start` for details.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,class_weight,"{""balanced""",None,"Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` The ""balanced_subsample"" mode is the same as ""balanced"" except that weights are computed based on the bootstrap sample for every tree grown. For multi-output, the weights of each column of y will be multiplied. Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,ccp_alpha,non-negative float,0.0,"Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. See :ref:`minimal_cost_complexity_pruning` for details. See :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py` for an example of such pruning.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,max_samples,int or float,None,"If bootstrap is True, the number of samples to draw from X to train each base estimator. - If None (default), then draw `X.shape[0]` samples. - If int, then draw `max_samples` samples. - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus, `max_samples` should be in the interval `(0.0, 1.0]`.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,monotonic_cst,array-like of int of shape (n_features),None,"Indicates the monotonicity constraint to enforce on each feature. - 1: monotonic increase - 0: no constraint - -1: monotonic decrease If monotonic_cst is None, no constraints are applied. Monotonicity constraints are not supported for: - multiclass classifications (i.e. when `n_classes > 2`), - multioutput classifications (i.e. when `n_outputs_ > 1`), - classifications trained on data with missing values. The constraints hold over the probability of the positive class. Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,estimator_,:class:`~sklearn.tree.DecisionTreeClassifier`,,The child estimator template used to create the collection of fitted sub-estimators. `base_estimator_` was renamed to `estimator_`.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,estimators_,list of DecisionTreeClassifier,,The collection of fitted sub-estimators.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,"The classes labels (single output problem), or a list of arrays of class labels (multi-output problem).",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,n_classes_,int or list,,"The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem).",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,n_outputs_,int,,The number of outputs when ``fit`` is performed.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,feature_importances_,ndarray of shape (n_features,,"The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.  It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,oob_score_,float,,Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when ``oob_score`` is True.,RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Attributes,oob_decision_function_,ndarray of shape (n_samples,,"Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob_decision_function_` might contain NaN. This attribute exists only when ``oob_score`` is True.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
,estimators_samples_,list of arrays,,"The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each subset is defined by an array of the indices selected.",RandomForestClassifier,<class 'sklearn.ensemble._forest.RandomForestClassifier'>
Parameters,n_estimators,int,100,The number of trees in the forest. The default value of ``n_estimators`` changed from 10 to 100 in 0.22.,RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,criterion,"{""squared_error""","""squared_error""","The function to measure the quality of a split. Supported criteria are ""squared_error"" for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, ""friedman_mse"", which uses mean squared error with Friedman's improvement score for potential splits, ""absolute_error"" for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and ""poisson"" which uses reduction in Poisson deviance to find splits. Training using ""absolute_error"" is significantly slower than when using ""squared_error"". Mean Absolute Error (MAE) criterion. Poisson criterion.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,max_depth,int,None,"The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split. Added float values for fractions.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node. Added float values for fractions.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,min_weight_fraction_leaf,float,0.0,The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.,RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,max_features,"{""sqrt""",1.0,"The number of features to consider when looking for the best split: - If int, then consider `max_features` features at each split. - If float, then `max_features` is a fraction and `max(1, int(max_features * n_features_in_))` features are considered at each split. - If ""sqrt"", then `max_features=sqrt(n_features)`. - If ""log2"", then `max_features=log2(n_features)`. - If None or 1.0, then `max_features=n_features`. The default of 1.0 is equivalent to bagged trees and more randomness can be achieved by setting smaller values, e.g. 0.3. The default of `max_features` changed from `""auto""` to 1.0. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,max_leaf_nodes,int,None,Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.,RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,bootstrap,bool,True,"Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,oob_score,bool or callable,False,"Whether to use out-of-bag samples to estimate the generalization score. By default, :func:`~sklearn.metrics.r2_score` is used. Provide a callable with signature `metric(y_true, y_pred)` to use a custom metric. Only available if `bootstrap=True`.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,n_jobs,int,None,"The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`, :meth:`decision_path` and :meth:`apply` are all parallelized over the trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,random_state,int,None,Controls both the randomness of the bootstrapping of the samples used when building trees (if ``bootstrap=True``) and the sampling of the features to consider when looking for the best split at each node (if ``max_features < n_features``). See :term:`Glossary <random_state>` for details.,RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,verbose,int,0,Controls the verbosity when fitting and predicting.,RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See :term:`Glossary <warm_start>` and :ref:`tree_ensemble_warm_start` for details.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,ccp_alpha,non-negative float,0.0,"Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ``ccp_alpha`` will be chosen. By default, no pruning is performed. See :ref:`minimal_cost_complexity_pruning` for details. See :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py` for an example of such pruning.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,max_samples,int or float,None,"If bootstrap is True, the number of samples to draw from X to train each base estimator. - If None (default), then draw `X.shape[0]` samples. - If int, then draw `max_samples` samples. - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus, `max_samples` should be in the interval `(0.0, 1.0]`.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Attributes,monotonic_cst,array-like of int of shape (n_features),None,"Indicates the monotonicity constraint to enforce on each feature. - 1: monotonically increasing - 0: no constraint - -1: monotonically decreasing If monotonic_cst is None, no constraints are applied. Monotonicity constraints are not supported for: - multioutput regressions (i.e. when `n_outputs_ > 1`), - regressions trained on data with missing values. Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Attributes,estimator_,:class:`~sklearn.tree.DecisionTreeRegressor`,,The child estimator template used to create the collection of fitted sub-estimators. `base_estimator_` was renamed to `estimator_`.,RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Attributes,estimators_,list of DecisionTreeRegressor,,The collection of fitted sub-estimators.,RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Attributes,feature_importances_,ndarray of shape (n_features,,"The impurity-based feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.  It is also known as the Gini importance. Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See :func:`sklearn.inspection.permutation_importance` as an alternative.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Attributes,n_outputs_,int,,The number of outputs when ``fit`` is performed.,RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Attributes,oob_score_,float,,Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when ``oob_score`` is True.,RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Attributes,oob_prediction_,ndarray of shape (n_samples,,Prediction computed with out-of-bag estimate on the training set. This attribute exists only when ``oob_score`` is True.,RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
,estimators_samples_,list of arrays,,"The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each subset is defined by an array of the indices selected.",RandomForestRegressor,<class 'sklearn.ensemble._forest.RandomForestRegressor'>
Parameters,n_estimators,int,100,Number of trees in the forest. The default value of ``n_estimators`` changed from 10 to 100 in 0.22.,RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Parameters,max_depth,int,5,"The maximum depth of each tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.",RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Parameters,min_samples_split,int or float,2,"The minimum number of samples required to split an internal node: - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` is the minimum number of samples for each split. Added float values for fractions.",RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Parameters,min_samples_leaf,int or float,1,"The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression. - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` is the minimum number of samples for each node. Added float values for fractions.",RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Parameters,min_weight_fraction_leaf,float,0.0,The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.,RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Parameters,max_leaf_nodes,int,None,Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.,RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Parameters,min_impurity_decrease,float,0.0,"A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:: N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) where ``N`` is the total number of samples, ``N_t`` is the number of samples at the current node, ``N_t_L`` is the number of samples in the left child, and ``N_t_R`` is the number of samples in the right child. ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum, if ``sample_weight`` is passed.",RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Parameters,sparse_output,bool,True,"Whether or not to return a sparse CSR matrix, as default behavior, or to return a dense array compatible with dense pipeline operators.",RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Parameters,n_jobs,int,None,"The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`, :meth:`decision_path` and :meth:`apply` are all parallelized over the trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Parameters,random_state,int,None,Controls the generation of the random `y` used to fit the trees and the draw of the splits for each feature at the trees' nodes. See :term:`Glossary <random_state>` for details.,RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Parameters,verbose,int,0,Controls the verbosity when fitting and predicting.,RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Attributes,warm_start,bool,False,"When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See :term:`Glossary <warm_start>` and :ref:`tree_ensemble_warm_start` for details.",RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Attributes,estimator_,:class:`~sklearn.tree.ExtraTreeRegressor` instance,,The child estimator template used to create the collection of fitted sub-estimators. `base_estimator_` was renamed to `estimator_`.,RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Attributes,estimators_,list of :class:`~sklearn.tree.ExtraTreeRegressor` instances,,The collection of fitted sub-estimators.,RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Attributes,feature_importances_,ndarray of shape (n_features,,"The feature importances (the higher, the more important the feature).",RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Attributes,n_outputs_,int,,The number of outputs when ``fit`` is performed.,RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Attributes,one_hot_encoder_,OneHotEncoder instance,,One-hot encoder used to create the sparse embedding.,RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
,estimators_samples_,list of arrays,,"The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each subset is defined by an array of the indices selected.",RandomTreesEmbedding,<class 'sklearn.ensemble._forest.RandomTreesEmbedding'>
Parameters,estimator,estimator object,,"An object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a ``score`` function, or ``scoring`` must be passed.",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Parameters,param_distributions,dict or list of dicts,,"Dictionary with parameters names (`str`) as keys and distributions or lists of parameters to try. Distributions must provide a ``rvs`` method for sampling (such as those from scipy.stats.distributions). If a list is given, it is sampled uniformly. If a list of dicts is given, first a dict is sampled uniformly, and then a parameter is sampled using that dict as above.",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Parameters,n_iter,int,10,Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution.,RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Parameters,scoring,str,None,"Strategy to evaluate the performance of the cross-validated model on the test set. If `scoring` represents a single score, one can use: - a single string (see :ref:`scoring_parameter`); - a callable (see :ref:`scoring_callable`) that returns a single value. If `scoring` represents multiple scores, one can use: - a list or tuple of unique strings; - a callable returning a dictionary where the keys are the metric names and the values are the metric scores; - a dictionary with metric names as keys and callables as values. See :ref:`multimetric_grid_search` for an example. If None, the estimator's score method is used.",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Parameters,n_jobs,int,None,Number of jobs to run in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details. `n_jobs` default changed from 1 to None,RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Parameters,refit,bool,True,"Refit an estimator using the best found parameters on the whole dataset. For multiple metric evaluation, this needs to be a `str` denoting the scorer that would be used to find the best parameters for refitting the estimator at the end. Where there are considerations other than maximum score in choosing a best estimator, ``refit`` can be set to a function which returns the selected ``best_index_`` given the ``cv_results_``. In that case, the ``best_estimator_`` and ``best_params_`` will be set according to the returned ``best_index_`` while the ``best_score_`` attribute will not be available. The refitted estimator is made available at the ``best_estimator_`` attribute and permits using ``predict`` directly on this ``RandomizedSearchCV`` instance. Also for multiple metric evaluation, the attributes ``best_index_``, ``best_score_`` and ``best_params_`` will only be available if ``refit`` is set and all of them will be determined w.r.t this specific scorer. See ``scoring`` parameter to know more about multiple metric evaluation. Support for callable added.",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used. These splitters are instantiated with `shuffle=False` so the splits will be the same across calls. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ``cv`` default value if None changed from 3-fold to 5-fold.",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Parameters,verbose,int,,"Controls the verbosity: the higher, the more messages. - >1 : the computation time for each fold and parameter candidate is displayed; - >2 : the score is also displayed; - >3 : the fold and candidate parameter indexes are also displayed together with the starting time of the computation.",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Parameters,pre_dispatch,int,'2*n_jobs',"Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A str, giving an expression as a function of n_jobs, as in '2*n_jobs'",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Parameters,random_state,int,None,Pseudo random number generator state used for random uniform sampling from lists of possible values instead of scipy.stats distributions. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Parameters,error_score,'raise' or numeric,np.nan,"Value to assign to the score if an error occurs in estimator fitting. If set to 'raise', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Attributes,return_train_score,bool,False,"If ``False``, the ``cv_results_`` attribute will not include training scores. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance. Default value was changed from ``True`` to ``False``",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Attributes,cv_results_,dict of numpy (masked) ndarrays,,"A dict with keys as column headers and values as columns, that can be imported into a pandas ``DataFrame``. For instance the below given table +--------------+-------------+-------------------+---+---------------+ | param_kernel | param_gamma | split0_test_score |...|rank_test_score| +==============+=============+===================+===+===============+ |    'rbf'     |     0.1     |       0.80        |...|       1       | +--------------+-------------+-------------------+---+---------------+ |    'rbf'     |     0.2     |       0.84        |...|       3       | +--------------+-------------+-------------------+---+---------------+ |    'rbf'     |     0.3     |       0.70        |...|       2       | +--------------+-------------+-------------------+---+---------------+ will be represented by a ``cv_results_`` dict of:: { 'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'], mask = False), 'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False), 'split0_test_score'  : [0.80, 0.84, 0.70], 'split1_test_score'  : [0.82, 0.50, 0.70], 'mean_test_score'    : [0.81, 0.67, 0.70], 'std_test_score'     : [0.01, 0.24, 0.00], 'rank_test_score'    : [1, 3, 2], 'split0_train_score' : [0.80, 0.92, 0.70], 'split1_train_score' : [0.82, 0.55, 0.70], 'mean_train_score'   : [0.81, 0.74, 0.70], 'std_train_score'    : [0.01, 0.19, 0.00], 'mean_fit_time'      : [0.73, 0.63, 0.43], 'std_fit_time'       : [0.01, 0.02, 0.01], 'mean_score_time'    : [0.01, 0.06, 0.04], 'std_score_time'     : [0.00, 0.00, 0.00], 'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...], } NOTE The key ``'params'`` is used to store a list of parameter settings dicts for all the parameter candidates. The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and ``std_score_time`` are all in seconds. For multi-metric evaluation, the scores for all the scorers are available in the ``cv_results_`` dict at the keys ending with that scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown above. ('split0_test_precision', 'mean_train_precision' etc.)",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Attributes,best_estimator_,estimator,,"Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if ``refit=False``. For multi-metric evaluation, this attribute is present only if ``refit`` is specified. See ``refit`` parameter for more information on allowed values.",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Attributes,best_score_,float,,"Mean cross-validated score of the best_estimator. For multi-metric evaluation, this is not available if ``refit`` is ``False``. See ``refit`` parameter for more information. This attribute is not available if ``refit`` is a function.",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Attributes,best_params_,dict,,"Parameter setting that gave the best results on the hold out data. For multi-metric evaluation, this is not available if ``refit`` is ``False``. See ``refit`` parameter for more information.",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Attributes,best_index_,int,,"The index (of the ``cv_results_`` arrays) which corresponds to the best candidate parameter setting. The dict at ``search.cv_results_['params'][search.best_index_]`` gives the parameter setting for the best model, that gives the highest mean score (``search.best_score_``). For multi-metric evaluation, this is not available if ``refit`` is ``False``. See ``refit`` parameter for more information.",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Attributes,scorer_,function or a dict,,"Scorer function used on the held out data to choose the best parameters for the model. For multi-metric evaluation, this attribute holds the validated ``scoring`` dict which maps the scorer key to the scorer callable.",RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Attributes,n_splits_,int,,The number of cross-validation splits (folds/iterations).,RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Attributes,refit_time_,float,,Seconds used for refitting the best model on the whole dataset. This is present only if ``refit`` is not False.,RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Attributes,multimetric_,bool,,Whether or not the scorers compute several metrics.,RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Attributes,classes_,ndarray of shape (n_classes,,The classes labels. This is present only if ``refit`` is specified and the underlying estimator is a classifier.,RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if `best_estimator_` is defined (see the documentation for the `refit` parameter for more details) and that `best_estimator_` exposes `n_features_in_` when fit.,RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if `best_estimator_` is defined (see the documentation for the `refit` parameter for more details) and that `best_estimator_` exposes `feature_names_in_` when fit.,RandomizedSearchCV,<class 'sklearn.model_selection._search.RandomizedSearchCV'>
Parameters,base_estimator,estimator,,The base estimator from which the regressor chain is built.,RegressorChain,<class 'sklearn.multioutput.RegressorChain'>
Parameters,order,array-like of shape (n_outputs,None,"If `None`, the order will be determined by the order of columns in the label matrix Y.:: order = [0, 1, 2, ..., Y.shape[1] - 1] The order of the chain can be explicitly set by providing a list of integers. For example, for a chain of length 5.:: order = [1, 3, 2, 4, 0] means that the first model in the chain will make predictions for column 1 in the Y matrix, the second model will make predictions for column 3, etc. If order is 'random' a random ordering will be used.",RegressorChain,<class 'sklearn.multioutput.RegressorChain'>
Parameters,cv,int,None,"Determines whether to use cross validated predictions or true labels for the results of previous estimators in the chain. Possible inputs for cv are: - None, to use true labels when fitting, - integer, to specify the number of folds in a (Stratified)KFold, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices.",RegressorChain,<class 'sklearn.multioutput.RegressorChain'>
Parameters,random_state,int,,"If ``order='random'``, determines random number generation for the chain order. In addition, it controls the random seed given at each `base_estimator` at each chaining iteration. Thus, it is only used when `base_estimator` exposes a `random_state`. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",RegressorChain,<class 'sklearn.multioutput.RegressorChain'>
Attributes,verbose,bool,False,"If True, chain progress is output as each model is completed.",RegressorChain,<class 'sklearn.multioutput.RegressorChain'>
Attributes,estimators_,list,,A list of clones of base_estimator.,RegressorChain,<class 'sklearn.multioutput.RegressorChain'>
Attributes,order_,list,,The order of labels in the classifier chain.,RegressorChain,<class 'sklearn.multioutput.RegressorChain'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying `base_estimator` exposes such an attribute when fit.,RegressorChain,<class 'sklearn.multioutput.RegressorChain'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RegressorChain,<class 'sklearn.multioutput.RegressorChain'>
Parameters,alpha,{float,1.0,"Constant that multiplies the L2 term, controlling regularization strength. `alpha` must be a non-negative float i.e. in `[0, inf)`. When `alpha = 0`, the objective is equivalent to ordinary least squares, solved by the :class:`LinearRegression` object. For numerical reasons, using `alpha = 0` with the `Ridge` object is not advised. Instead, you should use the :class:`LinearRegression` object. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.",Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
Parameters,fit_intercept,bool,True,"Whether to fit the intercept for this model. If set to false, no intercept will be used in calculations (i.e. ``X`` and ``y`` are expected to be centered).",Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
Parameters,copy_X,bool,True,"If True, X will be copied; else, it may be overwritten.",Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
Parameters,max_iter,int,None,"Maximum number of iterations for conjugate gradient solver. For 'sparse_cg' and 'lsqr' solvers, the default value is determined by scipy.sparse.linalg. For 'sag' solver, the default value is 1000. For 'lbfgs' solver, the default value is 15000.",Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
Parameters,tol,float,1e-4,"The precision of the solution (`coef_`) is determined by `tol` which specifies a different convergence criterion for each solver: - 'svd': `tol` has no impact. - 'cholesky': `tol` has no impact. - 'sparse_cg': norm of residuals smaller than `tol`. - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr, which control the norm of the residual vector in terms of the norms of matrix and coefficients. - 'sag' and 'saga': relative change of coef smaller than `tol`. - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals| smaller than `tol`. Default value changed from 1e-3 to 1e-4 for consistency with other linear models.",Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
Parameters,solver,{'auto','auto',"Solver to use in the computational routines: - 'auto' chooses the solver automatically based on the type of data. - 'svd' uses a Singular Value Decomposition of X to compute the Ridge coefficients. It is the most stable solver, in particular more stable for singular matrices than 'cholesky' at the cost of being slower. - 'cholesky' uses the standard scipy.linalg.solve function to obtain a closed-form solution. - 'sparse_cg' uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than 'cholesky' for large-scale data (possibility to set `tol` and `max_iter`). - 'lsqr' uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative procedure. - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses its improved, unbiased version named SAGA. Both methods also use an iterative procedure, and are often faster than other solvers when both n_samples and n_features are large. Note that 'sag' and 'saga' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing. - 'lbfgs' uses L-BFGS-B algorithm implemented in `scipy.optimize.minimize`. It can be used only when `positive` is True. All solvers except 'svd' support both dense and sparse data. However, only 'lsqr', 'sag', 'sparse_cg', and 'lbfgs' support sparse input when `fit_intercept` is True. Stochastic Average Gradient descent solver. SAGA solver.",Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
Parameters,positive,bool,False,"When set to ``True``, forces the coefficients to be positive. Only 'lbfgs' solver is supported in this case.",Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
Attributes,random_state,int,None,Used when ``solver`` == 'sag' or 'saga' to shuffle the data. See :term:`Glossary <random_state>` for details. `random_state` to support Stochastic Average Gradient.,Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
Attributes,coef_,ndarray of shape (n_features,,Weight vector(s).,Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
Attributes,intercept_,float or ndarray of shape (n_targets,,Independent term in decision function. Set to 0.0 if ``fit_intercept = False``.,Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
Attributes,n_iter_,None or ndarray of shape (n_targets,,Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None.,Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
,solver_,str,,The solver that was used at fit time by the computational routines.,Ridge,<class 'sklearn.linear_model._ridge.Ridge'>
Parameters,alphas,array-like of shape (n_alphas,(0.1,"Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear models such as :class:`~sklearn.linear_model.LogisticRegression` or :class:`~sklearn.svm.LinearSVC`. If using Leave-One-Out cross-validation, alphas must be strictly positive.",RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Parameters,scoring,str,None,"A string (see :ref:`scoring_parameter`) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. If None, the negative mean squared error if cv is 'auto' or None (i.e. when using leave-one-out cross-validation), and r2 score otherwise.",RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the efficient Leave-One-Out cross-validation - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if ``y`` is binary or multiclass, :class:`~sklearn.model_selection.StratifiedKFold` is used, else, :class:`~sklearn.model_selection.KFold` is used. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.",RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Parameters,gcv_mode,{'auto','auto',"Flag indicating which strategy to use when performing Leave-One-Out Cross-Validation. Options are:: 'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen' 'svd' : force use of singular value decomposition of X when X is dense, eigenvalue decomposition of X^T.X when X is sparse. 'eigen' : force computation via eigendecomposition of X.X^T The 'auto' mode is the default and is intended to pick the cheaper option of the two depending on the shape of the training data.",RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Parameters,store_cv_results,bool,False,Flag indicating if the cross-validation values corresponding to each alpha should be stored in the ``cv_results_`` attribute (see below). This flag is only compatible with ``cv=None`` (i.e. using Leave-One-Out Cross-Validation). Parameter name changed from `store_cv_values` to `store_cv_results`.,RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Parameters,alpha_per_target,bool,False,"Flag indicating whether to optimize the alpha value (picked from the `alphas` parameter list) for each target separately (for multi-output settings: multiple prediction targets). When set to `True`, after fitting, the `alpha_` attribute will contain a value for each target. When set to `False`, a single alpha is used for all targets.",RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Attributes,store_cv_values,bool,,Flag indicating if the cross-validation values corresponding to each alpha should be stored in the ``cv_values_`` attribute (see below). This flag is only compatible with ``cv=None`` (i.e. using Leave-One-Out Cross-Validation). `store_cv_values` is deprecated in version 1.5 in favor of `store_cv_results` and will be removed in version 1.7.,RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Attributes,cv_results_,ndarray of shape (n_samples,,"Cross-validation values for each alpha (only available if ``store_cv_results=True`` and ``cv=None``). After ``fit()`` has been called, this attribute will contain the mean squared errors if `scoring is None` otherwise it will contain standardized per point prediction values. `cv_values_` changed to `cv_results_`.",RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Attributes,coef_,ndarray of shape (n_features) or (n_targets,,Weight vector(s).,RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Attributes,intercept_,float or ndarray of shape (n_targets,,Independent term in decision function. Set to 0.0 if ``fit_intercept = False``.,RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Attributes,alpha_,float or ndarray of shape (n_targets,,"Estimated regularization parameter, or, if ``alpha_per_target=True``, the estimated regularization parameter for each target.",RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Attributes,best_score_,float or ndarray of shape (n_targets,,"Score of base estimator with best alpha, or, if ``alpha_per_target=True``, a score for each target.",RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RidgeCV,<class 'sklearn.linear_model._ridge.RidgeCV'>
Parameters,alpha,float,1.0,Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear models such as :class:`~sklearn.linear_model.LogisticRegression` or :class:`~sklearn.svm.LinearSVC`.,RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).",RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Parameters,copy_X,bool,True,"If True, X will be copied; else, it may be overwritten.",RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Parameters,max_iter,int,None,Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg.,RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Parameters,tol,float,1e-4,"The precision of the solution (`coef_`) is determined by `tol` which specifies a different convergence criterion for each solver: - 'svd': `tol` has no impact. - 'cholesky': `tol` has no impact. - 'sparse_cg': norm of residuals smaller than `tol`. - 'lsqr': `tol` is set as atol and btol of scipy.sparse.linalg.lsqr, which control the norm of the residual vector in terms of the norms of matrix and coefficients. - 'sag' and 'saga': relative change of coef smaller than `tol`. - 'lbfgs': maximum of the absolute (projected) gradient=max|residuals| smaller than `tol`. Default value changed from 1e-3 to 1e-4 for consistency with other linear models.",RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Parameters,class_weight,dict or 'balanced',None,"Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.",RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Parameters,solver,{'auto','auto',"Solver to use in the computational routines: - 'auto' chooses the solver automatically based on the type of data. - 'svd' uses a Singular Value Decomposition of X to compute the Ridge coefficients. It is the most stable solver, in particular more stable for singular matrices than 'cholesky' at the cost of being slower. - 'cholesky' uses the standard scipy.linalg.solve function to obtain a closed-form solution. - 'sparse_cg' uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than 'cholesky' for large-scale data (possibility to set `tol` and `max_iter`). - 'lsqr' uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative procedure. - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses its unbiased and more flexible version named SAGA. Both methods use an iterative procedure, and are often faster than other solvers when both n_samples and n_features are large. Note that 'sag' and 'saga' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing. Stochastic Average Gradient descent solver. SAGA solver. - 'lbfgs' uses L-BFGS-B algorithm implemented in `scipy.optimize.minimize`. It can be used only when `positive` is True.",RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Parameters,positive,bool,False,"When set to ``True``, forces the coefficients to be positive. Only 'lbfgs' solver is supported in this case.",RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Attributes,random_state,int,None,Used when ``solver`` == 'sag' or 'saga' to shuffle the data. See :term:`Glossary <random_state>` for details.,RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Attributes,coef_,ndarray of shape (1,,"Coefficient of the features in the decision function. ``coef_`` is of shape (1, n_features) when the given problem is binary.",RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Attributes,intercept_,float or ndarray of shape (n_targets,,Independent term in decision function. Set to 0.0 if ``fit_intercept = False``.,RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Attributes,n_iter_,None or ndarray of shape (n_targets,,Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None.,RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,The classes labels.,RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
,solver_,str,,The solver that was used at fit time by the computational routines.,RidgeClassifier,<class 'sklearn.linear_model._ridge.RidgeClassifier'>
Parameters,alphas,array-like of shape (n_alphas,(0.1,"Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``1 / (2C)`` in other linear models such as :class:`~sklearn.linear_model.LogisticRegression` or :class:`~sklearn.svm.LinearSVC`. If using Leave-One-Out cross-validation, alphas must be strictly positive.",RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).",RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Parameters,scoring,str,None,"A string (see :ref:`scoring_parameter`) or a scorer callable object / function with signature ``scorer(estimator, X, y)``.",RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the efficient Leave-One-Out cross-validation - integer, to specify the number of folds. - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.",RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Parameters,class_weight,dict or 'balanced',None,"Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.",RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Parameters,store_cv_results,bool,False,Flag indicating if the cross-validation results corresponding to each alpha should be stored in the ``cv_results_`` attribute (see below). This flag is only compatible with ``cv=None`` (i.e. using Leave-One-Out Cross-Validation). Parameter name changed from `store_cv_values` to `store_cv_results`.,RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Attributes,store_cv_values,bool,,Flag indicating if the cross-validation values corresponding to each alpha should be stored in the ``cv_values_`` attribute (see below). This flag is only compatible with ``cv=None`` (i.e. using Leave-One-Out Cross-Validation). `store_cv_values` is deprecated in version 1.5 in favor of `store_cv_results` and will be removed in version 1.7.,RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Attributes,cv_results_,ndarray of shape (n_samples,,"Cross-validation results for each alpha (only if ``store_cv_results=True`` and ``cv=None``). After ``fit()`` has been called, this attribute will contain the mean squared errors if `scoring is None` otherwise it will contain standardized per point prediction values. `cv_values_` changed to `cv_results_`.",RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Attributes,coef_,ndarray of shape (1,,"Coefficient of the features in the decision function. ``coef_`` is of shape (1, n_features) when the given problem is binary.",RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Attributes,intercept_,float or ndarray of shape (n_targets,,Independent term in decision function. Set to 0.0 if ``fit_intercept = False``.,RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Attributes,alpha_,float,,Estimated regularization parameter.,RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Attributes,best_score_,float,,Score of base estimator with best alpha.,RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Attributes,classes_,ndarray of shape (n_classes,,The classes labels.,RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RidgeClassifierCV,<class 'sklearn.linear_model._ridge.RidgeClassifierCV'>
Parameters,with_centering,bool,True,"If `True`, center the data before scaling. This will cause :meth:`transform` to raise an exception when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.",RobustScaler,<class 'sklearn.preprocessing._data.RobustScaler'>
Parameters,with_scaling,bool,True,"If `True`, scale the data to interquartile range.",RobustScaler,<class 'sklearn.preprocessing._data.RobustScaler'>
Parameters,quantile_range,tuple (q_min,(25.0,"Quantile range used to calculate `scale_`. By default this is equal to the IQR, i.e., `q_min` is the first quantile and `q_max` is the third quantile.",RobustScaler,<class 'sklearn.preprocessing._data.RobustScaler'>
Parameters,copy,bool,True,"If `False`, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.",RobustScaler,<class 'sklearn.preprocessing._data.RobustScaler'>
Attributes,unit_variance,bool,False,"If `True`, scale data so that normally distributed features have a variance of 1. In general, if the difference between the x-values of `q_max` and `q_min` for a standard normal distribution is greater than 1, the dataset will be scaled down. If less than 1, the dataset will be scaled up.",RobustScaler,<class 'sklearn.preprocessing._data.RobustScaler'>
Attributes,center_,array of floats,,The median value for each feature in the training set.,RobustScaler,<class 'sklearn.preprocessing._data.RobustScaler'>
Attributes,scale_,array of floats,,The (scaled) interquartile range for each feature in the training set. *scale_* attribute.,RobustScaler,<class 'sklearn.preprocessing._data.RobustScaler'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,RobustScaler,<class 'sklearn.preprocessing._data.RobustScaler'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,RobustScaler,<class 'sklearn.preprocessing._data.RobustScaler'>
Parameters,loss,{'hinge','hinge',"The loss function to be used. - 'hinge' gives a linear SVM. - 'log_loss' gives logistic regression, a probabilistic classifier. - 'modified_huber' is another smooth loss that brings tolerance to outliers as well as probability estimates. - 'squared_hinge' is like hinge but is quadratically penalized. - 'perceptron' is the linear loss used by the perceptron algorithm. - The other losses, 'squared_error', 'huber', 'epsilon_insensitive' and 'squared_epsilon_insensitive' are designed for regression but can be useful in classification as well; see :class:`~sklearn.linear_model.SGDRegressor` for a description. More details about the losses formulas can be found in the :ref:`User Guide <sgd_mathematical_formulation>` and you can find a visualisation of the loss functions in :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_loss_functions.py`.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,penalty,{'l2','l2',The penalty (aka regularization term) to be used. Defaults to 'l2' which is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might bring sparsity to the model (feature selection) not achievable with 'l2'. No penalty is added when set to `None`. You can see a visualisation of the penalties in :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`.,SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,alpha,float,0.0001,"Constant that multiplies the regularization term. The higher the value, the stronger the regularization. Also used to compute the learning rate when `learning_rate` is set to 'optimal'. Values must be in the range `[0.0, inf)`.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,l1_ratio,float,0.15,"The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Only used if `penalty` is 'elasticnet'. Values must be in the range `[0.0, 1.0]`.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,fit_intercept,bool,True,"Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,max_iter,int,1000,"The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method. Values must be in the range `[1, inf)`.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,tol,float or None,1e-3,"The stopping criterion. If it is not None, training will stop when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive epochs. Convergence is checked against the training loss or the validation loss depending on the `early_stopping` parameter. Values must be in the range `[0.0, inf)`.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,shuffle,bool,True,Whether or not the training data should be shuffled after each epoch.,SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,verbose,int,0,"The verbosity level. Values must be in the range `[0, inf)`.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,epsilon,float,0.1,"Epsilon in the epsilon-insensitive loss functions; only if `loss` is 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'. For 'huber', determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold. Values must be in the range `[0.0, inf)`.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,n_jobs,int,None,"The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,random_state,int,None,"Used for shuffling the data, when ``shuffle`` is set to ``True``. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`. Integer values must be in the range `[0, 2**32 - 1]`.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,learning_rate,str,'optimal',"The learning rate schedule: - 'constant': `eta = eta0` - 'optimal': `eta = 1.0 / (alpha * (t + t0))` where `t0` is chosen by a heuristic proposed by Leon Bottou. - 'invscaling': `eta = eta0 / pow(t, power_t)` - 'adaptive': `eta = eta0`, as long as the training keeps decreasing. Each time n_iter_no_change consecutive epochs fail to decrease the training loss by tol or fail to increase validation score by tol if `early_stopping` is `True`, the current learning rate is divided by 5. Added 'adaptive' option.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,eta0,float,0.0,"The initial learning rate for the 'constant', 'invscaling' or 'adaptive' schedules. The default value is 0.0 as eta0 is not used by the default schedule 'optimal'. Values must be in the range `[0.0, inf)`.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,power_t,float,0.5,"The exponent for inverse scaling learning rate. Values must be in the range `(-inf, inf)`.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,early_stopping,bool,False,"Whether to use early stopping to terminate training when validation score is not improving. If set to `True`, it will automatically set aside a stratified fraction of training data as validation and terminate training when validation score returned by the `score` method is not improving by at least tol for n_iter_no_change consecutive epochs. See :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_early_stopping.py` for an example of the effects of early stopping. Added 'early_stopping' option",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,validation_fraction,float,0.1,"The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if `early_stopping` is True. Values must be in the range `(0.0, 1.0)`. Added 'validation_fraction' option",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,n_iter_no_change,int,5,"Number of iterations with no improvement to wait before stopping fitting. Convergence is checked against the training loss or the validation loss depending on the `early_stopping` parameter. Integer values must be in the range `[1, max_iter)`. Added 'n_iter_no_change' option",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,class_weight,dict,None,"Preset for the class_weight fit parameter. Weights associated with classes. If not given, all classes are supposed to have weight one. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`. Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled. If a dynamic learning rate is used, the learning rate is adapted depending on the number of samples already seen. Calling ``fit`` resets this counter, while ``partial_fit`` will result in increasing the existing counter.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Attributes,average,bool or int,False,"When set to `True`, computes the averaged SGD weights across all updates and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches `average`. So ``average=10`` will begin averaging after seeing 10 samples. Integer values must be in the range `[1, n_samples]`.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Attributes,coef_,ndarray of shape (1,,Weights assigned to the features.,SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Attributes,intercept_,ndarray of shape (1,,Constants in decision function.,SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Attributes,n_iter_,int,,"The actual number of iterations before reaching the stopping criterion. For multiclass fits, it is the maximum over every binary fit.",SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Attributes,classes_,array of shape (n_classes,,,SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Attributes,t_,int,,Number of weight updates performed during training. Same as ``(n_iter_ * n_samples + 1)``.,SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SGDClassifier,<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>
Parameters,nu,float,0.5,"The nu parameter of the One Class SVM: an upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]. By default 0.5 will be taken.",SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Parameters,fit_intercept,bool,True,Whether the intercept should be estimated or not. Defaults to True.,SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Parameters,max_iter,int,1000,"The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the `partial_fit`. Defaults to 1000. Values must be in the range `[1, inf)`.",SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Parameters,tol,float or None,1e-3,"The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol). Defaults to 1e-3. Values must be in the range `[0.0, inf)`.",SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Parameters,shuffle,bool,True,Whether or not the training data should be shuffled after each epoch. Defaults to True.,SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Parameters,verbose,int,0,The verbosity level.,SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Parameters,random_state,int,None,"The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.",SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Parameters,learning_rate,{'constant','optimal',"The learning rate schedule to use with `fit`. (If using `partial_fit`, learning rate must be controlled directly). - 'constant': `eta = eta0` - 'optimal': `eta = 1.0 / (alpha * (t + t0))` where t0 is chosen by a heuristic proposed by Leon Bottou. - 'invscaling': `eta = eta0 / pow(t, power_t)` - 'adaptive': eta = eta0, as long as the training keeps decreasing. Each time n_iter_no_change consecutive epochs fail to decrease the training loss by tol or fail to increase validation score by tol if early_stopping is True, the current learning rate is divided by 5.",SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Parameters,eta0,float,0.0,"The initial learning rate for the 'constant', 'invscaling' or 'adaptive' schedules. The default value is 0.0 as eta0 is not used by the default schedule 'optimal'. Values must be in the range `[0.0, inf)`.",SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Parameters,power_t,float,0.5,"The exponent for inverse scaling learning rate. Values must be in the range `(-inf, inf)`.",SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Parameters,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`. Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled. If a dynamic learning rate is used, the learning rate is adapted depending on the number of samples already seen. Calling ``fit`` resets this counter, while ``partial_fit``  will result in increasing the existing counter.",SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Attributes,average,bool or int,False,"When set to True, computes the averaged SGD weights and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So ``average=10`` will begin averaging after seeing 10 samples.",SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Attributes,coef_,ndarray of shape (1,,Weights assigned to the features.,SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Attributes,offset_,ndarray of shape (1,,Offset used to define the decision function from the raw scores. We have the relation: decision_function = score_samples - offset.,SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Attributes,n_iter_,int,,The actual number of iterations to reach the stopping criterion.,SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Attributes,t_,int,,Number of weight updates performed during training. Same as ``(n_iter_ * n_samples + 1)``.,SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SGDOneClassSVM,<class 'sklearn.linear_model._stochastic_gradient.SGDOneClassSVM'>
Parameters,loss,str,'squared_error',"The loss function to be used. The possible values are 'squared_error', 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive' The 'squared_error' refers to the ordinary least squares fit. 'huber' modifies 'squared_error' to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is linear past that; this is the loss function used in SVR. 'squared_epsilon_insensitive' is the same but becomes squared loss past a tolerance of epsilon. More details about the losses formulas can be found in the :ref:`User Guide <sgd_mathematical_formulation>`.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,penalty,{'l2','l2',The penalty (aka regularization term) to be used. Defaults to 'l2' which is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might bring sparsity to the model (feature selection) not achievable with 'l2'. No penalty is added when set to `None`. You can see a visualisation of the penalties in :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_penalties.py`.,SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,alpha,float,0.0001,"Constant that multiplies the regularization term. The higher the value, the stronger the regularization. Also used to compute the learning rate when `learning_rate` is set to 'optimal'. Values must be in the range `[0.0, inf)`.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,l1_ratio,float,0.15,"The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Only used if `penalty` is 'elasticnet'. Values must be in the range `[0.0, 1.0]`.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,fit_intercept,bool,True,"Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,max_iter,int,1000,"The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the ``fit`` method, and not the :meth:`partial_fit` method. Values must be in the range `[1, inf)`.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,tol,float or None,1e-3,"The stopping criterion. If it is not None, training will stop when (loss > best_loss - tol) for ``n_iter_no_change`` consecutive epochs. Convergence is checked against the training loss or the validation loss depending on the `early_stopping` parameter. Values must be in the range `[0.0, inf)`.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,shuffle,bool,True,Whether or not the training data should be shuffled after each epoch.,SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,verbose,int,0,"The verbosity level. Values must be in the range `[0, inf)`.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,epsilon,float,0.1,"Epsilon in the epsilon-insensitive loss functions; only if `loss` is 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'. For 'huber', determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold. Values must be in the range `[0.0, inf)`.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,random_state,int,None,"Used for shuffling the data, when ``shuffle`` is set to ``True``. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,learning_rate,str,'invscaling',"The learning rate schedule: - 'constant': `eta = eta0` - 'optimal': `eta = 1.0 / (alpha * (t + t0))` where t0 is chosen by a heuristic proposed by Leon Bottou. - 'invscaling': `eta = eta0 / pow(t, power_t)` - 'adaptive': eta = eta0, as long as the training keeps decreasing. Each time n_iter_no_change consecutive epochs fail to decrease the training loss by tol or fail to increase validation score by tol if early_stopping is True, the current learning rate is divided by 5. Added 'adaptive' option.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,eta0,float,0.01,"The initial learning rate for the 'constant', 'invscaling' or 'adaptive' schedules. The default value is 0.01. Values must be in the range `[0.0, inf)`.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,power_t,float,0.25,"The exponent for inverse scaling learning rate. Values must be in the range `(-inf, inf)`.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,early_stopping,bool,False,"Whether to use early stopping to terminate training when validation score is not improving. If set to True, it will automatically set aside a fraction of training data as validation and terminate training when validation score returned by the `score` method is not improving by at least `tol` for `n_iter_no_change` consecutive epochs. See :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_early_stopping.py` for an example of the effects of early stopping. Added 'early_stopping' option",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,validation_fraction,float,0.1,"The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if `early_stopping` is True. Values must be in the range `(0.0, 1.0)`. Added 'validation_fraction' option",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,n_iter_no_change,int,5,"Number of iterations with no improvement to wait before stopping fitting. Convergence is checked against the training loss or the validation loss depending on the `early_stopping` parameter. Integer values must be in the range `[1, max_iter)`. Added 'n_iter_no_change' option",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,warm_start,bool,False,"When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. See :term:`the Glossary <warm_start>`. Repeatedly calling fit or partial_fit when warm_start is True can result in a different solution than when calling fit a single time because of the way the data is shuffled. If a dynamic learning rate is used, the learning rate is adapted depending on the number of samples already seen. Calling ``fit`` resets this counter, while ``partial_fit``  will result in increasing the existing counter.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Attributes,average,bool or int,False,"When set to True, computes the averaged SGD weights across all updates and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches `average`. So ``average=10`` will begin averaging after seeing 10 samples.",SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Attributes,coef_,ndarray of shape (n_features,,Weights assigned to the features.,SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Attributes,intercept_,ndarray of shape (1,,The intercept term.,SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Attributes,n_iter_,int,,The actual number of iterations before reaching the stopping criterion.,SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Attributes,t_,int,,Number of weight updates performed during training. Same as ``(n_iter_ * n_samples + 1)``.,SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SGDRegressor,<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>
Parameters,C,float,1.0,"Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty. For an intuitive visualization of the effects of scaling the regularization parameter C, see :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.",SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,kernel,{'linear','rbf',"Specifies the kernel type to be used in the algorithm. If none is given, 'rbf' will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape ``(n_samples, n_samples)``. For an intuitive visualization of different kernel types see :ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`.",SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,degree,int,3,Degree of the polynomial kernel function ('poly'). Must be non-negative. Ignored by all other kernels.,SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,gamma,{'scale','scale',"Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features - if float, must be non-negative. The default value of ``gamma`` changed from 'auto' to 'scale'.",SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,coef0,float,0.0,Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.,SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,shrinking,bool,True,Whether to use the shrinking heuristic. See the :ref:`User Guide <shrinking_svm>`.,SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,probability,bool,False,"Whether to enable probability estimates. This must be enabled prior to calling `fit`, will slow down that method as it internally uses 5-fold cross-validation, and `predict_proba` may be inconsistent with `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.",SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,tol,float,1e-3,Tolerance for stopping criterion.,SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,cache_size,float,200,Specify the size of the kernel cache (in MB).,SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,class_weight,dict or 'balanced',None,"Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The ""balanced"" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.",SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,verbose,bool,False,"Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.",SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,max_iter,int,-1,"Hard limit on iterations within solver, or -1 for no limit.",SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,decision_function_shape,{'ovo','ovr',"Whether to return a one-vs-rest ('ovr') decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one ('ovo') decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). However, note that internally, one-vs-one ('ovo') is always used as a multi-class strategy to train models; an ovr matrix is only constructed from the ovo matrix. The parameter is ignored for binary classification. decision_function_shape is 'ovr' by default. *decision_function_shape='ovr'* is recommended. Deprecated *decision_function_shape='ovo' and None*.",SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,break_ties,bool,False,"If true, ``decision_function_shape='ovr'``, and number of classes > 2, :term:`predict` will break ties according to the confidence values of :term:`decision_function`; otherwise the first class among the tied classes is returned. Please note that breaking ties comes at a relatively high computational cost compared to a simple predict. See :ref:`sphx_glr_auto_examples_svm_plot_svm_tie_breaking.py` for an example of its usage with ``decision_function_shape='ovr'``.",SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,random_state,int,None,Controls the pseudo random number generation for shuffling the data for probability estimates. Ignored when `probability` is False. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,class_weight_,ndarray of shape (n_classes,,Multipliers of parameter C for each class. Computed based on the ``class_weight`` parameter.,SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,classes_,ndarray of shape (n_classes,,The classes labels.,SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,coef_,ndarray of shape (n_classes * (n_classes - 1) / 2,,Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. `coef_` is a readonly property derived from `dual_coef_` and `support_vectors_`.,SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,dual_coef_,ndarray of shape (n_classes -1,,"Dual coefficients of the support vector in the decision function (see :ref:`sgd_mathematical_formulation`), multiplied by their targets. For multiclass, coefficient for all 1-vs-1 classifiers. The layout of the coefficients in the multiclass case is somewhat non-trivial. See the :ref:`multi-class section of the User Guide <svm_multi_class>` for details.",SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,fit_status_,int,,"0 if correctly fitted, 1 otherwise (will raise warning)",SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,intercept_,ndarray of shape (n_classes * (n_classes - 1) / 2,,Constants in decision function.,SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,n_iter_,ndarray of shape (n_classes * (n_classes - 1) // 2,,Number of iterations run by the optimization routine to fit the model. The shape of this attribute depends on the number of models optimized which in turn depends on the number of classes.,SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,support_,ndarray of shape (n_SV),,Indices of support vectors.,SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,support_vectors_,ndarray of shape (n_SV,,Support vectors. An empty array if kernel is precomputed.,SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,n_support_,ndarray of shape (n_classes,,Number of support vectors for each class.,SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,probA_,ndarray of shape (n_classes * (n_classes - 1) / 2),,,SVC,<class 'sklearn.svm._classes.SVC'>
Attributes,probB_,ndarray of shape (n_classes * (n_classes - 1) / 2),,"If `probability=True`, it corresponds to the parameters learned in Platt scaling to produce probability estimates from decision values. If `probability=False`, it's an empty array. Platt scaling uses the logistic function ``1 / (1 + exp(decision_value * probA_ + probB_))`` where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For more information on the multiclass case and training procedure see section 8 of [1]_.",SVC,<class 'sklearn.svm._classes.SVC'>
,shape_fit_,tuple of int of shape (n_dimensions_of_X,,Array dimensions of training vector ``X``.,SVC,<class 'sklearn.svm._classes.SVC'>
Parameters,kernel,{'linear','rbf',"Specifies the kernel type to be used in the algorithm. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix. For an intuitive visualization of different kernel types see :ref:`sphx_glr_auto_examples_svm_plot_svm_regression.py`",SVR,<class 'sklearn.svm._classes.SVR'>
Parameters,degree,int,3,Degree of the polynomial kernel function ('poly'). Must be non-negative. Ignored by all other kernels.,SVR,<class 'sklearn.svm._classes.SVR'>
Parameters,gamma,{'scale','scale',"Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. - if ``gamma='scale'`` (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma, - if 'auto', uses 1 / n_features - if float, must be non-negative. The default value of ``gamma`` changed from 'auto' to 'scale'.",SVR,<class 'sklearn.svm._classes.SVR'>
Parameters,coef0,float,0.0,Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.,SVR,<class 'sklearn.svm._classes.SVR'>
Parameters,tol,float,1e-3,Tolerance for stopping criterion.,SVR,<class 'sklearn.svm._classes.SVR'>
Parameters,C,float,1.0,"Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2. For an intuitive visualization of the effects of scaling the regularization parameter C, see :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.",SVR,<class 'sklearn.svm._classes.SVR'>
Parameters,epsilon,float,0.1,Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value. Must be non-negative.,SVR,<class 'sklearn.svm._classes.SVR'>
Parameters,shrinking,bool,True,Whether to use the shrinking heuristic. See the :ref:`User Guide <shrinking_svm>`.,SVR,<class 'sklearn.svm._classes.SVR'>
Parameters,cache_size,float,200,Specify the size of the kernel cache (in MB).,SVR,<class 'sklearn.svm._classes.SVR'>
Parameters,verbose,bool,False,"Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.",SVR,<class 'sklearn.svm._classes.SVR'>
Attributes,max_iter,int,-1,"Hard limit on iterations within solver, or -1 for no limit.",SVR,<class 'sklearn.svm._classes.SVR'>
Attributes,coef_,ndarray of shape (1,,Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. `coef_` is readonly property derived from `dual_coef_` and `support_vectors_`.,SVR,<class 'sklearn.svm._classes.SVR'>
Attributes,dual_coef_,ndarray of shape (1,,Coefficients of the support vector in the decision function.,SVR,<class 'sklearn.svm._classes.SVR'>
Attributes,fit_status_,int,,"0 if correctly fitted, 1 otherwise (will raise warning)",SVR,<class 'sklearn.svm._classes.SVR'>
Attributes,intercept_,ndarray of shape (1,,Constants in decision function.,SVR,<class 'sklearn.svm._classes.SVR'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SVR,<class 'sklearn.svm._classes.SVR'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SVR,<class 'sklearn.svm._classes.SVR'>
Attributes,n_iter_,int,,Number of iterations run by the optimization routine to fit the model.,SVR,<class 'sklearn.svm._classes.SVR'>
Attributes,n_support_,ndarray of shape (1,,Number of support vectors.,SVR,<class 'sklearn.svm._classes.SVR'>
Attributes,shape_fit_,tuple of int of shape (n_dimensions_of_X,,Array dimensions of training vector ``X``.,SVR,<class 'sklearn.svm._classes.SVR'>
Attributes,support_,ndarray of shape (n_SV,,Indices of support vectors.,SVR,<class 'sklearn.svm._classes.SVR'>
,support_vectors_,ndarray of shape (n_SV,,Support vectors.,SVR,<class 'sklearn.svm._classes.SVR'>
Parameters,score_func,callable,f_classif,"Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below ""See Also""). The default function only works with classification tasks.",SelectFdr,<class 'sklearn.feature_selection._univariate_selection.SelectFdr'>
Attributes,alpha,float,5e-2,The highest uncorrected p-value for features to keep.,SelectFdr,<class 'sklearn.feature_selection._univariate_selection.SelectFdr'>
Attributes,scores_,array-like of shape (n_features,,Scores of features.,SelectFdr,<class 'sklearn.feature_selection._univariate_selection.SelectFdr'>
Attributes,pvalues_,array-like of shape (n_features,,p-values of feature scores.,SelectFdr,<class 'sklearn.feature_selection._univariate_selection.SelectFdr'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SelectFdr,<class 'sklearn.feature_selection._univariate_selection.SelectFdr'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SelectFdr,<class 'sklearn.feature_selection._univariate_selection.SelectFdr'>
Parameters,score_func,callable,f_classif,"Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below ""See Also""). The default function only works with classification tasks.",SelectFpr,<class 'sklearn.feature_selection._univariate_selection.SelectFpr'>
Attributes,alpha,float,5e-2,Features with p-values less than `alpha` are selected.,SelectFpr,<class 'sklearn.feature_selection._univariate_selection.SelectFpr'>
Attributes,scores_,array-like of shape (n_features,,Scores of features.,SelectFpr,<class 'sklearn.feature_selection._univariate_selection.SelectFpr'>
Attributes,pvalues_,array-like of shape (n_features,,p-values of feature scores.,SelectFpr,<class 'sklearn.feature_selection._univariate_selection.SelectFpr'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SelectFpr,<class 'sklearn.feature_selection._univariate_selection.SelectFpr'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SelectFpr,<class 'sklearn.feature_selection._univariate_selection.SelectFpr'>
Parameters,estimator,object,,"The base estimator from which the transformer is built. This can be both a fitted (if ``prefit`` is set to True) or a non-fitted estimator. The estimator should have a ``feature_importances_`` or ``coef_`` attribute after fitting. Otherwise, the ``importance_getter`` parameter should be used.",SelectFromModel,<class 'sklearn.feature_selection._from_model.SelectFromModel'>
Parameters,threshold,str or float,None,"The threshold value to use for feature selection. Features whose absolute importance value is greater or equal are kept while the others are discarded. If ""median"" (resp. ""mean""), then the ``threshold`` value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., ""1.25*mean"") may also be used. If None and if the estimator has a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso), the threshold used is 1e-5. Otherwise, ""mean"" is used by default.",SelectFromModel,<class 'sklearn.feature_selection._from_model.SelectFromModel'>
Parameters,prefit,bool,False,"Whether a prefit model is expected to be passed into the constructor directly or not. If `True`, `estimator` must be a fitted estimator. If `False`, `estimator` is fitted and updated by calling `fit` and `partial_fit`, respectively.",SelectFromModel,<class 'sklearn.feature_selection._from_model.SelectFromModel'>
Parameters,norm_order,non-zero int,1,Order of the norm used to filter the vectors of coefficients below ``threshold`` in the case where the ``coef_`` attribute of the estimator is of dimension 2.,SelectFromModel,<class 'sklearn.feature_selection._from_model.SelectFromModel'>
Parameters,max_features,int,None,"The maximum number of features to select. - If an integer, then it specifies the maximum number of features to allow. - If a callable, then it specifies how to calculate the maximum number of features allowed by using the output of `max_features(X)`. - If `None`, then all features are kept. To only select based on ``max_features``, set ``threshold=-np.inf``. `max_features` accepts a callable.",SelectFromModel,<class 'sklearn.feature_selection._from_model.SelectFromModel'>
Attributes,importance_getter,str or callable,'auto',"If 'auto', uses the feature importance either through a ``coef_`` attribute or ``feature_importances_`` attribute of estimator. Also accepts a string that specifies an attribute name/path for extracting feature importance (implemented with `attrgetter`). For example, give `regressor_.coef_` in case of :class:`~sklearn.compose.TransformedTargetRegressor`  or `named_steps.clf.feature_importances_` in case of :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`. If `callable`, overrides the default feature importance getter. The callable is passed with the fitted estimator and it should return importance for each feature.",SelectFromModel,<class 'sklearn.feature_selection._from_model.SelectFromModel'>
Attributes,estimator_,estimator,,"The base estimator from which the transformer is built. This attribute exist only when `fit` has been called. - If `prefit=True`, it is a deep copy of `estimator`. - If `prefit=False`, it is a clone of `estimator` and fit on the data passed to `fit` or `partial_fit`.",SelectFromModel,<class 'sklearn.feature_selection._from_model.SelectFromModel'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,SelectFromModel,<class 'sklearn.feature_selection._from_model.SelectFromModel'>
Attributes,max_features_,int,,"Maximum number of features calculated during :term:`fit`. Only defined if the ``max_features`` is not `None`. - If `max_features` is an `int`, then `max_features_ = max_features`. - If `max_features` is a callable, then `max_features_ = max_features(X)`.",SelectFromModel,<class 'sklearn.feature_selection._from_model.SelectFromModel'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SelectFromModel,<class 'sklearn.feature_selection._from_model.SelectFromModel'>
,threshold_,float,,The threshold value used for feature selection.,SelectFromModel,<class 'sklearn.feature_selection._from_model.SelectFromModel'>
Parameters,score_func,callable,f_classif,"Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below ""See Also""). The default function only works with classification tasks.",SelectFwe,<class 'sklearn.feature_selection._univariate_selection.SelectFwe'>
Attributes,alpha,float,5e-2,The highest uncorrected p-value for features to keep.,SelectFwe,<class 'sklearn.feature_selection._univariate_selection.SelectFwe'>
Attributes,scores_,array-like of shape (n_features,,Scores of features.,SelectFwe,<class 'sklearn.feature_selection._univariate_selection.SelectFwe'>
Attributes,pvalues_,array-like of shape (n_features,,p-values of feature scores.,SelectFwe,<class 'sklearn.feature_selection._univariate_selection.SelectFwe'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SelectFwe,<class 'sklearn.feature_selection._univariate_selection.SelectFwe'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SelectFwe,<class 'sklearn.feature_selection._univariate_selection.SelectFwe'>
Parameters,score_func,callable,f_classif,"Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below ""See Also""). The default function only works with classification tasks.",SelectKBest,<class 'sklearn.feature_selection._univariate_selection.SelectKBest'>
Attributes,k,"int or ""all""",10,"Number of top features to select. The ""all"" option bypasses selection, for use in a parameter search.",SelectKBest,<class 'sklearn.feature_selection._univariate_selection.SelectKBest'>
Attributes,scores_,array-like of shape (n_features,,Scores of features.,SelectKBest,<class 'sklearn.feature_selection._univariate_selection.SelectKBest'>
Attributes,pvalues_,array-like of shape (n_features,,"p-values of feature scores, None if `score_func` returned only scores.",SelectKBest,<class 'sklearn.feature_selection._univariate_selection.SelectKBest'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SelectKBest,<class 'sklearn.feature_selection._univariate_selection.SelectKBest'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SelectKBest,<class 'sklearn.feature_selection._univariate_selection.SelectKBest'>
Parameters,score_func,callable,f_classif,"Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below ""See Also""). The default function only works with classification tasks.",SelectPercentile,<class 'sklearn.feature_selection._univariate_selection.SelectPercentile'>
Attributes,percentile,int,10,Percent of features to keep.,SelectPercentile,<class 'sklearn.feature_selection._univariate_selection.SelectPercentile'>
Attributes,scores_,array-like of shape (n_features,,Scores of features.,SelectPercentile,<class 'sklearn.feature_selection._univariate_selection.SelectPercentile'>
Attributes,pvalues_,array-like of shape (n_features,,"p-values of feature scores, None if `score_func` returned only scores.",SelectPercentile,<class 'sklearn.feature_selection._univariate_selection.SelectPercentile'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SelectPercentile,<class 'sklearn.feature_selection._univariate_selection.SelectPercentile'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SelectPercentile,<class 'sklearn.feature_selection._univariate_selection.SelectPercentile'>
Parameters,estimator,estimator object,,"An estimator object implementing `fit` and `predict_proba`. Invoking the `fit` method will fit a clone of the passed estimator, which will be stored in the `estimator_` attribute. `estimator` was added to replace `base_estimator`.",SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Parameters,base_estimator,estimator object,,"An estimator object implementing `fit` and `predict_proba`. Invoking the `fit` method will fit a clone of the passed estimator, which will be stored in the `estimator_` attribute. `base_estimator` was deprecated in 1.6 and will be removed in 1.8. Use `estimator` instead.",SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Parameters,threshold,float,0.75,"The decision threshold for use with `criterion='threshold'`. Should be in [0, 1). When using the `'threshold'` criterion, a :ref:`well calibrated classifier <calibration>` should be used.",SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Parameters,criterion,{'threshold','threshold',"The selection criterion used to select which labels to add to the training set. If `'threshold'`, pseudo-labels with prediction probabilities above `threshold` are added to the dataset. If `'k_best'`, the `k_best` pseudo-labels with highest prediction probabilities are added to the dataset. When using the 'threshold' criterion, a :ref:`well calibrated classifier <calibration>` should be used.",SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Parameters,k_best,int,10,The amount of samples to add in each iteration. Only used when `criterion='k_best'`.,SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Parameters,max_iter,int or None,10,"Maximum number of iterations allowed. Should be greater than or equal to 0. If it is `None`, the classifier will continue to predict labels until no new pseudo-labels are added, or all unlabeled samples have been labeled.",SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Attributes,verbose,bool,False,Enable verbose output.,SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Attributes,estimator_,estimator object,,The fitted estimator.,SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Attributes,classes_,ndarray or list of ndarray of shape (n_classes,,Class labels for each output. (Taken from the trained `estimator_`).,SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Attributes,transduction_,ndarray of shape (n_samples,,"The labels used for the final fit of the classifier, including pseudo-labels added during fit.",SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Attributes,labeled_iter_,ndarray of shape (n_samples,,"The iteration in which each sample was labeled. When a sample has iteration 0, the sample was already labeled in the original dataset. When a sample has iteration -1, the sample was not labeled in any iteration.",SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Attributes,n_iter_,int,,"The number of rounds of self-training, that is the number of times the base estimator is fitted on relabeled variants of the training set.",SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
,termination_condition_,{'max_iter',,The reason that fitting was stopped. - `'max_iter'`: `n_iter_` reached `max_iter`. - `'no_change'`: no new labels were predicted. - `'all_labeled'`: all unlabeled samples were labeled before `max_iter` was reached.,SelfTrainingClassifier,<class 'sklearn.semi_supervised._self_training.SelfTrainingClassifier'>
Parameters,estimator,estimator instance,,An unfitted estimator.,SequentialFeatureSelector,<class 'sklearn.feature_selection._sequential.SequentialFeatureSelector'>
Parameters,n_features_to_select,"""auto""","""auto""","If `""auto""`, the behaviour depends on the `tol` parameter: - if `tol` is not `None`, then features are selected while the score change does not exceed `tol`. - otherwise, half of the features are selected. If integer, the parameter is the absolute number of features to select. If float between 0 and 1, it is the fraction of features to select. The option `""auto""` was added in version 1.1. The default changed from `""warn""` to `""auto""` in 1.3.",SequentialFeatureSelector,<class 'sklearn.feature_selection._sequential.SequentialFeatureSelector'>
Parameters,tol,float,None,"If the score is not incremented by at least `tol` between two consecutive feature additions or removals, stop adding or removing. `tol` can be negative when removing features using `direction=""backward""`. `tol` is required to be strictly positive when doing forward selection. It can be useful to reduce the number of features at the cost of a small decrease in the score. `tol` is enabled only when `n_features_to_select` is `""auto""`.",SequentialFeatureSelector,<class 'sklearn.feature_selection._sequential.SequentialFeatureSelector'>
Parameters,direction,{'forward','forward',Whether to perform forward selection or backward selection.,SequentialFeatureSelector,<class 'sklearn.feature_selection._sequential.SequentialFeatureSelector'>
Parameters,scoring,str or callable,None,"A single str (see :ref:`scoring_parameter`) or a callable (see :ref:`scoring_callable`) to evaluate the predictions on the test set. NOTE that when using a custom scorer, it should return a single value. If None, the estimator's score method is used.",SequentialFeatureSelector,<class 'sklearn.feature_selection._sequential.SequentialFeatureSelector'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - :term:`CV splitter`, - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`~sklearn.model_selection.StratifiedKFold` is used. In all other cases, :class:`~sklearn.model_selection.KFold` is used. These splitters are instantiated with `shuffle=False` so the splits will be the same across calls. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.",SequentialFeatureSelector,<class 'sklearn.feature_selection._sequential.SequentialFeatureSelector'>
Attributes,n_jobs,int,None,"Number of jobs to run in parallel. When evaluating a new feature to add or remove, the cross-validation procedure is parallel over the folds. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",SequentialFeatureSelector,<class 'sklearn.feature_selection._sequential.SequentialFeatureSelector'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,SequentialFeatureSelector,<class 'sklearn.feature_selection._sequential.SequentialFeatureSelector'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SequentialFeatureSelector,<class 'sklearn.feature_selection._sequential.SequentialFeatureSelector'>
Attributes,n_features_to_select_,int,,The number of features that were selected.,SequentialFeatureSelector,<class 'sklearn.feature_selection._sequential.SequentialFeatureSelector'>
,support_,ndarray of shape (n_features,,The mask of selected features.,SequentialFeatureSelector,<class 'sklearn.feature_selection._sequential.SequentialFeatureSelector'>
Parameters,store_precision,bool,True,Specify if the estimated precision is stored.,ShrunkCovariance,<class 'sklearn.covariance._shrunk_covariance.ShrunkCovariance'>
Parameters,assume_centered,bool,False,"If True, data will not be centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data will be centered before computation.",ShrunkCovariance,<class 'sklearn.covariance._shrunk_covariance.ShrunkCovariance'>
Attributes,shrinkage,float,0.1,"Coefficient in the convex combination used for the computation of the shrunk estimate. Range is [0, 1].",ShrunkCovariance,<class 'sklearn.covariance._shrunk_covariance.ShrunkCovariance'>
Attributes,covariance_,ndarray of shape (n_features,,Estimated covariance matrix,ShrunkCovariance,<class 'sklearn.covariance._shrunk_covariance.ShrunkCovariance'>
Attributes,location_,ndarray of shape (n_features,,"Estimated location, i.e. the estimated mean.",ShrunkCovariance,<class 'sklearn.covariance._shrunk_covariance.ShrunkCovariance'>
Attributes,precision_,ndarray of shape (n_features,,Estimated pseudo inverse matrix. (stored only if store_precision is True),ShrunkCovariance,<class 'sklearn.covariance._shrunk_covariance.ShrunkCovariance'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,ShrunkCovariance,<class 'sklearn.covariance._shrunk_covariance.ShrunkCovariance'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,ShrunkCovariance,<class 'sklearn.covariance._shrunk_covariance.ShrunkCovariance'>
Parameters,missing_values,int,np.nan,"The placeholder for the missing values. All occurrences of `missing_values` will be imputed. For pandas' dataframes with nullable integer dtypes with missing values, `missing_values` can be set to either `np.nan` or `pd.NA`.",SimpleImputer,<class 'sklearn.impute._base.SimpleImputer'>
Parameters,strategy,str or Callable,'mean',"The imputation strategy. - If ""mean"", then replace missing values using the mean along each column. Can only be used with numeric data. - If ""median"", then replace missing values using the median along each column. Can only be used with numeric data. - If ""most_frequent"", then replace missing using the most frequent value along each column. Can be used with strings or numeric data. If there is more than one such value, only the smallest is returned. - If ""constant"", then replace missing values with fill_value. Can be used with strings or numeric data. - If an instance of Callable, then replace missing values using the scalar statistic returned by running the callable over a dense 1d array containing non-missing values of each column. strategy=""constant"" for fixed value imputation. strategy=callable for custom value imputation.",SimpleImputer,<class 'sklearn.impute._base.SimpleImputer'>
Parameters,fill_value,str or numerical value,None,"When strategy == ""constant"", `fill_value` is used to replace all occurrences of missing_values. For string or object data types, `fill_value` must be a string. If `None`, `fill_value` will be 0 when imputing numerical data and ""missing_value"" for strings or object data types.",SimpleImputer,<class 'sklearn.impute._base.SimpleImputer'>
Parameters,copy,bool,True,"If True, a copy of X will be created. If False, imputation will be done in-place whenever possible. Note that, in the following cases, a new copy will always be made, even if `copy=False`: - If `X` is not an array of floating values; - If `X` is encoded as a CSR matrix; - If `add_indicator=True`.",SimpleImputer,<class 'sklearn.impute._base.SimpleImputer'>
Parameters,add_indicator,bool,False,"If True, a :class:`MissingIndicator` transform will stack onto output of the imputer's transform. This allows a predictive estimator to account for missingness despite imputation. If a feature has no missing values at fit/train time, the feature won't appear on the missing indicator even if there are missing values at transform/test time.",SimpleImputer,<class 'sklearn.impute._base.SimpleImputer'>
Attributes,keep_empty_features,bool,False,"If True, features that consist exclusively of missing values when `fit` is called are returned in results when `transform` is called. The imputed value is always `0` except when `strategy=""constant""` in which case `fill_value` will be used instead. Currently, when `keep_empty_feature=False` and `strategy=""constant""`, empty features are not dropped. This behaviour will change in version 1.8. Set `keep_empty_feature=True` to preserve this behaviour.",SimpleImputer,<class 'sklearn.impute._base.SimpleImputer'>
Attributes,statistics_,array of shape (n_features,,"The imputation fill value for each feature. Computing statistics can result in `np.nan` values. During :meth:`transform`, features corresponding to `np.nan` statistics will be discarded.",SimpleImputer,<class 'sklearn.impute._base.SimpleImputer'>
Attributes,indicator_,:class:`~sklearn.impute.MissingIndicator`,,Indicator used to add binary indicators for missing values. `None` if `add_indicator=False`.,SimpleImputer,<class 'sklearn.impute._base.SimpleImputer'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SimpleImputer,<class 'sklearn.impute._base.SimpleImputer'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SimpleImputer,<class 'sklearn.impute._base.SimpleImputer'>
Parameters,skewedness,float,1.0,"""skewedness"" parameter of the kernel. Needs to be cross-validated.",SkewedChi2Sampler,<class 'sklearn.kernel_approximation.SkewedChi2Sampler'>
Parameters,n_components,int,100,Number of Monte Carlo samples per original feature. Equals the dimensionality of the computed feature space.,SkewedChi2Sampler,<class 'sklearn.kernel_approximation.SkewedChi2Sampler'>
Attributes,random_state,int,None,Pseudo-random number generator to control the generation of the random weights and random offset when fitting the training data. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,SkewedChi2Sampler,<class 'sklearn.kernel_approximation.SkewedChi2Sampler'>
Attributes,random_weights_,ndarray of shape (n_features,,"Weight array, sampled from a secant hyperbolic distribution, which will be used to linearly transform the log of the data.",SkewedChi2Sampler,<class 'sklearn.kernel_approximation.SkewedChi2Sampler'>
Attributes,random_offset_,ndarray of shape (n_features,,"Bias term, which will be added to the data. It is uniformly distributed between 0 and 2*pi.",SkewedChi2Sampler,<class 'sklearn.kernel_approximation.SkewedChi2Sampler'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SkewedChi2Sampler,<class 'sklearn.kernel_approximation.SkewedChi2Sampler'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SkewedChi2Sampler,<class 'sklearn.kernel_approximation.SkewedChi2Sampler'>
Parameters,dictionary,ndarray of shape (n_components,,The dictionary atoms used for sparse coding. Lines are assumed to be normalized to unit norm.,SparseCoder,<class 'sklearn.decomposition._dict_learning.SparseCoder'>
Parameters,transform_algorithm,{'lasso_lars','omp',Algorithm used to transform the data: - `'lars'`: uses the least angle regression method (`linear_model.lars_path`); - `'lasso_lars'`: uses Lars to compute the Lasso solution; - `'lasso_cd'`: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). `'lasso_lars'` will be faster if the estimated components are sparse; - `'omp'`: uses orthogonal matching pursuit to estimate the sparse solution; - `'threshold'`: squashes to zero all coefficients less than alpha from the projection ``dictionary * X'``.,SparseCoder,<class 'sklearn.decomposition._dict_learning.SparseCoder'>
Parameters,transform_n_nonzero_coefs,int,None,"Number of nonzero coefficients to target in each column of the solution. This is only used by `algorithm='lars'` and `algorithm='omp'` and is overridden by `alpha` in the `omp` case. If `None`, then `transform_n_nonzero_coefs=int(n_features / 10)`.",SparseCoder,<class 'sklearn.decomposition._dict_learning.SparseCoder'>
Parameters,transform_alpha,float,None,"If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the penalty applied to the L1 norm. If `algorithm='threshold'`, `alpha` is the absolute value of the threshold below which coefficients will be squashed to zero. If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides `n_nonzero_coefs`. If `None`, default to 1.",SparseCoder,<class 'sklearn.decomposition._dict_learning.SparseCoder'>
Parameters,split_sign,bool,False,Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers.,SparseCoder,<class 'sklearn.decomposition._dict_learning.SparseCoder'>
Parameters,n_jobs,int,None,Number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,SparseCoder,<class 'sklearn.decomposition._dict_learning.SparseCoder'>
Parameters,positive_code,bool,False,Whether to enforce positivity when finding the code.,SparseCoder,<class 'sklearn.decomposition._dict_learning.SparseCoder'>
Attributes,transform_max_iter,int,1000,Maximum number of iterations to perform if `algorithm='lasso_cd'` or `lasso_lars`.,SparseCoder,<class 'sklearn.decomposition._dict_learning.SparseCoder'>
Attributes,n_components_,int,,Number of atoms.,SparseCoder,<class 'sklearn.decomposition._dict_learning.SparseCoder'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SparseCoder,<class 'sklearn.decomposition._dict_learning.SparseCoder'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SparseCoder,<class 'sklearn.decomposition._dict_learning.SparseCoder'>
Parameters,n_components,int,None,"Number of sparse atoms to extract. If None, then ``n_components`` is set to ``n_features``.",SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Parameters,alpha,float,1,Sparsity controlling parameter. Higher values lead to sparser components.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Parameters,ridge_alpha,float,0.01,Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Parameters,max_iter,int,1000,Maximum number of iterations to perform.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Parameters,tol,float,1e-8,Tolerance for the stopping condition.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Parameters,method,{'lars','lars',Method to be used for optimization. lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Parameters,n_jobs,int,None,Number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Parameters,U_init,ndarray of shape (n_samples,None,Initial values for the loadings for warm restart scenarios. Only used if `U_init` and `V_init` are not None.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Parameters,V_init,ndarray of shape (n_components,None,Initial values for the components for warm restart scenarios. Only used if `U_init` and `V_init` are not None.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Parameters,verbose,int or bool,False,"Controls the verbosity; the higher, the more messages. Defaults to 0.",SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Attributes,random_state,int,None,Used during dictionary learning. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Attributes,components_,ndarray of shape (n_components,,Sparse components extracted from the data.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Attributes,error_,ndarray,,Vector of errors at each iteration.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Attributes,n_components_,int,,Estimated number of components.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Attributes,n_iter_,int,,Number of iterations run.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Attributes,mean_,ndarray of shape (n_features,,"Per-feature empirical mean, estimated from the training set. Equal to ``X.mean(axis=0)``.",SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SparsePCA,<class 'sklearn.decomposition._sparse_pca.SparsePCA'>
Parameters,n_components,int or 'auto','auto',Dimensionality of the target projection space. n_components can be automatically adjusted according to the number of samples in the dataset and the bound given by the Johnson-Lindenstrauss lemma. In that case the quality of the embedding is controlled by the ``eps`` parameter. It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset.,SparseRandomProjection,<class 'sklearn.random_projection.SparseRandomProjection'>
Parameters,density,float or 'auto','auto',"Ratio in the range (0, 1] of non-zero component in the random projection matrix. If density = 'auto', the value is set to the minimum density as recommended by Ping Li et al.: 1 / sqrt(n_features). Use density = 1 / 3.0 if you want to reproduce the results from Achlioptas, 2001.",SparseRandomProjection,<class 'sklearn.random_projection.SparseRandomProjection'>
Parameters,eps,float,0.1,Parameter to control the quality of the embedding according to the Johnson-Lindenstrauss lemma when n_components is set to 'auto'. This value should be strictly positive. Smaller values lead to better embedding and higher number of dimensions (n_components) in the target projection space.,SparseRandomProjection,<class 'sklearn.random_projection.SparseRandomProjection'>
Parameters,dense_output,bool,False,"If True, ensure that the output of the random projection is a dense numpy array even if the input and random projection matrix are both sparse. In practice, if the number of components is small the number of zero components in the projected data will be very small and it will be more CPU and memory efficient to use a dense representation. If False, the projected data uses a sparse representation if the input is sparse.",SparseRandomProjection,<class 'sklearn.random_projection.SparseRandomProjection'>
Parameters,compute_inverse_components,bool,False,"Learn the inverse transform by computing the pseudo-inverse of the components during fit. Note that the pseudo-inverse is always a dense array, even if the training data was sparse. This means that it might be necessary to call `inverse_transform` on a small batch of samples at a time to avoid exhausting the available memory on the host. Moreover, computing the pseudo-inverse does not scale well to large matrices.",SparseRandomProjection,<class 'sklearn.random_projection.SparseRandomProjection'>
Attributes,random_state,int,None,Controls the pseudo random number generator used to generate the projection matrix at fit time. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,SparseRandomProjection,<class 'sklearn.random_projection.SparseRandomProjection'>
Attributes,n_components_,int,,"Concrete number of components computed when n_components=""auto"".",SparseRandomProjection,<class 'sklearn.random_projection.SparseRandomProjection'>
Attributes,components_,sparse matrix of shape (n_components,,Random matrix used for the projection. Sparse matrix will be of CSR format.,SparseRandomProjection,<class 'sklearn.random_projection.SparseRandomProjection'>
Attributes,inverse_components_,ndarray of shape (n_features,,"Pseudo-inverse of the components, only computed if `compute_inverse_components` is True.",SparseRandomProjection,<class 'sklearn.random_projection.SparseRandomProjection'>
Attributes,density_,float in range 0.0 - 1.0,,"Concrete density computed from when density = ""auto"".",SparseRandomProjection,<class 'sklearn.random_projection.SparseRandomProjection'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SparseRandomProjection,<class 'sklearn.random_projection.SparseRandomProjection'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SparseRandomProjection,<class 'sklearn.random_projection.SparseRandomProjection'>
Parameters,n_clusters,int or tuple (n_row_clusters,3,The number of row and column clusters in the checkerboard structure.,SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Parameters,method,{'bistochastic','bistochastic',"Method of normalizing and converting singular vectors into biclusters. May be one of 'scale', 'bistochastic', or 'log'. The authors recommend using 'log'. If the data is sparse, however, log normalization will not work, which is why the default is 'bistochastic'. if `method='log'`, the data must not be sparse.",SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Parameters,n_components,int,6,Number of singular vectors to check.,SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Parameters,n_best,int,3,Number of best singular vectors to which to project the data for clustering.,SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Parameters,svd_method,{'randomized','randomized',"Selects the algorithm for finding singular vectors. May be 'randomized' or 'arpack'. If 'randomized', uses :func:`~sklearn.utils.extmath.randomized_svd`, which may be faster for large matrices. If 'arpack', uses `scipy.sparse.linalg.svds`, which is more accurate, but possibly slower in some cases.",SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Parameters,n_svd_vecs,int,None,Number of vectors to use in calculating the SVD. Corresponds to `ncv` when `svd_method=arpack` and `n_oversamples` when `svd_method` is 'randomized`.,SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Parameters,mini_batch,bool,False,"Whether to use mini-batch k-means, which is faster but may get different results.",SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Parameters,init,{'k-means++','k-means++',Method for initialization of k-means algorithm; defaults to 'k-means++'.,SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Parameters,n_init,int,10,"Number of random initializations that are tried with the k-means algorithm. If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen.",SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Attributes,random_state,int,None,Used for randomizing the singular value decomposition and the k-means initialization. Use an int to make the randomness deterministic. See :term:`Glossary <random_state>`.,SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Attributes,rows_,array-like of shape (n_row_clusters,,"Results of the clustering. `rows[i, r]` is True if cluster `i` contains row `r`. Available only after calling ``fit``.",SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Attributes,columns_,array-like of shape (n_column_clusters,,"Results of the clustering, like `rows`.",SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Attributes,row_labels_,array-like of shape (n_rows,,Row partition labels.,SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Attributes,column_labels_,array-like of shape (n_cols,,Column partition labels.,SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Attributes,biclusters_,tuple of two ndarrays,,The tuple contains the `rows_` and `columns_` arrays.,SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SpectralBiclustering,<class 'sklearn.cluster._bicluster.SpectralBiclustering'>
Parameters,n_clusters,int,8,The dimension of the projection subspace.,SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,eigen_solver,{'arpack',None,"The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities. If None, then ``'arpack'`` is used. See [4]_ for more details regarding `'lobpcg'`.",SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,n_components,int,None,"Number of eigenvectors to use for the spectral embedding. If None, defaults to `n_clusters`.",SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,random_state,int,None,"A pseudo random number generator used for the initialization of the lobpcg eigenvectors decomposition when `eigen_solver == 'amg'`, and for the K-Means initialization. Use an int to make the results deterministic across calls (See :term:`Glossary <random_state>`). When using `eigen_solver == 'amg'`, it is necessary to also fix the global numpy seed with `np.random.seed(int)` to get deterministic results. See https://github.com/pyamg/pyamg/issues/139 for further information.",SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,n_init,int,10,Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. Only used if ``assign_labels='kmeans'``.,SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,gamma,float,1.0,"Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels. Ignored for ``affinity='nearest_neighbors'``, ``affinity='precomputed'`` or ``affinity='precomputed_nearest_neighbors'``.",SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,affinity,str or callable,'rbf',"How to construct the affinity matrix. - 'nearest_neighbors': construct the affinity matrix by computing a graph of nearest neighbors. - 'rbf': construct the affinity matrix using a radial basis function (RBF) kernel. - 'precomputed': interpret ``X`` as a precomputed affinity matrix, where larger values indicate greater similarity between instances. - 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graph of precomputed distances, and construct a binary affinity matrix from the ``n_neighbors`` nearest neighbors of each instance. - one of the kernels supported by :func:`~sklearn.metrics.pairwise.pairwise_kernels`. Only kernels that produce similarity scores (non-negative values that increase with similarity) should be used. This property is not checked by the clustering algorithm.",SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,n_neighbors,int,10,Number of neighbors to use when constructing the affinity matrix using the nearest neighbors method. Ignored for ``affinity='rbf'``.,SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,eigen_tol,float,"""auto""","Stopping criterion for eigen decomposition of the Laplacian matrix. If `eigen_tol=""auto""` then the passed tolerance will depend on the `eigen_solver`: - If `eigen_solver=""arpack""`, then `eigen_tol=0.0`; - If `eigen_solver=""lobpcg""` or `eigen_solver=""amg""`, then `eigen_tol=None` which configures the underlying `lobpcg` solver to automatically resolve the value according to their heuristics. See, :func:`scipy.sparse.linalg.lobpcg` for details. Note that when using `eigen_solver=""lobpcg""` or `eigen_solver=""amg""` values of `tol<1e-5` may lead to convergence issues and should be avoided. Added 'auto' option.",SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,assign_labels,{'kmeans','kmeans',"The strategy for assigning labels in the embedding space. There are two ways to assign labels after the Laplacian embedding. k-means is a popular choice, but it can be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization [3]_. The cluster_qr method [5]_ directly extract clusters from eigenvectors in spectral clustering. In contrast to k-means and discretization, cluster_qr has no tuning parameters and runs no iterations, yet may outperform k-means and discretization in terms of both quality and speed. Added new labeling method 'cluster_qr'.",SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,degree,float,3,Degree of the polynomial kernel. Ignored by other kernels.,SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,coef0,float,1,Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.,SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,kernel_params,dict of str to any,None,Parameters (keyword arguments) and values for kernel passed as callable object. Ignored by other kernels.,SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,n_jobs,int,None,The number of parallel jobs to run when `affinity='nearest_neighbors'` or `affinity='precomputed_nearest_neighbors'`. The neighbors search will be done in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Attributes,verbose,bool,False,Verbosity mode.,SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Attributes,affinity_matrix_,array-like of shape (n_samples,,Affinity matrix used for clustering. Available only after calling ``fit``.,SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Attributes,labels_,ndarray of shape (n_samples,,Labels of each point,SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SpectralClustering,<class 'sklearn.cluster._spectral.SpectralClustering'>
Parameters,n_clusters,int,3,The number of biclusters to find.,SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
Parameters,svd_method,{'randomized','randomized',"Selects the algorithm for finding singular vectors. May be 'randomized' or 'arpack'. If 'randomized', use :func:`sklearn.utils.extmath.randomized_svd`, which may be faster for large matrices. If 'arpack', use :func:`scipy.sparse.linalg.svds`, which is more accurate, but possibly slower in some cases.",SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
Parameters,n_svd_vecs,int,None,Number of vectors to use in calculating the SVD. Corresponds to `ncv` when `svd_method=arpack` and `n_oversamples` when `svd_method` is 'randomized`.,SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
Parameters,mini_batch,bool,False,"Whether to use mini-batch k-means, which is faster but may get different results.",SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
Parameters,init,{'k-means++','k-means++',Method for initialization of k-means algorithm; defaults to 'k-means++'.,SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
Parameters,n_init,int,10,"Number of random initializations that are tried with the k-means algorithm. If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen.",SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
Attributes,random_state,int,None,Used for randomizing the singular value decomposition and the k-means initialization. Use an int to make the randomness deterministic. See :term:`Glossary <random_state>`.,SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
Attributes,rows_,array-like of shape (n_row_clusters,,"Results of the clustering. `rows[i, r]` is True if cluster `i` contains row `r`. Available only after calling ``fit``.",SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
Attributes,columns_,array-like of shape (n_column_clusters,,"Results of the clustering, like `rows`.",SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
Attributes,row_labels_,array-like of shape (n_rows,,The bicluster label of each row.,SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
Attributes,column_labels_,array-like of shape (n_cols,,The bicluster label of each column.,SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
Attributes,biclusters_,tuple of two ndarrays,,The tuple contains the `rows_` and `columns_` arrays.,SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SpectralCoclustering,<class 'sklearn.cluster._bicluster.SpectralCoclustering'>
Parameters,n_components,int,2,The dimension of the projected subspace.,SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>
Parameters,affinity,{'nearest_neighbors','nearest_neighbors',"How to construct the affinity matrix. - 'nearest_neighbors' : construct the affinity matrix by computing a graph of nearest neighbors. - 'rbf' : construct the affinity matrix by computing a radial basis function (RBF) kernel. - 'precomputed' : interpret ``X`` as a precomputed affinity matrix. - 'precomputed_nearest_neighbors' : interpret ``X`` as a sparse graph of precomputed nearest neighbors, and constructs the affinity matrix by selecting the ``n_neighbors`` nearest neighbors. - callable : use passed in function as affinity the function takes in data matrix (n_samples, n_features) and return affinity matrix (n_samples, n_samples).",SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>
Parameters,gamma,float,None,"Kernel coefficient for rbf kernel. If None, gamma will be set to 1/n_features.",SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>
Parameters,random_state,int,None,"A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when `eigen_solver == 'amg'`, and for the K-Means initialization. Use an int to make the results deterministic across calls (See :term:`Glossary <random_state>`). When using `eigen_solver == 'amg'`, it is necessary to also fix the global numpy seed with `np.random.seed(int)` to get deterministic results. See https://github.com/pyamg/pyamg/issues/139 for further information.",SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>
Parameters,eigen_solver,{'arpack',None,"The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems. If None, then ``'arpack'`` is used.",SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>
Parameters,eigen_tol,float,"""auto""","Stopping criterion for eigendecomposition of the Laplacian matrix. If `eigen_tol=""auto""` then the passed tolerance will depend on the `eigen_solver`: - If `eigen_solver=""arpack""`, then `eigen_tol=0.0`; - If `eigen_solver=""lobpcg""` or `eigen_solver=""amg""`, then `eigen_tol=None` which configures the underlying `lobpcg` solver to automatically resolve the value according to their heuristics. See, :func:`scipy.sparse.linalg.lobpcg` for details. Note that when using `eigen_solver=""lobpcg""` or `eigen_solver=""amg""` values of `tol<1e-5` may lead to convergence issues and should be avoided.",SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>
Parameters,n_neighbors,int,None,"Number of nearest neighbors for nearest_neighbors graph building. If None, n_neighbors will be set to max(n_samples/10, 1).",SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>
Attributes,n_jobs,int,None,The number of parallel jobs to run. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>
Attributes,embedding_,ndarray of shape (n_samples,,Spectral embedding of the training matrix.,SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>
Attributes,affinity_matrix_,ndarray of shape (n_samples,,Affinity_matrix constructed from samples or precomputed.,SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>
,n_neighbors_,int,,Number of nearest neighbors effectively used.,SpectralEmbedding,<class 'sklearn.manifold._spectral_embedding.SpectralEmbedding'>
Parameters,n_knots,int,5,"Number of knots of the splines if `knots` equals one of {'uniform', 'quantile'}. Must be larger or equal 2. Ignored if `knots` is array-like.",SplineTransformer,<class 'sklearn.preprocessing._polynomial.SplineTransformer'>
Parameters,degree,int,3,The polynomial degree of the spline basis. Must be a non-negative integer.,SplineTransformer,<class 'sklearn.preprocessing._polynomial.SplineTransformer'>
Parameters,knots,{'uniform','uniform',"Set knot positions such that first knot <= features <= last knot. - If 'uniform', `n_knots` number of knots are distributed uniformly from min to max values of the features. - If 'quantile', they are distributed uniformly along the quantiles of the features. - If an array-like is given, it directly specifies the sorted knot positions including the boundary knots. Note that, internally, `degree` number of knots are added before the first knot, the same after the last knot.",SplineTransformer,<class 'sklearn.preprocessing._polynomial.SplineTransformer'>
Parameters,extrapolation,{'error','constant',"If 'error', values outside the min and max values of the training features raises a `ValueError`. If 'constant', the value of the splines at minimum and maximum value of the features is used as constant extrapolation. If 'linear', a linear extrapolation is used. If 'continue', the splines are extrapolated as is, i.e. option `extrapolate=True` in :class:`scipy.interpolate.BSpline`. If 'periodic', periodic splines with a periodicity equal to the distance between the first and last knot are used. Periodic splines enforce equal function values and derivatives at the first and last knot. For example, this makes it possible to avoid introducing an arbitrary jump between Dec 31st and Jan 1st in spline features derived from a naturally periodic ""day-of-year"" input feature. In this case it is recommended to manually set the knot values to control the period.",SplineTransformer,<class 'sklearn.preprocessing._polynomial.SplineTransformer'>
Parameters,include_bias,bool,True,"If False, then the last spline element inside the data range of a feature is dropped. As B-splines sum to one over the spline basis functions for each data point, they implicitly include a bias term, i.e. a column of ones. It acts as an intercept term in a linear models.",SplineTransformer,<class 'sklearn.preprocessing._polynomial.SplineTransformer'>
Parameters,order,{'C','C',"Order of output array in the dense case. `'F'` order is faster to compute, but may slow down subsequent estimators.",SplineTransformer,<class 'sklearn.preprocessing._polynomial.SplineTransformer'>
Attributes,sparse_output,bool,False,Will return sparse CSR matrix if set True else will return an array. This option is only available with `scipy>=1.8`.,SplineTransformer,<class 'sklearn.preprocessing._polynomial.SplineTransformer'>
Attributes,bsplines_,list of shape (n_features,,"List of BSplines objects, one for each feature.",SplineTransformer,<class 'sklearn.preprocessing._polynomial.SplineTransformer'>
Attributes,n_features_in_,int,,The total number of input features.,SplineTransformer,<class 'sklearn.preprocessing._polynomial.SplineTransformer'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,SplineTransformer,<class 'sklearn.preprocessing._polynomial.SplineTransformer'>
,n_features_out_,int,,"The total number of output features, which is computed as `n_features * n_splines`, where `n_splines` is the number of bases elements of the B-splines, `n_knots + degree - 1` for non-periodic splines and `n_knots - 1` for periodic ones. If `include_bias=False`, then it is only `n_features * (n_splines - 1)`.",SplineTransformer,<class 'sklearn.preprocessing._polynomial.SplineTransformer'>
Parameters,estimators,list of (str,,"Base estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. An estimator can be set to 'drop' using `set_params`. The type of estimator is generally expected to be a classifier. However, one can pass a regressor for some use case (e.g. ordinal regression).",StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
Parameters,final_estimator,estimator,None,A classifier which will be used to combine the base estimators. The default classifier is a :class:`~sklearn.linear_model.LogisticRegression`.,StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy used in `cross_val_predict` to train `final_estimator`. Possible inputs for cv are: * None, to use the default 5-fold cross validation, * integer, to specify the number of folds in a (Stratified) KFold, * An object to be used as a cross-validation generator, * An iterable yielding train, test splits, * `""prefit""`, to assume the `estimators` are prefit. In this case, the estimators will not be refitted. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, :class:`~sklearn.model_selection.StratifiedKFold` is used. In all other cases, :class:`~sklearn.model_selection.KFold` is used. These splitters are instantiated with `shuffle=False` so the splits will be the same across calls. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. If ""prefit"" is passed, it is assumed that all `estimators` have been fitted already. The `final_estimator_` is trained on the `estimators` predictions on the full training set and are **not** cross validated predictions. Please note that if the models have been trained on the same data to train the stacking model, there is a very high risk of overfitting. The 'prefit' option was added in 1.1 A larger number of split will provide no benefits if the number of training samples is large enough. Indeed, the training time will increase. ``cv`` is not used for model evaluation but for prediction.",StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
Parameters,stack_method,{'auto','auto',"Methods called for each base estimator. It can be: * if 'auto', it will try to invoke, for each estimator, `'predict_proba'`, `'decision_function'` or `'predict'` in that order. * otherwise, one of `'predict_proba'`, `'decision_function'` or `'predict'`. If the method is not implemented by the estimator, it will raise an error.",StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
Parameters,n_jobs,int,None,The number of jobs to run in parallel for `fit` of all `estimators`. `None` means 1 unless in a `joblib.parallel_backend` context. -1 means using all processors. See :term:`Glossary <n_jobs>` for more details.,StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
Parameters,passthrough,bool,False,"When False, only the predictions of estimators will be used as training data for `final_estimator`. When True, the `final_estimator` is trained on the predictions as well as the original training data.",StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
Attributes,verbose,int,0,Verbosity level.,StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,Class labels.,StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
Attributes,estimators_,list of estimators,,"The elements of the `estimators` parameter, having been fitted on the training data. If an estimator has been set to `'drop'`, it will not appear in `estimators_`. When `cv=""prefit""`, `estimators_` is set to `estimators` and is not fitted again.",StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
Attributes,named_estimators_,:class:`~sklearn.utils.Bunch`,,Attribute to access any fitted sub-estimators by name.,StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if the underlying estimators expose such an attribute when fit.,StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
Attributes,final_estimator_,estimator,,The classifier fit on the output of `estimators_` and responsible for final predictions.,StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
,stack_method_,list of str,,The method used by each base estimator.,StackingClassifier,<class 'sklearn.ensemble._stacking.StackingClassifier'>
Parameters,estimators,list of (str,,Base estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. An estimator can be set to 'drop' using `set_params`.,StackingRegressor,<class 'sklearn.ensemble._stacking.StackingRegressor'>
Parameters,final_estimator,estimator,None,A regressor which will be used to combine the base estimators. The default regressor is a :class:`~sklearn.linear_model.RidgeCV`.,StackingRegressor,<class 'sklearn.ensemble._stacking.StackingRegressor'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy used in `cross_val_predict` to train `final_estimator`. Possible inputs for cv are: * None, to use the default 5-fold cross validation, * integer, to specify the number of folds in a (Stratified) KFold, * An object to be used as a cross-validation generator, * An iterable yielding train, test splits, * `""prefit""`, to assume the `estimators` are prefit. In this case, the estimators will not be refitted. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, :class:`~sklearn.model_selection.StratifiedKFold` is used. In all other cases, :class:`~sklearn.model_selection.KFold` is used. These splitters are instantiated with `shuffle=False` so the splits will be the same across calls. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. If ""prefit"" is passed, it is assumed that all `estimators` have been fitted already. The `final_estimator_` is trained on the `estimators` predictions on the full training set and are **not** cross validated predictions. Please note that if the models have been trained on the same data to train the stacking model, there is a very high risk of overfitting. The 'prefit' option was added in 1.1 A larger number of split will provide no benefits if the number of training samples is large enough. Indeed, the training time will increase. ``cv`` is not used for model evaluation but for prediction.",StackingRegressor,<class 'sklearn.ensemble._stacking.StackingRegressor'>
Parameters,n_jobs,int,None,The number of jobs to run in parallel for `fit` of all `estimators`. `None` means 1 unless in a `joblib.parallel_backend` context. -1 means using all processors. See :term:`Glossary <n_jobs>` for more details.,StackingRegressor,<class 'sklearn.ensemble._stacking.StackingRegressor'>
Parameters,passthrough,bool,False,"When False, only the predictions of estimators will be used as training data for `final_estimator`. When True, the `final_estimator` is trained on the predictions as well as the original training data.",StackingRegressor,<class 'sklearn.ensemble._stacking.StackingRegressor'>
Attributes,verbose,int,0,Verbosity level.,StackingRegressor,<class 'sklearn.ensemble._stacking.StackingRegressor'>
Attributes,estimators_,list of estimators,,"The elements of the `estimators` parameter, having been fitted on the training data. If an estimator has been set to `'drop'`, it will not appear in `estimators_`. When `cv=""prefit""`, `estimators_` is set to `estimators` and is not fitted again.",StackingRegressor,<class 'sklearn.ensemble._stacking.StackingRegressor'>
Attributes,named_estimators_,:class:`~sklearn.utils.Bunch`,,Attribute to access any fitted sub-estimators by name.,StackingRegressor,<class 'sklearn.ensemble._stacking.StackingRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,StackingRegressor,<class 'sklearn.ensemble._stacking.StackingRegressor'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if the underlying estimators expose such an attribute when fit.,StackingRegressor,<class 'sklearn.ensemble._stacking.StackingRegressor'>
Attributes,final_estimator_,estimator,,The regressor fit on the output of `estimators_` and responsible for final predictions.,StackingRegressor,<class 'sklearn.ensemble._stacking.StackingRegressor'>
,stack_method_,list of str,,The method used by each base estimator.,StackingRegressor,<class 'sklearn.ensemble._stacking.StackingRegressor'>
Parameters,copy,bool,True,"If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.",StandardScaler,<class 'sklearn.preprocessing._data.StandardScaler'>
Parameters,with_mean,bool,True,"If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory.",StandardScaler,<class 'sklearn.preprocessing._data.StandardScaler'>
Attributes,with_std,bool,True,"If True, scale the data to unit variance (or equivalently, unit standard deviation).",StandardScaler,<class 'sklearn.preprocessing._data.StandardScaler'>
Attributes,scale_,ndarray of shape (n_features,,"Per feature relative scaling of the data to achieve zero mean and unit variance. Generally this is calculated using `np.sqrt(var_)`. If a variance is zero, we can't achieve unit variance, and the data is left as-is, giving a scaling factor of 1. `scale_` is equal to `None` when `with_std=False`. *scale_*",StandardScaler,<class 'sklearn.preprocessing._data.StandardScaler'>
Attributes,mean_,ndarray of shape (n_features,,The mean value for each feature in the training set. Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.,StandardScaler,<class 'sklearn.preprocessing._data.StandardScaler'>
Attributes,var_,ndarray of shape (n_features,,The variance for each feature in the training set. Used to compute `scale_`. Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.,StandardScaler,<class 'sklearn.preprocessing._data.StandardScaler'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,StandardScaler,<class 'sklearn.preprocessing._data.StandardScaler'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,StandardScaler,<class 'sklearn.preprocessing._data.StandardScaler'>
,n_samples_seen_,int or ndarray of shape (n_features,,"The number of samples processed by the estimator for each feature. If there are no missing samples, the ``n_samples_seen`` will be an integer, otherwise it will be an array of dtype int. If `sample_weights` are used it will be a float (if no missing data) or an array of dtype float that sums the weights seen so far. Will be reset on new calls to fit, but increments across ``partial_fit`` calls.",StandardScaler,<class 'sklearn.preprocessing._data.StandardScaler'>
Parameters,n_components,int,2,Dimension of the embedded space.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,perplexity,float,30.0,The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. Different values can result in significantly different results. The perplexity must be less than the number of samples.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,early_exaggeration,float,12.0,"Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. For larger values, the space between natural clusters will be larger in the embedded space. Again, the choice of this parameter is not very critical. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high.",TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,learning_rate,"float or ""auto""","""auto""","The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a 'ball' with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help. Note that many other t-SNE implementations (bhtsne, FIt-SNE, openTSNE, etc.) use a definition of learning_rate that is 4 times smaller than ours. So our learning_rate=200 corresponds to learning_rate=800 in those other implementations. The 'auto' option sets the learning_rate to `max(N / early_exaggeration / 4, 50)` where N is the sample size, following [4] and [5]. The default value changed to `""auto""`.",TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,max_iter,int,1000,Maximum number of iterations for the optimization. Should be at least 250. Parameter name changed from `n_iter` to `max_iter`.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,n_iter_without_progress,int,300,"Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50. parameter *n_iter_without_progress* to control stopping criteria.",TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,min_grad_norm,float,1e-7,"If the gradient norm is below this threshold, the optimization will be stopped.",TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,metric,str or callable,'euclidean',"The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is ""precomputed"", X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is ""euclidean"" which is interpreted as squared euclidean distance.",TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,metric_params,dict,None,Additional keyword arguments for the metric function.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,init,"{""random""","""pca""","Initialization of embedding. PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization. The default value changed to `""pca""`.",TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,verbose,int,0,Verbosity level.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,random_state,int,None,Determines the random number generator. Pass an int for reproducible results across multiple function calls. Note that different initializations might result in different local minima of the cost function. See :term:`Glossary <random_state>`.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,method,{'barnes_hut','barnes_hut',"By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method='exact' will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples. Approximate optimization *method* via the Barnes-Hut.",TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,angle,float,0.5,Only used if method='barnes_hut' This is the trade-off between speed and accuracy for Barnes-Hut T-SNE. 'angle' is the angular size (referred to as theta in [3]) of a distant node as measured from a point. If this size is below 'angle' then it is used as a summary node of all points contained within it. This method is not very sensitive to changes in this parameter in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing computation time and angle greater 0.8 has quickly increasing error.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,n_jobs,int,None,"The number of parallel jobs to run for neighbors search. This parameter has no impact when ``metric=""precomputed""`` or (``metric=""euclidean""`` and ``method=""exact""``). ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Attributes,n_iter,int,,Maximum number of iterations for the optimization. Should be at least 250. `n_iter` was deprecated in version 1.5 and will be removed in 1.7. Please use `max_iter` instead.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Attributes,embedding_,array-like of shape (n_samples,,Stores the embedding vectors.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Attributes,kl_divergence_,float,,Kullback-Leibler divergence after optimization.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Attributes,learning_rate_,float,,Effective learning rate.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
,n_iter_,int,,Number of iterations run.,TSNE,<class 'sklearn.manifold._t_sne.TSNE'>
Parameters,categories,"""auto"" or list of shape (n_features","""auto""","Categories (unique values) per feature: - `""auto""` : Determine categories automatically from the training data. - list : `categories[i]` holds the categories expected in the i-th column. The passed categories should not mix strings and numeric values within a single feature, and should be sorted in case of numeric values. The used categories are stored in the `categories_` fitted attribute.",TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>
Parameters,target_type,"{""auto""","""auto""","Type of target. - `""auto""` : Type of target is inferred with :func:`~sklearn.utils.multiclass.type_of_target`. - `""continuous""` : Continuous target - `""binary""` : Binary target - `""multiclass""` : Multiclass target The type of target inferred with `""auto""` may not be the desired target type used for modeling. For example, if the target consisted of integers between 0 and 100, then :func:`~sklearn.utils.multiclass.type_of_target` will infer the target as `""multiclass""`. In this case, setting `target_type=""continuous""` will specify the target as a regression problem. The `target_type_` attribute gives the target type used by the encoder. Added the option 'multiclass'.",TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>
Parameters,smooth,"""auto"" or float","""auto""","The amount of mixing of the target mean conditioned on the value of the category with the global target mean. A larger `smooth` value will put more weight on the global target mean. If `""auto""`, then `smooth` is set to an empirical Bayes estimate.",TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>
Parameters,cv,int,5,"Determines the number of folds in the :term:`cross fitting` strategy used in :meth:`fit_transform`. For classification targets, `StratifiedKFold` is used and for continuous targets, `KFold` is used.",TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>
Parameters,shuffle,bool,True,Whether to shuffle the data in :meth:`fit_transform` before splitting into folds. Note that the samples within each split will not be shuffled.,TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>
Attributes,random_state,int,None,"When `shuffle` is True, `random_state` affects the ordering of the indices, which controls the randomness of each fold. Otherwise, this parameter has no effect. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.",TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>
Attributes,encodings_,list of shape (n_features,,"Encodings learnt on all of `X`. For feature `i`, `encodings_[i]` are the encodings matching the categories listed in `categories_[i]`. When `target_type_` is ""multiclass"", the encoding for feature `i` and class `j` is stored in `encodings_[j + (i * len(classes_))]`. E.g., for 2 features (f) and 3 classes (c), encodings are ordered: f0_c0, f0_c1, f0_c2, f1_c0, f1_c1, f1_c2,",TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>
Attributes,categories_,list of shape (n_features,,The categories of each input feature determined during fitting or specified in `categories` (in order of the features in `X` and corresponding with the output of :meth:`transform`).,TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>
Attributes,target_type_,str,,Type of target.,TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>
Attributes,target_mean_,float,,The overall mean of the target. This value is only used in :meth:`transform` to encode categories.,TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>
Attributes,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>
,classes_,ndarray or None,,"If `target_type_` is 'binary' or 'multiclass', holds the label for each class, otherwise `None`.",TargetEncoder,<class 'sklearn.preprocessing._target_encoder.TargetEncoder'>
Parameters,norm,{'l1','l2',"Each output row will have unit norm, either: - 'l2': Sum of squares of vector elements is 1. The cosine similarity between two vectors is their dot product when l2 norm has been applied. - 'l1': Sum of absolute values of vector elements is 1. See :func:`~sklearn.preprocessing.normalize`. - None: No normalization.",TfidfTransformer,<class 'sklearn.feature_extraction.text.TfidfTransformer'>
Parameters,use_idf,bool,True,"Enable inverse-document-frequency reweighting. If False, idf(t) = 1.",TfidfTransformer,<class 'sklearn.feature_extraction.text.TfidfTransformer'>
Parameters,smooth_idf,bool,True,"Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.",TfidfTransformer,<class 'sklearn.feature_extraction.text.TfidfTransformer'>
Attributes,sublinear_tf,bool,False,"Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).",TfidfTransformer,<class 'sklearn.feature_extraction.text.TfidfTransformer'>
Attributes,idf_,array of shape (n_features),,The inverse document frequency (IDF) vector; only defined if  ``use_idf`` is True.,TfidfTransformer,<class 'sklearn.feature_extraction.text.TfidfTransformer'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,TfidfTransformer,<class 'sklearn.feature_extraction.text.TfidfTransformer'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,TfidfTransformer,<class 'sklearn.feature_extraction.text.TfidfTransformer'>
Parameters,input,{'filename','content',"- If `'filename'`, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze. - If `'file'`, the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory. - If `'content'`, the input is expected to be a sequence of items that can be of type string or byte.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,encoding,str,'utf-8',"If bytes or files are given to analyze, this encoding is used to decode.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,decode_error,{'strict','strict',"Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,strip_accents,{'ascii',None,Remove accents and perform other character normalization during the preprocessing step. 'ascii' is a fast method that only works on characters that have a direct ASCII mapping. 'unicode' is a slightly slower method that works on any characters. None (default) means no character normalization is performed. Both 'ascii' and 'unicode' use NFKD normalization from :func:`unicodedata.normalize`.,TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,lowercase,bool,True,Convert all characters to lowercase before tokenizing.,TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,preprocessor,callable,None,Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if ``analyzer`` is not callable.,TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,tokenizer,callable,None,Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if ``analyzer == 'word'``.,TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,analyzer,{'word','word',"Whether the feature should be made of word or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space. If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data is first read from the file and then passed to the given callable analyzer.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,stop_words,{'english'},None,"If a string, it is passed to _check_stop_list and the appropriate stop list is returned. 'english' is currently the only supported string value. There are several known issues with 'english' and you should consider an alternative (see :ref:`stop_words`). If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if ``analyzer == 'word'``. If None, no stop words will be used. In this case, setting `max_df` to a higher value, such as in the range (0.7, 1.0), can automatically detect and filter stop words based on intra corpus document frequency of terms.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,token_pattern,str,"r""(?u)\\b\\w\\w+\\b""","Regular expression denoting what constitutes a ""token"", only used if ``analyzer == 'word'``. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). If there is a capturing group in token_pattern then the captured group content, not the entire match, becomes the token. At most one capturing group is permitted.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,ngram_range,tuple (min_n,(1,"The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means only bigrams. Only applies if ``analyzer`` is not callable.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,max_df,float or int,1.0,"When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float in range [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,min_df,float or int,1,"When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float in range of [0.0, 1.0], the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,max_features,int,None,"If not None, build a vocabulary that only consider the top `max_features` ordered by term frequency across the corpus. Otherwise, all features are used. This parameter is ignored if vocabulary is not None.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,vocabulary,Mapping or iterable,None,"Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,binary,bool,False,"If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set `binary` to True, `use_idf` to False and `norm` to None to get 0/1 outputs).",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,dtype,dtype,float64,Type of the matrix returned by fit_transform() or transform().,TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,norm,{'l1','l2',"Each output row will have unit norm, either: - 'l2': Sum of squares of vector elements is 1. The cosine similarity between two vectors is their dot product when l2 norm has been applied. - 'l1': Sum of absolute values of vector elements is 1. See :func:`~sklearn.preprocessing.normalize`. - None: No normalization.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,use_idf,bool,True,"Enable inverse-document-frequency reweighting. If False, idf(t) = 1.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,smooth_idf,bool,True,"Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Attributes,sublinear_tf,bool,False,"Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).",TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Attributes,vocabulary_,dict,,A mapping of terms to feature indices.,TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Attributes,fixed_vocabulary_,bool,,True if a fixed vocabulary of term to indices mapping is provided by the user.,TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
,idf_,array of shape (n_features,,The inverse document frequency (IDF) vector; only defined if ``use_idf`` is True.,TfidfVectorizer,<class 'sklearn.feature_extraction.text.TfidfVectorizer'>
Parameters,fit_intercept,bool,True,"Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations.",TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Parameters,copy_X,bool,True,"If True, X will be copied; else, it may be overwritten. `copy_X` was deprecated in 1.6 and will be removed in 1.8. It has no effect as a copy is always made.",TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Parameters,max_subpopulation,int,1e4,"Instead of computing with a set of cardinality 'n choose k', where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if 'n choose k' is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed. Note that the data type should be int but floats such as 1e4 can be accepted too.",TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Parameters,n_subsamples,int,None,"Number of samples to calculate the parameters. This is at least the number of features (plus 1 if fit_intercept=True) and the number of samples as a maximum. A lower number leads to a higher breakdown point and a low efficiency while a high number leads to a low breakdown point and a high efficiency. If None, take the minimum number of subsamples leading to maximal robustness. If n_subsamples is set to n_samples, Theil-Sen is identical to least squares.",TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Parameters,max_iter,int,300,Maximum number of iterations for the calculation of spatial median.,TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Parameters,tol,float,1e-3,Tolerance when calculating spatial median.,TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Parameters,random_state,int,None,A random number generator instance to define the state of the random permutations generator. Pass an int for reproducible output across multiple function calls. See :term:`Glossary <random_state>`.,TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Parameters,n_jobs,int,None,Number of CPUs to use during the cross validation. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Attributes,verbose,bool,False,Verbose mode when fitting the model.,TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Attributes,coef_,ndarray of shape (n_features,,Coefficients of the regression model (median of distribution).,TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Attributes,intercept_,float,,Estimated intercept of regression model.,TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Attributes,breakdown_,float,,Approximated breakdown point.,TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Attributes,n_iter_,int,,Number of iterations needed for the spatial median.,TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Attributes,n_subpopulation_,int,,"Number of combinations taken into account from 'n choose k', where n is the number of samples and k is the number of subsamples.",TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,TheilSenRegressor,<class 'sklearn.linear_model._theil_sen.TheilSenRegressor'>
Parameters,regressor,object,None,"Regressor object such as derived from :class:`~sklearn.base.RegressorMixin`. This regressor will automatically be cloned each time prior to fitting. If `regressor is None`, :class:`~sklearn.linear_model.LinearRegression` is created and used.",TransformedTargetRegressor,<class 'sklearn.compose._target.TransformedTargetRegressor'>
Parameters,transformer,object,None,"Estimator object such as derived from :class:`~sklearn.base.TransformerMixin`. Cannot be set at the same time as `func` and `inverse_func`. If `transformer is None` as well as `func` and `inverse_func`, the transformer will be an identity transformer. Note that the transformer will be cloned during fitting. Also, the transformer is restricting `y` to be a numpy array.",TransformedTargetRegressor,<class 'sklearn.compose._target.TransformedTargetRegressor'>
Parameters,func,function,None,"Function to apply to `y` before passing to :meth:`fit`. Cannot be set at the same time as `transformer`. If `func is None`, the function used will be the identity function. If `func` is set, `inverse_func` also needs to be provided. The function needs to return a 2-dimensional array.",TransformedTargetRegressor,<class 'sklearn.compose._target.TransformedTargetRegressor'>
Parameters,inverse_func,function,None,"Function to apply to the prediction of the regressor. Cannot be set at the same time as `transformer`. The inverse function is used to return predictions to the same space of the original training labels. If `inverse_func` is set, `func` also needs to be provided. The inverse function needs to return a 2-dimensional array.",TransformedTargetRegressor,<class 'sklearn.compose._target.TransformedTargetRegressor'>
Attributes,check_inverse,bool,True,Whether to check that `transform` followed by `inverse_transform` or `func` followed by `inverse_func` leads to the original targets.,TransformedTargetRegressor,<class 'sklearn.compose._target.TransformedTargetRegressor'>
Attributes,regressor_,object,,Fitted regressor.,TransformedTargetRegressor,<class 'sklearn.compose._target.TransformedTargetRegressor'>
Attributes,transformer_,object,,Transformer used in :meth:`fit` and :meth:`predict`.,TransformedTargetRegressor,<class 'sklearn.compose._target.TransformedTargetRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying regressor exposes such an attribute when fit.,TransformedTargetRegressor,<class 'sklearn.compose._target.TransformedTargetRegressor'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,TransformedTargetRegressor,<class 'sklearn.compose._target.TransformedTargetRegressor'>
Parameters,n_components,int,2,"Desired dimensionality of output data. If algorithm='arpack', must be strictly less than the number of features. If algorithm='randomized', must be less than or equal to the number of features. The default value is useful for visualisation. For LSA, a value of 100 is recommended.",TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>
Parameters,algorithm,{'arpack','randomized',"SVD solver to use. Either ""arpack"" for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or ""randomized"" for the randomized algorithm due to Halko (2009).",TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>
Parameters,n_iter,int,5,Number of iterations for randomized SVD solver. Not used by ARPACK. The default is larger than the default in :func:`~sklearn.utils.extmath.randomized_svd` to handle sparse matrices that may have large slowly decaying spectrum.,TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>
Parameters,n_oversamples,int,10,Number of oversamples for randomized SVD solver. Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd` for a complete description.,TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>
Parameters,power_iteration_normalizer,{'auto','auto',Power iteration normalizer for randomized SVD solver. Not used by ARPACK. See :func:`~sklearn.utils.extmath.randomized_svd` for more details.,TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>
Parameters,random_state,int,None,Used during randomized svd. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`.,TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>
Attributes,tol,float,0.0,Tolerance for ARPACK. 0 means machine precision. Ignored by randomized SVD solver.,TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>
Attributes,components_,ndarray of shape (n_components,,The right singular vectors of the input data.,TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>
Attributes,explained_variance_,ndarray of shape (n_components,,The variance of the training samples transformed by a projection to each component.,TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>
Attributes,explained_variance_ratio_,ndarray of shape (n_components,,Percentage of variance explained by each of the selected components.,TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>
Attributes,singular_values_,ndarray of shape (n_components,,The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the ``n_components`` variables in the lower-dimensional space.,TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,TruncatedSVD,<class 'sklearn.decomposition._truncated_svd.TruncatedSVD'>
Parameters,estimator,estimator instance,,"The classifier, fitted or not, for which we want to optimize the decision threshold used during `predict`.",TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Parameters,scoring,str or callable,"""balanced_accuracy""",The objective metric to be optimized. Can be one of: * a string associated to a scoring function for binary classification (see :ref:`scoring_parameter`); * a scorer callable object created with :func:`~sklearn.metrics.make_scorer`;,TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Parameters,response_method,"{""auto""","""auto""","Methods by the classifier `estimator` corresponding to the decision function for which we want to find a threshold. It can be: * if `""auto""`, it will try to invoke, for each classifier, `""predict_proba""` or `""decision_function""` in that order. * otherwise, one of `""predict_proba""` or `""decision_function""`. If the method is not implemented by the classifier, it will raise an error.",TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Parameters,thresholds,int or array-like,100,The number of decision threshold to use when discretizing the output of the classifier `method`. Pass an array-like to manually specify the thresholds to use.,TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Parameters,cv,int,None,"Determines the cross-validation splitting strategy to train classifier. Possible inputs for cv are: * `None`, to use the default 5-fold stratified K-fold cross validation; * An integer number, to specify the number of folds in a stratified k-fold; * A float number, to specify a single shuffle split. The floating number should be in (0, 1) and represent the size of the validation set; * An object to be used as a cross-validation generator; * An iterable yielding train, test splits; * `""prefit""`, to bypass the cross-validation. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. Using `cv=""prefit""` and passing the same dataset for fitting `estimator` and tuning the cut-off point is subject to undesired overfitting. You can refer to :ref:`TunedThresholdClassifierCV_no_cv` for an example. This option should only be used when the set used to fit `estimator` is different from the one used to tune the cut-off point (by calling :meth:`TunedThresholdClassifierCV.fit`).",TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Parameters,refit,bool,True,"Whether or not to refit the classifier on the entire training set once the decision threshold has been found. Note that forcing `refit=False` on cross-validation having more than a single split will raise an error. Similarly, `refit=True` in conjunction with `cv=""prefit""` will raise an error.",TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Parameters,n_jobs,int,None,"The number of jobs to run in parallel. When `cv` represents a cross-validation strategy, the fitting and scoring on each data split is done in parallel. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.",TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Parameters,random_state,int,None,Controls the randomness of cross-validation when `cv` is a float. See :term:`Glossary <random_state>`.,TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Attributes,store_cv_results,bool,False,Whether to store all scores and thresholds computed during the cross-validation process.,TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Attributes,estimator_,estimator instance,,The fitted classifier used when predicting.,TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Attributes,best_threshold_,float,,The new decision threshold.,TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Attributes,best_score_,float or None,,"The optimal score of the objective metric, evaluated at `best_threshold_`.",TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Attributes,cv_results_,dict or None,,"A dictionary containing the scores and thresholds computed during the cross-validation process. Only exist if `store_cv_results=True`. The keys are `""thresholds""` and `""scores""`.",TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Attributes,classes_,ndarray of shape (n_classes,,The class labels.,TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if the underlying estimator exposes such an attribute when fit.,TunedThresholdClassifierCV,<class 'sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV'>
Parameters,power,float,0,"The power determines the underlying target distribution according to the following table: +-------+------------------------+ | Power | Distribution           | +=======+========================+ | 0     | Normal                 | +-------+------------------------+ | 1     | Poisson                | +-------+------------------------+ | (1,2) | Compound Poisson Gamma | +-------+------------------------+ | 2     | Gamma                  | +-------+------------------------+ | 3     | Inverse Gaussian       | +-------+------------------------+ For ``0 < power < 1``, no distribution exists.",TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
Parameters,alpha,float,1,"Constant that multiplies the L2 penalty term and determines the regularization strength. ``alpha = 0`` is equivalent to unpenalized GLMs. In this case, the design matrix `X` must have full column rank (no collinearities). Values of `alpha` must be in the range `[0.0, inf)`.",TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
Parameters,fit_intercept,bool,True,Specifies if a constant (a.k.a. bias or intercept) should be added to the linear predictor (`X @ coef + intercept`).,TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
Parameters,link,{'auto','auto',"The link function of the GLM, i.e. mapping from linear predictor `X @ coeff + intercept` to prediction `y_pred`. Option 'auto' sets the link depending on the chosen `power` parameter as follows: - 'identity' for ``power <= 0``, e.g. for the Normal distribution - 'log' for ``power > 0``, e.g. for Poisson, Gamma and Inverse Gaussian distributions",TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
Parameters,solver,{'lbfgs','lbfgs',"Algorithm to use in the optimization problem: 'lbfgs' Calls scipy's L-BFGS-B optimizer. 'newton-cholesky' Uses Newton-Raphson steps (in arbitrary precision arithmetic equivalent to iterated reweighted least squares) with an inner Cholesky based solver. This solver is a good choice for `n_samples` >> `n_features`, especially with one-hot encoded categorical features with rare categories. Be aware that the memory usage of this solver has a quadratic dependency on `n_features` because it explicitly computes the Hessian matrix.",TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
Parameters,max_iter,int,100,"The maximal number of iterations for the solver. Values must be in the range `[1, inf)`.",TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
Parameters,tol,float,1e-4,"Stopping criterion. For the lbfgs solver, the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol`` where ``g_j`` is the j-th component of the gradient (derivative) of the objective function. Values must be in the range `(0.0, inf)`.",TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
Parameters,warm_start,bool,False,"If set to ``True``, reuse the solution of the previous call to ``fit`` as initialization for ``coef_`` and ``intercept_`` .",TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
Attributes,verbose,int,0,"For the lbfgs solver set verbose to any positive number for verbosity. Values must be in the range `[0, inf)`.",TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
Attributes,coef_,array of shape (n_features,,Estimated coefficients for the linear predictor (`X @ coef_ + intercept_`) in the GLM.,TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
Attributes,intercept_,float,,Intercept (a.k.a. bias) added to linear predictor.,TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
Attributes,n_iter_,int,,Actual number of iterations used in the solver.,TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,TweedieRegressor,<class 'sklearn.linear_model._glm.glm.TweedieRegressor'>
Attributes,threshold,float,0,"Features with a training-set variance lower than this threshold will be removed. The default is to keep all features with non-zero variance, i.e. remove the features that have the same value in all samples.",VarianceThreshold,<class 'sklearn.feature_selection._variance_threshold.VarianceThreshold'>
Attributes,variances_,array,,Variances of individual features.,VarianceThreshold,<class 'sklearn.feature_selection._variance_threshold.VarianceThreshold'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`.,VarianceThreshold,<class 'sklearn.feature_selection._variance_threshold.VarianceThreshold'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Defined only when `X` has feature names that are all strings.,VarianceThreshold,<class 'sklearn.feature_selection._variance_threshold.VarianceThreshold'>
Parameters,estimators,list of (str,,Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones of those original estimators that will be stored in the class attribute ``self.estimators_``. An estimator can be set to ``'drop'`` using :meth:`set_params`. ``'drop'`` is accepted. Using None was deprecated in 0.22 and support was removed in 0.24.,VotingClassifier,<class 'sklearn.ensemble._voting.VotingClassifier'>
Parameters,voting,{'hard','hard',"If 'hard', uses predicted class labels for majority rule voting. Else if 'soft', predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.",VotingClassifier,<class 'sklearn.ensemble._voting.VotingClassifier'>
Parameters,weights,array-like of shape (n_classifiers,None,Sequence of weights (`float` or `int`) to weight the occurrences of predicted class labels (`hard` voting) or class probabilities before averaging (`soft` voting). Uses uniform weights if `None`.,VotingClassifier,<class 'sklearn.ensemble._voting.VotingClassifier'>
Parameters,n_jobs,int,None,The number of jobs to run in parallel for ``fit``. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,VotingClassifier,<class 'sklearn.ensemble._voting.VotingClassifier'>
Parameters,flatten_transform,bool,True,"Affects shape of transform output only when voting='soft' If voting='soft' and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes). If flatten_transform=False, it returns (n_classifiers, n_samples, n_classes).",VotingClassifier,<class 'sklearn.ensemble._voting.VotingClassifier'>
Attributes,verbose,bool,False,"If True, the time elapsed while fitting will be printed as it is completed.",VotingClassifier,<class 'sklearn.ensemble._voting.VotingClassifier'>
Attributes,estimators_,list of classifiers,,The collection of fitted sub-estimators as defined in ``estimators`` that are not 'drop'.,VotingClassifier,<class 'sklearn.ensemble._voting.VotingClassifier'>
Attributes,named_estimators_,:class:`~sklearn.utils.Bunch`,,Attribute to access any fitted sub-estimators by name.,VotingClassifier,<class 'sklearn.ensemble._voting.VotingClassifier'>
Attributes,le_,:class:`~sklearn.preprocessing.LabelEncoder`,,Transformer used to encode the labels during fit and decode during prediction.,VotingClassifier,<class 'sklearn.ensemble._voting.VotingClassifier'>
Attributes,classes_,ndarray of shape (n_classes,,The classes labels.,VotingClassifier,<class 'sklearn.ensemble._voting.VotingClassifier'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying classifier exposes such an attribute when fit.,VotingClassifier,<class 'sklearn.ensemble._voting.VotingClassifier'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if the underlying estimators expose such an attribute when fit.,VotingClassifier,<class 'sklearn.ensemble._voting.VotingClassifier'>
Parameters,estimators,list of (str,,Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones of those original estimators that will be stored in the class attribute ``self.estimators_``. An estimator can be set to ``'drop'`` using :meth:`set_params`. ``'drop'`` is accepted. Using None was deprecated in 0.22 and support was removed in 0.24.,VotingRegressor,<class 'sklearn.ensemble._voting.VotingRegressor'>
Parameters,weights,array-like of shape (n_regressors,None,Sequence of weights (`float` or `int`) to weight the occurrences of predicted values before averaging. Uses uniform weights if `None`.,VotingRegressor,<class 'sklearn.ensemble._voting.VotingRegressor'>
Parameters,n_jobs,int,None,The number of jobs to run in parallel for ``fit``. ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means using all processors. See :term:`Glossary <n_jobs>` for more details.,VotingRegressor,<class 'sklearn.ensemble._voting.VotingRegressor'>
Attributes,verbose,bool,False,"If True, the time elapsed while fitting will be printed as it is completed.",VotingRegressor,<class 'sklearn.ensemble._voting.VotingRegressor'>
Attributes,estimators_,list of regressors,,The collection of fitted sub-estimators as defined in ``estimators`` that are not 'drop'.,VotingRegressor,<class 'sklearn.ensemble._voting.VotingRegressor'>
Attributes,named_estimators_,:class:`~sklearn.utils.Bunch`,,Attribute to access any fitted sub-estimators by name.,VotingRegressor,<class 'sklearn.ensemble._voting.VotingRegressor'>
Attributes,n_features_in_,int,,Number of features seen during :term:`fit`. Only defined if the underlying regressor exposes such an attribute when fit.,VotingRegressor,<class 'sklearn.ensemble._voting.VotingRegressor'>
,feature_names_in_,ndarray of shape (`n_features_in_`,,Names of features seen during :term:`fit`. Only defined if the underlying estimators expose such an attribute when fit.,VotingRegressor,<class 'sklearn.ensemble._voting.VotingRegressor'>
