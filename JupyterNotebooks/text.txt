 1/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")

example_dict = {}
 1/2:
from Datasets import GenerateSKModelDoc
sklearn_model_df,param_df = GenerateSKModelDoc()
display(sklearn_model_df) 
display(param_df)
 1/3:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")

example_dict = {}
 1/4:
from DataSets import FakeBaseballstats 

fake_bb_df = FakeBaseballstats()
example_dict['FakeBaseballstats']=1
fake_bb_df
 1/5:
from Datasets import diabetes_df
example_dict['diabetes_df']=1
diabetes_df
 1/6:
from DataSets import diabetes_df
example_dict['diabetes_df']=1
diabetes_df
 1/7:
from DataSets import GenerateSKModelDoc
sklearn_model_df,param_df = GenerateSKModelDoc()
display(sklearn_model_df) 
display(param_df)
 1/8:
from DataSets import GenerateSKModelDoc
sklearn_model_df,param_df = GenerateSKModelDoc()
display(sklearn_model_df.head()) 
display(param_df.head())
 2/1:
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")
 2/2:
from MLPipeline import SKLearnModelList

model_list = SKLearnModelList()
model_list
#estimator_class.__doc__
 2/3:

model_list['Estimator Class'].iloc[0].__doc___
 2/4:

model_list['Estimator Class'].iloc[0].__doc__
 3/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import time
import sys

sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")
 3/2:
from DataSets import iris_df
from MLPipeline import MLPipelineSample

df = iris_df.copy()

results_df = MLPipelineSample(df,
                              scaler='normal',
                              ml_model_type='classifier',
                              target_column='Target',
                              run_all_size_models=1,
                              test_size=0.2)


results_df
 3/3:
from DataSets import iris_df
from MLPipeline import MLPipelineSample

df = iris_df.copy()

results_df = MLPipelineSample(df,
                              scaler='normal',
                              ml_model_type='classifier',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)


results_df
 3/4: df
 1/9:
from DataSets import diabetes_df,iris_df
example_dict['diabetes_df']=1
example_dict['iris_df']=1

display(diabetes_df.head())
display(iris_df.head())
 3/5:
from DataSets import iris_df,diabetes_df
from MLPipeline import MLPipelineSample

df = iris_df.copy()
df1 = diabetes_df.copy()



results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              ml_model_type='classifier',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
 3/6:
from DataSets import iris_df,diabetes_df
from MLPipeline import MLPipelineSample

df = iris_df.copy()
df1 = diabetes_df.copy()



results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
 3/7:
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

from DFProcessing import ConvertDicttoDF

# Example actual and predicted values
y = np.array([100, 200, 300, 400, 500])
y_pred = np.array([110, 190, 290, 410, 480])

pd.DataFrame([[y,y_pred]])
 3/8:
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

from DFProcessing import ConvertDicttoDF

# Example actual and predicted values
y = np.array([100, 200, 300, 400, 500])
y_pred = np.array([110, 190, 290, 410, 480])

pd.DataFrame([y,y_pred])
 3/9:
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

from DFProcessing import ConvertDicttoDF

# Example actual and predicted values
y = np.array([100, 200, 300, 400, 500])
y_pred = np.array([110, 190, 290, 410, 480])

pd.DataFrame([[y],[y_pred]])
3/10:
def ConvertListstoDF(dict_lists):
    '''
    
    
    '''

    return pd.DataFrame(dict_lists)

ConvertDicttoDF({'y':y,'y_pred':y_pred})
3/11: {'y':y,'y_pred':y_pred}
3/12:
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

from DFProcessing import ConvertDicttoDF

# Example actual and predicted values
y = [100, 200, 300, 400, 500]
y_pred = [110, 190, 290, 410, 480]

pd.DataFrame([y,y_pred])
3/13:
def ConvertListstoDF(dict_lists):
    '''
    
    
    '''

    return pd.DataFrame(dict_lists)

ConvertDicttoDF({'y':y,'y_pred':y_pred})
3/14: {'y':y,'y_pred':y_pred}
3/15:

y = [100, 200, 300, 400, 500]
y_pred = [110, 190, 290, 410, 480]

def ConvertListstoDF(dict_lists):
    '''
    
    
    '''

    return pd.DataFrame(dict_lists)

ConvertListstoDF({'y':y,'y_pred':y_pred})
3/16:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(mse)
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - r2) * (n - 1) / (n - p - 1)

    return dict_
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred},2)
3/17:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(mse)
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - r2) * (n - 1) / (n - p - 1)

    return dict_
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/18:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(mse)
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - r2) * (n - 1) / (n - p - 1)

    return dict_
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/19: ConvertListstoDF({'y':y,'y_pred':y_pred})
3/20: ConvertListstoDF({'y':y,'y_pred':y_pred})['y']
3/21: ConvertListstoDF({'y':y,'y_pred':y_pred})['y'].to_array()
3/22: ConvertListstoDF({'y':y,'y_pred':y_pred})['y'].to_numpy()
3/23:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = y.to_numpy()
    y_pred = y_pred.to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(mse)
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - r2) * (n - 1) / (n - p - 1)

    return dict_
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/24:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(mse)
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - r2) * (n - 1) / (n - p - 1)

    return dict_
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/25:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - r2) * (n - 1) / (n - p - 1)

    return dict_
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/26:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/27:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return ConvertListstoDF(dict_)
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/28:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/29:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),8)
3/30:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/31:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return ConvertDicttoDF(dict_)
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/32:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return ConvertDicttoDF(dict_,key_name='Metric')
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/33:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return ConvertDicttoDF(dict_,key_name='Metric').T
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/34:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return pd.DataFrame(dict_.values,index=dict_.keys).T
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/35:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return pd.DataFrame(dict_.values(),index=dict_.keys().T
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/36:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/37:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
a = CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)

a.keys()
3/38:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
a = CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)

pd.DataFrame(a.keys())
3/39:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
a = CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)

pd.DataFrame(a.values(),index=a.keys()
3/40:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
a = CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)

pd.DataFrame(a.values(),index=a.keys())
3/41:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
a = CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)

pd.DataFrame(a.values(),index=a.keys(),columns=['VALUES']).reset_index().rename(columns={'index':'METRIC'})
3/42:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
a = CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)

pd.DataFrame(a.values(),index=a.keys(),columns=['VALUES']).reset_index().rename(columns={'index':'METRIC'}).T
3/43:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
a = CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)

pd.DataFrame(a.values(),index=a.keys(),columns=['VALUES'])T.reset_index().rename(columns={'index':'METRIC'})
3/44:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
a = CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)

pd.DataFrame(a.values(),index=a.keys(),columns=['VALUES'])T.reset_index()
3/45:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
a = CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)

pd.DataFrame(a.values(),index=a.keys(),columns=['VALUES']).T.reset_index()
3/46:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['mae'] = mean_absolute_error(y, y_pred)
    dict_['mse'] = mean_squared_error(y, y_pred)
    dict_['rmse'] = np.sqrt(dict_['mse'])
    dict_['r2'] = r2_score(y, y_pred)
    dict_['mape'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['smape'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['adj_r2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
a = CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)

pd.DataFrame(a.values(),index=a.keys(),columns=['VALUES']).T
3/47:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['MAE'] = mean_absolute_error(y, y_pred)
    dict_['MSE'] = mean_squared_error(y, y_pred)
    dict_['RMSE'] = np.sqrt(dict_['mse'])
    dict_['R2'] = r2_score(y, y_pred)
    dict_['MAPE'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['SMAPE'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['ADJ_R2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
a = CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)

pd.DataFrame(a.values(),index=a.keys(),columns=['VALUES']).T
3/48:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['MAE'] = mean_absolute_error(y, y_pred)
    dict_['MSE'] = mean_squared_error(y, y_pred)
    dict_['RMSE'] = np.sqrt(dict_['MSE'])
    dict_['R2'] = r2_score(y, y_pred)
    dict_['MAPE'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['SMAPE'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['ADJ_R2'] = 1 - (1 - dict_['r2']) * (n - 1) / (n - p - 1)

    return dict_
    
a = CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)

pd.DataFrame(a.values(),index=a.keys(),columns=['VALUES']).T
3/49:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['MAE'] = mean_absolute_error(y, y_pred)
    dict_['MSE'] = mean_squared_error(y, y_pred)
    dict_['RMSE'] = np.sqrt(dict_['MSE'])
    dict_['R2'] = r2_score(y, y_pred)
    dict_['MAPE'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['SMAPE'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['ADJ_R2'] = 1 - (1 - dict_['R2']) * (n - 1) / (n - p - 1)

    return dict_
    
a = CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)

pd.DataFrame(a.values(),index=a.keys(),columns=['VALUES']).T
3/50:


def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df['y'].to_numpy()
    y_pred = df['y_pred'].to_numpy()
    
    dict_ = {}
    dict_['MAE'] = mean_absolute_error(y, y_pred)
    dict_['MSE'] = mean_squared_error(y, y_pred)
    dict_['RMSE'] = np.sqrt(dict_['MSE'])
    dict_['R2'] = r2_score(y, y_pred)
    dict_['MAPE'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['SMAPE'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['ADJ_R2'] = 1 - (1 - dict_['R2']) * (n - 1) / (n - p - 1)

    return pd.DataFrame(dict_.values(),index=dict_.keys(),columns=['VALUES']).T
    
CalculateRegressionPerformance(ConvertListstoDF({'y':y,'y_pred':y_pred}),2)
3/51:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]

    if len(df)>5000 & sample_override==0:
        df = df.sample(frac=.15).copy()
        
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                    print(AUC_Score)
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X_test),y=target_column,y='ACTUAL',y_pred='PREDICITION')
            
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {elapsed}.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
3/52:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]

    if len(df)>5000 & sample_override==0:
        df = df.sample(frac=.15).copy()
        
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                    print(AUC_Score)
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X_test),y=target_column,y_pred='PREDICITION')
            
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {elapsed}.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
 5/1:
from DataSets import iris_df,diabetes_df
from MLPipeline import MLPipelineSample

df = iris_df.copy()
df1 = diabetes_df.copy()

# results_df = MLPipelineSample(df,
#                               scaler='normal',
#                               ml_model_type='classifier',
#                               target_column='Target',
#                               sample_override=1,
#                               run_all_size_models=1,
#                               test_size=0.2)

results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
 5/2:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import time
import sys

sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")
 5/3:
from DataSets import iris_df,diabetes_df
from MLPipeline import MLPipelineSample

df = iris_df.copy()
df1 = diabetes_df.copy()

# results_df = MLPipelineSample(df,
#                               scaler='normal',
#                               ml_model_type='classifier',
#                               target_column='Target',
#                               sample_override=1,
#                               run_all_size_models=1,
#                               test_size=0.2)

results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
 5/4:

from UtilityFunctions import InspectFunction
InspectFunction(MLPipelineSample)
 6/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import time
import sys

sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")
 6/2:
from DataSets import iris_df,diabetes_df
from MLPipeline import MLPipelineSample

df = iris_df.copy()
df1 = diabetes_df.copy()

# results_df = MLPipelineSample(df,
#                               scaler='normal',
#                               ml_model_type='classifier',
#                               target_column='Target',
#                               sample_override=1,
#                               run_all_size_models=1,
#                               test_size=0.2)

results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
 6/3:

from DataSets import MNIST_SKLEARN
from MLPipeline import MLPipelineSample

df,z = MNIST_SKLEARN(normalize=False,flatten=True,return_value='df')

results_df = MLPipelineSample(df,
                              scaler='normal',
                              ml_model_type='classifier',
                              target_column='Target',
                              test_size=0.2)

results_df
 7/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import time
import sys

sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")
 7/2:

from DataSets import MNIST_SKLEARN
from MLPipeline import MLPipelineSample

df,z = MNIST_SKLEARN(normalize=False,flatten=True,return_value='df')

results_df = MLPipelineSample(df,
                              scaler='normal',
                              ml_model_type='classifier',
                              target_column='Target',
                              test_size=0.2)
results_df
 7/3:
from DataSets import iris_df,diabetes_df
from MLPipeline import MLPipelineSample

df = iris_df.copy()
df1 = diabetes_df.copy()

# results_df = MLPipelineSample(df,
#                               scaler='normal',
#                               ml_model_type='classifier',
#                               target_column='Target',
#                               sample_override=1,
#                               run_all_size_models=1,
#                               test_size=0.2)

results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
 7/4:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]

    if len(df)>5000 & sample_override==0:
        df = df.sample(frac=.15).copy()
        
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                    print(AUC_Score)
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                return binary_df
                
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X_test),y=target_column,y_pred='PREDICITION')
            
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    

from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split

results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
 7/5: diabetes_df
 7/6:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]

    if len(df)>5000 & sample_override==0:
        df = df.sample(frac=.15).copy()

    print(len(df))
        
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                    print(AUC_Score)
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                return binary_df
                
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X_test),y=target_column,y_pred='PREDICITION')
            
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    
from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split

results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
 7/7:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]

    print(len(df))
    
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()

    print(len(df))
        
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                    print(AUC_Score)
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                return binary_df
                
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X_test),y=target_column,y_pred='PREDICITION')
            
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    
from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split
 7/8:
results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
 7/9:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]

    print(len(df))
    
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()

    print(len(df))
        
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                    print(AUC_Score)
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                return binary_df
                
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X_test),y=target_column,y_pred='PREDICITION')
            
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    
from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split
7/10: results_df1.head(1)
7/11: CalculateRegressionPerformance(results_df1,p=len(X_test.columns)y='ACTUAL',y_pred='PREDICTION')
7/12: df1
7/13: X_test = df1.drop('Target',axis=1).copy()
7/14: CalculateRegressionPerformance(results_df1,p=len(X_test.columns)y='ACTUAL',y_pred='PREDICTION')
7/15: CalculateRegressionPerformance(results_df1,p=len(X_test.columns),y='ACTUAL',y_pred='PREDICTION')
7/16:
def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df[y].to_numpy()
    y_pred = df[y_pred].to_numpy()
    
    dict_ = {}
    dict_['MAE'] = mean_absolute_error(y, y_pred)
    dict_['MSE'] = mean_squared_error(y, y_pred)
    dict_['RMSE'] = np.sqrt(dict_['MSE'])
    dict_['R2'] = r2_score(y, y_pred)
    dict_['MAPE'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['SMAPE'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['ADJ_R2'] = 1 - (1 - dict_['R2']) * (n - 1) / (n - p - 1)

    return pd.DataFrame(dict_.values(),index=dict_.keys(),columns=['VALUES']).T
7/17: CalculateRegressionPerformance(results_df1,p=len(X_test.columns),y='ACTUAL',y_pred='PREDICTION')
7/18:
from sklearn.metrics import roc_auc_score,mean_absolute_error, mean_squared_error, r2_score

def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df[y].to_numpy()
    y_pred = df[y_pred].to_numpy()
    
    dict_ = {}
    dict_['MAE'] = mean_absolute_error(y, y_pred)
    dict_['MSE'] = mean_squared_error(y, y_pred)
    dict_['RMSE'] = np.sqrt(dict_['MSE'])
    dict_['R2'] = r2_score(y, y_pred)
    dict_['MAPE'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['SMAPE'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['ADJ_R2'] = 1 - (1 - dict_['R2']) * (n - 1) / (n - p - 1)

    return pd.DataFrame(dict_.values(),index=dict_.keys(),columns=['VALUES']).T
7/19: CalculateRegressionPerformance(results_df1,p=len(X_test.columns),y='ACTUAL',y_pred='PREDICTION')
7/20:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]

    print(len(df))
    
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()

    print(len(df))
        
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                    print(AUC_Score)
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X_test),y=target_column,y_pred='PREDICITION')
                return model_perfom_stats
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    
from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split
results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
7/21:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]

    print(len(df))
    
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()

    print(len(df))
        
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                    print(AUC_Score)
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                return binary_df
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X_test.columns),y=target_column,y_pred='PREDICITION')
                return model_perfom_stats
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    
from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split
results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
7/22:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
  
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()
     
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                return binary_df
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X_test.columns),y=target_column,y_pred='PREDICITION')
                return model_perfom_stats
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    
from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split
results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
7/23:
from sklearn.metrics import roc_auc_score,mean_absolute_error, mean_squared_error, r2_score

def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df[y].to_numpy()
    y_pred = df[y_pred].to_numpy()
    
    dict_ = {}
    dict_['MAE'] = mean_absolute_error(y, y_pred)
    dict_['MSE'] = mean_squared_error(y, y_pred)
    dict_['RMSE'] = np.sqrt(dict_['MSE'])
    dict_['R2'] = r2_score(y, y_pred)
    dict_['MAPE'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['SMAPE'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['ADJ_R2'] = 1 - (1 - dict_['R2']) * (n - 1) / (n - p - 1)

    return pd.DataFrame(dict_.values(),index=dict_.keys(),columns=['VALUES']).T
CalculateRegressionPerformance(results_df1,p=len(X_test.columns),y='ACTUAL',y_pred='PREDICTION')
7/24:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
  
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()
     
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X_test.columns),y='ACTUAL',y_pred='PREDICITION')
                return model_perfom_stats
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    
from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split
results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
7/25:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
  
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()
     
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X.columns),y='ACTUAL',y_pred='PREDICITION')
                return model_perfom_stats
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    
from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split
results_df1 = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df1
7/26: results_df1
7/27:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
  
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()
     
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                return binary_df
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X.columns),y='ACTUAL',y_pred='PREDICITION')
                return model_perfom_stats
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    
from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split
binary_df = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

binary_df
7/28: X_test
7/29:
from sklearn.metrics import roc_auc_score,mean_absolute_error, mean_squared_error, r2_score

def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df[y].to_numpy()
    y_pred = df[y_pred].to_numpy()
    
    dict_ = {}
    dict_['MAE'] = mean_absolute_error(y, y_pred)
    dict_['MSE'] = mean_squared_error(y, y_pred)
    dict_['RMSE'] = np.sqrt(dict_['MSE'])
    dict_['R2'] = r2_score(y, y_pred)
    dict_['MAPE'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['SMAPE'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['ADJ_R2'] = 1 - (1 - dict_['R2']) * (n - 1) / (n - p - 1)

    return pd.DataFrame(dict_.values(),index=dict_.keys(),columns=['VALUES']).T
    
CalculateRegressionPerformance(binary_df,p=len(X_test.columns),y='ACTUAL',y_pred='PREDICTION')
7/30:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
  
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()
     
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X.columns),y='ACTUAL',y_pred='PREDICITION')
                return model_perfom_stats
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    
from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split
results_df = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df
 8/1:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
  
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()
     
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                return binary_df,X

                
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X.columns),y='ACTUAL',y_pred='PREDICITION')
                return model_perfom_stats
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    
from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split
results_df = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

results_df
 8/2:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import time
import sys

sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")

from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split
 8/3:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
  
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()
     
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                return binary_df,X

                
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X.columns),y='ACTUAL',y_pred='PREDICITION')
                return model_perfom_stats
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    

results_df = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

binary_df,X = results_df
 8/4:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import time
import sys

sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")

from MLPipeline import CalculateRegressionPerformance,SKLearnModelList,apply_scaling
from sklearn.model_selection import train_test_split


from DataSets import iris_df,diabetes_df
from MLPipeline import MLPipelineSample

df = iris_df.copy()
df1 = diabetes_df.copy()
 8/5:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
  
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()
     
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                return binary_df,X

                
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X.columns),y='ACTUAL',y_pred='PREDICITION')
                return model_perfom_stats
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    

results_df = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)

binary_df,X = results_df
 8/6:
from sklearn.metrics import roc_auc_score,mean_absolute_error, mean_squared_error, r2_score

def CalculateRegressionPerformance(df,
                                   p,
                                   y='y',
                                   y_pred='y_pred'):
    
    '''
    Function to Generate a series of Performance Metrics for Regression Models.

    Parameters:
        df (Dataframe)
        p (int): Predictors, Number of X Variables (Independent Variables in Model)
        y (float): Target Value, Actual Observed Value
        y_pred (float) : Y-Hat, Model Predicted Value
    '''

    y = df[y].to_numpy()
    y_pred = df[y_pred].to_numpy()
    
    dict_ = {}
    dict_['MAE'] = mean_absolute_error(y, y_pred)
    dict_['MSE'] = mean_squared_error(y, y_pred)
    dict_['RMSE'] = np.sqrt(dict_['MSE'])
    dict_['R2'] = r2_score(y, y_pred)
    dict_['MAPE'] = np.mean(np.abs((y - y_pred) / y)) * 100
    dict_['SMAPE'] = 100 * np.mean(2 * np.abs(y_pred - y) / (np.abs(y) + np.abs(y_pred)))

    # Adjusted R^2 (assuming p predictors, here p=2 as example)
    n = len(y)
    dict_['ADJ_R2'] = 1 - (1 - dict_['R2']) * (n - 1) / (n - p - 1)

    return pd.DataFrame(dict_.values(),index=dict_.keys(),columns=['VALUES']).T
    
CalculateRegressionPerformance(binary_df,p=len(X.columns),y='ACTUAL',y_pred='PREDICTION')
 8/7:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
  
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()
     
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X.columns),y='ACTUAL',y_pred='PREDICITION')
                return model_perfom_stats
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    

MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)
 8/8:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
  
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()
     
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X.columns),y='ACTUAL',y_pred='PREDICTION')
                return model_perfom_stats
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    

MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)
 8/9:
def MLPipelineSample(df, 
                     scaler,
                     ml_model_type='regressor',
                     target_column='Target',
                     sample_override=0,
                     run_all_size_models=0,
                     test_size=0.2):
    """
    Function to run the relevant models in SKlearn against Data set. Primary input is ml_model_type, which determines which class of models to run. 
    Models are selected from SKlearn library based on model Type. 

    This function is really meant to be a first pass on a sample of to provide insights into which models are likely to perform well.

    Parameters:
        df (DataFrame): Input dataset in form of DataFrame, with Target column labeled Target as default (it can be specified to change).
        scaler (str): One of None, 'normal', or 'standard'.
        ml_model_type (str): 'classifier', 'regressor', 'cluster', 'transformer'
        target_column (str): Name of the target column.
        sample_override (int): Binary flag to determine whether the entirety of Dataset should be run. Default is to NOT override and run a sample as defined by test size.
        run_all_size_models: (int): Binary flag to determine whether all models of the seleect model type should be run. Default is only to run models of relevant size, however
        override provided, espcially in the case of Smaller models, or models with limit parameters, which might run quickly against all models. Change implemented for Iris Dataset
        which ran in 1 second for selected models and 3 seconds for ALL models, opposed to MNIST, which timed out on all models.
        test_size (float): Proportion of data used for testing.

    Returns:
        pd.DataFrame: Summary of model performance.
    """

    # Get model list (you must have this function already working)
    sklearn_models_df = SKLearnModelList()

    if run_all_size_models==0:
        if len(df)<5000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('small',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)<100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('medium',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
        elif len(df)>100000*(1+test_size):
            model_list = sklearn_models_df[(sklearn_models_df['Dataset Size'].str.contains('large',case=False))&(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
    else:
        model_list = sklearn_models_df[(sklearn_models_df['Estimator Type'].str.contains(ml_model_type,case=False))]
  
    if (len(df)>5000) & (sample_override==0):
        df = df.sample(frac=.15).copy()
     
    # Prepare data
    X = df.drop(columns=[target_column])
    y = df[target_column]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # Apply scaler
    X_train, X_test, _ = apply_scaling(X_train, X_test, scaler=scaler)

    results_df = pd.DataFrame()

    for _, row in model_list.iterrows():
        name = row['Model Name']
        estimator_class = row['Estimator Class']
        print(f'Generating Predicition for {name}, {estimator_class}')
               
        try:
            start_time = time.time()
            model = estimator_class()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            binary_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test).rename(columns={target_column:'ACTUAL'}).reset_index(drop=True)],axis=1) 

            if ml_model_type == 'classifier':
                # Calculate AUC Score Outside of Classification as it requires information not being passed.
                if hasattr(model, "predict_proba"):
                    y_proba = model.predict_proba(X_test)
                    AUC_Score = roc_auc_score(y_test, y_proba, multi_class="ovr")
                else:
                    AUC_Score = 0            
                   
                model_perfom_stats =  ClassificationMetrics(binary_df,AUC_Score=AUC_Score)
                
            elif ml_model_type == 'regressor':
                model_perfom_stats = CalculateRegressionPerformance(binary_df,p=len(X.columns),y='ACTUAL',y_pred='PREDICTION')
                
            else:
                print('Need to develop Performance Measurement Metrics')
                pass
            
            model_perfom_stats['Model'] = name
            model_perfom_stats['Scaler'] = scaler
            model_perfom_stats['Time'] = time.time() - start_time
            results_df = pd.concat([results_df,model_perfom_stats])
            print(f'{name} Successfully Completed in {time.time() - start_time:.2f} seconds.')

        except Exception as e:
            print(f"Model Generation and Results Failed: {e}\n")
    return results_df
    

MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)
8/10:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import time
import sys

sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")

from MLPipeline import MLPipelineSample,
#CalculateRegressionPerformance,SKLearnModelList,apply_scaling
#from sklearn.model_selection import train_test_split

from DataSets import iris_df,diabetes_df

df = iris_df.copy()
df1 = diabetes_df.copy()
 9/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import time
import sys

sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")

from MLPipeline import MLPipelineSample
#CalculateRegressionPerformance,SKLearnModelList,apply_scaling
#from sklearn.model_selection import train_test_split

from DataSets import iris_df,diabetes_df

df = iris_df.copy()
df1 = diabetes_df.copy()
 9/2:
from MLPipeline import MLPipelineSample

results_df = MLPipelineSample(df1,
                              scaler='normal',
                              target_column='Target',
                              sample_override=1,
                              run_all_size_models=1,
                              test_size=0.2)
 9/3: results_df
 9/4: len(results_df)
 9/5: results_df
 9/6: results_df.sort_values('RMSE')
10/1: import numpy as np
11/1:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
12/1: import numpy as np
12/2: !conda install numpy -y
12/3: !conda install matplotlib -y
12/4: !conda install pandas
12/5: !conda install pandas -y
12/6:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
12/7: !conda instal tensorflow
12/8: !conda install tensorflow -y
12/9:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
12/10:
# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
X_train = x_train.reshape(60000, 784)
X_test = x_test.reshape(10000, 784)
X_train = X_train.astype("float32") / 255
X_test = X_test.astype("float32") / 255
12/11:

model  = keras.Sequential([
    layers.Dense(512, activation="relu", name="layer1"),
    layers.Dense(10, activation="softmax")
])

model.compile(optimizer="rmsprop", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

history = model.fit(X_train, y_train, epochs=10, batch_size=128,validation_split=.1)
12/12:
import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
12/13:
def plot_training_history(history, train_metric='accuracy', validation_metric='val_accuracy', 
                          title='Model Accuracy', y_label='Accuracy', x_label='Epoch'):
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns

    # Plot accuracy
    axes[0].plot(history.history[train_metric])
    axes[0].plot(history.history[validation_metric])
    axes[0].set_title(title)
    axes[0].set_ylabel(y_label)
    axes[0].set_xlabel(x_label)
    axes[0].legend(['Train', 'Validation'], loc='upper left')

    # Plot loss
    axes[1].plot(history.history['loss'])
    axes[1].plot(history.history['val_loss'])
    axes[1].set_title('Model Loss')
    axes[1].set_ylabel('Loss')
    axes[1].set_xlabel(x_label)
    axes[1].legend(['Train', 'Validation'], loc='upper left')

    plt.tight_layout()
    plt.show()
                                         
plot_training_history(history)
13/1:
import re

df = pd.DataFrame({
    'text': [
        'Transaction from BCCA001 and ONCA-fund',
        'Nothing to remove here',
        'Another bcca999 item',
        'Words withonca inside'
    ]
})

# Regex to remove entire words containing 'bcca' or 'onca' (case-insensitive)
pattern = r'\b\w*(bcca|onca)\w*\b'

# Clean the text
df['cleaned'] = df['text'].str.replace(pattern, '', regex=True, case=False).str.replace(r'\s+', ' ', regex=True).str.strip()

print(df)
13/2:
import re
import pandas as pd

df = pd.DataFrame({
    'text': [
        'Transaction from BCCA001 and ONCA-fund',
        'Nothing to remove here',
        'Another bcca999 item',
        'Words withonca inside'
    ]
})

# Regex to remove entire words containing 'bcca' or 'onca' (case-insensitive)
pattern = r'\b\w*(bcca|onca)\w*\b'

# Clean the text
df['cleaned'] = df['text'].str.replace(pattern, '', regex=True, case=False).str.replace(r'\s+', ' ', regex=True).str.strip()

print(df)
15/1:
from tensorflow.keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
15/2: train_images.shape
15/3: len(train_labels)
15/4: train_labels
15/5: test_images.shape
15/6: len(test_labels)
15/7: test_labels
15/8:
from tensorflow import keras
from tensorflow.keras import layers
model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(10, activation="softmax")
])
15/9:
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
15/10:
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype("float32") / 255
15/11: model.fit(train_images, train_labels, epochs=5, batch_size=128)
15/12:
test_digits = test_images[0:10]
predictions = model.predict(test_digits)
predictions[0]
15/13: predictions[0].argmax()
15/14: predictions[0][7]
15/15: test_labels[0]
15/16:
test_loss, test_acc = model.evaluate(test_images, test_labels)
print(f"test_acc: {test_acc}")
15/17:
import numpy as np
x = np.array(12)
x
15/18: x.ndim
15/19:
x = np.array([12, 3, 6, 14, 7])
x
15/20: x.ndim
15/21:
x = np.array([[5, 78, 2, 34, 0],
              [6, 79, 3, 35, 1],
              [7, 80, 4, 36, 2]])
x.ndim
15/22:
x = np.array([[[5, 78, 2, 34, 0],
               [6, 79, 3, 35, 1],
               [7, 80, 4, 36, 2]],
              [[5, 78, 2, 34, 0],
               [6, 79, 3, 35, 1],
               [7, 80, 4, 36, 2]],
              [[5, 78, 2, 34, 0],
               [6, 79, 3, 35, 1],
               [7, 80, 4, 36, 2]]])
x.ndim
15/23:
from tensorflow.keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
15/24: train_images.ndim
15/25: train_images.shape
15/26: train_images.dtype
15/27:
import matplotlib.pyplot as plt
digit = train_images[4]
plt.imshow(digit, cmap=plt.cm.binary)
plt.show()
15/28: train_labels[4]
15/29:
my_slice = train_images[10:100]
my_slice.shape
15/30:
my_slice = train_images[10:100, :, :]
my_slice.shape
15/31:
my_slice = train_images[10:100, 0:28, 0:28]
my_slice.shape
15/32: my_slice = train_images[:, 14:, 14:]
15/33: my_slice = train_images[:, 7:-7, 7:-7]
15/34: batch = train_images[:128]
15/35: batch = train_images[128:256]
15/36:
n = 3
batch = train_images[128 * n:128 * (n + 1)]
15/37:
def naive_relu(x):
    assert len(x.shape) == 2
    x = x.copy()
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] = max(x[i, j], 0)
    return x
15/38:
def naive_add(x, y):
    assert len(x.shape) == 2
    assert x.shape == y.shape
    x = x.copy()
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] += y[i, j]
    return x
15/39:
import time

x = np.random.random((20, 100))
y = np.random.random((20, 100))

t0 = time.time()
for _ in range(1000):
    z = x + y
    z = np.maximum(z, 0.)
print("Took: {0:.2f} s".format(time.time() - t0))
15/40:
t0 = time.time()
for _ in range(1000):
    z = naive_add(x, y)
    z = naive_relu(z)
print("Took: {0:.2f} s".format(time.time() - t0))
15/41:
import numpy as np
X = np.random.random((32, 10))
y = np.random.random((10,))
15/42: y = np.expand_dims(y, axis=0)
15/43: Y = np.concatenate([y] * 32, axis=0)
15/44:
def naive_add_matrix_and_vector(x, y):
    assert len(x.shape) == 2
    assert len(y.shape) == 1
    assert x.shape[1] == y.shape[0]
    x = x.copy()
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] += y[j]
    return x
15/45:
import numpy as np
x = np.random.random((64, 3, 32, 10))
y = np.random.random((32, 10))
z = np.maximum(x, y)
15/46:
x = np.random.random((32,))
y = np.random.random((32,))
z = np.dot(x, y)
15/47:
def naive_vector_dot(x, y):
    assert len(x.shape) == 1
    assert len(y.shape) == 1
    assert x.shape[0] == y.shape[0]
    z = 0.
    for i in range(x.shape[0]):
        z += x[i] * y[i]
    return z
15/48:
def naive_matrix_vector_dot(x, y):
    assert len(x.shape) == 2
    assert len(y.shape) == 1
    assert x.shape[1] == y.shape[0]
    z = np.zeros(x.shape[0])
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            z[i] += x[i, j] * y[j]
    return z
15/49:
def naive_matrix_vector_dot(x, y):
    z = np.zeros(x.shape[0])
    for i in range(x.shape[0]):
        z[i] = naive_vector_dot(x[i, :], y)
    return z
15/50:
def naive_matrix_dot(x, y):
    assert len(x.shape) == 2
    assert len(y.shape) == 2
    assert x.shape[1] == y.shape[0]
    z = np.zeros((x.shape[0], y.shape[1]))
    for i in range(x.shape[0]):
        for j in range(y.shape[1]):
            row_x = x[i, :]
            column_y = y[:, j]
            z[i, j] = naive_vector_dot(row_x, column_y)
    return z
15/51: train_images = train_images.reshape((60000, 28 * 28))
15/52:
x = np.array([[0., 1.],
             [2., 3.],
             [4., 5.]])
x.shape
15/53:
x = x.reshape((6, 1))
x
15/54:
x = np.zeros((300, 20))
x = np.transpose(x)
x.shape
15/55:
import tensorflow as tf
x = tf.Variable(0.)
with tf.GradientTape() as tape:
    y = 2 * x + 3
grad_of_y_wrt_x = tape.gradient(y, x)
15/56:
x = tf.Variable(tf.random.uniform((2, 2)))
with tf.GradientTape() as tape:
    y = 2 * x + 3
grad_of_y_wrt_x = tape.gradient(y, x)
15/57:
W = tf.Variable(tf.random.uniform((2, 2)))
b = tf.Variable(tf.zeros((2,)))
x = tf.random.uniform((2, 2))
with tf.GradientTape() as tape:
    y = tf.matmul(x, W) + b
grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b])
15/58:
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype("float32") / 255
15/59:
model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(10, activation="softmax")
])
15/60:
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
15/61: model.fit(train_images, train_labels, epochs=5, batch_size=128)
15/62:
import tensorflow as tf

class NaiveDense:
    def __init__(self, input_size, output_size, activation):
        self.activation = activation

        w_shape = (input_size, output_size)
        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)
        self.W = tf.Variable(w_initial_value)

        b_shape = (output_size,)
        b_initial_value = tf.zeros(b_shape)
        self.b = tf.Variable(b_initial_value)

    def __call__(self, inputs):
        return self.activation(tf.matmul(inputs, self.W) + self.b)

    @property
    def weights(self):
        return [self.W, self.b]
15/63:
class NaiveSequential:
    def __init__(self, layers):
        self.layers = layers

    def __call__(self, inputs):
        x = inputs
        for layer in self.layers:
           x = layer(x)
        return x

    @property
    def weights(self):
       weights = []
       for layer in self.layers:
           weights += layer.weights
       return weights
15/64:
model = NaiveSequential([
    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),
    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)
])
assert len(model.weights) == 4
15/65:
import math

class BatchGenerator:
    def __init__(self, images, labels, batch_size=128):
        assert len(images) == len(labels)
        self.index = 0
        self.images = images
        self.labels = labels
        self.batch_size = batch_size
        self.num_batches = math.ceil(len(images) / batch_size)

    def next(self):
        images = self.images[self.index : self.index + self.batch_size]
        labels = self.labels[self.index : self.index + self.batch_size]
        self.index += self.batch_size
        return images, labels
15/66:
def one_training_step(model, images_batch, labels_batch):
    with tf.GradientTape() as tape:
        predictions = model(images_batch)
        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(
            labels_batch, predictions)
        average_loss = tf.reduce_mean(per_sample_losses)
    gradients = tape.gradient(average_loss, model.weights)
    update_weights(gradients, model.weights)
    return average_loss
15/67:
learning_rate = 1e-3

def update_weights(gradients, weights):
    for g, w in zip(gradients, weights):
        w.assign_sub(g * learning_rate)
15/68:
from tensorflow.keras import optimizers

optimizer = optimizers.SGD(learning_rate=1e-3)

def update_weights(gradients, weights):
    optimizer.apply_gradients(zip(gradients, weights))
15/69:
def fit(model, images, labels, epochs, batch_size=128):
    for epoch_counter in range(epochs):
        print(f"Epoch {epoch_counter}")
        batch_generator = BatchGenerator(images, labels)
        for batch_counter in range(batch_generator.num_batches):
            images_batch, labels_batch = batch_generator.next()
            loss = one_training_step(model, images_batch, labels_batch)
            if batch_counter % 100 == 0:
                print(f"loss at batch {batch_counter}: {loss:.2f}")
15/70:
from tensorflow.keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype("float32") / 255

fit(model, train_images, train_labels, epochs=10, batch_size=128)
15/71:
predictions = model(test_images)
predictions = predictions.numpy()
predicted_labels = np.argmax(predictions, axis=1)
matches = predicted_labels == test_labels
print(f"accuracy: {matches.mean():.2f}")
17/1:
import sys

assert sys.version_info >= (3, 7)
17/2:
from packaging import version
import sklearn

assert version.parse(sklearn.__version__) >= version.parse("1.0.1")
19/1:
import tensorflow as tf
x = tf.ones(shape=(2, 1))
print(x)
20/1:
import tensorflow as tf
x = tf.ones(shape=(2, 1))
print(x)
20/2:
x = tf.zeros(shape=(2, 1))
print(x)
20/3:
x = tf.random.normal(shape=(3, 1), mean=0., stddev=1.)
print(x)
20/4:
x = tf.random.uniform(shape=(3, 1), minval=0., maxval=1.)
print(x)
20/5:
import numpy as np
x = np.ones(shape=(2, 2))
x[0, 0] = 0.
20/6:
v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))
print(v)
20/7: v.assign(tf.ones((3, 1)))
20/8: v[0, 0].assign(3.)
20/9: v.assign_add(tf.ones((3, 1)))
20/10:
a = tf.ones((2, 2))
b = tf.square(a)
c = tf.sqrt(a)
d = b + c
e = tf.matmul(a, b)
e *= d
20/11:
input_var = tf.Variable(initial_value=3.)
with tf.GradientTape() as tape:
   result = tf.square(input_var)
gradient = tape.gradient(result, input_var)
20/12:
input_const = tf.constant(3.)
with tf.GradientTape() as tape:
   tape.watch(input_const)
   result = tf.square(input_const)
gradient = tape.gradient(result, input_const)
20/13:
time = tf.Variable(0.)
with tf.GradientTape() as outer_tape:
    with tf.GradientTape() as inner_tape:
        position =  4.9 * time ** 2
    speed = inner_tape.gradient(position, time)
acceleration = outer_tape.gradient(speed, time)
20/14:
time = tf.Variable(0.)
with tf.GradientTape() as outer_tape:
    with tf.GradientTape() as inner_tape:
        position =  4.9 * time ** 2
    speed = inner_tape.gradient(position, time)
acceleration = outer_tape.gradient(speed, time)
21/1:
import tensorflow as tf
x = tf.ones(shape=(2, 1))
print(x)
21/2:
x = tf.zeros(shape=(2, 1))
print(x)
21/3:
x = tf.random.normal(shape=(3, 1), mean=0., stddev=1.)
print(x)
21/4:
x = tf.random.uniform(shape=(3, 1), minval=0., maxval=1.)
print(x)
21/5:
import numpy as np
x = np.ones(shape=(2, 2))
x[0, 0] = 0.
21/6:
v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))
print(v)
21/7: v.assign(tf.ones((3, 1)))
21/8: v[0, 0].assign(3.)
21/9: v.assign_add(tf.ones((3, 1)))
21/10:
a = tf.ones((2, 2))
b = tf.square(a)
c = tf.sqrt(a)
d = b + c
e = tf.matmul(a, b)
e *= d
21/11:
input_var = tf.Variable(initial_value=3.)
with tf.GradientTape() as tape:
   result = tf.square(input_var)
gradient = tape.gradient(result, input_var)
21/12:
input_const = tf.constant(3.)
with tf.GradientTape() as tape:
   tape.watch(input_const)
   result = tf.square(input_const)
gradient = tape.gradient(result, input_const)
21/13:
time = tf.Variable(0.)
with tf.GradientTape() as outer_tape:
    with tf.GradientTape() as inner_tape:
        position =  4.9 * time ** 2
    speed = inner_tape.gradient(position, time)
acceleration = outer_tape.gradient(speed, time)
21/14:
num_samples_per_class = 1000
negative_samples = np.random.multivariate_normal(
    mean=[0, 3],
    cov=[[1, 0.5],[0.5, 1]],
    size=num_samples_per_class)
positive_samples = np.random.multivariate_normal(
    mean=[3, 0],
    cov=[[1, 0.5],[0.5, 1]],
    size=num_samples_per_class)
21/15: inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)
21/16:
targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype="float32"),
                     np.ones((num_samples_per_class, 1), dtype="float32")))
21/17:
import matplotlib.pyplot as plt
plt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0])
plt.show()
21/18:
input_dim = 2
output_dim = 1
W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))
b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))
21/19:
def model(inputs):
    return tf.matmul(inputs, W) + b
21/20:
def square_loss(targets, predictions):
    per_sample_losses = tf.square(targets - predictions)
    return tf.reduce_mean(per_sample_losses)
21/21:
learning_rate = 0.1

def training_step(inputs, targets):
    with tf.GradientTape() as tape:
        predictions = model(inputs)
        loss = square_loss(targets, predictions)
    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])
    W.assign_sub(grad_loss_wrt_W * learning_rate)
    b.assign_sub(grad_loss_wrt_b * learning_rate)
    return loss
21/22:
for step in range(40):
    loss = training_step(inputs, targets)
    print(f"Loss at step {step}: {loss:.4f}")
21/23:
predictions = model(inputs)
plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)
plt.show()
21/24:
x = np.linspace(-1, 4, 100)
y = - W[0] /  W[1] * x + (0.5 - b) / W[1]
plt.plot(x, y, "-r")
plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)
21/25:
from tensorflow import keras

class SimpleDense(keras.layers.Layer):

    def __init__(self, units, activation=None):
        super().__init__()
        self.units = units
        self.activation = activation

    def build(self, input_shape):
        input_dim = input_shape[-1]
        self.W = self.add_weight(shape=(input_dim, self.units),
                                 initializer="random_normal")
        self.b = self.add_weight(shape=(self.units,),
                                 initializer="zeros")

    def call(self, inputs):
        y = tf.matmul(inputs, self.W) + self.b
        if self.activation is not None:
            y = self.activation(y)
        return y
21/26:
my_dense = SimpleDense(units=32, activation=tf.nn.relu)
input_tensor = tf.ones(shape=(2, 784))
output_tensor = my_dense(input_tensor)
print(output_tensor.shape)
21/27:
from tensorflow.keras import layers
layer = layers.Dense(32, activation="relu")
21/28:
from tensorflow.keras import models
from tensorflow.keras import layers
model = models.Sequential([
    layers.Dense(32, activation="relu"),
    layers.Dense(32)
])
21/29:
model = keras.Sequential([
    SimpleDense(32, activation="relu"),
    SimpleDense(64, activation="relu"),
    SimpleDense(32, activation="relu"),
    SimpleDense(10, activation="softmax")
])
21/30:
model = keras.Sequential([keras.layers.Dense(1)])
model.compile(optimizer="rmsprop",
              loss="mean_squared_error",
              metrics=["accuracy"])
21/31:
model.compile(optimizer=keras.optimizers.RMSprop(),
              loss=keras.losses.MeanSquaredError(),
              metrics=[keras.metrics.BinaryAccuracy()])
21/32:
history = model.fit(
    inputs,
    targets,
    epochs=5,
    batch_size=128
)
21/33: history.history
21/34:
model = keras.Sequential([keras.layers.Dense(1)])
model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.1),
              loss=keras.losses.MeanSquaredError(),
              metrics=[keras.metrics.BinaryAccuracy()])

indices_permutation = np.random.permutation(len(inputs))
shuffled_inputs = inputs[indices_permutation]
shuffled_targets = targets[indices_permutation]

num_validation_samples = int(0.3 * len(inputs))
val_inputs = shuffled_inputs[:num_validation_samples]
val_targets = shuffled_targets[:num_validation_samples]
training_inputs = shuffled_inputs[num_validation_samples:]
training_targets = shuffled_targets[num_validation_samples:]
model.fit(
    training_inputs,
    training_targets,
    epochs=5,
    batch_size=16,
    validation_data=(val_inputs, val_targets)
)
21/35:
predictions = model.predict(val_inputs, batch_size=128)
print(predictions[:10])
22/1:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
22/2:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
22/3:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
23/1:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
23/2:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")

example_dict = {}
23/3:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
23/4:
from Connections import DownloadFilesFromGit,BackUpGoogleSheets,ParamterMapping

example_dict['DownloadFilesFromGit']=1
example_dict['BackUpGoogleSheets']=1
DownloadFilesFromGit(output_folder='/Users/derekdewald/Documents/Python/Github_Repo/JupyterNotebooks/ArchivePyFunctions/')
BackUpGoogleSheets()
23/5:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
24/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")

example_dict = {}
24/2:
from Visualization import visualize_hex_color, hex_color_list

visualize_hex_color(hex_color_list[:150]
24/3:
from Visualization import visualize_hex_color, hex_color_list

visualize_hex_color(hex_color_list[:150])
25/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")

example_dict = {}
25/2:
from Visualization import visualize_hex_color, hex_color_list

visualize_hex_color(hex_color_list[:150])
25/3:
from Visualization import visualize_hex_color, hex_color_list

visualize_hex_color(hex_color_list[:150])
26/1:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
26/2:
from tensorflow.keras.datasets import imdb

(X_train,y_train),(X_test,y_test) = imdb.load_data(num_words=10000)
26/3: X_train
26/4: y_train
26/5: X_train[0]
26/6:
word_index = imdb.get_word_index()

word_index
26/7:
word_index = imdb.get_word_index()

pd.DataFrame(word_index)
26/8: word_index = imdb.get_word_index()
26/9: pd.DataFrame(word_index)
26/10:
from DFProcessing import ConvertDicttoDF

ConvertDicttoDF(word_index)
26/11: !conda install statsmodels -y
26/12:
from DFProcessing import ConvertDicttoDF

ConvertDicttoDF(word_index)
26/13: !conda install sklearn
26/14: !conda install scikit-learn -y
26/15:
from DFProcessing import ConvertDicttoDF

ConvertDicttoDF(word_index)
26/16: pd.DataFrame(X_train[0])
26/17: pd.DataFrame(X_train[0],columns='VALUE')
26/18: pd.DataFrame(X_train[0],columns=['VALUE'])
26/19: pd.DataFrame(X_train[0],columns=['VALUE']).merge(ConvertDicttoDF(word_index),on='VALUE',how='left')
26/20: d = [(value,key) for (key,value) in word_index.items()]
26/21: [(value,key) for (key,value) in word_index.items()]
26/22: d = dict([(value,key) for (key,value) in word_index.items()])
26/23:
d = dict([(value,key) for (key,value) in word_index.items()])

" ".join([d.get(i-3,"?") for i in X_train[0])
26/24:
d = dict([(value,key) for (key,value) in word_index.items()])

" ".join([d.get(i-3,"?") for i in X_train[0]])
26/25:
d = dict([(value,key) for (key,value) in word_index.items()])

" ".join([d.get(i-3,"?") for i in X_train[2]])
26/26:
d = dict([(value,key) for (key,value) in word_index.items()])

" ".join([d.get(i-3,"?") for i in X_train[4]])
26/27:
d = dict([(value,key) for (key,value) in word_index.items()])

" ".join([d.get(i-3,"?") for i in X_train[6]])
27/1:
from tensorflow.keras.datasets import imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
    num_words=10000)
27/2: train_data[0]
27/3:
word_index = imdb.get_word_index()
reverse_word_index = dict(
    [(value, key) for (key, value) in word_index.items()])
decoded_review = " ".join(
    [reverse_word_index.get(i - 3, "?") for i in train_data[0]])
27/4:
word_index = imdb.get_word_index()
reverse_word_index = dict(
    [(value, key) for (key, value) in word_index.items()])
decoded_review = " ".join(
    [reverse_word_index.get(i - 3, "?") for i in train_data[0]])

decoded_review
27/5:
import numpy as np
def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        for j in sequence:
            results[i, j] = 1.
    return results
    
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
27/6: x_train[0]
27/7: x_train[0].shape
27/8: x_train[0]
27/9: train_data
27/10: train_data[0]
27/11: x_train[0][:4]
27/12: x_train[0][:10]
27/13:
y_train = np.asarray(train_labels).astype("float32")
y_test = np.asarray(test_labels).astype("float32")
27/14:
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
27/15:
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
27/16:
x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]
27/17:
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
27/18:
history_dict = history.history
history_dict.keys()
27/19:
import matplotlib.pyplot as plt
history_dict = history.history
loss_values = history_dict["loss"]
val_loss_values = history_dict["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()
27/20:
plt.clf()
acc = history_dict["accuracy"]
val_acc = history_dict["val_accuracy"]
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()
27/21:
model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
model.fit(x_train, y_train, epochs=4, batch_size=512)
results = model.evaluate(x_test, y_test)
27/22: results
27/23: model.predict(x_test)
27/24:
from tensorflow.keras.datasets import reuters
(train_data, train_labels), (test_data, test_labels) = reuters.load_data(
    num_words=10000)
27/25: len(train_data)
27/26: len(test_data)
27/27: train_data[10]
27/28:
word_index = reuters.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
decoded_newswire = " ".join([reverse_word_index.get(i - 3, "?") for i in
    train_data[0]])
27/29: train_labels[10]
27/30:
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)
27/31:
def to_one_hot(labels, dimension=46):
    results = np.zeros((len(labels), dimension))
    for i, label in enumerate(labels):
        results[i, label] = 1.
    return results
y_train = to_one_hot(train_labels)
y_test = to_one_hot(test_labels)
27/32:
from tensorflow.keras.utils import to_categorical
y_train = to_categorical(train_labels)
y_test = to_categorical(test_labels)
27/33:
model = keras.Sequential([
    layers.Dense(64, activation="relu"),
    layers.Dense(64, activation="relu"),
    layers.Dense(46, activation="softmax")
])
27/34:
model.compile(optimizer="rmsprop",
              loss="categorical_crossentropy",
              metrics=["accuracy"])
27/35:
x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = y_train[:1000]
partial_y_train = y_train[1000:]
27/36:
history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))
27/37:
loss = history.history["loss"]
val_loss = history.history["val_loss"]
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()
27/38:
plt.clf()
acc = history.history["accuracy"]
val_acc = history.history["val_accuracy"]
plt.plot(epochs, acc, "bo", label="Training accuracy")
plt.plot(epochs, val_acc, "b", label="Validation accuracy")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()
27/39:
model = keras.Sequential([
  layers.Dense(64, activation="relu"),
  layers.Dense(64, activation="relu"),
  layers.Dense(46, activation="softmax")
])
model.compile(optimizer="rmsprop",
              loss="categorical_crossentropy",
              metrics=["accuracy"])
model.fit(x_train,
          y_train,
          epochs=9,
          batch_size=512)
results = model.evaluate(x_test, y_test)
27/40:
import copy
test_labels_copy = copy.copy(test_labels)
np.random.shuffle(test_labels_copy)
hits_array = np.array(test_labels) == np.array(test_labels_copy)
hits_array.mean()
27/41: predictions = model.predict(x_test)
27/42: predictions[0].shape
27/43: np.sum(predictions[0])
27/44: np.argmax(predictions[0])
27/45:
y_train = np.array(train_labels)
y_test = np.array(test_labels)
27/46:
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
27/47:
model = keras.Sequential([
    layers.Dense(64, activation="relu"),
    layers.Dense(4, activation="relu"),
    layers.Dense(46, activation="softmax")
])
model.compile(optimizer="rmsprop",
              loss="categorical_crossentropy",
              metrics=["accuracy"])
model.fit(partial_x_train,
          partial_y_train,
          epochs=20,
          batch_size=128,
          validation_data=(x_val, y_val))
27/48: decoded_newswire
27/49: y_train
27/50:
def to_one_hot(labels, dimension=46):
    results = np.zeros((len(labels), dimension))
    for i, label in enumerate(labels):
        results[i, label] = 1.
    return results
y_train = to_one_hot(train_labels)
y_test = to_one_hot(test_labels)
27/51: y_train
27/52: y_train[0
27/53: y_train[0]
27/54: train_data
27/55: train_data[0]
27/56: x_train
27/57:
from tensorflow.keras.datasets import boston_housing
(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()
27/58: train_data.shape
27/59: test_data.shape
27/60: train_targets
27/61:
mean = train_data.mean(axis=0)
train_data -= mean
std = train_data.std(axis=0)
train_data /= std
test_data -= mean
test_data /= std
27/62:
def build_model():
    model = keras.Sequential([
        layers.Dense(64, activation="relu"),
        layers.Dense(64, activation="relu"),
        layers.Dense(1)
    ])
    model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
    return model
27/63:
k = 4
num_val_samples = len(train_data) // k
num_epochs = 100
all_scores = []
for i in range(k):
    print(f"Processing fold #{i}")
    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]
    partial_train_data = np.concatenate(
        [train_data[:i * num_val_samples],
         train_data[(i + 1) * num_val_samples:]],
        axis=0)
    partial_train_targets = np.concatenate(
        [train_targets[:i * num_val_samples],
         train_targets[(i + 1) * num_val_samples:]],
        axis=0)
    model = build_model()
    model.fit(partial_train_data, partial_train_targets,
              epochs=num_epochs, batch_size=16, verbose=0)
    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
    all_scores.append(val_mae)
27/64: all_scores
27/65: np.mean(all_scores)
27/66:
num_epochs = 500
all_mae_histories = []
for i in range(k):
    print(f"Processing fold #{i}")
    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]
    partial_train_data = np.concatenate(
        [train_data[:i * num_val_samples],
         train_data[(i + 1) * num_val_samples:]],
        axis=0)
    partial_train_targets = np.concatenate(
        [train_targets[:i * num_val_samples],
         train_targets[(i + 1) * num_val_samples:]],
        axis=0)
    model = build_model()
    history = model.fit(partial_train_data, partial_train_targets,
                        validation_data=(val_data, val_targets),
                        epochs=num_epochs, batch_size=16, verbose=0)
    mae_history = history.history["val_mae"]
    all_mae_histories.append(mae_history)
27/67:
average_mae_history = [
    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]
27/68:
plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)
plt.xlabel("Epochs")
plt.ylabel("Validation MAE")
plt.show()
27/69:
truncated_mae_history = average_mae_history[10:]
plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)
plt.xlabel("Epochs")
plt.ylabel("Validation MAE")
plt.show()
27/70:
model = build_model()
model.fit(train_data, train_targets,
          epochs=130, batch_size=16, verbose=0)
test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)
27/71:
model = build_model()
model.fit(train_data, train_targets,
          epochs=130, batch_size=16, verbose=0)
test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)
27/72: test_mae_score
27/73:
predictions = model.predict(test_data)
predictions[0]
33/1:
from tensorflow.keras.datasets import mnist
import numpy as np

(train_images, train_labels), _ = mnist.load_data()
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255

train_images_with_noise_channels = np.concatenate(
    [train_images, np.random.random((len(train_images), 784))], axis=1)

train_images_with_zeros_channels = np.concatenate(
    [train_images, np.zeros((len(train_images), 784))], axis=1)
33/2:
from tensorflow import keras
from tensorflow.keras import layers

def get_model():
    model = keras.Sequential([
        layers.Dense(512, activation="relu"),
        layers.Dense(10, activation="softmax")
    ])
    model.compile(optimizer="rmsprop",
                  loss="sparse_categorical_crossentropy",
                  metrics=["accuracy"])
    return model

model = get_model()
history_noise = model.fit(
    train_images_with_noise_channels, train_labels,
    epochs=10,
    batch_size=128,
    validation_split=0.2)

model = get_model()
history_zeros = model.fit(
    train_images_with_zeros_channels, train_labels,
    epochs=10,
    batch_size=128,
    validation_split=0.2)
33/3:
import matplotlib.pyplot as plt
val_acc_noise = history_noise.history["val_accuracy"]
val_acc_zeros = history_zeros.history["val_accuracy"]
epochs = range(1, 11)
plt.plot(epochs, val_acc_noise, "b-",
         label="Validation accuracy with noise channels")
plt.plot(epochs, val_acc_zeros, "b--",
         label="Validation accuracy with zeros channels")
plt.title("Effect of noise channels on validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
33/4:
(train_images, train_labels), _ = mnist.load_data()
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255

random_train_labels = train_labels[:]
np.random.shuffle(random_train_labels)

model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(10, activation="softmax")
])
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
model.fit(train_images, random_train_labels,
          epochs=100,
          batch_size=128,
          validation_split=0.2)
33/5:
(train_images, train_labels), _ = mnist.load_data()
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255

model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(10, activation="softmax")
])
model.compile(optimizer=keras.optimizers.RMSprop(1.),
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
model.fit(train_images, train_labels,
          epochs=10,
          batch_size=128,
          validation_split=0.2)
33/6:
model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(10, activation="softmax")
])
model.compile(optimizer=keras.optimizers.RMSprop(1e-2),
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
model.fit(train_images, train_labels,
          epochs=10,
          batch_size=128,
          validation_split=0.2)
33/7:
model = keras.Sequential([layers.Dense(10, activation="softmax")])
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
history_small_model = model.fit(
    train_images, train_labels,
    epochs=20,
    batch_size=128,
    validation_split=0.2)
33/8:
import matplotlib.pyplot as plt
val_loss = history_small_model.history["val_loss"]
epochs = range(1, 21)
plt.plot(epochs, val_loss, "b--",
         label="Validation loss")
plt.title("Effect of insufficient model capacity on validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
33/9:
model = keras.Sequential([
    layers.Dense(96, activation="relu"),
    layers.Dense(96, activation="relu"),
    layers.Dense(10, activation="softmax"),
])
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
history_large_model = model.fit(
    train_images, train_labels,
    epochs=20,
    batch_size=128,
    validation_split=0.2)
33/10:
from tensorflow.keras.datasets import imdb
(train_data, train_labels), _ = imdb.load_data(num_words=10000)

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results
train_data = vectorize_sequences(train_data)

model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
history_original = model.fit(train_data, train_labels,
                             epochs=20, batch_size=512, validation_split=0.4)
33/11:
model = keras.Sequential([
    layers.Dense(4, activation="relu"),
    layers.Dense(4, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
history_smaller_model = model.fit(
    train_data, train_labels,
    epochs=20, batch_size=512, validation_split=0.4)
33/12:
model = keras.Sequential([
    layers.Dense(512, activation="relu"),
    layers.Dense(512, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
history_larger_model = model.fit(
    train_data, train_labels,
    epochs=20, batch_size=512, validation_split=0.4)
33/13:
from tensorflow.keras import regularizers
model = keras.Sequential([
    layers.Dense(16,
                 kernel_regularizer=regularizers.l2(0.002),
                 activation="relu"),
    layers.Dense(16,
                 kernel_regularizer=regularizers.l2(0.002),
                 activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
history_l2_reg = model.fit(
    train_data, train_labels,
    epochs=20, batch_size=512, validation_split=0.4)
33/14:
from tensorflow.keras import regularizers
regularizers.l1(0.001)
regularizers.l1_l2(l1=0.001, l2=0.001)
33/15:
model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(16, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
history_dropout = model.fit(
    train_data, train_labels,
    epochs=20, batch_size=512, validation_split=0.4)
34/1:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
34/2:
from DateFunctions import MonthSelector


MonthSelector(1)
35/1:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
35/2:
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF()
df
35/3: from collections import counter
35/4: from collections import Counter
36/1:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
36/2:
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF()
df
36/3:
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(100,10)
df
36/4: df['MEMBERNBR'].value_counts()
36/5: df['MEMBERNBR'].unique()
36/6: pd.DataFrame(df['MEMBERNBR'].unique())
36/7: pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])
36/8:
branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']]
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = []
duration = []
random = []



pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])
36/9:
branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = []
duration = []
random = []



pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])
36/10:
branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']


df1 = pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])
df1['BRANCHNAME'] = np.random.choice(branches)
36/11:
branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']


df1 = pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])
df1['BRANCHNAME'] = np.random.choice(branches)
df1
36/12: [np.random.choice(branches) for x in range(len(df1)]]
36/13: [np.random.choice(branches) for x in range(len(df1)]
36/14: [np.random.choice(branches) for x in range(len(df1))]
36/15:
branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']


df1 = pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])
df1['BRANCHNAME'] = [np.random.choice(branches) for x in range(len(df1))]
df1
36/16:
branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']


df1 = pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])
df1['BRANCHNAME'] = [np.random.choice(branches) for x in range(len(df1))]
df1['CITY'] = [np.random.choice(city) for x in range(len(df1))]
df1['LOB'] = [np.random.choice(lob) for x in range(len(df1))]
df1['DURACTION'] = [np.random.choice(duration) for x in range(len(df1))]


df1
36/17:
final_df = df.merge(df1,on='MEMBERNBR',how='left')
final_df
36/18:
import pandas as pd
import itertools

def summarize_by_combinations_multiagg(df, group_vars, agg_col, agg_funcs=None):
    """
    Summarizes a DataFrame over all combinations of N group variables with multiple aggregations.

    Parameters:
    - df: input DataFrame
    - group_vars: list of column names to group by
    - agg_col: column to aggregate
    - agg_funcs: list of aggregation functions (e.g., ['sum', 'min', 'max', 'mean'])

    Returns:
    - summary_df: summarized DataFrame with all group combinations and aggregation results
    """
    if agg_funcs is None:
        agg_funcs = ['sum', 'min', 'max', 'mean']
    
    result_frames = []

    for i in range(1, len(group_vars) + 1):
        for combo in itertools.combinations(group_vars, i):
            group_df = df.groupby(list(combo))[agg_col].agg(agg_funcs).reset_index()
            for col in group_vars:
                if col not in combo:
                    group_df[col] = 'All'
            # Ensure column order and consistent column names
            group_df = group_df[group_vars + agg_funcs]
            result_frames.append(group_df)

    # Overall total (All for all group columns)
    overall = pd.DataFrame({col: ['All'] for col in group_vars})
    overall_stats = df[agg_col].agg(agg_funcs).to_frame().T.reset_index(drop=True)
    overall = pd.concat([overall, overall_stats], axis=1)
    result_frames.append(overall)

    summary_df = pd.concat(result_frames, ignore_index=True)
    return summary_df
36/19: summarize_by_combinations_multiagg(final_df,['BRANCHNAME','CITY','LOB','DURATION'])
36/20: final_df
36/21: summarize_by_combinations_multiagg(final_df,['BRANCHNAME','CITY','LOB','DURATION'],['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE'])
36/22:
def summarize_by_combinations_multiagg_multi_column(df, group_vars, agg_cols, agg_funcs=None):
    """
    Summarizes a DataFrame over all combinations of group_vars for multiple agg_cols and agg_funcs.

    Parameters:
    - df: input DataFrame
    - group_vars: list of columns to group by
    - agg_cols: list of numeric columns to aggregate
    - agg_funcs: list of aggregation functions (default: ['sum', 'min', 'max', 'mean'])

    Returns:
    - summary_df: DataFrame of all combinations with requested aggregations
    """
    import pandas as pd
    import itertools

    if agg_funcs is None:
        agg_funcs = ['sum', 'min', 'max', 'mean']
    
    result_frames = []

    for i in range(1, len(group_vars) + 1):
        for combo in itertools.combinations(group_vars, i):
            group_df = df.groupby(list(combo))[agg_cols].agg(agg_funcs).reset_index()

            # Flatten MultiIndex columns (e.g., ('DEPOSIT', 'sum') → 'DEPOSIT_sum')
            group_df.columns = [
                f"{col[0]}_{col[1]}" if isinstance(col, tuple) else col for col in group_df.columns
            ]

            # Add "All" for the missing group levels
            for col in group_vars:
                if col not in combo:
                    group_df[col] = 'All'

            # Reorder columns so grouping columns are first
            ordered_cols = group_vars + [col for col in group_df.columns if col not in group_vars]
            group_df = group_df[ordered_cols]

            result_frames.append(group_df)

    # Overall total row (All for all groups)
    overall = pd.DataFrame({col: ['All'] for col in group_vars})
    overall_stats = df[agg_cols].agg(agg_funcs).T
    overall_stats.columns = [f"{col}_{func}" for func in agg_funcs for col in [overall_stats.index[0]]]
    overall_stats = overall_stats.stack().reset_index()
    overall_stats = overall_stats.pivot(columns="level_1", values=0).reset_index(drop=True)
    overall_row = pd.concat([overall, overall_stats], axis=1)
    result_frames.append(overall_row)

    summary_df = pd.concat(result_frames, ignore_index=True)
    return summary_df
36/23: summarize_by_combinations_multiagg(final_df,['BRANCHNAME','CITY','LOB','DURATION'],['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE'])
36/24:
def summarize_by_combinations_multiagg_multi_column(df, group_vars, agg_cols, agg_funcs=None):
    """
    Summarizes a DataFrame over all combinations of group_vars for multiple agg_cols and agg_funcs.

    Parameters:
    - df: input DataFrame
    - group_vars: list of columns to group by
    - agg_cols: list of numeric columns to aggregate
    - agg_funcs: list of aggregation functions (default: ['sum', 'min', 'max', 'mean'])

    Returns:
    - summary_df: DataFrame of all combinations with requested aggregations
    """
    import pandas as pd
    import itertools

    if agg_funcs is None:
        agg_funcs = ['sum', 'min', 'max', 'mean']
    
    result_frames = []

    for i in range(1, len(group_vars) + 1):
        for combo in itertools.combinations(group_vars, i):
            group_df = df.groupby(list(combo))[agg_cols].agg(agg_funcs).reset_index()

            # Flatten MultiIndex columns (e.g., ('DEPOSIT', 'sum') → 'DEPOSIT_sum')
            group_df.columns = [
                f"{col[0]}_{col[1]}" if isinstance(col, tuple) else col for col in group_df.columns
            ]

            # Add "All" for the missing group levels
            for col in group_vars:
                if col not in combo:
                    group_df[col] = 'All'

            # Reorder columns so grouping columns are first
            ordered_cols = group_vars + [col for col in group_df.columns if col not in group_vars]
            group_df = group_df[ordered_cols]

            result_frames.append(group_df)

    # Overall total row (All for all groups)
    overall = pd.DataFrame({col: ['All'] for col in group_vars})
    overall_stats = df[agg_cols].agg(agg_funcs).T
    overall_stats.columns = [f"{col}_{func}" for func in agg_funcs for col in [overall_stats.index[0]]]
    overall_stats = overall_stats.stack().reset_index()
    overall_stats = overall_stats.pivot(columns="level_1", values=0).reset_index(drop=True)
    overall_row = pd.concat([overall, overall_stats], axis=1)
    result_frames.append(overall_row)

    summary_df = pd.concat(result_frames, ignore_index=True)
    return summary_df
36/25: summarize_by_combinations_multiagg(final_df,['BRANCHNAME','CITY','LOB','DURATION'],['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE'])
36/26:
def summarize_by_combinations_multiagg(df, group_vars, agg_cols, agg_funcs=None):
    """
    Summarizes a DataFrame over all combinations of group_vars for multiple agg_cols and agg_funcs.

    Parameters:
    - df: input DataFrame
    - group_vars: list of columns to group by
    - agg_cols: list of numeric columns to aggregate
    - agg_funcs: list of aggregation functions (default: ['sum', 'min', 'max', 'mean'])

    Returns:
    - summary_df: DataFrame of all combinations with requested aggregations
    """
    import pandas as pd
    import itertools

    if agg_funcs is None:
        agg_funcs = ['sum', 'min', 'max', 'mean']
    
    result_frames = []

    for i in range(1, len(group_vars) + 1):
        for combo in itertools.combinations(group_vars, i):
            group_df = df.groupby(list(combo))[agg_cols].agg(agg_funcs).reset_index()

            # Flatten MultiIndex columns (e.g., ('DEPOSIT', 'sum') → 'DEPOSIT_sum')
            group_df.columns = [
                f"{col[0]}_{col[1]}" if isinstance(col, tuple) else col for col in group_df.columns
            ]

            # Add "All" for the missing group levels
            for col in group_vars:
                if col not in combo:
                    group_df[col] = 'All'

            # Reorder columns so grouping columns are first
            ordered_cols = group_vars + [col for col in group_df.columns if col not in group_vars]
            group_df = group_df[ordered_cols]

            result_frames.append(group_df)

    # Overall total row (All for all groups)
    overall = pd.DataFrame({col: ['All'] for col in group_vars})
    overall_stats = df[agg_cols].agg(agg_funcs).T
    overall_stats.columns = [f"{col}_{func}" for func in agg_funcs for col in [overall_stats.index[0]]]
    overall_stats = overall_stats.stack().reset_index()
    overall_stats = overall_stats.pivot(columns="level_1", values=0).reset_index(drop=True)
    overall_row = pd.concat([overall, overall_stats], axis=1)
    result_frames.append(overall_row)

    summary_df = pd.concat(result_frames, ignore_index=True)
    return summary_df
36/27: summarize_by_combinations_multiagg(final_df,['BRANCHNAME','CITY','LOB','DURATION'],['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE'])
36/28: final_df
36/29: summarize_by_combinations_multiagg(final_df,['BRANCHNAME','CITY','LOB','DURATION'],['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE'])
36/30: final_df
36/31:
branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']
df1 = pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])
df1['BRANCHNAME'] = [np.random.choice(branches) for x in range(len(df1))]
df1['CITY'] = [np.random.choice(city) for x in range(len(df1))]
df1['LOB'] = [np.random.choice(lob) for x in range(len(df1))]
df1['DURATION'] = [np.random.choice(duration) for x in range(len(df1))]

from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(100,10)
final_df = df.merge(df1,on='MEMBERNBR',how='left')
36/32:
def summarize_by_combinations_multiagg(df, group_vars, agg_cols, agg_funcs=None):
    """
    Summarizes a DataFrame over all combinations of group_vars for multiple agg_cols and agg_funcs.

    Parameters:
    - df: input DataFrame
    - group_vars: list of columns to group by
    - agg_cols: list of numeric columns to aggregate
    - agg_funcs: list of aggregation functions (default: ['sum', 'min', 'max', 'mean'])

    Returns:
    - summary_df: DataFrame of all combinations with requested aggregations
    """
    import pandas as pd
    import itertools

    if agg_funcs is None:
        agg_funcs = ['sum', 'min', 'max', 'mean']
    
    result_frames = []

    for i in range(1, len(group_vars) + 1):
        for combo in itertools.combinations(group_vars, i):
            group_df = df.groupby(list(combo))[agg_cols].agg(agg_funcs).reset_index()

            # Flatten MultiIndex columns (e.g., ('DEPOSIT', 'sum') → 'DEPOSIT_sum')
            group_df.columns = [
                f"{col[0]}_{col[1]}" if isinstance(col, tuple) else col for col in group_df.columns
            ]

            # Add "All" for the missing group levels
            for col in group_vars:
                if col not in combo:
                    group_df[col] = 'All'

            # Reorder columns so grouping columns are first
            ordered_cols = group_vars + [col for col in group_df.columns if col not in group_vars]
            group_df = group_df[ordered_cols]

            result_frames.append(group_df)

    # Overall total row (All for all groups)
    overall = pd.DataFrame({col: ['All'] for col in group_vars})
    overall_stats = df[agg_cols].agg(agg_funcs).T
    overall_stats.columns = [f"{col}_{func}" for func in agg_funcs for col in [overall_stats.index[0]]]
    overall_stats = overall_stats.stack().reset_index()
    overall_stats = overall_stats.pivot(columns="level_1", values=0).reset_index(drop=True)
    overall_row = pd.concat([overall, overall_stats], axis=1)
    result_frames.append(overall_row)

    summary_df = pd.concat(result_frames, ignore_index=True)
    return summary_df
36/33: summarize_by_combinations_multiagg(final_df,['BRANCHNAME','CITY','LOB','DURATION'],['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE'])
36/34: print(final_df.columns.tolist())
36/35:
def SummarizeDFbyColumns(df, group_vars, agg_cols, agg_funcs=None):
    """
    Summarizes a DataFrame over all combinations of group_vars for multiple agg_cols and agg_funcs.

    Parameters:
    - df: input DataFrame
    - group_vars: list of columns to group by
    - agg_cols: list of numeric columns to aggregate
    - agg_funcs: list of aggregation functions (default: ['sum', 'min', 'max', 'mean'])

    Returns:
    - summary_df: DataFrame of all combinations with requested aggregations
    """
    import pandas as pd
    import itertools

    if agg_funcs is None:
        agg_funcs = ['sum', 'min', 'max', 'mean']
    
    result_frames = []

    for i in range(1, len(group_vars) + 1):
        for combo in itertools.combinations(group_vars, i):
            group_df = df.groupby(list(combo))[agg_cols].agg(agg_funcs).reset_index()

            # Flatten MultiIndex columns (e.g., ('DEPOSIT', 'sum') → 'DEPOSIT_sum')
            group_df.columns = [
                f"{col[0]}_{col[1]}" if isinstance(col, tuple) else col for col in group_df.columns
            ]

            # Add "All" for the missing group levels
            for col in group_vars:
                if col not in combo:
                    group_df[col] = 'All'

            # Reorder columns so grouping columns are first
            ordered_cols = group_vars + [col for col in group_df.columns if col not in group_vars]
            group_df = group_df[ordered_cols]

            result_frames.append(group_df)

    # Overall total row (All for all groups)
    overall = pd.DataFrame({col: ['All'] for col in group_vars})
    overall_stats = df[agg_cols].agg(agg_funcs).T
    overall_stats.columns = [f"{col}_{func}" for func in agg_funcs for col in [overall_stats.index[0]]]
    overall_stats = overall_stats.stack().reset_index()
    overall_stats = overall_stats.pivot(columns="level_1", values=0).reset_index(drop=True)
    overall_row = pd.concat([overall, overall_stats], axis=1)
    result_frames.append(overall_row)

    summary_df = pd.concat(result_frames, ignore_index=True)
    return summary_df
36/36: SummarizeDFbyColumns(final_df,['BRANCHNAME','CITY','LOB','DURATION'],['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE'])
36/37:
import pandas as pd
import itertools

def SummarizeDFbyColumns(df, group_vars, agg_cols, agg_funcs=None):
    """
    Summarizes a DataFrame over all combinations of group_vars for multiple agg_cols and agg_funcs.

    Parameters:
    - df: input DataFrame
    - group_vars: list of columns to group by
    - agg_cols: list of numeric columns to aggregate
    - agg_funcs: list of aggregation functions (default: ['sum', 'min', 'max', 'mean'])

    Returns:
    - summary_df: DataFrame of all combinations with requested aggregations
    """
    if agg_funcs is None:
        agg_funcs = ['sum', 'min', 'max', 'mean']
    
    result_frames = []

    for i in range(1, len(group_vars) + 1):
        for combo in itertools.combinations(group_vars, i):
            group_df = df.groupby(list(combo))[agg_cols].agg(agg_funcs).reset_index()

            # Flatten MultiIndex columns (e.g., ('DEPOSIT', 'sum') → 'DEPOSIT_sum')
            group_df.columns = [
                f"{col[0]}_{col[1]}" if isinstance(col, tuple) else col for col in group_df.columns
            ]

            # Add "All" for the missing group levels
            for col in group_vars:
                if col not in combo:
                    group_df[col] = 'All'

            # Reorder columns so grouping columns are first
            ordered_cols = group_vars + [col for col in group_df.columns if col not in group_vars]
            group_df = group_df[ordered_cols]

            result_frames.append(group_df)

    # Overall total row (All for all groups)
    overall = pd.DataFrame({col: ['All'] for col in group_vars})
    overall_stats = df[agg_cols].agg(agg_funcs)

    # Flatten MultiIndex: ('DEPOSIT', 'sum') → 'DEPOSIT_sum'
    overall_stats.index = [f"{i[0]}_{i[1]}" for i in overall_stats.index]
    overall_stats = overall_stats.to_frame().T.reset_index(drop=True)

    overall_row = pd.concat([overall, overall_stats], axis=1)
    result_frames.append(overall_row)

    summary_df = pd.concat(result_frames, ignore_index=True)
    return summary_df
36/38: SummarizeDFbyColumns(final_df,['BRANCHNAME','CITY','LOB','DURATION'],['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE'])
36/39:
import pandas as pd
import itertools

def SummarizeDFbyColumns(df, group_vars, agg_cols, agg_funcs=None):
    """
    Summarizes a DataFrame over all combinations of group_vars for multiple agg_cols and agg_funcs.
    """
    if agg_funcs is None:
        agg_funcs = ['sum', 'min', 'max', 'mean']

    result_frames = []

    for i in range(1, len(group_vars) + 1):
        for combo in itertools.combinations(group_vars, i):
            group_df = df.groupby(list(combo), dropna=False)[agg_cols].agg(agg_funcs).reset_index()

            # Flatten MultiIndex columns (e.g., ('DEPOSIT', 'sum') → 'DEPOSIT_sum')
            group_df.columns = [
                f"{col[0]}_{col[1]}" if isinstance(col, tuple) else col for col in group_df.columns
            ]

            # Fill in 'All' for unused group_vars
            for col in group_vars:
                if col not in combo:
                    group_df[col] = 'All'

            # Ensure consistent column order
            existing_group_vars = [col for col in group_vars if col in group_df.columns]
            other_cols = [col for col in group_df.columns if col not in existing_group_vars]
            group_df = group_df[existing_group_vars + other_cols]

            result_frames.append(group_df)

    # Add overall totals row
    overall = pd.DataFrame({col: ['All'] for col in group_vars})
    overall_stats = df[agg_cols].agg(agg_funcs)
    overall_stats.index = [f"{col[0]}_{col[1]}" for col in overall_stats.index]
    overall_stats = overall_stats.to_frame().T.reset_index(drop=True)
    overall_row = pd.concat([overall, overall_stats], axis=1)

    result_frames.append(overall_row)

    summary_df = pd.concat(result_frames, ignore_index=True)
    return summary_df
36/40: SummarizeDFbyColumns(final_df,['BRANCHNAME','CITY','LOB','DURATION'],['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE'])
36/41:
import pandas as pd
import itertools

def SummarizeDFbyColumns(df, group_vars, agg_cols, agg_funcs=None):
    """
    Summarizes a DataFrame over all combinations of group_vars for multiple agg_cols and agg_funcs.
    """
    if agg_funcs is None:
        agg_funcs = ['sum', 'min', 'max', 'mean']

    result_frames = []

    for i in range(1, len(group_vars) + 1):
        for combo in itertools.combinations(group_vars, i):
            group_df = df.groupby(list(combo), dropna=False)[agg_cols].agg(agg_funcs).reset_index()

            # Flatten MultiIndex columns (e.g., ('DEPOSIT', 'sum') → 'DEPOSIT_sum')
            group_df.columns = [
                f"{col[0]}_{col[1]}" if isinstance(col, tuple) else col for col in group_df.columns
            ]

            # Fill in 'All' for unused group_vars
            for col in group_vars:
                if col not in combo:
                    group_df[col] = 'All'

            # Ensure consistent column order
            existing_group_vars = [col for col in group_vars if col in group_df.columns]
            other_cols = [col for col in group_df.columns if col not in existing_group_vars]
            group_df = group_df[existing_group_vars + other_cols]

            result_frames.append(group_df)

    # Add overall totals row
    overall = pd.DataFrame({col: ['All'] for col in group_vars})
    overall_stats = df[agg_cols].agg(agg_funcs)
    overall_stats.index = [f"{col[0]}_{col[1]}" for col in overall_stats.index]
    overall_stats = overall_stats.reset_index(drop=True)
    overall_row = pd.concat([overall, overall_stats], axis=1)

    result_frames.append(overall_row)

    summary_df = pd.concat(result_frames, ignore_index=True)
    return summary_df
36/42: SummarizeDFbyColumns(final_df,['BRANCHNAME','CITY','LOB','DURATION'],['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE'])
36/43:
group_vars = ['BRANCHNAME','CITY','LOB','DURATION']

agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE'])

agg_funcs = ['sum', 'min', 'max', 'mean']

for i in range(1, len(group_vars) + 1):
        for combo in itertools.combinations(group_vars, i):
            group_df = df.groupby(list(combo), dropna=False)[agg_cols].agg(agg_funcs).reset_index()



group_df
36/44:
group_vars = ['BRANCHNAME','CITY','LOB','DURATION']

agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']

agg_funcs = ['sum', 'min', 'max', 'mean']

for i in range(1, len(group_vars) + 1):
        for combo in itertools.combinations(group_vars, i):
            group_df = df.groupby(list(combo), dropna=False)[agg_cols].agg(agg_funcs).reset_index()



group_df
36/45:
df = final_df.copy()

group_vars = ['BRANCHNAME','CITY','LOB','DURATION']

agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']

agg_funcs = ['sum', 'min', 'max', 'mean']

for i in range(1, len(group_vars) + 1):
        for combo in itertools.combinations(group_vars, i):
            group_df = df.groupby(list(combo), dropna=False)[agg_cols].agg(agg_funcs).reset_index()

group_df
36/46: final_df
36/47:
df = final_df.copy()

group_vars = ['BRANCHNAME','CITY','LOB','DURATION']

agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']

agg_funcs = ['sum', 'min', 'max', 'mean','count']

for i in range(1, len(group_vars) + 1):
        for combo in itertools.combinations(group_vars, i):
            group_df = df.groupby(list(combo), dropna=False)[agg_cols].agg(agg_funcs).reset_index()

group_df
36/48:
from DFProcessing import TransposePivotTable


TransposePivotTable(groupby)
36/49:
from DFProcessing import TransposePivotTable


TransposePivotTable(group_df)
36/50: from DFProcessing import TranposeNonTimeSeriesDF
36/51:
from DFProcessing import TranposeNonTimeSeriesDF


TranposeNonTimeSeriesDF(group_df,group_vars,)
36/52: group_df.columns
36/53:
from DFProcessing import TranposeNonTimeSeriesDF


TranposeNonTimeSeriesDF(group_df,group_vars,(   'DEPOSIT',   'sum'))
36/54:
from DFProcessing import TranposeNonTimeSeriesDF


TranposeNonTimeSeriesDF(group_df,group_vars,[(   'DEPOSIT',   'sum')])
36/55:
from DFProcessing import TranposeNonTimeSeriesDF


TranposeNonTimeSeriesDF(group_df.reset_index(),group_vars,[(   'DEPOSIT',   'sum')])
36/56: group_df.columns
36/57: group_df[(   'DEPOSIT',   'sum')
36/58: group_df[(   'DEPOSIT',   'sum')]
36/59: group_df[(   'DEPOSIT',   'sum')].sum()
36/60: final_df['DEPOSIT'].sum()
36/61:
df = final_df.copy()

group_vars = ['BRANCHNAME','CITY','LOB','DURATION']

agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']

agg_funcs = ['sum', 'min', 'max', 'mean','count']

for i in range(1, len(group_vars) + 1):
    for combo in itertools.combinations(group_vars, i):
        group_df = df.groupby(list(combo), dropna=False)[agg_cols].agg(agg_funcs).reset_index()

group_df
36/62:
df = final_df.copy()

group_vars = ['BRANCHNAME','CITY','LOB','DURATION']

agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']

agg_funcs = ['sum', 'min', 'max', 'mean','count']

for i in range(1, len(group_vars) + 1):
    for combo in itertools.combinations(group_vars, i):
group_df = df.groupby(list(combo), dropna=False)[agg_cols].agg(agg_funcs).reset_index()

group_df
36/63:
df = final_df.copy()

group_vars = ['BRANCHNAME','CITY','LOB','DURATION']

agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']

agg_funcs = ['sum', 'min', 'max', 'mean','count']

for i in range(1, len(group_vars) + 1):
    for combo in itertools.combinations(group_vars, i):
        #group_df = df.groupby(list(combo), dropna=False)[agg_cols].agg(agg_funcs).reset_index()
        print(combo)

group_df
36/64:
df = final_df.copy()

group_vars = ['BRANCHNAME','CITY','LOB','DURATION']

agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']

agg_funcs = ['sum', 'mean','count']

for i in range(1, len(group_vars) + 1):
    for combo in itertools.combinations(group_vars, i):
        #group_df = df.groupby(list(combo), dropna=False)[agg_cols].agg(agg_funcs).reset_index()
        print(combo)

group_df
36/65:
import pandas as pd
import itertools


def long_format_summary(df, group_vars, agg_cols, agg_funcs):
    """
    Summarizes df grouped by group_vars, with one row per (group, metric, function) and a single VALUE column.

    Parameters:
    - df: input DataFrame
    - group_vars: list of columns to group by
    - agg_cols: list of numeric columns to summarize
    - agg_funcs: list of aggregation functions (e.g., ['sum', 'mean', 'min', 'max'])

    Returns:
    - long_df: tidy DataFrame with columns: group_vars + ['METRIC', 'FUNCTION', 'VALUE']
    """
    # Multi-level aggregation
    agg_df = df.groupby(group_vars)[agg_cols].agg(agg_funcs).reset_index()

    # Flatten multi-index columns
    agg_df.columns = group_vars + [f"{col}_{func}" for col in agg_cols for func in agg_funcs]

    # Melt to long format
    long_df = agg_df.melt(id_vars=group_vars, var_name='METRIC_FUNC', value_name='VALUE')

    # Split "DEPOSIT_sum" → "DEPOSIT", "sum"
    long_df[['METRIC', 'FUNCTION']] = long_df['METRIC_FUNC'].str.rsplit('_', n=1, expand=True)
    long_df = long_df.drop(columns='METRIC_FUNC')

    # Optional: sort for readability
    return long_df.sort_values(by=group_vars + ['METRIC', 'FUNCTION']).reset_index(drop=True)


group_vars = ['BRANCHNAME','CITY','LOB','DURATION']
agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
agg_funcs = ['sum', 'mean','count']


long_format_summary(final_df,group_vars,agg_cols,agg_funcs)
36/66:
import pandas as pd
import itertools


def long_format_summary(df, group_vars, agg_cols, agg_funcs):
    """
    Summarizes df grouped by group_vars, with one row per (group, metric, function) and a single VALUE column.

    Parameters:
    - df: input DataFrame
    - group_vars: list of columns to group by
    - agg_cols: list of numeric columns to summarize
    - agg_funcs: list of aggregation functions (e.g., ['sum', 'mean', 'min', 'max'])

    Returns:
    - long_df: tidy DataFrame with columns: group_vars + ['METRIC', 'FUNCTION', 'VALUE']
    """
    # Multi-level aggregation
    agg_df = df.groupby(group_vars)[agg_cols].agg(agg_funcs).reset_index()

    # Flatten multi-index columns
    agg_df.columns = group_vars + [f"{col}_{func}" for col in agg_cols for func in agg_funcs]

    # Melt to long format
    long_df = agg_df.melt(id_vars=group_vars, var_name='METRIC_FUNC', value_name='VALUE')

    # Split "DEPOSIT_sum" → "DEPOSIT", "sum"
    long_df[['METRIC', 'FUNCTION']] = long_df['METRIC_FUNC'].str.rsplit('_', n=1, expand=True)
    long_df = long_df.drop(columns='METRIC_FUNC')

    # Optional: sort for readability
    return long_df.sort_values(by=group_vars + ['METRIC', 'FUNCTION']).reset_index(drop=True)


group_vars = ['BRANCHNAME','CITY','LOB','DURATION']
agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
agg_funcs = ['sum', 'mean','count']


d = long_format_summary(final_df,group_vars,agg_cols,agg_funcs)
36/67:
import pandas as pd
import itertools


def long_format_summary(df, group_vars, agg_cols, agg_funcs):
    """
    Summarizes df grouped by group_vars, with one row per (group, metric, function) and a single VALUE column.

    Parameters:
    - df: input DataFrame
    - group_vars: list of columns to group by
    - agg_cols: list of numeric columns to summarize
    - agg_funcs: list of aggregation functions (e.g., ['sum', 'mean', 'min', 'max'])

    Returns:
    - long_df: tidy DataFrame with columns: group_vars + ['METRIC', 'FUNCTION', 'VALUE']
    """
    # Multi-level aggregation
    agg_df = df.groupby(group_vars)[agg_cols].agg(agg_funcs).reset_index()

    # Flatten multi-index columns
    agg_df.columns = group_vars + [f"{col}_{func}" for col in agg_cols for func in agg_funcs]

    # Melt to long format
    long_df = agg_df.melt(id_vars=group_vars, var_name='METRIC_FUNC', value_name='VALUE')

    # Split "DEPOSIT_sum" → "DEPOSIT", "sum"
    long_df[['METRIC', 'FUNCTION']] = long_df['METRIC_FUNC'].str.rsplit('_', n=1, expand=True)
    long_df = long_df.drop(columns='METRIC_FUNC')

    # Optional: sort for readability
    return long_df.sort_values(by=group_vars + ['METRIC', 'FUNCTION']).reset_index(drop=True)


group_vars = ['BRANCHNAME','CITY','LOB','DURATION']
agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
agg_funcs = ['sum', 'mean','count']


d = long_format_summary(final_df,group_vars,agg_cols,agg_funcs)

d
36/68: d[d['FUNCTION']=='sum'].pivot_table(index=group_vars,columns='METRIC',values='VALUE',aggfunc='sum')
36/69: df
36/70:
df = final_df.copy()
group_vars = ['BRANCHNAME','CITY','LOB','DURATION','MONTH']
agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
agg_funcs = ['sum', 'mean','count']


d = long_format_summary(final_df,group_vars,agg_cols,agg_funcs)
d
36/71: d[d['FUNCTION']=='sum'].pivot_table(index=group_vars,columns='METRIC',values='VALUE',aggfunc='sum')
36/72: d[d['FUNCTION']=='sum'].pivot_table(index=group_vars,columns='MONTH',values='VALUE',aggfunc='sum')
36/73: d[d['FUNCTION']=='sum'].pivot_table(index=group_vars,columns='METRIC',values='VALUE',aggfunc='sum')
36/74: d[d['FUNCTION']=='sum'].pivot_table(index=group_vars,columns='MONTH',values='VALUE',aggfunc='sum')
36/75: d[d['FUNCTION']=='sum'].pivot_table(index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='VALUE',aggfunc='sum')
36/76: d[d['FUNCTION']=='sum'].pivot_table(index=['BRANCHNAME','CITY','LOB','DURATION','METRIC'],columns='MONTH',values='VALUE',aggfunc='sum')
36/77: final_df
36/78: final_df.head()
36/79: from DFProcessing import CreatePivotTableFromTimeSeries
36/80:
from DFProcessing import CreatePivotTableFromTimeSeries

CreatePivotTableFromTimeSeries(final_df,index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',value='DEPOSIT'])
36/81:
from DFProcessing import CreatePivotTableFromTimeSeries

CreatePivotTableFromTimeSeries(final_df,index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT'])
36/82:
from DFProcessing import CreatePivotTableFromTimeSeries

CreatePivotTableFromTimeSeries(final_df,index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT')
36/83:
branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']



df1 = pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])
df1['BRANCHNAME'] = [np.random.choice(branches) for x in range(len(df1))]
df1['CITY'] = [np.random.choice(city) for x in range(len(df1))]
df1['LOB'] = [np.random.choice(lob) for x in range(len(df1))]
df1['DURATION'] = [np.random.choice(duration) for x in range(len(df1))]

from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(100,20)
final_df = df.merge(df1,on='MEMBERNBR',how='left')
36/84:
from DFProcessing import CreatePivotTableFromTimeSeries

CreatePivotTableFromTimeSeries(final_df,index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT')
36/85:
from DFProcessing import CreatePivotTableFromTimeSeries

CreatePivotTableFromTimeSeries(final_df,index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT').to_csv('test.csv')
36/86:
from DFProcessing import CreatePivotTableFromTimeSeries

CreatePivotTableFromTimeSeries(final_df,index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT')
36/87:
from DFProcessing import CreatePivotTableFromTimeSeries

CreatePivotTableFromTimeSeries(final_df[final_df['MONTH']==0],index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT')
38/1:
branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']

df1 = pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])
df1['BRANCHNAME'] = [np.random.choice(branches) for x in range(len(df1))]
df1['CITY'] = [np.random.choice(city) for x in range(len(df1))]
df1['LOB'] = [np.random.choice(lob) for x in range(len(df1))]
df1['DURATION'] = [np.random.choice(duration) for x in range(len(df1))]

from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(100,20)
final_df = df.merge(df1,on='MEMBERNBR',how='left')
38/2:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
38/3:
branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']

df1 = pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])
df1['BRANCHNAME'] = [np.random.choice(branches) for x in range(len(df1))]
df1['CITY'] = [np.random.choice(city) for x in range(len(df1))]
df1['LOB'] = [np.random.choice(lob) for x in range(len(df1))]
df1['DURATION'] = [np.random.choice(duration) for x in range(len(df1))]

from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(100,20)
final_df = df.merge(df1,on='MEMBERNBR',how='left')
38/4:
df = GenerateFakeMemberDF(100,20)


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']

df1 = pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])
df1['BRANCHNAME'] = [np.random.choice(branches) for x in range(len(df1))]
df1['CITY'] = [np.random.choice(city) for x in range(len(df1))]
df1['LOB'] = [np.random.choice(lob) for x in range(len(df1))]
df1['DURATION'] = [np.random.choice(duration) for x in range(len(df1))]

from DataSets import GenerateFakeMemberDF


final_df = df.merge(df1,on='MEMBERNBR',how='left')
38/5:
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(100,20)


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']

df1 = pd.DataFrame(df['MEMBERNBR'].unique(),columns=['MEMBERNBR'])
df1['BRANCHNAME'] = [np.random.choice(branches) for x in range(len(df1))]
df1['CITY'] = [np.random.choice(city) for x in range(len(df1))]
df1['LOB'] = [np.random.choice(lob) for x in range(len(df1))]
df1['DURATION'] = [np.random.choice(duration) for x in range(len(df1))]

final_df = df.merge(df1,on='MEMBERNBR',how='left')
38/6: final_df.head()
38/7:
from DFProcessing import CreatePivotTableFromTimeSeries

CreatePivotTableFromTimeSeries(final_df[final_df['MONTH']==0],index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT')
38/8:
from DFProcessing import CreatePivotTableFromTimeSeries

CreatePivotTableFromTimeSeries(final_df[final_df['MONTH']<=3],index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT')
38/9:
from DFProcessing import CreatePivotTableFromTimeSeries

CreatePivotTableFromTimeSeries(final_df[final_df['MONTH']<=6],index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT')
38/10: DataFrameColumnObservations(final_df,['BRANCHNAME','CITY','LOB'])
38/11:
from DFProcessing import DataFrameColumnObservations
DataFrameColumnObservations(final_df,['BRANCHNAME','CITY','LOB'])
38/12: from Visualization import SimpleBar
38/13: !conda install seaborn -y
38/14: from Visualization import SimpleBar
38/15:
from DFProcessing import DataFrameColumnObservations
d = DataFrameColumnObservations(final_df,['BRANCHNAME','CITY','LOB'])
d
38/16:
from Visualization import SimpleBar



SimpleBar(d['BRANCHNAME'].drop_duplicates('BRANCHNAME'], x = 'BRANCHNAME',y='BRANCHNAME_OBS')
38/17:
from Visualization import SimpleBar



SimpleBar(d['BRANCHNAME'].drop_duplicates('BRANCHNAME'), x = 'BRANCHNAME',y='BRANCHNAME_OBS')
38/18:
from Visualization import SimpleBar



SimpleBar(d['BRANCHNAME'].drop_duplicates('BRANCHNAME'),'BRANCHNAME','BRANCHNAME_OBS')
38/19:
from Visualization import SimpleBar



SimpleBar(d.drop_duplicates('BRANCHNAME'),'BRANCHNAME','BRANCHNAME_OBS')
38/20: SimpleBar(d.drop_duplicates('CITY'),'CITY','CITY_OBS')
38/21: final_df
38/22:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index()
            final_df = pd.concat([final_df,temp])

    return final_df

CreateMultiplePivotTableFromTimeSeries(final_df,index,'MONTH',metrics)
38/23:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index()
            final_df = pd.concat([final_df,temp])

    return final_df

CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       columns= 'MONTH')
38/24:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index()
            final_df = pd.concat([final_df,temp])

    return final_df

CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
38/25:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index()

            print(temp.head())
            
            final_df = pd.concat([final_df,temp])

    return final_df

CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
38/26:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            print(key,metric)
            #temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index()

            #print(temp.head())
            
            #final_df = pd.concat([final_df,temp])
    return pass
    return final_df

CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
38/27:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            print(key,metric)
            #temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index()

            #print(temp.head())
            
            #final_df = pd.concat([final_df,temp])
    
    #return final_df

CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
38/28:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            print(key,metric)
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index()
            print(temp.head())
            
            #final_df = pd.concat([final_df,temp])
    
    #return final_df

CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
38/29:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            print(key,metric)
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index()
            print(temp.head(1))
            
            #final_df = pd.concat([final_df,temp])
    
    #return final_df

CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
38/30:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            print(key,metric)
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index()
            print(temp.head(1))
            
            final_df = pd.concat([final_df,temp])
    return final_df

CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
38/31:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index()            
            final_df = pd.concat([final_df,temp])
    return final_df

CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
38/32:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index()            
            final_df = pd.concat([final_df,temp])
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e.iloc[0]
38/33:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index() 
            for missing in [x for x in metric_list if x != metric]
            temp[metric] = 'All'
            final_df = pd.concat([final_df,temp])
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e.iloc[0]
38/34:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index() 
            for missing in [x for x in metric_list if x != metric]:
            temp[metric] = 'All'
            final_df = pd.concat([final_df,temp])
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e.iloc[0]
38/35:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index() 
            for missing in [x for x in metric_list if x != metric]:
                temp[metric] = 'All'
            final_df = pd.concat([final_df,temp])
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e.iloc[0]
38/36:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index() 
            for missing in [x for x in metric_list if x != metric]:
                temp[metric] = 'All'
            final_df = pd.concat([final_df,temp])
    return temp

e = CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e.iloc[0]
38/37:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index() 
            for missing in [x for x in index if x != index]:
                temp[missing] = 'All'
            final_df = pd.concat([final_df,temp])
    return temp

e = CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e.iloc[0]
38/38:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            final_df = pd.concat([final_df,temp])
    return temp

e = CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e.iloc[0]
38/39:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            final_df = pd.concat([final_df,temp])
    return temp

e = CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/40:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            final_df = pd.concat([final_df,temp])
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/41:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
                temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/42:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
                temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e.to_csv('test.csv')
38/43:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df,
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/44:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<6],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/45:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

columns='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,columns=columns,values=metric).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/46: e[e['BRANCHNAM']!='ALL']
38/47: e[e['BRANCHNAME']!='ALL']
38/48: e[e['BRANCHNAME']!='All']
38/49:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
    return pd.concat([final_dfCreatePivotTableFromTimeSeries(df,index=index_list,values=metric_list,columns=column).reset_index()

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/50:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
    return pd.concat([final_dfCreatePivotTableFromTimeSeries(df,index=index_list,values=metric_list,columns=column).reset_index()]

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/51:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
    return pd.concat([final_df,
                      CreatePivotTableFromTimeSeries(df,index=index_list,values=metric_list,columns=column).reset_index()]

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/52:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    for key in index_list:
        for metric in metric_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
    return pd.concat([final_df,
                      CreatePivotTableFromTimeSeries(df,index=index_list,values=metric_list,columns=column).reset_index()])

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/53: e[e['BRANCHNAME']!='All']
38/54:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric_list,columns=column).reset_index()
        final_df = pd.concat([final_df,temp])

    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/55: e[e['BRANCHNAME']!='Fenway Park']
38/56: e[e['BRANCHNAME']=='Fenway Park']
38/57:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/58: e[e['BRANCHNAME']=='Fenway Park']
38/59:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/60:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e.to_csv('test.csv',index=False)
38/61: final_df
38/62: final_df.pivot_table(columns='MONTH',values='DEPOSIT',aggfunc='sum')
38/63:
CreatePivotTableFromTimeSeries(df,index="",
                               values='DEPOSIT,
                               columns=column).reset_index()
38/64:
CreatePivotTableFromTimeSeries(df,index="",
                               values='DEPOSIT',
                               columns=column).reset_index()
38/65:
CreatePivotTableFromTimeSeries(df,index="",
                               values='DEPOSIT',
                               columns='MONTH').reset_index()
38/66:
CreatePivotTableFromTimeSeries(df,
                               values='DEPOSIT',
                               columns='MONTH').reset_index()
38/67:
metric_list = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']

for metric in metric_list:
    final_df.pivot_table(columns=metric_list,value='DEPOSIT',aggfunc='sum')
38/68:
metric_list = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']

for metric in metric_list:
    final_df.pivot_table(columns=metric_list,values=metric,aggfunc='sum')
38/69: final_df
38/70:
metric_list = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']

for metric in metric_list:
    final_df.pivot_table(columns=metric_list,values='MONTH',aggfunc='sum')
38/71:
metric_list = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']

for metric in metric_list:
    final_df.pivot_table(columns=metric_list,values='MONTH',aggfunc='sum')

final_df.pivot_table(columns=metric_list,values='MONTH',aggfunc='sum')
38/72:
metric_list = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']

for metric in metric_list:
    final_df.pivot_table(columns=metric_list,values='MONTH',aggfunc='sum')

final_df.pivot_table(columns='MONTH',values='DEPOSIT',aggfunc='sum')
38/73:
def CreatePivotTableFromTimeSeries(df,
                                   index,
                                   columns,
                                   values,
                                   aggfunc='sum',
                                   skipna=True):
    
    '''
    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.
    
    
    
    '''
    
    # 1. Pivot
    if index=None:
        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)

    else:
        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)

    # 2. Capture original month columns IMMEDIATELY after pivot
    month_cols = df1.columns.tolist()
 
    # 3. Add rolling window stats
    if len(month_cols) >= 3:
        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)
        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]
    
    if len(month_cols) >= 6:
        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)
        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]
        
    if len(month_cols) >= 12:
        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)
        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]
    
    try:
        df1['CHG_MOM'] = df1[month_cols[-1]]-df1[month_cols[-12]]
    except:
        pass

    try:
        df1['CHG_DF']  = df1[month_cols[-1]]-df1[month_cols[0]]
    except:
        pass
    
    # 4. Now calculate global stats **only using the original month columns**
    stats = pd.DataFrame({
        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),
        'STD': df1[month_cols].std(axis=1, skipna=skipna),
        'MAX': df1[month_cols].max(axis=1, skipna=skipna),
        'MIN': df1[month_cols].min(axis=1, skipna=skipna),
        'COUNT': df1[month_cols].count(axis=1)
    })

    # 5. Merge the stats
    df1 = pd.concat([df1, stats], axis=1)
    
    return df1.fillna(0)
38/74:
def CreatePivotTableFromTimeSeries(df,
                                   index,
                                   columns,
                                   values,
                                   aggfunc='sum',
                                   skipna=True):
    
    '''
    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.
    
    
    
    '''
    
    # 1. Pivot
    if index==None:
        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)

    else:
        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)

    # 2. Capture original month columns IMMEDIATELY after pivot
    month_cols = df1.columns.tolist()
 
    # 3. Add rolling window stats
    if len(month_cols) >= 3:
        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)
        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]
    
    if len(month_cols) >= 6:
        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)
        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]
        
    if len(month_cols) >= 12:
        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)
        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]
    
    try:
        df1['CHG_MOM'] = df1[month_cols[-1]]-df1[month_cols[-12]]
    except:
        pass

    try:
        df1['CHG_DF']  = df1[month_cols[-1]]-df1[month_cols[0]]
    except:
        pass
    
    # 4. Now calculate global stats **only using the original month columns**
    stats = pd.DataFrame({
        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),
        'STD': df1[month_cols].std(axis=1, skipna=skipna),
        'MAX': df1[month_cols].max(axis=1, skipna=skipna),
        'MIN': df1[month_cols].min(axis=1, skipna=skipna),
        'COUNT': df1[month_cols].count(axis=1)
    })

    # 5. Merge the stats
    df1 = pd.concat([df1, stats], axis=1)
    
    return df1.fillna(0)
38/75:
def CreatePivotTableFromTimeSeries(df,
                                   index,
                                   columns,
                                   values,
                                   aggfunc='sum',
                                   skipna=True):
    
    '''
    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.
    
    
    
    '''
    
    # 1. Pivot
    if index==None:
        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)
    else:
        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)

    # 2. Capture original month columns IMMEDIATELY after pivot
    month_cols = df1.columns.tolist()
 
    # 3. Add rolling window stats
    if len(month_cols) >= 3:
        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)
        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]
    
    if len(month_cols) >= 6:
        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)
        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]
        
    if len(month_cols) >= 12:
        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)
        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]
    
    try:
        df1['CHG_MOM'] = df1[month_cols[-1]]-df1[month_cols[-12]]
    except:
        pass

    try:
        df1['CHG_DF']  = df1[month_cols[-1]]-df1[month_cols[0]]
    except:
        pass
    
    # 4. Now calculate global stats **only using the original month columns**
    stats = pd.DataFrame({
        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),
        'STD': df1[month_cols].std(axis=1, skipna=skipna),
        'MAX': df1[month_cols].max(axis=1, skipna=skipna),
        'MIN': df1[month_cols].min(axis=1, skipna=skipna),
        'COUNT': df1[month_cols].count(axis=1)
    })

    # 5. Merge the stats
    df1 = pd.concat([df1, stats], axis=1)
    
    return df1.fillna(0)
38/76: CreatePivotTableFromTimeSeries(index=None,columns='MONTH',values='DEPOSIT',aggfunc='sum')
38/77: CreatePivotTableFromTimeSeries(df=final_df,index=None,columns='MONTH',values='DEPOSIT',aggfunc='sum')
38/78:
test = CreatePivotTableFromTimeSeries(df=final_df,index=None,columns='MONTH',values='DEPOSIT',aggfunc='sum')

test.reset_index().
38/79:
test = CreatePivotTableFromTimeSeries(df=final_df,index=None,columns='MONTH',values='DEPOSIT',aggfunc='sum')

test.reset_index()
38/80:
test = CreatePivotTableFromTimeSeries(df=final_df,index=None,columns='MONTH',values='DEPOSIT',aggfunc='sum')

test.reset_index(drop=True)
38/81:
test = CreatePivotTableFromTimeSeries(df=final_df,index=None,columns='MONTH',values='DEPOSIT',aggfunc='sum')

test = test.reset_index(drop=True)
for key in index:
    test[index] = 'All'

test
38/82:
test = CreatePivotTableFromTimeSeries(df=final_df,index=None,columns='MONTH',values='DEPOSIT',aggfunc='sum')

test = test.reset_index(drop=True)
test.columns
38/83: list(test.columns)
38/84:
cols = list(test.columns)

cols.insert(0,'Derek')
cols
38/85:
test = CreatePivotTableFromTimeSeries(df=final_df,index=None,columns='MONTH',values='DEPOSIT',aggfunc='sum')

test = test.reset_index(drop=True)
cols = list(test.columns)

for key in index:
    cols.insert(0,key)
    test[key] = 'All'

test[cols]
38/86:
test = CreatePivotTableFromTimeSeries(df=final_df,index=None,columns='MONTH',values='DEPOSIT',aggfunc='sum')

cols = list(test.columns)
test = test.reset_index(drop=True)
test['Metric'] = 'DEPOSIT'
cols.insert(0,'DEPOSIT')


for key in index:
    cols.insert(0,key)
    test[key] = 'All'
    

test[cols]
38/87:
test = CreatePivotTableFromTimeSeries(df=final_df,index=None,columns='MONTH',values='DEPOSIT',aggfunc='sum')

cols = list(test.columns)
test = test.reset_index(drop=True)
test['Metric'] = 'DEPOSIT'
cols.insert(0,'DEPOSIT')


for key in index:
    cols.insert(0,key)
    test[key] = 'All'
    

test
38/88:
test = CreatePivotTableFromTimeSeries(df=final_df,index=None,columns='MONTH',values='DEPOSIT',aggfunc='sum')

cols = list(test.columns)
test = test.reset_index(drop=True)
test['METRIC'] = 'DEPOSIT'
cols.insert(0,'Metric')

for key in index:
    cols.insert(0,key)
    test[key] = 'All'
  
test
38/89:
test = CreatePivotTableFromTimeSeries(df=final_df,index=None,columns='MONTH',values='DEPOSIT',aggfunc='sum')

cols = list(test.columns)
test = test.reset_index(drop=True)
test['METRIC'] = 'DEPOSIT'
cols.insert(0,'Metric')

for key in index:
    cols.insert(0,key)
    test[key] = 'All'
  
test[cols]
38/90:
test = CreatePivotTableFromTimeSeries(df=final_df,index=None,columns='MONTH',values='DEPOSIT',aggfunc='sum')

cols = list(test.columns)
test = test.reset_index(drop=True)
test['METRIC'] = 'DEPOSIT'
cols.insert(0,'METRIC')

for key in index:
    cols.insert(0,key)
    test[key] = 'All'
  
test[cols]
38/91:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    
    
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:
        all_df = CreatePivotTableFromTimeSeries(df=final_df,index=None,columns='MONTH',values=metric,aggfunc='sum')
        
        cols = list(all_df.columns)
        all_df = all_df.reset_index(drop=True)
        all_df['METRIC'] = metric
        cols.insert(0,'METRIC')

        for key in index:
            cols.insert(0,key)
            all_df[key] = 'All'

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/92:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    
    
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:
        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum')
        
        cols = list(all_df.columns)
        all_df = all_df.reset_index(drop=True)
        all_df['METRIC'] = metric
        cols.insert(0,'METRIC')

        for key in index:
            cols.insert(0,key)
            all_df[key] = 'All'

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/93:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    
    
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:
        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum')
        
        cols = list(all_df.columns)
        all_df = all_df.reset_index(drop=True)
        all_df['METRIC'] = metric
        cols.insert(0,'METRIC')

        for key in index:
            cols.insert(0,key)
            all_df[key] = 'All'

        final_df = pd.concat([final_df,all_df])

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/94:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    
    
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:
        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum')
        
        cols = list(all_df.columns)
        all_df = all_df.reset_index(drop=True)
        all_df['METRIC'] = metric
        cols.insert(0,'METRIC')

        for key in index:
            cols.insert(0,key)
            all_df[key] = 'All'

        print(all_df)

        final_df = pd.concat([all_df,final_df])

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/95:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    
    
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:
        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum')
        
        cols = list(all_df.columns)
        all_df = all_df.reset_index(drop=True)
        all_df['METRIC'] = metric
        cols.insert(0,'METRIC')

        for key in index:
            cols.insert(0,key)
            all_df[key] = 'All'

        final_df = pd.concat([final_df,all_df[cols]])

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/96: e[e['BRANCHNAME']=='Fenway Park']
38/97: e[e['BRANCHNAME']=='Fenway Park'][0].sum()
38/98: final_df['DEPOSIT'].sum()
38/99: final_df
38/100: final_df[final_df['DEPOSIT']==0]['DEPOSIT'].sum()
38/101: final_df[final_df['MONTH']==0]['DEPOSIT'].sum()
38/102: e[e['BRANCHNAME']=='Fenway Park'][0].sum()/2
38/103:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    
    
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:
        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum')
        
        cols = list(all_df.columns)
        all_df = all_df.reset_index(drop=True)
        all_df['METRIC'] = metric
        cols.insert(0,'METRIC')

        for key in index:
            cols.insert(0,key)
            all_df[key] = 'All'

        final_df = pd.concat([final_df,all_df[cols]])

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e.to_csv('test.csv',index=False)
38/104: e
38/105:

def CreatePivotTableFromTimeSeries(df,
                                   index,
                                   columns,
                                   values,
                                   aggfunc='sum',
                                   skipna=True):
    
    '''
    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.
    
    
    
    '''
    
    # 1. Pivot
    if index==None:
        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)
    else:
        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)

    # 2. Capture original month columns IMMEDIATELY after pivot
    month_cols = df1.columns.tolist()
 
    # 3. Add rolling window stats
    if len(month_cols) >= 3:
        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)
        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]
        try:
            df1['PERC_CHG_3M'] = (df1[month_cols[-3]-df1[month_cols[-1])/df1[month_cols[-3]
        except:
            df1['PERC_CHG_3M'] = 0
    
    if len(month_cols) >= 6:
        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)
        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]
        try:
            df1['PERC_CHG_6M'] = (df1[month_cols[-6]-df1[month_cols[-1])/df1[month_cols[-3]
        except:
            df1['PERC_CHG_6M'] = 0
            
    if len(month_cols) >= 12:
        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)
        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]
        
        try:
            df1['PERC_CHG_12M'] = (df1[month_cols[-12]-df1[month_cols[-1])/df1[month_cols[-3]
        except:
            df1['PERC_CHG_12M'] = 0
        
    # 4. Now calculate global stats **only using the original month columns**
    stats = pd.DataFrame({
        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),
        'STD': df1[month_cols].std(axis=1, skipna=skipna),
        'MAX': df1[month_cols].max(axis=1, skipna=skipna),
        'MIN': df1[month_cols].min(axis=1, skipna=skipna),
        'COUNT': df1[month_cols].count(axis=1)
    })

    # 5. Merge the stats
    df1 = pd.concat([df1, stats], axis=1)
    
    return df1.fillna(0)
38/106:

def CreatePivotTableFromTimeSeries(df,
                                   index,
                                   columns,
                                   values,
                                   aggfunc='sum',
                                   skipna=True):
    
    '''
    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.
    
    
    
    '''
    
    # 1. Pivot
    if index==None:
        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)
    else:
        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)

    # 2. Capture original month columns IMMEDIATELY after pivot
    month_cols = df1.columns.tolist()
 
    # 3. Add rolling window stats
    if len(month_cols) >= 3:
        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)
        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]
        try:
            df1['PERC_CHG_3M'] = (df1[month_cols[-3]]-df1[month_cols[-1]])/df1[month_cols[-3]
        except:
            df1['PERC_CHG_3M'] = 0
    
    if len(month_cols) >= 6:
        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)
        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]
        try:
            df1['PERC_CHG_6M'] = (df1[month_cols[-6]-df1[month_cols[-1])/df1[month_cols[-3]
        except:
            df1['PERC_CHG_6M'] = 0
            
    if len(month_cols) >= 12:
        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)
        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]
        
        try:
            df1['PERC_CHG_12M'] = (df1[month_cols[-12]-df1[month_cols[-1])/df1[month_cols[-3]
        except:
            df1['PERC_CHG_12M'] = 0
        
    # 4. Now calculate global stats **only using the original month columns**
    stats = pd.DataFrame({
        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),
        'STD': df1[month_cols].std(axis=1, skipna=skipna),
        'MAX': df1[month_cols].max(axis=1, skipna=skipna),
        'MIN': df1[month_cols].min(axis=1, skipna=skipna),
        'COUNT': df1[month_cols].count(axis=1)
    })

    # 5. Merge the stats
    df1 = pd.concat([df1, stats], axis=1)
    
    return df1.fillna(0)
38/107:

def CreatePivotTableFromTimeSeries(df,
                                   index,
                                   columns,
                                   values,
                                   aggfunc='sum',
                                   skipna=True):
    
    '''
    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.
    
    
    
    '''
    
    # 1. Pivot
    if index==None:
        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)
    else:
        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)

    # 2. Capture original month columns IMMEDIATELY after pivot
    month_cols = df1.columns.tolist()
 
    # 3. Add rolling window stats
    if len(month_cols) >= 3:
        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)
        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]
        try:
            df1['PERC_CHG_3M'] = (df1[month_cols[-3]]-df1[month_cols[-1]])/df1[month_cols[-3]
        except:
            df1['PERC_CHG_3M'] = 0
    
    if len(month_cols) >= 6:
        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)
        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]
        try:
            df1['PERC_CHG_6M'] = (df1[month_cols[-6]]-df1[month_cols[-1]])/df1[month_cols[-3]
        except:
            df1['PERC_CHG_6M'] = 0
            
    if len(month_cols) >= 12:
        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)
        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]
        
        try:
            df1['PERC_CHG_12M'] = (df1[month_cols[-12]]-df1[month_cols[-1]])/df1[month_cols[-3]
        except:
            df1['PERC_CHG_12M'] = 0
        
    # 4. Now calculate global stats **only using the original month columns**
    stats = pd.DataFrame({
        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),
        'STD': df1[month_cols].std(axis=1, skipna=skipna),
        'MAX': df1[month_cols].max(axis=1, skipna=skipna),
        'MIN': df1[month_cols].min(axis=1, skipna=skipna),
        'COUNT': df1[month_cols].count(axis=1)
    })

    # 5. Merge the stats
    df1 = pd.concat([df1, stats], axis=1)
    
    return df1.fillna(0)
38/108:

def CreatePivotTableFromTimeSeries(df,
                                   index,
                                   columns,
                                   values,
                                   aggfunc='sum',
                                   skipna=True):
    
    '''
    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.
    
    
    
    '''
    
    # 1. Pivot
    if index==None:
        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)
    else:
        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)

    # 2. Capture original month columns IMMEDIATELY after pivot
    month_cols = df1.columns.tolist()
 
    # 3. Add rolling window stats
    if len(month_cols) >= 3:
        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)
        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]
        try:
            df1['PERC_CHG_3M'] = (df1[month_cols[-3]]-df1[month_cols[-1]])/df1[month_cols[-3]
        except:
            df1['PERC_CHG_3M'] = 0
    

        
    # 4. Now calculate global stats **only using the original month columns**
    stats = pd.DataFrame({
        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),
        'STD': df1[month_cols].std(axis=1, skipna=skipna),
        'MAX': df1[month_cols].max(axis=1, skipna=skipna),
        'MIN': df1[month_cols].min(axis=1, skipna=skipna),
        'COUNT': df1[month_cols].count(axis=1)
    })

    # 5. Merge the stats
    df1 = pd.concat([df1, stats], axis=1)
    
    return df1.fillna(0)
38/109:

def CreatePivotTableFromTimeSeries(df,
                                   index,
                                   columns,
                                   values,
                                   aggfunc='sum',
                                   skipna=True):
    
    '''
    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.
    
    
    
    '''
    
    # 1. Pivot
    if index==None:
        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)
    else:
        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)

    # 2. Capture original month columns IMMEDIATELY after pivot
    month_cols = df1.columns.tolist()
 
    # 3. Add rolling window stats
    if len(month_cols) >= 3:
        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)
        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]
        try:
            df1['PERC_CHG_3M'] = (df1[month_cols[-3]]-df1[month_cols[-1]])/df1[month_cols[-3]]
        except:
            df1['PERC_CHG_3M'] = 0
    
    if len(month_cols) >= 6:
        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)
        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]
        try:
            df1['PERC_CHG_6M'] = (df1[month_cols[-6]]-df1[month_cols[-1]])/df1[month_cols[-3]]
        except:
            df1['PERC_CHG_6M'] = 0
            
    if len(month_cols) >= 12:
        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)
        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]
        try:
            df1['PERC_CHG_12M'] = (df1[month_cols[-12]]-df1[month_cols[-1]])/df1[month_cols[-3]]
        except:
            df1['PERC_CHG_12M'] = 0
        
    # 4. Now calculate global stats **only using the original month columns**
    stats = pd.DataFrame({
        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),
        'STD': df1[month_cols].std(axis=1, skipna=skipna),
        'MAX': df1[month_cols].max(axis=1, skipna=skipna),
        'MIN': df1[month_cols].min(axis=1, skipna=skipna),
        'COUNT': df1[month_cols].count(axis=1)
    })

    # 5. Merge the stats
    df1 = pd.concat([df1, stats], axis=1)
    
    return df1.fillna(0)
38/110:
df = final_df.copy()
group_vars = ['BRANCHNAME','CITY','LOB','DURATION','MONTH']
agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
agg_funcs = ['sum', 'mean','count']


d = long_format_summary(final_df,group_vars,agg_cols,agg_funcs)
d
38/111:
from DFProcessing import CreatePivotTableFromTimeSeries

CreatePivotTableFromTimeSeries(final_df[final_df['MONTH']<=6],index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT')
38/112:
df = final_df.copy()
group_vars = ['BRANCHNAME','CITY','LOB','DURATION','MONTH']
agg_cols = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
agg_funcs = ['sum', 'mean','count']
38/113:
from DFProcessing import CreatePivotTableFromTimeSeries

CreatePivotTableFromTimeSeries(final_df[final_df['MONTH']<=6],index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT')
38/114: CreatePivotTableFromTimeSeries(final_df[final_df['MONTH']<=6],index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT')
38/115:

def CreatePivotTableFromTimeSeries(df,
                                   index,
                                   columns,
                                   values,
                                   aggfunc='sum',
                                   skipna=True):
    
    '''
    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.
    
    
    
    '''
    
    # 1. Pivot
    if index==None:
        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)
    else:
        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)

    # 2. Capture original month columns IMMEDIATELY after pivot
    month_cols = df1.columns.tolist()
 
    # 3. Add rolling window stats
    if len(month_cols) >= 3:
        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)
        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]
        try:
            df1['PERC_CHG_3M'] = (df1[month_cols[-3]]-df1[month_cols[-1]])/df1[month_cols[-3]]
        except:
            df1['PERC_CHG_3M'] = 0
    
    if len(month_cols) >= 6:
        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)
        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]
        try:
            df1['PERC_CHG_6M'] = (df1[month_cols[-6]]-df1[month_cols[-1]])/df1[month_cols[-3]]
        except:
            df1['PERC_CHG_6M'] = 0
            
    if len(month_cols) >= 12:
        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)
        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]
        try:
            df1['PERC_CHG_12M'] = (df1[month_cols[-12]]-df1[month_cols[-1]])/df1[month_cols[-3]]
        except:
            df1['PERC_CHG_12M'] = 0
        
    # 4. Now calculate global stats **only using the original month columns**
    stats = pd.DataFrame({
        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),
        'STD': df1[month_cols].std(axis=1, skipna=skipna),
        'MAX': df1[month_cols].max(axis=1, skipna=skipna),
        'MIN': df1[month_cols].min(axis=1, skipna=skipna),
        'COUNT': df1[month_cols].count(axis=1)
    })

    # 5. Merge the stats
    df1 = pd.concat([df1, stats], axis=1)
    
    return df1.fillna(0)
38/116: CreatePivotTableFromTimeSeries(final_df[final_df['MONTH']<=6],index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT')
38/117:

def CreatePivotTableFromTimeSeries(df,
                                   index,
                                   columns,
                                   values,
                                   aggfunc='sum',
                                   skipna=True):
    
    '''
    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.
    
    
    
    '''
    
    # 1. Pivot
    if index==None:
        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)
    else:
        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)

    # 2. Capture original month columns IMMEDIATELY after pivot
    month_cols = df1.columns.tolist()
 
    # 3. Add rolling window stats
    if len(month_cols) >= 3:
        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)
        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]
        try:
            df1['PERC_CHG_3M'] = df1['CHG_3M']/df1[month_cols[-3]]
        except:
            df1['PERC_CHG_3M'] = 0
    
    if len(month_cols) >= 6:
        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)
        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]
        try:
            df1['PERC_CHG_6M'] = df1['CHG_6M']/df1[month_cols[-6]]
        except:
            df1['PERC_CHG_6M'] = 0
            
    if len(month_cols) >= 12:
        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)
        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]
        try:
            df1['PERC_CHG_12M'] = df1['CHG_12M']/df1[month_cols[-12]]
        except:
            df1['PERC_CHG_12M'] = 0

    df1['CHG_DF']  = df1[month_cols[-1]]-df1[month_cols[0]]
    df1['AVG_DF'] = df1[month_cols[-1:]].mean(axis=1, skipna=skipna)
    df1['PERC_CHG_DF'] = df1['AVG_DF']/df1[month_cols[-1]]

    
    # 4. Now calculate global stats **only using the original month columns**
    stats = pd.DataFrame({
        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),
        'STD': df1[month_cols].std(axis=1, skipna=skipna),
        'MAX': df1[month_cols].max(axis=1, skipna=skipna),
        'MIN': df1[month_cols].min(axis=1, skipna=skipna),
        'COUNT': df1[month_cols].count(axis=1)
    })

    # 5. Merge the stats
    df1 = pd.concat([df1, stats], axis=1)
    
    return df1.fillna(0)
38/118: CreatePivotTableFromTimeSeries(final_df[final_df['MONTH']<=6],index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT')
38/119: CreatePivotTableFromTimeSeries(final_df[final_df['MONTH']<=6],index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT').to_csv('test1.csv')
38/120: CreatePivotTableFromTimeSeries(final_df[final_df['MONTH']<=6],index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT')
38/121: CreatePivotTableFromTimeSeries(final_df[final_df['MONTH']<=6],index=['BRANCHNAME','CITY','LOB','DURATION'],columns='MONTH',values='DEPOSIT').iloc[0]
38/122:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    
    
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:
        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum')
        
        cols = list(all_df.columns)
        all_df = all_df.reset_index(drop=True)
        all_df['METRIC'] = metric
        cols.insert(0,'METRIC')

        for key in index:
            cols.insert(0,key)
            all_df[key] = 'All'

        final_df = pd.concat([final_df,all_df[cols]])

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/123:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    
    
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:
        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum')
        
        cols = list(all_df.columns)
        all_df = all_df.reset_index(drop=True)
        all_df['METRIC'] = metric
        cols.insert(0,'METRIC')

        for key in index:
            cols.insert(0,key)
            all_df[key] = 'All'

        final_df = pd.concat([final_df,all_df[cols]])

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e.to_csv('test2.csv',index=False)
38/124:
def CreateMultiplePivotTableFromTimeSeries(df, index_list, metric_list, column):
    '''
    Creates multiple pivot tables for each metric and index combination, including record counts.

    Parameters:
        df (pd.DataFrame): Input data
        index_list (list): List of index column names
        metric_list (list): List of metric column names
        column (str): Time column for pivoting (e.g., month, year)

    Returns:
        pd.DataFrame: Combined pivot with metrics and record counts
    '''
    df = df.copy()
    df['RECORD_COUNT'] = 1
    final_df = pd.DataFrame()

    for metric in metric_list:
        # ===== All Index = None
        value_df = CreatePivotTableFromTimeSeries(df, index=None, columns=column, values=metric, aggfunc='sum')
        count_df = CreatePivotTableFromTimeSeries(df, index=None, columns=column, values='RECORD_COUNT', aggfunc='count')
        combined = value_df.copy()
        for col in count_df.columns:
            combined[f'{col}_COUNT'] = count_df[col]

        combined = combined.reset_index(drop=True)
        combined['METRIC'] = metric
        for key in index_list[::-1]:  # insert from end to preserve order
            combined.insert(0, key, 'All')
        combined.insert(0, 'METRIC', metric)

        final_df = pd.concat([final_df, combined])

        # ===== Individual Index
        for key in index_list:
            value_df = CreatePivotTableFromTimeSeries(df, index=key, columns=column, values=metric, aggfunc='sum').reset_index()
            count_df = CreatePivotTableFromTimeSeries(df, index=key, columns=column, values='RECORD_COUNT', aggfunc='count').reset_index()
            merged = pd.merge(value_df, count_df, on=[key], suffixes=('', '_COUNT'))
            merged['METRIC'] = metric
            for missing in [x for x in index_list if x != key]:
                merged[missing] = 'All'
            final_df = pd.concat([final_df, merged])

        # ===== Full Index Combination
        value_df = CreatePivotTableFromTimeSeries(df, index=index_list, columns=column, values=metric, aggfunc='sum').reset_index()
        count_df = CreatePivotTableFromTimeSeries(df, index=index_list, columns=column, values='RECORD_COUNT', aggfunc='count').reset_index()
        merged = pd.merge(value_df, count_df, on=index_list, suffixes=('', '_COUNT'))
        merged['METRIC'] = metric
        final_df = pd.concat([final_df, merged])

    return final_df.reset_index(drop=True)



e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
38/125:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE','COUNT']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    
    
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:
        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') 
        cols = list(all_df.columns)
        all_df = all_df.reset_index(drop=True)
        all_df['METRIC'] = metric
        cols.insert(0,'METRIC')

        for key in index:
            cols.insert(0,key)
            all_df[key] = 'All'

        final_df = pd.concat([final_df,all_df[cols]])

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/126:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE','RECORD_COUNT']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

df['RECORD_COUNT']=1

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    
    
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:
        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') 
        cols = list(all_df.columns)
        all_df = all_df.reset_index(drop=True)
        all_df['METRIC'] = metric
        cols.insert(0,'METRIC')

        for key in index:
            cols.insert(0,key)
            all_df[key] = 'All'

        final_df = pd.concat([final_df,all_df[cols]])

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/127:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE','RECORD_COUNT']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

final_df['RECORD_COUNT']=1

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    
    
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:
        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') 
        cols = list(all_df.columns)
        all_df = all_df.reset_index(drop=True)
        all_df['METRIC'] = metric
        cols.insert(0,'METRIC')

        for key in index:
            cols.insert(0,key)
            all_df[key] = 'All'

        final_df = pd.concat([final_df,all_df[cols]])

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e
38/128:
metrics = ['DEPOSIT','LENDING','TXN_COUNT','TXN_VALUE','RECORD_COUNT']
index=['BRANCHNAME','CITY','LOB','DURATION']

column='MONTH'

final_df['RECORD_COUNT']=1

def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):
    '''
    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's
    



    '''
    
    
    final_df = pd.DataFrame()
    
    # Iterate through all Possible Metrics Selected.
    for metric in metric_list:
        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') 
        cols = list(all_df.columns)
        all_df = all_df.reset_index(drop=True)
        all_df['METRIC'] = metric
        cols.insert(0,'METRIC')

        for key in index:
            cols.insert(0,key)
            all_df[key] = 'All'

        final_df = pd.concat([final_df,all_df[cols]])

        # Iterate through all Index Items Individually
        for key in index_list:
            temp = CreatePivotTableFromTimeSeries(df,index=key,
                                                  values=metric,
                                                  columns=column).reset_index() 
            for missing in [x for x in index if x != key]:
                temp[missing] = 'All'
            temp['METRIC'] = metric
            final_df = pd.concat([final_df,temp])
        
        # Add Value for Metric with Entire Index Combination
        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()
        temp['METRIC'] = metric
        final_df = pd.concat([final_df,temp])
    
    return final_df

e = CreateMultiplePivotTableFromTimeSeries(df=final_df[final_df['MONTH']<3],
                                       index_list= index,
                                       metric_list=metrics,
                                       column= 'MONTH')
        

e[e['METRIC']=='RECORD_COUNT']
38/129: final_df
38/130: e
38/131: np.random.randint(0.02,.05)
38/132: np.random.uniform(0.02, 0.05)
38/133: final_df['INTEREST_RATE'] = final_df.apply(lambda x:np.np.random.uniform(0.02, 0.07))
38/134: final_df['INTEREST_RATE'] = final_df.apply(lambda x:np.random.uniform(0.02, 0.07))
38/135: final_df.head()
38/136:
final_df['INTEREST_RATE'] = final_df['MONTH'].apply(lambda x:np.random.uniform(0.02, 0.07))
final_df.head()
38/137: final_df.head()
38/138:

import itertools


def custom_metric_aggregator(df, index_cols, metrics):
    """
    Computes custom derived metrics grouped by index columns.

    Parameters:
        df (pd.DataFrame): Input data
        index_cols (list): Grouping dimensions (e.g. branch, lob, duration)
        metrics (list of dicts): Each dict defines one derived metric:
            {
                'name': str,
                'numerator': str or callable,
                'denominator': str or callable,
                'function': callable (e.g. lambda x, y: x / y)
            }

    Returns:
        pd.DataFrame: Aggregated metrics including 'All' combinations
    """
    
    all_results = []

    for r in range(len(index_cols) + 1):
        for group in itertools.combinations(index_cols, r):
            group = list(group)
            grouped = df.groupby(group)

            result_rows = []
            for keys, group_df in grouped:
                row = {col: 'All' for col in index_cols}
                if isinstance(keys, tuple):
                    for col, val in zip(group, keys):
                        row[col] = val
                elif group:
                    row[group[0]] = keys

                for metric in metrics:
                    # compute numerator
                    num = (
                        group_df[metric['numerator']].sum()
                        if isinstance(metric['numerator'], str)
                        else metric['numerator'](group_df)
                    )
                    # compute denominator
                    denom = (
                        group_df[metric['denominator']].sum()
                        if isinstance(metric['denominator'], str)
                        else metric['denominator'](group_df)
                    )
                    # apply function
                    try:
                        row[metric['name']] = metric['function'](num, denom)
                    except ZeroDivisionError:
                        row[metric['name']] = None

                result_rows.append(row)

            all_results.extend(result_rows)

    return pd.DataFrame(all_results)



metrics = [
    {
        'name': 'WEIGHTED_INTEREST',
        'numerator': lambda df: (df['INTEREST_RATE'] * df['LENDING']).sum(),
        'denominator': 'LENDING',
        'function': lambda x, y: x / y
    },
    # {
    #     'name': 'RENEWAL_RATE',
    #     'numerator': 'RENEWED_AMOUNT',
    #     'denominator': 'MATURED_AMOUNT',
    #     'function': lambda x, y: x / y
    # }
]

custom_df = custom_metric_aggregator(
    df,
    index_cols=['BRANCHNAME', 'LOB', 'DURATION'],
    metrics=metrics
)
38/139:

import itertools


def custom_metric_aggregator(df, index_cols, metrics):
    """
    Computes custom derived metrics grouped by index columns.

    Parameters:
        df (pd.DataFrame): Input data
        index_cols (list): Grouping dimensions (e.g. branch, lob, duration)
        metrics (list of dicts): Each dict defines one derived metric:
            {
                'name': str,
                'numerator': str or callable,
                'denominator': str or callable,
                'function': callable (e.g. lambda x, y: x / y)
            }

    Returns:
        pd.DataFrame: Aggregated metrics including 'All' combinations
    """
    
    all_results = []

    for r in range(len(index_cols) + 1):
        for group in itertools.combinations(index_cols, r):
            group = list(group)
            grouped = df.groupby(group)

            result_rows = []
            for keys, group_df in grouped:
                row = {col: 'All' for col in index_cols}
                if isinstance(keys, tuple):
                    for col, val in zip(group, keys):
                        row[col] = val
                elif group:
                    row[group[0]] = keys

                for metric in metrics:
                    # compute numerator
                    num = (
                        group_df[metric['numerator']].sum()
                        if isinstance(metric['numerator'], str)
                        else metric['numerator'](group_df)
                    )
                    # compute denominator
                    denom = (
                        group_df[metric['denominator']].sum()
                        if isinstance(metric['denominator'], str)
                        else metric['denominator'](group_df)
                    )
                    # apply function
                    try:
                        row[metric['name']] = metric['function'](num, denom)
                    except ZeroDivisionError:
                        row[metric['name']] = None

                result_rows.append(row)

            all_results.extend(result_rows)

    return pd.DataFrame(all_results)



metrics = [
    {
        'name': 'WEIGHTED_INTEREST',
        'numerator': lambda df: (df['INTEREST_RATE'] * df['LENDING']).sum(),
        'denominator': 'LENDING',
        'function': lambda x, y: x / y
    },
    # {
    #     'name': 'RENEWAL_RATE',
    #     'numerator': 'RENEWED_AMOUNT',
    #     'denominator': 'MATURED_AMOUNT',
    #     'function': lambda x, y: x / y
    # }
]

custom_df = custom_metric_aggregator(
    final_df,
    index_cols=['BRANCHNAME', 'LOB', 'DURATION'],
    metrics=metrics
)
38/140:

import itertools


def custom_metric_aggregator(df, index_cols, metrics):
    """
    Computes custom derived metrics grouped by index columns.

    Parameters:
        df (pd.DataFrame): Input data
        index_cols (list): Grouping dimensions (e.g. branch, lob, duration)
        metrics (list of dicts): Each dict defines one derived metric:
            {
                'name': str,
                'numerator': str or callable,
                'denominator': str or callable,
                'function': callable (e.g. lambda x, y: x / y)
            }

    Returns:
        pd.DataFrame: Aggregated metrics including 'All' combinations
    """
    
    all_results = []

    for r in range(len(index_cols) + 1):
        for group in itertools.combinations(index_cols, r):
            group = list(group)
            grouped = df.groupby(group)

            result_rows = []
            for keys, group_df in grouped:
                row = {col: 'All' for col in index_cols}
                if isinstance(keys, tuple):
                    for col, val in zip(group, keys):
                        row[col] = val
                elif group:
                    row[group[0]] = keys

                for metric in metrics:
                    # compute numerator
                    num = (
                        group_df[metric['numerator']].sum()
                        if isinstance(metric['numerator'], str)
                        else metric['numerator'](group_df)
                    )
                    # compute denominator
                    denom = (
                        group_df[metric['denominator']].sum()
                        if isinstance(metric['denominator'], str)
                        else metric['denominator'](group_df)
                    )
                    # apply function
                    try:
                        row[metric['name']] = metric['function'](num, denom)
                    except ZeroDivisionError:
                        row[metric['name']] = None

                result_rows.append(row)

            all_results.extend(result_rows)

    return pd.DataFrame(all_results)



custom_metrics = [
    {
        'name': 'WEIGHTED_INTEREST',
        'numerator': lambda df: (df['INTEREST_RATE'] * df['LENDING']).sum(),
        'denominator': 'LENDING',
        'function': lambda x, y: x / y
    },
    # {
    #     'name': 'RENEWAL_RATE',
    #     'numerator': 'RENEWED_AMOUNT',
    #     'denominator': 'MATURED_AMOUNT',
    #     'function': lambda x, y: x / y
    # }
]

if custom_metrics:
    for metric_def in custom_metrics:
        name = metric_def['name']
        func = metric_def['function']

        for group in all_index_combinations:  # you already generate these
            filtered_df = df.copy()
            for col, val in group.items():
                if val != "All":
                    filtered_df = filtered_df[filtered_df[col] == val]
            result_value = func(filtered_df)
            row = {**group, 'METRIC': name, 'VALUE': result_value}
            final_custom_rows.append(row)
)
38/141: final_df
38/142:
def CombineLists(list_,
                 combo=1,
                 r=2):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            return list(map(list,combinations(list_,r)))
        else:
            return list(map(list,permutations(list_,r)))
    
    return list(map(list,product(*list_)))

list_of_lists = [branches,city,lob,duration]

CombineLists(list_of_lists)
38/143:
from itertools import product,permutations,combinations

def CombineLists(list_,
                 combo=1,
                 r=2):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            return list(map(list,combinations(list_,r)))
        else:
            return list(map(list,permutations(list_,r)))
    
    return list(map(list,product(*list_)))

list_of_lists = [branches,city,lob,duration]

CombineLists(list_of_lists)
38/144:
from itertools import product,permutations,combinations

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    if add_metric:
        for x in list_:
            x.append(add_metric)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            return list(map(list,combinations(list_,r)))
        else:
            return list(map(list,permutations(list_,r)))
    
    return list(map(list,product(*list_)))

list_of_lists = [branches,city,lob,duration]

CombineLists(list_of_lists,add_metric='All')
38/145:
from itertools import product,permutations,combinations

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    if add_metric:
        for x in list_:
            x.append(add_metric)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            return list(map(list,combinations(list_,r)))
        else:
            return list(map(list,permutations(list_,r)))
    
    return list(map(list,product(*list_)))

list_of_lists = [branches,city,lob,duration]

pd.DataFrame(CombineLists(list_of_lists,add_metric='All'))
38/146:
from itertools import product,permutations,combinations

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    if add_metric:
        for x in list_:
            x.append(add_metric)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            return list(map(list,combinations(list_,r)))
        else:
            return list(map(list,permutations(list_,r)))
    
    return list(map(list,product(*list_)))

list_of_lists = [branches,city,lob,duration]

pd.DataFrame(CombineLists(list_of_lists,add_metric='All')).drop_duplicates(
38/147:
from itertools import product,permutations,combinations

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    if add_metric:
        for x in list_:
            x.append(add_metric)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            return list(map(list,combinations(list_,r)))
        else:
            return list(map(list,permutations(list_,r)))
    
    return list(map(list,product(*list_)))

list_of_lists = [branches,city,lob,duration]

pd.DataFrame(CombineLists(list_of_lists,add_metric='All')).drop_duplicates()
38/148:
from itertools import product,permutations,combinations

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    if add_metric:
        for x in list_:
            x.append(add_metric)
            print(x)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            return list(map(list,combinations(list_,r)))
        else:
            return list(map(list,permutations(list_,r)))
    
    return list(map(list,product(*list_)))

list_of_lists = [branches,city,lob,duration]

pd.DataFrame(CombineLists(list_of_lists,add_metric='All')).drop_duplicates()
38/149:
from itertools import product,permutations,combinations


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']
list_of_lists = [branches,city,lob,duration]

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    list_ = List_.copy()

    if add_metric:
        for x in list_:
            x.append(add_metric)
            print(x)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            return list(map(list,combinations(list_,r)))
        else:
            return list(map(list,permutations(list_,r)))
    
    return list(map(list,product(*list_)))

list_of_lists = [branches,city,lob,duration]

pd.DataFrame(CombineLists(list_of_lists,add_metric='All')).drop_duplicates()
38/150:
from itertools import product,permutations,combinations


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']
list_of_lists = [branches,city,lob,duration]

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    list_ = list_.copy()

    if add_metric:
        for x in list_:
            x.append(add_metric)
            print(x)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            return list(map(list,combinations(list_,r)))
        else:
            return list(map(list,permutations(list_,r)))
    
    return list(map(list,product(*list_)))

list_of_lists = [branches,city,lob,duration]

pd.DataFrame(CombineLists(list_of_lists,add_metric='All')).drop_duplicates()
38/151:
from itertools import product,permutations,combinations


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']
list_of_lists = [branches,city,lob,duration]

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    list_ = list_.copy()

    if add_metric:
        for x in list_:
            x.append(add_metric)
            print(x)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            return list(map(list,combinations(list_,r)))
        else:
            return list(map(list,permutations(list_,r)))
    
    return list(map(list,product(*list_)))

list_of_lists = [branches,city,lob,duration]

pd.DataFrame(CombineLists(list_of_lists,add_metric='All'))
38/152:
from itertools import product,permutations,combinations


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']
list_of_lists = [branches,city,lob,duration]

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    list_ = list_.copy()

    if add_metric:
        for x in list_:
            x.append(add_metric)
            print(x)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            return list(map(list,combinations(list_,r)))
        else:
            return list(map(list,permutations(list_,r)))
    
    return list(map(list,product(*list_)))

list_of_lists = [branches,city,lob,duration]

pd.DataFrame(CombineLists(list_of_lists,add_metric='All')).drop_duplicates()
38/153:
from itertools import product,permutations,combinations


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']
list_of_lists = [branches,city,lob,duration]

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    list_ = list_.copy()

    if add_metric:
        for x in list_:
            x.append(add_metric)
            print(x)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            return list(map(list,combinations(list_,r)))
        else:
            return list(map(list,permutations(list_,r)))
    
    if return_value == 'list_':
        return list(map(list,product(*list_)))
    else:
        return pd.DataFrame(list(map(list,product(*list_)))).drop_duplicates()

list_of_lists = [branches,city,lob,duration]
38/154:
from itertools import product,permutations,combinations


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']
list_of_lists = [branches,city,lob,duration]

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    list_ = list_.copy()

    if add_metric:
        for x in list_:
            x.append(add_metric)
            print(x)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            value = list(map(list,combinations(list_,r)))
            if return_value == 'list_':
                return value
            else:
                return pd.DataFrame(value)
        else:
            value  list(map(list,permutations(list_,r)))
            if return_value == 'list_':
                return value
            else:
                return pd.DataFrame(value)
    
    else:
        value = list(map(list,product(*list_)))

        if return_value == 'list_':
            return value
        else:
            return pd.DataFrame(value)
    
list_of_lists = [branches,city,lob,duration]
38/155:
from itertools import product,permutations,combinations


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']
list_of_lists = [branches,city,lob,duration]

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    list_ = list_.copy()

    if add_metric:
        for x in list_:
            x.append(add_metric)
            print(x)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            value = list(map(list,combinations(list_,r)))
            if return_value == 'list_':
                return value
            else:
                return pd.DataFrame(value)
        else:
            value = list(map(list,permutations(list_,r)))
            if return_value == 'list_':
                return value
            else:
                return pd.DataFrame(value)
    
    else:
        value = list(map(list,product(*list_)))

        if return_value == 'list_':
            return value
        else:
            return pd.DataFrame(value)
    
list_of_lists = [branches,city,lob,duration]
38/156:
from itertools import product,permutations,combinations


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']
list_of_lists = [branches,city,lob,duration]

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    list_ = list_.copy()

    if add_metric:
        for x in list_:
            x.append(add_metric)
            print(x)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            value = list(map(list,combinations(list_,r)))
            if return_value == 'list_':
                return value
            else:
                return pd.DataFrame(value)
        else:
            value = list(map(list,permutations(list_,r)))
            if return_value == 'list_':
                return value
            else:
                return pd.DataFrame(value)
    
    else:
        value = list(map(list,product(*list_)))

        if return_value == 'list_':
            return value
        else:
            return pd.DataFrame(value)
    
list_of_lists = [branches,city,lob,duration]

list_of_lists
38/157:
from itertools import product,permutations,combinations


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']
list_of_lists = [branches,city,lob,duration]

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    list_ = list_.copy()

    if add_metric:
        for x in list_:
            x.append(add_metric)
            print(x)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            value = list(map(list,combinations(list_,r)))
            if return_value == 'list_':
                return value
            else:
                return pd.DataFrame(value)
        else:
            value = list(map(list,permutations(list_,r)))
            if return_value == 'list_':
                return value
            else:
                return pd.DataFrame(value)
    
    else:
        value = list(map(list,product(*list_)))

        if return_value == 'list_':
            return value
        else:
            return pd.DataFrame(value)
    
list_of_lists = [branches,city,lob,duration]

CombineLists(list_of_lists)
38/158:
from itertools import product,permutations,combinations


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']
list_of_lists = [branches,city,lob,duration]

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
        
    '''
    Function to 
    
    
    Parameters:
        
    
    Return:
        
    '''

    list_ = list_.copy()

    if add_metric:
        for x in list_:
            x.append(add_metric)
            print(x)
    
    items = sum([1 if isinstance(x,list) else 0 for x in list_])

    if items==0:
        if combo==1:
            value = list(map(list,combinations(list_,r)))
            if return_value == 'list_':
                return value
            else:
                return pd.DataFrame(value)
        else:
            value = list(map(list,permutations(list_,r)))
            if return_value == 'list_':
                return value
            else:
                return pd.DataFrame(value)
    
    else:
        value = list(map(list,product(*list_)))

        if return_value == 'list_':
            return value
        else:
            return pd.DataFrame(value)
    
list_of_lists = [branches,city,lob,duration]

CombineLists(list_of_lists,'All')
38/159:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value,return_value='df')

    


CombineLists(list_of_lists,'All')
38/160:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value)

    


CombineLists(list_of_lists,'All',return_value='df')
38/161:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value).drop_duplicates()

    


CombineLists(list_of_lists,'All',return_value='df')
38/162:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value).drop_duplicates()

    


CombineLists(list_of_lists,'All',return_value='df',combo=0)
38/163:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value)

    


CombineLists(list_of_lists,'All',return_value='df',combo=0)
38/164:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value)

    


CombineLists(list_of_lists,return_value='df',combo=0)
38/165:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value)

    
CombineLists(list_=list_of_lists,return_value='df',combo=0)
38/166:
from itertools import product,permutations,combinations


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']
list_of_lists = [branches,city,lob,duration]
38/167:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value)

    
CombineLists(list_=list_of_lists,return_value='df',combo=0)
38/168:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value)

    
CombineLists(list_=list_of_lists,return_value='df',combo=1)
38/169:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value)

    
CombineLists(list_=list_of_lists,add_metric='All',return_value='df',combo=1)
38/170:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value)

    
CombineLists(list_=list_of_lists,add_metric='All',return_value='df',combo=0)
38/171:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value)

    
CombineLists(list_=list_of_lists,add_metric='All',return_value='df',combo=0)
38/172:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value)

    
CombineLists(list_=['d','dd','dee','dere'],add_metric='All',return_value='df',combo=0)
38/173:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value)

    
CombineLists(list_=['d','dd','dee','dere'],add_metric='All',return_value='df',combo=1)
38/174:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value)

    
CombineLists(list_=['d','dd','dee','dere'],return_value='df',combo=1)
38/175:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value)

    
CombineLists(list_=['d','dd','dee','dere'],return_value='df',combo=0)
38/176:
from itertools import combinations, permutations, product
import pandas as pd

def CombineLists(list_,
                 add_metric=None,
                 combo=1,
                 r=2,
                 return_value='list_'):
    '''
    Function to generate an exhaustive list from multiple lists. 
    Handles both Cartesian product (list of lists) and flat combinations/permutations.

    If List of Lists, it by-passes Combniation and Permutations by making items equal to 1.

    Parameters:
        list_ (list): Flat list or list of lists
        add_metric (any): Optional value to append to each sublist
        combo (int): 1 = combinations, 0 = permutations
        r (int): size of each combination/permutation
        return_value (str): 'list_' or 'df'
        
    Returns:
        list or pd.DataFrame
    '''

    import copy
    
    list_ = copy.deepcopy(list_)

    if add_metric:
        list_ = [x + [add_metric] if isinstance(x, list) else [x, add_metric] for x in list_]

    items = sum([1 if isinstance(x, list) else 0 for x in list_])

    if items == 0:
        if combo == 1:
            value = list(map(list, combinations(list_, r)))
        else:
            value = list(map(list, permutations(list_, r)))
    else:
        value = list(map(list, product(*list_)))

    if return_value == 'list_':
        return value
    else:
        return pd.DataFrame(value).drop_duplicates()

    
CombineLists(list_=list_of_lists,add_metric='ALL',return_value='df')
38/177: final_df
38/178:
def CreateCalculatedField(df,primary_keys,include_all=1):
    
    final_list = []
    for col in primary_keys:
        temp_list = df[col].unique().tolist()
        if include_all==1:
            temp_list.append('All')

        final_list.append(temp_list)



    return final_list

    

    


CreateCalculatedField(final_df['BRANCHNAME','CITY','LOB','DURATION'])
    
#CombineLists(list_=list_of_lists,add_metric='ALL',return_value='df')
38/179: final_df['BRANCHNAME']
38/180: final_df['BRANCHNAME'].unique()
38/181: final_df['BRANCHNAME'].unique().tolist()
38/182: final_df['BRANCHNAME'].unique().tolist().append('all')
38/183:
def CreateCalculatedField(df,primary_key,include_all=1):
    
    final_list = []
    for col in primary_key:
        print(col)
        temp_list = df[col].unique().tolist()
        if include_all==1:
            temp_list.append('All')

        final_list.append(temp_list)



    return final_list

    

    


CreateCalculatedField(final_df['BRANCHNAME','CITY','LOB','DURATION'])
    
#CombineLists(list_=list_of_lists,add_metric='ALL',return_value='df')
38/184:
def CreateCalculatedField(df,primary_key,include_all=1):
    
    final_list = []
    for col in primary_key:
        print(col)
        temp_list = df[col].unique().tolist()
        if include_all==1:
            temp_list.append('All')

        final_list.append(temp_list)



    return final_list

    

    


CreateCalculatedField(final_df,['BRANCHNAME','CITY','LOB','DURATION'])
    
#CombineLists(list_=list_of_lists,add_metric='ALL',return_value='df')
38/185:
def CreateCalculatedField(df,primary_key,include_all=1):
    
    final_list = []
    for col in primary_key:
        temp_list = df[col].unique().tolist()
        if include_all==1:
            temp_list.append('All')

        final_list.append(temp_list)



    return final_list

    

    


CreateCalculatedField(final_df,['BRANCHNAME','CITY','LOB','DURATION'])
    
#CombineLists(list_=list_of_lists,add_metric='ALL',return_value='df')
38/186:
def CreateCalculatedField(df,primary_key,include_all=1):
    
    final_list = []
    # Iterate through Columns to Generate List of lists.
    for col in primary_key:
        temp_list = df[col].unique().tolist()
        if include_all==1:
            temp_list.append('All')
        final_list.append(temp_list)



    return CombineLists(list_=final_list,return_value='df')

    

    


CreateCalculatedField(final_df,['BRANCHNAME','CITY','LOB','DURATION'])
    
#C
38/187:
from itertools import combinations, product
import pandas as pd

def CreateCalculatedField(df, primary_key, include_all=1):
    """
    Efficiently creates a dataframe of all group combinations with optional 'All' rollups,
    and calculates aggregates (sum, weighted average) using a single groupby + synthetic rollups.

    Parameters:
        df (pd.DataFrame): Source dataframe
        primary_key (list): Columns to group by
        include_all (int): 1 to include 'All' combinations (default)

    Returns:
        pd.DataFrame: Result with TOTAL_LENDING and WEIGHTED_INTEREST
    """

    # Step 1: Base groupby aggregation
    grouped = df.groupby(primary_key, dropna=False).agg(
        TOTAL_LENDING=('LENDING', 'sum'),
        _WEIGHT_NUM=('INTEREST_RATE', lambda x: (df.loc[x.index, 'LENDING'] * x).sum())
    ).reset_index()
    grouped['WEIGHTED_INTEREST'] = grouped['_WEIGHT_NUM'] / grouped['TOTAL_LENDING']
    grouped = grouped.drop(columns=['_WEIGHT_NUM'])

    result_frames = [grouped.copy()]

    if include_all:
        rollup_rows = []

        # Step 2: All partial groupings
        for r in range(1, len(primary_key)):
            for group_cols in combinations(primary_key, r):
                temp = grouped.groupby(list(group_cols)).agg({
                    'TOTAL_LENDING': 'sum',
                    'WEIGHTED_INTEREST': lambda x: (
                        (x * grouped.loc[x.index, 'TOTAL_LENDING']).sum() /
                        grouped.loc[x.index, 'TOTAL_LENDING'].sum()
                    )
                }).reset_index()

                # Fill missing columns with 'All'
                for col in primary_key:
                    if col not in group_cols:
                        temp[col] = 'All'

                # Reorder columns
                temp = temp[primary_key + ['TOTAL_LENDING', 'WEIGHTED_INTEREST']]
                rollup_rows.append(temp)

        # Step 3: Full 'All' row
        grand_total = grouped['TOTAL_LENDING'].sum()
        grand_weighted = (
            (grouped['TOTAL_LENDING'] * grouped['WEIGHTED_INTEREST']).sum() / grand_total
            if grand_total > 0 else 0
        )
        all_row = pd.DataFrame([{
            **{col: 'All' for col in primary_key},
            'TOTAL_LENDING': grand_total,
            'WEIGHTED_INTEREST': grand_weighted
        }])
        rollup_rows.append(all_row)

        result_frames += rollup_rows

    # Final result
    final_df = pd.concat(result_frames, ignore_index=True)
    return final_df



CreateCalculatedField(final_df,['BRANCHNAME','CITY','LOB','DURATION'])
38/188:
from itertools import combinations
import pandas as pd

def CreateCalculatedField(df, primary_key, include_all=1, calcs=None):
    """
    Efficient aggregation with support for sum and weighted averages,
    including 'All' rollup rows.

    Parameters:
        df (pd.DataFrame): Source data
        primary_key (list): Columns to group by
        include_all (int): Add 'All' rows (1 = yes)
        calcs (list of dicts): Calculations to perform. Each dict:
            {
                'name': 'TOTAL_LENDING',
                'type': 'sum',
                'column': 'LENDING'
            }
            or
            {
                'name': 'WEIGHTED_INTEREST',
                'type': 'weighted_avg',
                'column': 'INTEREST_RATE',
                'weight_by': 'LENDING'
            }

    Returns:
        pd.DataFrame: Aggregated output
    """
    if calcs is None:
        calcs = []

    # Preprocess for groupby
    agg_dict = {}
    for calc in calcs:
        if calc['type'] == 'sum':
            agg_dict[calc['name']] = (calc['column'], 'sum')
        elif calc['type'] == 'weighted_avg':
            # Do numerator in groupby, compute avg manually later
            num_col = f"__{calc['name']}_NUM"
            agg_dict[num_col] = (calc['column'], lambda x: (df.loc[x.index, calc['weight_by']] * x).sum())
            agg_dict[f"__{calc['name']}_DEN"] = (calc['weight_by'], 'sum')

    # Group
    grouped = df.groupby(primary_key, dropna=False).agg(**agg_dict).reset_index()

    # Compute weighted averages
    for calc in calcs:
        if calc['type'] == 'weighted_avg':
            num_col = f"__{calc['name']}_NUM"
            denom_col = f"__{calc['name']}_DEN"
            grouped[calc['name']] = grouped[num_col] / grouped[denom_col]
            grouped = grouped.drop(columns=[num_col, denom_col])

    result_frames = [grouped.copy()]

    # Add 'All' combinations
    if include_all:
        rollup_rows = []

        for r in range(1, len(primary_key)):
            for group_cols in combinations(primary_key, r):
                group_cols = list(group_cols)

                temp = grouped.groupby(group_cols).agg({c['name']: 'sum' if c['type'] == 'sum' else 'first' for c in calcs}).reset_index()

                # Reweight weighted averages
                for calc in calcs:
                    if calc['type'] == 'weighted_avg':
                        name = calc['name']
                        weights = grouped.loc[grouped.set_index(group_cols).index.isin(temp.set_index(group_cols).index), f"TOTAL_LENDING"]
                        weighted_vals = grouped.loc[grouped.set_index(group_cols).index.isin(temp.set_index(group_cols).index), name]
                        temp[name] = (weights * weighted_vals).groupby(temp.index).sum() / weights.groupby(temp.index).sum()

                for col in primary_key:
                    if col not in group_cols:
                        temp[col] = 'All'
                temp = temp[primary_key + [c['name'] for c in calcs]]
                rollup_rows.append(temp)

        # Grand total
        grand = {}
        for calc in calcs:
            if calc['type'] == 'sum':
                grand[calc['name']] = grouped[calc['name']].sum()
            elif calc['type'] == 'weighted_avg':
                weights = grouped[calc['weight_by']]
                vals = grouped[calc['name']]
                grand[calc['name']] = (weights * vals).sum() / weights.sum()

        grand_row = {col: 'All' for col in primary_key}
        grand_row.update(grand)
        rollup_rows.append(pd.DataFrame([grand_row]))

        result_frames += rollup_rows

    return pd.concat(result_frames, ignore_index=True)

calcs = [
    {'name': 'TOTAL_LENDING', 'type': 'sum', 'column': 'LENDING'},
    {'name': 'WEIGHTED_INTEREST', 'type': 'weighted_avg', 'column': 'INTEREST_RATE', 'weight_by': 'LENDING'},
    {'name': 'TXN_COUNT_TOTAL', 'type': 'sum', 'column': 'TXN_COUNT'}
]

result = CreateCalculatedField(final_df, ['BRANCHNAME', 'CITY'], include_all=1, calcs=calcs)


CreateCalculatedField(final_df,['BRANCHNAME','CITY','LOB','DURATION'])
38/189:
from itertools import combinations
import pandas as pd

def CreateCalculatedField(df, primary_key, include_all=1, calcs=None):
    """
    Efficient aggregation with support for sum and weighted averages,
    including 'All' rollup rows.

    Parameters:
        df (pd.DataFrame): Source data
        primary_key (list): Columns to group by
        include_all (int): Add 'All' rows (1 = yes)
        calcs (list of dicts): Calculations to perform. Each dict:
            {
                'name': 'TOTAL_LENDING',
                'type': 'sum',
                'column': 'LENDING'
            }
            or
            {
                'name': 'WEIGHTED_INTEREST',
                'type': 'weighted_avg',
                'column': 'INTEREST_RATE',
                'weight_by': 'LENDING'
            }

    Returns:
        pd.DataFrame: Aggregated output
    """
    if calcs is None:
        calcs = []

    # Preprocess for groupby
    agg_dict = {}
    for calc in calcs:
        if calc['type'] == 'sum':
            agg_dict[calc['name']] = (calc['column'], 'sum')
        elif calc['type'] == 'weighted_avg':
            # Do numerator in groupby, compute avg manually later
            num_col = f"__{calc['name']}_NUM"
            agg_dict[num_col] = (calc['column'], lambda x: (df.loc[x.index, calc['weight_by']] * x).sum())
            agg_dict[f"__{calc['name']}_DEN"] = (calc['weight_by'], 'sum')

    # Group
    grouped = df.groupby(primary_key, dropna=False).agg(**agg_dict).reset_index()

    # Compute weighted averages
    for calc in calcs:
        if calc['type'] == 'weighted_avg':
            num_col = f"__{calc['name']}_NUM"
            denom_col = f"__{calc['name']}_DEN"
            grouped[calc['name']] = grouped[num_col] / grouped[denom_col]
            grouped = grouped.drop(columns=[num_col, denom_col])

    result_frames = [grouped.copy()]

    # Add 'All' combinations
    if include_all:
        rollup_rows = []

        for r in range(1, len(primary_key)):
            for group_cols in combinations(primary_key, r):
                group_cols = list(group_cols)

                temp = grouped.groupby(group_cols).agg({c['name']: 'sum' if c['type'] == 'sum' else 'first' for c in calcs}).reset_index()

                # Reweight weighted averages
                for calc in calcs:
                    if calc['type'] == 'weighted_avg':
                        name = calc['name']
                        weights = grouped.loc[grouped.set_index(group_cols).index.isin(temp.set_index(group_cols).index), f"TOTAL_LENDING"]
                        weighted_vals = grouped.loc[grouped.set_index(group_cols).index.isin(temp.set_index(group_cols).index), name]
                        temp[name] = (weights * weighted_vals).groupby(temp.index).sum() / weights.groupby(temp.index).sum()

                for col in primary_key:
                    if col not in group_cols:
                        temp[col] = 'All'
                temp = temp[primary_key + [c['name'] for c in calcs]]
                rollup_rows.append(temp)

        # Grand total
        grand = {}
        for calc in calcs:
            if calc['type'] == 'sum':
                grand[calc['name']] = grouped[calc['name']].sum()
            elif calc['type'] == 'weighted_avg':
                weights = grouped[calc['weight_by']]
                vals = grouped[calc['name']]
                grand[calc['name']] = (weights * vals).sum() / weights.sum()

        grand_row = {col: 'All' for col in primary_key}
        grand_row.update(grand)
        rollup_rows.append(pd.DataFrame([grand_row]))

        result_frames += rollup_rows

    return pd.concat(result_frames, ignore_index=True)

calcs = [
    {'name': 'WEIGHTED_INTEREST', 'type': 'weighted_avg', 'column': 'INTEREST_RATE', 'weight_by': 'LENDING'},
    {'name': 'TXN_COUNT_TOTAL', 'type': 'sum', 'column': 'TXN_COUNT'}
]

result = CreateCalculatedField(final_df, ['BRANCHNAME','CITY','LOB','DURATION'], include_all=1, calcs=calcs)


CreateCalculatedField(final_df,['BRANCHNAME','CITY','LOB','DURATION'])
38/190:
from itertools import combinations, product
import pandas as pd

def CreateCalculatedField(df, primary_key, include_all=1):
    """
    Efficiently creates a dataframe of all group combinations with optional 'All' rollups,
    and calculates aggregates (sum, weighted average) using a single groupby + synthetic rollups.

    Parameters:
        df (pd.DataFrame): Source dataframe
        primary_key (list): Columns to group by
        include_all (int): 1 to include 'All' combinations (default)

    Returns:
        pd.DataFrame: Result with TOTAL_LENDING and WEIGHTED_INTEREST
    """

    # Step 1: Base groupby aggregation
    grouped = df.groupby(primary_key, dropna=False).agg(
        TOTAL_LENDING=('LENDING', 'sum'),
        _WEIGHT_NUM=('INTEREST_RATE', lambda x: (df.loc[x.index, 'LENDING'] * x).sum())
    ).reset_index()
    grouped['WEIGHTED_INTEREST'] = grouped['_WEIGHT_NUM'] / grouped['TOTAL_LENDING']
    grouped = grouped.drop(columns=['_WEIGHT_NUM'])

    result_frames = [grouped.copy()]

    if include_all:
        rollup_rows = []

        # Step 2: All partial groupings
        for r in range(1, len(primary_key)):
            for group_cols in combinations(primary_key, r):
                temp = grouped.groupby(list(group_cols)).agg({
                    'TOTAL_LENDING': 'sum',
                    'WEIGHTED_INTEREST': lambda x: (
                        (x * grouped.loc[x.index, 'TOTAL_LENDING']).sum() /
                        grouped.loc[x.index, 'TOTAL_LENDING'].sum()
                    )
                }).reset_index()

                # Fill missing columns with 'All'
                for col in primary_key:
                    if col not in group_cols:
                        temp[col] = 'All'

                # Reorder columns
                temp = temp[primary_key + ['TOTAL_LENDING', 'WEIGHTED_INTEREST']]
                rollup_rows.append(temp)

        # Step 3: Full 'All' row
        grand_total = grouped['TOTAL_LENDING'].sum()
        grand_weighted = (
            (grouped['TOTAL_LENDING'] * grouped['WEIGHTED_INTEREST']).sum() / grand_total
            if grand_total > 0 else 0
        )
        all_row = pd.DataFrame([{
            **{col: 'All' for col in primary_key},
            'TOTAL_LENDING': grand_total,
            'WEIGHTED_INTEREST': grand_weighted
        }])
        rollup_rows.append(all_row)

        result_frames += rollup_rows

    # Final result
    final_df = pd.concat(result_frames, ignore_index=True)
    return final_df


CreateCalculatedField(final_df,['BRANCHNAME','CITY','LOB','DURATION'])
38/191:
import pandas as pd
from itertools import combinations

def CreateCalculatedField(df, primary_key, calc_instructions, include_all=1):
    """
    Generalized groupby calculation with optional rollups and support for:
    - Sum
    - Weighted Average

    Parameters:
        df (pd.DataFrame): Source data
        primary_key (list): Grouping columns
        calc_instructions (list of dict): 
            Each dict should contain:
                - type: 'sum' or 'weighted_average'
                - value1: main column (or weight column for weighted)
                - value2: column to average (for weighted_average)
                - name: output column name
        include_all (int): Whether to include 'All' rollups

    Returns:
        pd.DataFrame: Result with calculated columns
    """

    # Step 1: Initial aggregation
    agg_map = {}
    for ci in calc_instructions:
        if ci['type'] == 'sum':
            agg_map[ci['name']] = (ci['value1'], 'sum')
        elif ci['type'] == 'weighted_average':
            # Split numerator and denominator to calculate later
            agg_map[f"__{ci['name']}_NUM"] = (
                ci['value2'], lambda x, wcol=ci['value1']: (df.loc[x.index, wcol] * x).sum()
            )
            agg_map[f"__{ci['name']}_DEN"] = (ci['value1'], 'sum')

    grouped = df.groupby(primary_key, dropna=False).agg(**agg_map).reset_index()

    # Calculate weighted averages
    for ci in calc_instructions:
        if ci['type'] == 'weighted_average':
            num_col = f"__{ci['name']}_NUM"
            den_col = f"__{ci['name']}_DEN"
            grouped[ci['name']] = grouped[num_col] / grouped[den_col]
            grouped.loc[grouped[den_col] == 0, ci['name']] = 0  # Avoid div-by-zero
            grouped.drop(columns=[num_col, den_col], inplace=True)

    result_frames = [grouped.copy()]

    # Step 2: Generate rollup combinations
    if include_all:
        rollup_rows = []
        value_cols = [ci['name'] for ci in calc_instructions]

        for r in range(1, len(primary_key)):
            for group_cols in combinations(primary_key, r):
                group_cols = list(group_cols)

                temp = grouped.groupby(group_cols)[value_cols].agg('sum').reset_index()

                # Recalculate weighted averages using weighted sum logic
                for ci in calc_instructions:
                    if ci['type'] == 'weighted_average':
                        numerator = (
                            grouped.set_index(group_cols)[ci['name']] *
                            grouped.set_index(group_cols)[next(c['name'] for c in calc_instructions if c['type'] == 'sum' and c['value1'] == ci['value1'])]
                        ).groupby(level=0).sum()
                        denominator = grouped.set_index(group_cols)[next(c['name'] for c in calc_instructions if c['type'] == 'sum' and c['value1'] == ci['value1'])].groupby(level=0).sum()

                        temp.set_index(group_cols, inplace=True)
                        temp[ci['name']] = (numerator / denominator).fillna(0)
                        temp.reset_index(inplace=True)

                # Fill missing grouping cols with 'All'
                for col in primary_key:
                    if col not in group_cols:
                        temp[col] = 'All'

                temp = temp[primary_key + value_cols]
                rollup_rows.append(temp)

        # Step 3: Grand total row
        grand_row = {col: 'All' for col in primary_key}
        for ci in calc_instructions:
            if ci['type'] == 'sum':
                grand_row[ci['name']] = grouped[ci['name']].sum()
            elif ci['type'] == 'weighted_average':
                weight_col_name = next(c['name'] for c in calc_instructions if c['type'] == 'sum' and c['value1'] == ci['value1'])
                weights = grouped[weight_col_name]
                values = grouped[ci['name']]
                grand_row[ci['name']] = (weights * values).sum() / weights.sum() if weights.sum() > 0 else 0

        rollup_rows.append(pd.DataFrame([grand_row]))
        result_frames += rollup_rows

    return pd.concat(result_frames, ignore_index=True)



calc_instructions = [
    {
        'type': 'sum',
        'value1': 'LENDING',
        'name': 'Total Lending'
    },
    {
        'type': 'weighted_average',
        'value1': 'LENDING',
        'value2': 'INTEREST_RATE',
        'name': 'Weighted Interest Rate'
    },
    {
        'type': 'sum',
        'value1': 'TXN_COUNT',
        'name': 'Total Transactions'
    }
]

output = CreateCalculatedField(final_df, ['BRANCHNAME', 'CITY', 'LOB', 'DURATION'], calc_instructions)
38/192:
import pandas as pd
from itertools import combinations

def CreateCalculatedField(df, primary_key, calc_instructions, include_all=1):
    """
    Generalized groupby calculation with optional rollups and support for:
    - Sum
    - Weighted Average

    Parameters:
        df (pd.DataFrame): Source data
        primary_key (list): Grouping columns
        calc_instructions (list of dict): 
            Each dict should contain:
                - type: 'sum' or 'weighted_average'
                - value1: main column (or weight column for weighted)
                - value2: column to average (for weighted_average)
                - name: output column name
        include_all (int): Whether to include 'All' rollups

    Returns:
        pd.DataFrame: Result with calculated columns
    """

    # Step 1: Initial aggregation
    agg_map = {}
    for ci in calc_instructions:
        if ci['type'] == 'sum':
            agg_map[ci['name']] = (ci['value1'], 'sum')
        elif ci['type'] == 'weighted_average':
            # Split numerator and denominator to calculate later
            agg_map[f"__{ci['name']}_NUM"] = (
                ci['value2'], lambda x, wcol=ci['value1']: (df.loc[x.index, wcol] * x).sum()
            )
            agg_map[f"__{ci['name']}_DEN"] = (ci['value1'], 'sum')

    grouped = df.groupby(primary_key, dropna=False).agg(**agg_map).reset_index()

    # Calculate weighted averages
    for ci in calc_instructions:
        if ci['type'] == 'weighted_average':
            num_col = f"__{ci['name']}_NUM"
            den_col = f"__{ci['name']}_DEN"
            grouped[ci['name']] = grouped[num_col] / grouped[den_col]
            grouped.loc[grouped[den_col] == 0, ci['name']] = 0  # Avoid div-by-zero
            grouped.drop(columns=[num_col, den_col], inplace=True)

    result_frames = [grouped.copy()]

    # Step 2: Generate rollup combinations
    if include_all:
        rollup_rows = []
        value_cols = [ci['name'] for ci in calc_instructions]

        for r in range(1, len(primary_key)):
            for group_cols in combinations(primary_key, r):
                group_cols = list(group_cols)

                temp = grouped.groupby(group_cols)[value_cols].agg('sum').reset_index()

                # Recalculate weighted averages using weighted sum logic
                for ci in calc_instructions:
                    if ci['type'] == 'weighted_average':
                        numerator = (
                            grouped.set_index(group_cols)[ci['name']] *
                            grouped.set_index(group_cols)[next(c['name'] for c in calc_instructions if c['type'] == 'sum' and c['value1'] == ci['value1'])]
                        ).groupby(level=0).sum()
                        denominator = grouped.set_index(group_cols)[next(c['name'] for c in calc_instructions if c['type'] == 'sum' and c['value1'] == ci['value1'])].groupby(level=0).sum()

                        temp.set_index(group_cols, inplace=True)
                        temp[ci['name']] = (numerator / denominator).fillna(0)
                        temp.reset_index(inplace=True)

                # Fill missing grouping cols with 'All'
                for col in primary_key:
                    if col not in group_cols:
                        temp[col] = 'All'

                temp = temp[primary_key + value_cols]
                rollup_rows.append(temp)

        # Step 3: Grand total row
        grand_row = {col: 'All' for col in primary_key}
        for ci in calc_instructions:
            if ci['type'] == 'sum':
                grand_row[ci['name']] = grouped[ci['name']].sum()
            elif ci['type'] == 'weighted_average':
                weight_col_name = next(c['name'] for c in calc_instructions if c['type'] == 'sum' and c['value1'] == ci['value1'])
                weights = grouped[weight_col_name]
                values = grouped[ci['name']]
                grand_row[ci['name']] = (weights * values).sum() / weights.sum() if weights.sum() > 0 else 0

        rollup_rows.append(pd.DataFrame([grand_row]))
        result_frames += rollup_rows

    return pd.concat(result_frames, ignore_index=True)



calc_instructions = [
    {
        'type': 'sum',
        'value1': 'LENDING',
        'name': 'Total Lending'
    },
    {
        'type': 'weighted_average',
        'value1': 'LENDING',
        'value2': 'INTEREST_RATE',
        'name': 'Weighted Interest Rate'
    },
    {
        'type': 'sum',
        'value1': 'TXN_COUNT',
        'name': 'Total Transactions'
    }
]

output = CreateCalculatedField(final_df, ['BRANCHNAME', 'CITY', 'LOB', 'DURATION'], calc_instructions)
output
38/193: d
38/194: e
38/195: final_df
38/196: final_df['MATURING_AMOUNT'] = final_df['LENDING'].apply(lambda x:x*np.random.uniform(0, 1))
38/197:
final_df['MATURING_AMOUNT'] = final_df['LENDING'].apply(lambda x:x*np.random.uniform(0, 1)) 
final_df['RENEWAL_AMOUNT'] = final_df['MATURING_AMOUNT'].apply(lambda x:x*.95)
38/198:
import pandas as pd
from itertools import combinations

import pandas as pd
from itertools import combinations

def CreateCalculatedField(df, primary_key, calc_instructions, include_all=1):
    """
    Generalized groupby calculation with optional rollups.
    Supports: sum, weighted_average, and ratio (value1 / value2).

    Parameters:
        df (pd.DataFrame): Input dataframe
        primary_key (list): List of grouping columns
        calc_instructions (list of dict): Each dict must include:
            - type: 'sum', 'weighted_average', or 'ratio'
            - value1: numerator (or base for sum)
            - value2: denominator (only for weighted_average or ratio)
            - name: output column name
        include_all (int): Whether to include 'All' rollups

    Returns:
        pd.DataFrame
    """

    agg_map = {}

    for ci in calc_instructions:
        if ci['type'] == 'sum':
            agg_map[ci['name']] = (ci['value1'], 'sum')

        elif ci['type'] == 'weighted_average':
            agg_map[f"__{ci['name']}_NUM"] = (
                ci['value2'], lambda x, wcol=ci['value1']: (df.loc[x.index, wcol] * x).sum()
            )
            agg_map[f"__{ci['name']}_DEN"] = (ci['value1'], 'sum')

        elif ci['type'] == 'ratio':
            agg_map[f"__{ci['name']}_NUM"] = (ci['value1'], 'sum')
            agg_map[f"__{ci['name']}_DEN"] = (ci['value2'], 'sum')

    grouped = df.groupby(primary_key, dropna=False).agg(**agg_map).reset_index()

    # Post-process derived metrics
    for ci in calc_instructions:
        if ci['type'] in ['weighted_average', 'ratio']:
            num_col = f"__{ci['name']}_NUM"
            den_col = f"__{ci['name']}_DEN"
            grouped[ci['name']] = grouped[num_col] / grouped[den_col]
            grouped.loc[grouped[den_col] == 0, ci['name']] = 0
            grouped.drop(columns=[num_col, den_col], inplace=True)

    result_frames = [grouped.copy()]
    value_cols = [ci['name'] for ci in calc_instructions]

    # Rollups
    if include_all:
        rollup_rows = []

        for r in range(1, len(primary_key)):
            for group_cols in combinations(primary_key, r):
                group_cols = list(group_cols)
                temp = grouped.groupby(group_cols)[value_cols].sum().reset_index()

                # Handle weighted averages and ratios
                for ci in calc_instructions:
                    if ci['type'] in ['weighted_average', 'ratio']:
                        weight_name = next(
                            c['name'] for c in calc_instructions
                            if c['type'] == 'sum' and c['value1'] == ci['value1']
                        )
                        numerator = (grouped.set_index(group_cols)[ci['name']] *
                                     grouped.set_index(group_cols)[weight_name]).groupby(level=0).sum()
                        denominator = grouped.set_index(group_cols)[weight_name].groupby(level=0).sum()
                        temp.set_index(group_cols, inplace=True)
                        temp[ci['name']] = (numerator / denominator).fillna(0)
                        temp.reset_index(inplace=True)

                for col in primary_key:
                    if col not in group_cols:
                        temp[col] = 'All'

                temp = temp[primary_key + value_cols]
                rollup_rows.append(temp)

        # Grand total
        grand_row = {col: 'All' for col in primary_key}
        for ci in calc_instructions:
            if ci['type'] == 'sum':
                grand_row[ci['name']] = grouped[ci['name']].sum()
            elif ci['type'] in ['weighted_average', 'ratio']:
                weight_name = next(
                    c['name'] for c in calc_instructions
                    if c['type'] == 'sum' and c['value1'] == ci['value1']
                )
                weights = grouped[weight_name]
                values = grouped[ci['name']]
                grand_row[ci['name']] = (weights * values).sum() / weights.sum() if weights.sum() > 0 else 0

        rollup_rows.append(pd.DataFrame([grand_row]))
        result_frames += rollup_rows

    return pd.concat(result_frames, ignore_index=True)



calc_instructions = [
    {
        'type': 'sum',
        'value1': 'LENDING',
        'name': 'Total Lending'
    },
    {
        'type': 'weighted_average',
        'value1': 'LENDING',
        'value2': 'INTEREST_RATE',
        'name': 'Weighted Interest Rate'
    },
    {
        'type': 'ratio',         # Division: value1 / value2
        'value1': 'RENEWED_AMOUNT',
        'value2': 'MATURED_AMOUNT',
        'name': 'Renewal Rate'
    }
]

output = CreateCalculatedField(final_df, ['BRANCHNAME', 'CITY', 'LOB', 'DURATION'], calc_instructions)
output
38/199:
from itertools import product,permutations,combinations


branches = ['Fenway Park','Wrigely Field','The Forum','Maple Leaf Gardens','GM Place','Safeco','Lambeau Stadium']
city = ['Boston','Chicago','Philly','Toronto','Vancouver','Chicago','Seattle','Green Bay']
lob = ['Retail','Corporate','Mid Market']
duration = ['1) Less than 30d','2) 30D - 1Y', '3) 1Y - 5Y','4)5Y+']
list_of_lists = [branches,city,lob,duration]
final_df['MATURED_AMOUNT'] = final_df['LENDING'].apply(lambda x:x*np.random.uniform(0, 1)) 
final_df['RENEWAL_AMOUNT'] = final_df['MATURED_AMOUNT'].apply(lambda x:x*.95)
38/200:
import pandas as pd
from itertools import combinations

import pandas as pd
from itertools import combinations

def CreateCalculatedField(df, primary_key, calc_instructions, include_all=1):
    """
    Generalized groupby calculation with optional rollups.
    Supports: sum, weighted_average, and ratio (value1 / value2).

    Parameters:
        df (pd.DataFrame): Input dataframe
        primary_key (list): List of grouping columns
        calc_instructions (list of dict): Each dict must include:
            - type: 'sum', 'weighted_average', or 'ratio'
            - value1: numerator (or base for sum)
            - value2: denominator (only for weighted_average or ratio)
            - name: output column name
        include_all (int): Whether to include 'All' rollups

    Returns:
        pd.DataFrame
    """

    agg_map = {}

    for ci in calc_instructions:
        if ci['type'] == 'sum':
            agg_map[ci['name']] = (ci['value1'], 'sum')

        elif ci['type'] == 'weighted_average':
            agg_map[f"__{ci['name']}_NUM"] = (
                ci['value2'], lambda x, wcol=ci['value1']: (df.loc[x.index, wcol] * x).sum()
            )
            agg_map[f"__{ci['name']}_DEN"] = (ci['value1'], 'sum')

        elif ci['type'] == 'ratio':
            agg_map[f"__{ci['name']}_NUM"] = (ci['value1'], 'sum')
            agg_map[f"__{ci['name']}_DEN"] = (ci['value2'], 'sum')

    grouped = df.groupby(primary_key, dropna=False).agg(**agg_map).reset_index()

    # Post-process derived metrics
    for ci in calc_instructions:
        if ci['type'] in ['weighted_average', 'ratio']:
            num_col = f"__{ci['name']}_NUM"
            den_col = f"__{ci['name']}_DEN"
            grouped[ci['name']] = grouped[num_col] / grouped[den_col]
            grouped.loc[grouped[den_col] == 0, ci['name']] = 0
            grouped.drop(columns=[num_col, den_col], inplace=True)

    result_frames = [grouped.copy()]
    value_cols = [ci['name'] for ci in calc_instructions]

    # Rollups
    if include_all:
        rollup_rows = []

        for r in range(1, len(primary_key)):
            for group_cols in combinations(primary_key, r):
                group_cols = list(group_cols)
                temp = grouped.groupby(group_cols)[value_cols].sum().reset_index()

                # Handle weighted averages and ratios
                for ci in calc_instructions:
                    if ci['type'] in ['weighted_average', 'ratio']:
                        weight_name = next(
                            c['name'] for c in calc_instructions
                            if c['type'] == 'sum' and c['value1'] == ci['value1']
                        )
                        numerator = (grouped.set_index(group_cols)[ci['name']] *
                                     grouped.set_index(group_cols)[weight_name]).groupby(level=0).sum()
                        denominator = grouped.set_index(group_cols)[weight_name].groupby(level=0).sum()
                        temp.set_index(group_cols, inplace=True)
                        temp[ci['name']] = (numerator / denominator).fillna(0)
                        temp.reset_index(inplace=True)

                for col in primary_key:
                    if col not in group_cols:
                        temp[col] = 'All'

                temp = temp[primary_key + value_cols]
                rollup_rows.append(temp)

        # Grand total
        grand_row = {col: 'All' for col in primary_key}
        for ci in calc_instructions:
            if ci['type'] == 'sum':
                grand_row[ci['name']] = grouped[ci['name']].sum()
            elif ci['type'] in ['weighted_average', 'ratio']:
                weight_name = next(
                    c['name'] for c in calc_instructions
                    if c['type'] == 'sum' and c['value1'] == ci['value1']
                )
                weights = grouped[weight_name]
                values = grouped[ci['name']]
                grand_row[ci['name']] = (weights * values).sum() / weights.sum() if weights.sum() > 0 else 0

        rollup_rows.append(pd.DataFrame([grand_row]))
        result_frames += rollup_rows

    return pd.concat(result_frames, ignore_index=True)



calc_instructions = [
    {
        'type': 'sum',
        'value1': 'LENDING',
        'name': 'Total Lending'
    },
    {
        'type': 'weighted_average',
        'value1': 'LENDING',
        'value2': 'INTEREST_RATE',
        'name': 'Weighted Interest Rate'
    },
    {
        'type': 'ratio',         # Division: value1 / value2
        'value1': 'RENEWED_AMOUNT',
        'value2': 'MATURED_AMOUNT',
        'name': 'Renewal Rate'
    }
]

output = CreateCalculatedField(final_df, ['BRANCHNAME', 'CITY', 'LOB', 'DURATION'], calc_instructions)
output
38/201: final_df['RENEWED_AMOUNT'] = final_df['MATURED_AMOUNT'].apply(lambda x:x*.95)
38/202:
import pandas as pd
from itertools import combinations

import pandas as pd
from itertools import combinations

def CreateCalculatedField(df, primary_key, calc_instructions, include_all=1):
    """
    Generalized groupby calculation with optional rollups.
    Supports: sum, weighted_average, and ratio (value1 / value2).

    Parameters:
        df (pd.DataFrame): Input dataframe
        primary_key (list): List of grouping columns
        calc_instructions (list of dict): Each dict must include:
            - type: 'sum', 'weighted_average', or 'ratio'
            - value1: numerator (or base for sum)
            - value2: denominator (only for weighted_average or ratio)
            - name: output column name
        include_all (int): Whether to include 'All' rollups

    Returns:
        pd.DataFrame
    """

    agg_map = {}

    for ci in calc_instructions:
        if ci['type'] == 'sum':
            agg_map[ci['name']] = (ci['value1'], 'sum')

        elif ci['type'] == 'weighted_average':
            agg_map[f"__{ci['name']}_NUM"] = (
                ci['value2'], lambda x, wcol=ci['value1']: (df.loc[x.index, wcol] * x).sum()
            )
            agg_map[f"__{ci['name']}_DEN"] = (ci['value1'], 'sum')

        elif ci['type'] == 'ratio':
            agg_map[f"__{ci['name']}_NUM"] = (ci['value1'], 'sum')
            agg_map[f"__{ci['name']}_DEN"] = (ci['value2'], 'sum')

    grouped = df.groupby(primary_key, dropna=False).agg(**agg_map).reset_index()

    # Post-process derived metrics
    for ci in calc_instructions:
        if ci['type'] in ['weighted_average', 'ratio']:
            num_col = f"__{ci['name']}_NUM"
            den_col = f"__{ci['name']}_DEN"
            grouped[ci['name']] = grouped[num_col] / grouped[den_col]
            grouped.loc[grouped[den_col] == 0, ci['name']] = 0
            grouped.drop(columns=[num_col, den_col], inplace=True)

    result_frames = [grouped.copy()]
    value_cols = [ci['name'] for ci in calc_instructions]

    # Rollups
    if include_all:
        rollup_rows = []

        for r in range(1, len(primary_key)):
            for group_cols in combinations(primary_key, r):
                group_cols = list(group_cols)
                temp = grouped.groupby(group_cols)[value_cols].sum().reset_index()

                # Handle weighted averages and ratios
                for ci in calc_instructions:
                    if ci['type'] in ['weighted_average', 'ratio']:
                        weight_name = next(
                            c['name'] for c in calc_instructions
                            if c['type'] == 'sum' and c['value1'] == ci['value1']
                        )
                        numerator = (grouped.set_index(group_cols)[ci['name']] *
                                     grouped.set_index(group_cols)[weight_name]).groupby(level=0).sum()
                        denominator = grouped.set_index(group_cols)[weight_name].groupby(level=0).sum()
                        temp.set_index(group_cols, inplace=True)
                        temp[ci['name']] = (numerator / denominator).fillna(0)
                        temp.reset_index(inplace=True)

                for col in primary_key:
                    if col not in group_cols:
                        temp[col] = 'All'

                temp = temp[primary_key + value_cols]
                rollup_rows.append(temp)

        # Grand total
        grand_row = {col: 'All' for col in primary_key}
        for ci in calc_instructions:
            if ci['type'] == 'sum':
                grand_row[ci['name']] = grouped[ci['name']].sum()
            elif ci['type'] in ['weighted_average', 'ratio']:
                weight_name = next(
                    c['name'] for c in calc_instructions
                    if c['type'] == 'sum' and c['value1'] == ci['value1']
                )
                weights = grouped[weight_name]
                values = grouped[ci['name']]
                grand_row[ci['name']] = (weights * values).sum() / weights.sum() if weights.sum() > 0 else 0

        rollup_rows.append(pd.DataFrame([grand_row]))
        result_frames += rollup_rows

    return pd.concat(result_frames, ignore_index=True)



calc_instructions = [
    {
        'type': 'sum',
        'value1': 'LENDING',
        'name': 'Total Lending'
    },
    {
        'type': 'weighted_average',
        'value1': 'LENDING',
        'value2': 'INTEREST_RATE',
        'name': 'Weighted Interest Rate'
    },
    {
        'type': 'ratio',         # Division: value1 / value2
        'value1': 'RENEWED_AMOUNT',
        'value2': 'MATURED_AMOUNT',
        'name': 'Renewal Rate'
    }
]

output = CreateCalculatedField(final_df, ['BRANCHNAME', 'CITY', 'LOB', 'DURATION'], calc_instructions)
output
38/203:
import pandas as pd
from itertools import combinations

import pandas as pd
from itertools import combinations

def CreateCalculatedField(df, primary_key, calc_instructions, include_all=1):
    """
    Generalized groupby calculation with optional rollups.
    Supports: sum, weighted_average, and ratio (value1 / value2).

    Parameters:
        df (pd.DataFrame): Input dataframe
        primary_key (list): List of grouping columns
        calc_instructions (list of dict): Each dict must include:
            - type: 'sum', 'weighted_average', or 'ratio'
            - value1: numerator (or base for sum)
            - value2: denominator (only for weighted_average or ratio)
            - name: output column name
        include_all (int): Whether to include 'All' rollups

    Returns:
        pd.DataFrame
    """

    agg_map = {}

    for ci in calc_instructions:
        if ci['type'] == 'sum':
            agg_map[ci['name']] = (ci['value1'], 'sum')

        elif ci['type'] == 'weighted_average':
            agg_map[f"__{ci['name']}_NUM"] = (
                ci['value2'], lambda x, wcol=ci['value1']: (df.loc[x.index, wcol] * x).sum()
            )
            agg_map[f"__{ci['name']}_DEN"] = (ci['value1'], 'sum')

        elif ci['type'] == 'ratio':
            agg_map[f"__{ci['name']}_NUM"] = (ci['value1'], 'sum')
            agg_map[f"__{ci['name']}_DEN"] = (ci['value2'], 'sum')

    grouped = df.groupby(primary_key, dropna=False).agg(**agg_map).reset_index()

    # Post-process derived metrics
    for ci in calc_instructions:
        if ci['type'] in ['weighted_average', 'ratio']:
            num_col = f"__{ci['name']}_NUM"
            den_col = f"__{ci['name']}_DEN"
            grouped[ci['name']] = grouped[num_col] / grouped[den_col]
            grouped.loc[grouped[den_col] == 0, ci['name']] = 0
            grouped.drop(columns=[num_col, den_col], inplace=True)

    result_frames = [grouped.copy()]
    value_cols = [ci['name'] for ci in calc_instructions]

    # Rollups
    if include_all:
        rollup_rows = []

        for r in range(1, len(primary_key)):
            for group_cols in combinations(primary_key, r):
                group_cols = list(group_cols)
                temp = grouped.groupby(group_cols)[value_cols].sum().reset_index()

                # Handle weighted averages and ratios
                for ci in calc_instructions:
                    if ci['type'] in ['weighted_average', 'ratio']:
                        weight_name = next(
                            c['name'] for c in calc_instructions
                            if c['type'] == 'sum' and c['value1'] == ci['value1']
                        )
                        numerator = (grouped.set_index(group_cols)[ci['name']] *
                                     grouped.set_index(group_cols)[weight_name]).groupby(level=0).sum()
                        denominator = grouped.set_index(group_cols)[weight_name].groupby(level=0).sum()
                        temp.set_index(group_cols, inplace=True)
                        temp[ci['name']] = (numerator / denominator).fillna(0)
                        temp.reset_index(inplace=True)

                for col in primary_key:
                    if col not in group_cols:
                        temp[col] = 'All'

                temp = temp[primary_key + value_cols]
                rollup_rows.append(temp)

        # Grand total
        grand_row = {col: 'All' for col in primary_key}
        for ci in calc_instructions:
            if ci['type'] == 'sum':
                grand_row[ci['name']] = grouped[ci['name']].sum()
            elif ci['type'] in ['weighted_average', 'ratio']:
                weight_name = next(
                    c['name'] for c in calc_instructions
                    if c['type'] == 'sum' and c['value1'] == ci['value1']
                )
                weights = grouped[weight_name]
                values = grouped[ci['name']]
                grand_row[ci['name']] = (weights * values).sum() / weights.sum() if weights.sum() > 0 else 0

        rollup_rows.append(pd.DataFrame([grand_row]))
        result_frames += rollup_rows

    return pd.concat(result_frames, ignore_index=True)



calc_instructions = [
    {
        'type': 'sum',
        'value1': 'LENDING',
        'name': 'Total Lending'
    },
    {
        'type': 'weighted_average',
        'value1': 'LENDING',
        'value2': 'INTEREST_RATE',
        'name': 'Weighted Interest Rate'
    },
    {
        'type': 'ratio',         # Division: value1 / value2
        'value1': 'RENEWED_AMOUNT',
        'value2': 'MATURED_AMOUNT',
        'name': 'Renewal Rate'
    }
]

output = CreateCalculatedField(final_df, ['BRANCHNAME', 'CITY', 'LOB', 'DURATION'], calc_instructions)
output
38/204:
import pandas as pd
from itertools import combinations

import pandas as pd
from itertools import combinations

def CreateCalculatedField(df, primary_key, calc_instructions, include_all=1):
    """
    Generalized groupby calculation with optional rollups.
    Supports: sum, weighted_average, and ratio (value1 / value2).

    Parameters:
        df (pd.DataFrame): Input dataframe
        primary_key (list): List of grouping columns
        calc_instructions (list of dict): Each dict must include:
            - type: 'sum', 'weighted_average', or 'ratio'
            - value1: numerator (or base for sum)
            - value2: denominator (only for weighted_average or ratio)
            - name: output column name
        include_all (int): Whether to include 'All' rollups

    Returns:
        pd.DataFrame
    """

    agg_map = {}

    for ci in calc_instructions:
        if ci['type'] == 'sum':
            agg_map[ci['name']] = (ci['value1'], 'sum')

        elif ci['type'] == 'weighted_average':
            agg_map[f"__{ci['name']}_NUM"] = (
                ci['value2'], lambda x, wcol=ci['value1']: (df.loc[x.index, wcol] * x).sum()
            )
            agg_map[f"__{ci['name']}_DEN"] = (ci['value1'], 'sum')

        elif ci['type'] == 'ratio':
            agg_map[f"__{ci['name']}_NUM"] = (ci['value1'], 'sum')
            agg_map[f"__{ci['name']}_DEN"] = (ci['value2'], 'sum')

    grouped = df.groupby(primary_key, dropna=False).agg(**agg_map).reset_index()

    # Post-process derived metrics
    for ci in calc_instructions:
        if ci['type'] in ['weighted_average', 'ratio']:
            num_col = f"__{ci['name']}_NUM"
            den_col = f"__{ci['name']}_DEN"
            grouped[ci['name']] = grouped[num_col] / grouped[den_col]
            grouped.loc[grouped[den_col] == 0, ci['name']] = 0
            grouped.drop(columns=[num_col, den_col], inplace=True)

    result_frames = [grouped.copy()]
    value_cols = [ci['name'] for ci in calc_instructions]

    # Rollups
    if include_all:
        rollup_rows = []

        for r in range(1, len(primary_key)):
            for group_cols in combinations(primary_key, r):
                group_cols = list(group_cols)
                temp = grouped.groupby(group_cols)[value_cols].sum().reset_index()

                # Handle weighted averages and ratios
                for ci in calc_instructions:
                    if ci['type'] in ['weighted_average', 'ratio']:
                        weight_name = next(
                            c['name'] for c in calc_instructions
                            if c['type'] == 'sum' and c['value1'] == ci['value1']
                        )
                        numerator = (grouped.set_index(group_cols)[ci['name']] *
                                     grouped.set_index(group_cols)[weight_name]).groupby(level=0).sum()
                        denominator = grouped.set_index(group_cols)[weight_name].groupby(level=0).sum()
                        temp.set_index(group_cols, inplace=True)
                        temp[ci['name']] = (numerator / denominator).fillna(0)
                        temp.reset_index(inplace=True)

                for col in primary_key:
                    if col not in group_cols:
                        temp[col] = 'All'

                temp = temp[primary_key + value_cols]
                rollup_rows.append(temp)

        # Grand total
        grand_row = {col: 'All' for col in primary_key}
        for ci in calc_instructions:
            if ci['type'] == 'sum':
                grand_row[ci['name']] = grouped[ci['name']].sum()
            elif ci['type'] in ['weighted_average', 'ratio']:
                weight_name = next(
                    c['name'] for c in calc_instructions
                    if c['type'] == 'sum' and c['value1'] == ci['value1']
                )
                weights = grouped[weight_name]
                values = grouped[ci['name']]
                grand_row[ci['name']] = (weights * values).sum() / weights.sum() if weights.sum() > 0 else 0

        rollup_rows.append(pd.DataFrame([grand_row]))
        result_frames += rollup_rows

    return pd.concat(result_frames, ignore_index=True)


calc_instructions = [
    {'type': 'sum', 'value1': 'LENDING', 'name': 'TOTAL_LENDING'},
    {'type': 'weighted_average', 'value1': 'LENDING', 'value2': 'INTEREST_RATE', 'name': 'WEIGHTED_INTEREST'},
    {'type': 'ratio', 'value1': 'RENEWED_AMOUNT', 'value2': 'MATURED_AMOUNT', 'name': 'RENEWAL_RATE'}
]

output = CreateCalculatedField(final_df, ['BRANCHNAME', 'CITY', 'LOB', 'DURATION'], calc_instructions)

output
38/205:
import pandas as pd
import numpy as np
from itertools import combinations

def CreateCalculatedField(df, primary_key, calc_instructions, include_all=1):
    base_aggs = {}
    
    # Collect all fields we need
    for calc in calc_instructions:
        if calc['type'] == 'sum':
            base_aggs[calc['name']] = (calc['value1'], 'sum')
        elif calc['type'] == 'weighted_average':
            base_aggs[f"__{calc['name']}_NUM"] = (
                calc['value2'], lambda x, col=calc['value1']: (df.loc[x.index, col] * x).sum()
            )
            base_aggs[f"__{calc['name']}_DEN"] = (calc['value1'], 'sum')
        elif calc['type'] == 'ratio':
            base_aggs[f"__{calc['name']}_NUM"] = (calc['value1'], 'sum')
            base_aggs[f"__{calc['name']}_DEN"] = (calc['value2'], 'sum')

    # Base groupby
    grouped = df.groupby(primary_key, dropna=False).agg(**base_aggs).reset_index()

    # Compute post-aggregates
    for calc in calc_instructions:
        if calc['type'] == 'sum':
            continue
        elif calc['type'] == 'weighted_average':
            num = grouped[f"__{calc['name']}_NUM"]
            den = grouped[f"__{calc['name']}_DEN"]
            grouped[calc['name']] = np.where(den != 0, num / den, np.nan)
            grouped.drop(columns=[f"__{calc['name']}_NUM", f"__{calc['name']}_DEN"], inplace=True)
        elif calc['type'] == 'ratio':
            num = grouped[f"__{calc['name']}_NUM"]
            den = grouped[f"__{calc['name']}_DEN"]
            grouped[calc['name']] = np.where(den != 0, num / den, np.nan)
            grouped.drop(columns=[f"__{calc['name']}_NUM", f"__{calc['name']}_DEN"], inplace=True)

    result_frames = [grouped.copy()]

    # Rollup combinations
    if include_all:
        for r in range(1, len(primary_key)):
            for group_cols in combinations(primary_key, r):
                temp = df.copy()
                for col in primary_key:
                    if col not in group_cols:
                        temp[col] = 'All'
                temp_group = CreateCalculatedField(temp, primary_key, calc_instructions, include_all=0)
                result_frames.append(temp_group)

        # Full 'All' row
        temp = df.copy()
        for col in primary_key:
            temp[col] = 'All'
        temp_group = CreateCalculatedField(temp, primary_key, calc_instructions, include_all=0)
        result_frames.append(temp_group)

    return pd.concat(result_frames, ignore_index=True)



calc_instructions = [
    {'type': 'sum', 'value1': 'LENDING', 'name': 'TOTAL_LENDING'},
    {'type': 'weighted_average', 'value1': 'LENDING', 'value2': 'INTEREST_RATE', 'name': 'WEIGHTED_INTEREST'},
    {'type': 'ratio', 'value1': 'RENEWED_AMOUNT', 'value2': 'MATURED_AMOUNT', 'name': 'RENEWAL_RATE'}
]

output = CreateCalculatedField(final_df, ['BRANCHNAME', 'CITY', 'LOB', 'DURATION'], calc_instructions)

output
39/1:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
39/2:
model  = keras.Sequential([
    layers.Dense(512, activation="relu", name="layer1"),
    layers.Dense(10, activation="softmax")
])

model.compile(optimizer="rmsprop", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
39/3:
# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
X_train = x_train.reshape(60000, 784)
X_test = x_test.reshape(10000, 784)
X_train = X_train.astype("float32") / 255
X_test = X_test.astype("float32") / 255
39/4: X_test
39/5:
model  = keras.Sequential([
    layers.Dense(512, activation="relu", name="layer1"),
    layers.Dense(10, activation="softmax")
])

model.compile(optimizer="rmsprop", loss="sparse_categorical_crossentropy", metrics=["accuracy"])

history = model.fit(X_train, y_train, epochs=10, batch_size=128,validation_split=.1)
39/6:
def plot_training_history(history, 
                          train_metric='accuracy', 
                          validation_metric='val_accuracy', 
                          title='Model Accuracy', 
                          y_label='Accuracy', 
                          x_label='Epoch'):
    '''
    
    
    
    '''
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns

    # Plot accuracy
    axes[0].plot(history.history[train_metric])
    axes[0].plot(history.history[validation_metric])
    axes[0].set_title(title)
    axes[0].set_ylabel(y_label)
    axes[0].set_xlabel(x_label)
    axes[0].legend(['Train', 'Validation'], loc='upper left')

    # Plot loss
    axes[1].plot(history.history['loss'])
    axes[1].plot(history.history['val_loss'])
    axes[1].set_title('Model Loss')
    axes[1].set_ylabel('Loss')
    axes[1].set_xlabel(x_label)
    axes[1].legend(['Train', 'Validation'], loc='upper left')

    plt.tight_layout()
    plt.show()
                                         
plot_training_history(history)
39/7:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import pandas as pd
import numpy as np
import matplotlib.pyplot as ply
39/8:
def plot_training_history(history, 
                          train_metric='accuracy', 
                          validation_metric='val_accuracy', 
                          title='Model Accuracy', 
                          y_label='Accuracy', 
                          x_label='Epoch'):
    '''
    
    
    
    '''
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns

    # Plot accuracy
    axes[0].plot(history.history[train_metric])
    axes[0].plot(history.history[validation_metric])
    axes[0].set_title(title)
    axes[0].set_ylabel(y_label)
    axes[0].set_xlabel(x_label)
    axes[0].legend(['Train', 'Validation'], loc='upper left')

    # Plot loss
    axes[1].plot(history.history['loss'])
    axes[1].plot(history.history['val_loss'])
    axes[1].set_title('Model Loss')
    axes[1].set_ylabel('Loss')
    axes[1].set_xlabel(x_label)
    axes[1].legend(['Train', 'Validation'], loc='upper left')

    plt.tight_layout()
    plt.show()
                                         
plot_training_history(history)
39/9:
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
39/10:
def plot_training_history(history, 
                          train_metric='accuracy', 
                          validation_metric='val_accuracy', 
                          title='Model Accuracy', 
                          y_label='Accuracy', 
                          x_label='Epoch'):
    '''
    
    
    
    '''
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns

    # Plot accuracy
    axes[0].plot(history.history[train_metric])
    axes[0].plot(history.history[validation_metric])
    axes[0].set_title(title)
    axes[0].set_ylabel(y_label)
    axes[0].set_xlabel(x_label)
    axes[0].legend(['Train', 'Validation'], loc='upper left')

    # Plot loss
    axes[1].plot(history.history['loss'])
    axes[1].plot(history.history['val_loss'])
    axes[1].set_title('Model Loss')
    axes[1].set_ylabel('Loss')
    axes[1].set_xlabel(x_label)
    axes[1].legend(['Train', 'Validation'], loc='upper left')

    plt.tight_layout()
    plt.show()
                                         
plot_training_history(history)
39/11: model.weights
39/12: model.weights[0
39/13: model.weights[0]
39/14: model.summary
39/15: model.summary(
39/16: model.summary()
40/1:
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(64, activation="relu"),
    layers.Dense(10, activation="softmax")
])
40/2:
model = keras.Sequential()
model.add(layers.Dense(64, activation="relu"))
model.add(layers.Dense(10, activation="softmax"))
40/3:
model.build(input_shape=(None, 3))
model.weights
40/4: model.summary()
40/5:
model = keras.Sequential(name="my_example_model")
model.add(layers.Dense(64, activation="relu", name="my_first_layer"))
model.add(layers.Dense(10, activation="softmax", name="my_last_layer"))
model.build((None, 3))
model.summary()
40/6:
model = keras.Sequential()
model.add(keras.Input(shape=(3,)))
model.add(layers.Dense(64, activation="relu"))
40/7: model.summary()
40/8:
model.add(layers.Dense(10, activation="softmax"))
model.summary()
40/9:
inputs = keras.Input(shape=(3,), name="my_input")
features = layers.Dense(64, activation="relu")(inputs)
outputs = layers.Dense(10, activation="softmax")(features)
model = keras.Model(inputs=inputs, outputs=outputs)
40/10: inputs = keras.Input(shape=(3,), name="my_input")
40/11: inputs.shape
40/12: inputs.dtype
40/13: features = layers.Dense(64, activation="relu")(inputs)
40/14: features.shape
40/15:
outputs = layers.Dense(10, activation="softmax")(features)
model = keras.Model(inputs=inputs, outputs=outputs)
40/16: model.summary()
40/17:
vocabulary_size = 10000
num_tags = 100
num_departments = 4

title = keras.Input(shape=(vocabulary_size,), name="title")
text_body = keras.Input(shape=(vocabulary_size,), name="text_body")
tags = keras.Input(shape=(num_tags,), name="tags")

features = layers.Concatenate()([title, text_body, tags])
features = layers.Dense(64, activation="relu")(features)

priority = layers.Dense(1, activation="sigmoid", name="priority")(features)
department = layers.Dense(
    num_departments, activation="softmax", name="department")(features)

model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])
40/18:
import numpy as np

num_samples = 1280

title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))
text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))
tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))

priority_data = np.random.random(size=(num_samples, 1))
department_data = np.random.randint(0, 2, size=(num_samples, num_departments))

model.compile(optimizer="rmsprop",
              loss=["mean_squared_error", "categorical_crossentropy"],
              metrics=[["mean_absolute_error"], ["accuracy"]])
model.fit([title_data, text_body_data, tags_data],
          [priority_data, department_data],
          epochs=1)
model.evaluate([title_data, text_body_data, tags_data],
               [priority_data, department_data])
priority_preds, department_preds = model.predict([title_data, text_body_data, tags_data])
40/19:
model.compile(optimizer="rmsprop",
              loss={"priority": "mean_squared_error", "department": "categorical_crossentropy"},
              metrics={"priority": ["mean_absolute_error"], "department": ["accuracy"]})
model.fit({"title": title_data, "text_body": text_body_data, "tags": tags_data},
          {"priority": priority_data, "department": department_data},
          epochs=1)
model.evaluate({"title": title_data, "text_body": text_body_data, "tags": tags_data},
               {"priority": priority_data, "department": department_data})
priority_preds, department_preds = model.predict(
    {"title": title_data, "text_body": text_body_data, "tags": tags_data})
40/20: keras.utils.plot_model(model, "ticket_classifier.png")
40/21: keras.utils.plot_model(model, "ticket_classifier_with_shape_info.png", show_shapes=True)
40/22: model.layers
40/23: model.layers[3].input
40/24: model.layers[3].output
40/25:
features = model.layers[4].output
difficulty = layers.Dense(3, activation="softmax", name="difficulty")(features)

new_model = keras.Model(
    inputs=[title, text_body, tags],
    outputs=[priority, department, difficulty])
40/26: keras.utils.plot_model(new_model, "updated_ticket_classifier.png", show_shapes=True)
40/27:
class CustomerTicketModel(keras.Model):

    def __init__(self, num_departments):
        super().__init__()
        self.concat_layer = layers.Concatenate()
        self.mixing_layer = layers.Dense(64, activation="relu")
        self.priority_scorer = layers.Dense(1, activation="sigmoid")
        self.department_classifier = layers.Dense(
            num_departments, activation="softmax")

    def call(self, inputs):
        title = inputs["title"]
        text_body = inputs["text_body"]
        tags = inputs["tags"]

        features = self.concat_layer([title, text_body, tags])
        features = self.mixing_layer(features)
        priority = self.priority_scorer(features)
        department = self.department_classifier(features)
        return priority, department
40/28:
model = CustomerTicketModel(num_departments=4)

priority, department = model(
    {"title": title_data, "text_body": text_body_data, "tags": tags_data})
40/29:
model.compile(optimizer="rmsprop",
              loss=["mean_squared_error", "categorical_crossentropy"],
              metrics=[["mean_absolute_error"], ["accuracy"]])
model.fit({"title": title_data,
           "text_body": text_body_data,
           "tags": tags_data},
          [priority_data, department_data],
          epochs=1)
model.evaluate({"title": title_data,
                "text_body": text_body_data,
                "tags": tags_data},
               [priority_data, department_data])
priority_preds, department_preds = model.predict({"title": title_data,
                                                  "text_body": text_body_data,
                                                  "tags": tags_data})
40/30:
class Classifier(keras.Model):

    def __init__(self, num_classes=2):
        super().__init__()
        if num_classes == 2:
            num_units = 1
            activation = "sigmoid"
        else:
            num_units = num_classes
            activation = "softmax"
        self.dense = layers.Dense(num_units, activation=activation)

    def call(self, inputs):
        return self.dense(inputs)

inputs = keras.Input(shape=(3,))
features = layers.Dense(64, activation="relu")(inputs)
outputs = Classifier(num_classes=10)(features)
model = keras.Model(inputs=inputs, outputs=outputs)
40/31:
inputs = keras.Input(shape=(64,))
outputs = layers.Dense(1, activation="sigmoid")(inputs)
binary_classifier = keras.Model(inputs=inputs, outputs=outputs)

class MyModel(keras.Model):

    def __init__(self, num_classes=2):
        super().__init__()
        self.dense = layers.Dense(64, activation="relu")
        self.classifier = binary_classifier

    def call(self, inputs):
        features = self.dense(inputs)
        return self.classifier(features)

model = MyModel()
40/32:
from tensorflow.keras.datasets import mnist

def get_mnist_model():
    inputs = keras.Input(shape=(28 * 28,))
    features = layers.Dense(512, activation="relu")(inputs)
    features = layers.Dropout(0.5)(features)
    outputs = layers.Dense(10, activation="softmax")(features)
    model = keras.Model(inputs, outputs)
    return model

(images, labels), (test_images, test_labels) = mnist.load_data()
images = images.reshape((60000, 28 * 28)).astype("float32") / 255
test_images = test_images.reshape((10000, 28 * 28)).astype("float32") / 255
train_images, val_images = images[10000:], images[:10000]
train_labels, val_labels = labels[10000:], labels[:10000]

model = get_mnist_model()
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
model.fit(train_images, train_labels,
          epochs=3,
          validation_data=(val_images, val_labels))
test_metrics = model.evaluate(test_images, test_labels)
predictions = model.predict(test_images)
40/33:
import tensorflow as tf

class RootMeanSquaredError(keras.metrics.Metric):

    def __init__(self, name="rmse", **kwargs):
        super().__init__(name=name, **kwargs)
        self.mse_sum = self.add_weight(name="mse_sum", initializer="zeros")
        self.total_samples = self.add_weight(
            name="total_samples", initializer="zeros", dtype="int32")

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.one_hot(y_true, depth=tf.shape(y_pred)[1])
        mse = tf.reduce_sum(tf.square(y_true - y_pred))
        self.mse_sum.assign_add(mse)
        num_samples = tf.shape(y_pred)[0]
        self.total_samples.assign_add(num_samples)

    def result(self):
        return tf.sqrt(self.mse_sum / tf.cast(self.total_samples, tf.float32))

    def reset_state(self):
        self.mse_sum.assign(0.)
        self.total_samples.assign(0)
40/34:
model = get_mnist_model()
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy", RootMeanSquaredError()])
model.fit(train_images, train_labels,
          epochs=3,
          validation_data=(val_images, val_labels))
test_metrics = model.evaluate(test_images, test_labels)
40/35:
callbacks_list = [
    keras.callbacks.EarlyStopping(
        monitor="val_accuracy",
        patience=2,
    ),
    keras.callbacks.ModelCheckpoint(
        filepath="checkpoint_path.keras",
        monitor="val_loss",
        save_best_only=True,
    )
]
model = get_mnist_model()
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
model.fit(train_images, train_labels,
          epochs=10,
          callbacks=callbacks_list,
          validation_data=(val_images, val_labels))
40/36: model = keras.models.load_model("checkpoint_path.keras")
40/37:
from matplotlib import pyplot as plt

class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs):
        self.per_batch_losses = []

    def on_batch_end(self, batch, logs):
        self.per_batch_losses.append(logs.get("loss"))

    def on_epoch_end(self, epoch, logs):
        plt.clf()
        plt.plot(range(len(self.per_batch_losses)), self.per_batch_losses,
                 label="Training loss for each batch")
        plt.xlabel(f"Batch (epoch {epoch})")
        plt.ylabel("Loss")
        plt.legend()
        plt.savefig(f"plot_at_epoch_{epoch}")
        self.per_batch_losses = []
40/38:
model = get_mnist_model()
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
model.fit(train_images, train_labels,
          epochs=10,
          callbacks=[LossHistory()],
          validation_data=(val_images, val_labels))
40/39:
model = get_mnist_model()
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

tensorboard = keras.callbacks.TensorBoard(
    log_dir="/full_path_to_your_log_dir",
)
model.fit(train_images, train_labels,
          epochs=10,
          validation_data=(val_images, val_labels),
          callbacks=[tensorboard])
40/40: title_data
40/41: text_body_data
40/42: text_body_data
40/43: tags_data
40/44: !conda install pydot
40/45: !conda install pydot -y
40/46: keras.utils.plot_model(model, "ticket_classifier.png")
40/47: !conda install -c conda-forge graphviz -y
40/48: keras.utils.plot_model(model, "ticket_classifier.png")
40/49: keras.utils.plot_model(model, "ticket_classifier.png")
41/1:
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(64, activation="relu"),
    layers.Dense(10, activation="softmax")
])
41/2:
model = keras.Sequential()
model.add(layers.Dense(64, activation="relu"))
model.add(layers.Dense(10, activation="softmax"))
41/3:
model.build(input_shape=(None, 3))
model.weights
41/4: model.summary()
41/5:
model = keras.Sequential(name="my_example_model")
model.add(layers.Dense(64, activation="relu", name="my_first_layer"))
model.add(layers.Dense(10, activation="softmax", name="my_last_layer"))
model.build((None, 3))
model.summary()
41/6:
model = keras.Sequential()
model.add(keras.Input(shape=(3,)))
model.add(layers.Dense(64, activation="relu"))
41/7: model.summary()
41/8:
model.add(layers.Dense(10, activation="softmax"))
model.summary()
41/9:
inputs = keras.Input(shape=(3,), name="my_input")
features = layers.Dense(64, activation="relu")(inputs)
outputs = layers.Dense(10, activation="softmax")(features)
model = keras.Model(inputs=inputs, outputs=outputs)
41/10: inputs = keras.Input(shape=(3,), name="my_input")
41/11: inputs.shape
41/12: inputs.dtype
41/13: features = layers.Dense(64, activation="relu")(inputs)
41/14: features.shape
41/15:
outputs = layers.Dense(10, activation="softmax")(features)
model = keras.Model(inputs=inputs, outputs=outputs)
41/16: model.summary()
41/17:
vocabulary_size = 10000
num_tags = 100
num_departments = 4

title = keras.Input(shape=(vocabulary_size,), name="title")
text_body = keras.Input(shape=(vocabulary_size,), name="text_body")
tags = keras.Input(shape=(num_tags,), name="tags")

features = layers.Concatenate()([title, text_body, tags])
features = layers.Dense(64, activation="relu")(features)

priority = layers.Dense(1, activation="sigmoid", name="priority")(features)
department = layers.Dense(
    num_departments, activation="softmax", name="department")(features)

model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])
41/18:
import numpy as np

num_samples = 1280

title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))
text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))
tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))

priority_data = np.random.random(size=(num_samples, 1))
department_data = np.random.randint(0, 2, size=(num_samples, num_departments))

model.compile(optimizer="rmsprop",
              loss=["mean_squared_error", "categorical_crossentropy"],
              metrics=[["mean_absolute_error"], ["accuracy"]])
model.fit([title_data, text_body_data, tags_data],
          [priority_data, department_data],
          epochs=1)
model.evaluate([title_data, text_body_data, tags_data],
               [priority_data, department_data])
priority_preds, department_preds = model.predict([title_data, text_body_data, tags_data])
41/19:
model.compile(optimizer="rmsprop",
              loss={"priority": "mean_squared_error", "department": "categorical_crossentropy"},
              metrics={"priority": ["mean_absolute_error"], "department": ["accuracy"]})
model.fit({"title": title_data, "text_body": text_body_data, "tags": tags_data},
          {"priority": priority_data, "department": department_data},
          epochs=1)
model.evaluate({"title": title_data, "text_body": text_body_data, "tags": tags_data},
               {"priority": priority_data, "department": department_data})
priority_preds, department_preds = model.predict(
    {"title": title_data, "text_body": text_body_data, "tags": tags_data})
41/20: keras.utils.plot_model(model, "ticket_classifier.png")
41/21: keras.utils.plot_model(model, "ticket_classifier_with_shape_info.png", show_shapes=True)
41/22: model.layers
41/23: model.layers[3].input
41/24: model.layers[3].output
41/25:
features = model.layers[4].output
difficulty = layers.Dense(3, activation="softmax", name="difficulty")(features)

new_model = keras.Model(
    inputs=[title, text_body, tags],
    outputs=[priority, department, difficulty])
41/26: keras.utils.plot_model(new_model, "updated_ticket_classifier.png", show_shapes=True)
41/27:
class CustomerTicketModel(keras.Model):

    def __init__(self, num_departments):
        super().__init__()
        self.concat_layer = layers.Concatenate()
        self.mixing_layer = layers.Dense(64, activation="relu")
        self.priority_scorer = layers.Dense(1, activation="sigmoid")
        self.department_classifier = layers.Dense(
            num_departments, activation="softmax")

    def call(self, inputs):
        title = inputs["title"]
        text_body = inputs["text_body"]
        tags = inputs["tags"]

        features = self.concat_layer([title, text_body, tags])
        features = self.mixing_layer(features)
        priority = self.priority_scorer(features)
        department = self.department_classifier(features)
        return priority, department
41/28:
model = CustomerTicketModel(num_departments=4)

priority, department = model(
    {"title": title_data, "text_body": text_body_data, "tags": tags_data})
41/29:
model.compile(optimizer="rmsprop",
              loss=["mean_squared_error", "categorical_crossentropy"],
              metrics=[["mean_absolute_error"], ["accuracy"]])
model.fit({"title": title_data,
           "text_body": text_body_data,
           "tags": tags_data},
          [priority_data, department_data],
          epochs=1)
model.evaluate({"title": title_data,
                "text_body": text_body_data,
                "tags": tags_data},
               [priority_data, department_data])
priority_preds, department_preds = model.predict({"title": title_data,
                                                  "text_body": text_body_data,
                                                  "tags": tags_data})
41/30:
class Classifier(keras.Model):

    def __init__(self, num_classes=2):
        super().__init__()
        if num_classes == 2:
            num_units = 1
            activation = "sigmoid"
        else:
            num_units = num_classes
            activation = "softmax"
        self.dense = layers.Dense(num_units, activation=activation)

    def call(self, inputs):
        return self.dense(inputs)

inputs = keras.Input(shape=(3,))
features = layers.Dense(64, activation="relu")(inputs)
outputs = Classifier(num_classes=10)(features)
model = keras.Model(inputs=inputs, outputs=outputs)
41/31:
inputs = keras.Input(shape=(64,))
outputs = layers.Dense(1, activation="sigmoid")(inputs)
binary_classifier = keras.Model(inputs=inputs, outputs=outputs)

class MyModel(keras.Model):

    def __init__(self, num_classes=2):
        super().__init__()
        self.dense = layers.Dense(64, activation="relu")
        self.classifier = binary_classifier

    def call(self, inputs):
        features = self.dense(inputs)
        return self.classifier(features)

model = MyModel()
41/32:
from tensorflow.keras.datasets import mnist

def get_mnist_model():
    inputs = keras.Input(shape=(28 * 28,))
    features = layers.Dense(512, activation="relu")(inputs)
    features = layers.Dropout(0.5)(features)
    outputs = layers.Dense(10, activation="softmax")(features)
    model = keras.Model(inputs, outputs)
    return model

(images, labels), (test_images, test_labels) = mnist.load_data()
images = images.reshape((60000, 28 * 28)).astype("float32") / 255
test_images = test_images.reshape((10000, 28 * 28)).astype("float32") / 255
train_images, val_images = images[10000:], images[:10000]
train_labels, val_labels = labels[10000:], labels[:10000]

model = get_mnist_model()
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
model.fit(train_images, train_labels,
          epochs=3,
          validation_data=(val_images, val_labels))
test_metrics = model.evaluate(test_images, test_labels)
predictions = model.predict(test_images)
41/33:
import tensorflow as tf

class RootMeanSquaredError(keras.metrics.Metric):

    def __init__(self, name="rmse", **kwargs):
        super().__init__(name=name, **kwargs)
        self.mse_sum = self.add_weight(name="mse_sum", initializer="zeros")
        self.total_samples = self.add_weight(
            name="total_samples", initializer="zeros", dtype="int32")

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.one_hot(y_true, depth=tf.shape(y_pred)[1])
        mse = tf.reduce_sum(tf.square(y_true - y_pred))
        self.mse_sum.assign_add(mse)
        num_samples = tf.shape(y_pred)[0]
        self.total_samples.assign_add(num_samples)

    def result(self):
        return tf.sqrt(self.mse_sum / tf.cast(self.total_samples, tf.float32))

    def reset_state(self):
        self.mse_sum.assign(0.)
        self.total_samples.assign(0)
41/34:
model = get_mnist_model()
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy", RootMeanSquaredError()])
model.fit(train_images, train_labels,
          epochs=3,
          validation_data=(val_images, val_labels))
test_metrics = model.evaluate(test_images, test_labels)
41/35:
callbacks_list = [
    keras.callbacks.EarlyStopping(
        monitor="val_accuracy",
        patience=2,
    ),
    keras.callbacks.ModelCheckpoint(
        filepath="checkpoint_path.keras",
        monitor="val_loss",
        save_best_only=True,
    )
]
model = get_mnist_model()
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
model.fit(train_images, train_labels,
          epochs=10,
          callbacks=callbacks_list,
          validation_data=(val_images, val_labels))
41/36: model = keras.models.load_model("checkpoint_path.keras")
41/37:
from matplotlib import pyplot as plt

class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs):
        self.per_batch_losses = []

    def on_batch_end(self, batch, logs):
        self.per_batch_losses.append(logs.get("loss"))

    def on_epoch_end(self, epoch, logs):
        plt.clf()
        plt.plot(range(len(self.per_batch_losses)), self.per_batch_losses,
                 label="Training loss for each batch")
        plt.xlabel(f"Batch (epoch {epoch})")
        plt.ylabel("Loss")
        plt.legend()
        plt.savefig(f"plot_at_epoch_{epoch}")
        self.per_batch_losses = []
41/38:
model = get_mnist_model()
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
model.fit(train_images, train_labels,
          epochs=10,
          callbacks=[LossHistory()],
          validation_data=(val_images, val_labels))
41/39:
model = get_mnist_model()
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

tensorboard = keras.callbacks.TensorBoard(
    log_dir="/full_path_to_your_log_dir",
)
model.fit(train_images, train_labels,
          epochs=10,
          validation_data=(val_images, val_labels),
          callbacks=[tensorboard])
42/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import datetime
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions")

example_dict = {}
42/2: from Visualization import JupyterNotebookMarkdown
42/3:
from Visualization import JupyterNotebookMarkdown


from Connections import GoogleProcessSheetLinks
42/4:
from Connections import DownloadFilesFromGit,BackUpGoogleSheets,ParamterMapping

example_dict['DownloadFilesFromGit']=1
example_dict['BackUpGoogleSheets']=1
DownloadFilesFromGit(output_folder='/Users/derekdewald/Documents/Python/Github_Repo/JupyterNotebooks/ArchivePyFunctions/')
BackUpGoogleSheets()
42/5:
from Visualization import JupyterNotebookMarkdown


from Connections import GoogleProcessSheetLinks
42/6:
from UtilityFunctions import InspectFunction

InspectFunction(GoogleProcessSheetLinks)
42/7:
df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSwDznLz-GKgWFT1uN0XZYm3bsos899I9MS-pSvEoDC-Cjqo9CWeEuSdjitxjqzF3O39LmjJB0_Fg-B/pub?output=csv')


df
42/8: df1  = GoogleProcessSheetLinks()
42/9: df1
42/10: df1[df1['Title']=='Building in TensorFlow']
42/11: df1['Title'].value_counts()
42/12: df1[df1['Title']=='Building In TensorFlow']
42/13: df2 = df1[df1['Title']=='Building In TensorFlow']
42/14: JupyterNotebookMarkdown(df2)
42/15: InspectFunction(JupyterNotebookMarkdown)
42/16:
from Visualization import JupyterNotebookMarkdown
from Connections import GoogleProcessSheetLinks
df1  = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSwDznLz-GKgWFT1uN0XZYm3bsos899I9MS-pSvEoDC-Cjqo9CWeEuSdjitxjqzF3O39LmjJB0_Fg-B/pub?output=csv')
df2 = df1[df1['Title']=='Building In TensorFlow']

JupyterNotebookMarkdown(df2)
42/17: df1
42/18:
from Visualization import JupyterNotebookMarkdown

df1  = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSQF2lNc4WPeTRQ_VzWPkqSZp4RODFkbap8AqmolWp5bKoMaslP2oRVVG21x2POu_JcbF1tGRcBgodu/pub?output=csv')
df1
42/19:
df2 = df1[df1['Title']=='Building In TensorFlow']
JupyterNotebookMarkdown(df2)
42/20: InspectFunction(JupyterNotebookMarkdown)
42/21:
from IPython.display import display, HTML

def JupyterNotebookMarkdown(df, return_value=""):
    '''
    Function to Create a Markdown file from Process DF, which is a data frame of the structure, 
    Title, Header, Description

    Args:
        df (DataFrame): Must include columns Title, Header, Description
        return_value (str): 
            If "", renders HTML in notebook.
            If text, returns HTML Markdown string.
    
    Returns:
        str or display: Based on return_value
    '''
    try:
        df1 = df[['Title', 'Header', 'Description']]
    except:
        print('DataFrame must include columns: Title, Header, Description')
        return ''

    text = ""
    step_number = 1
    last_title = None
    last_header = None
    open_l2 = False  # Track if L2 <ul> is open
    open_l3 = False  # Track if L3 <ul> is open

    for _, row in df1.iterrows():
        curr_title = row['Title']
        curr_header = row['Header']
        curr_description = row['Description']

        # If new Title
        if curr_title != last_title:
            if open_l3:
                text += "</ul>\n"
                open_l3 = False
            if open_l2:
                text += "</ul>\n"
                open_l2 = False
            if last_title is not None:
                text += "</ul>\n"  # Close previous title's outer <ul>

            text += f"<h4>{step_number}. {curr_title}</h4>\n<ul>\n"
            step_number += 1
            last_title = curr_title
            last_header = None  # Reset header context

        # If new Header
        if curr_header != last_header and isinstance(curr_header, str) and curr_header.strip():
            if open_l3:
                text += "</ul>\n"
                open_l3 = False
            if open_l2:
                text += "</ul>\n"
                open_l2 = False

            text += f"  <ul><li>{curr_header}</li>\n"
            open_l2 = True
            last_header = curr_header

        # If Description exists
        if isinstance(curr_description, str) and curr_description.strip():
            if not open_l3:
                text += "    <ul>\n"
                open_l3 = True
            text += f"      <li>{curr_description}</li>\n"

    # Close any open lists at the end
    if open_l3:
        text += "    </ul>\n"
    if open_l2:
        text += "  </ul>\n"
    text += "</ul>\n"

    if return_value == "":
        display(HTML(text))
    else:
        return text

df2 = df1[df1['Title']=='Building In TensorFlow']
JupyterNotebookMarkdown(df2)
42/22:
df1  = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSQF2lNc4WPeTRQ_VzWPkqSZp4RODFkbap8AqmolWp5bKoMaslP2oRVVG21x2POu_JcbF1tGRcBgodu/pub?output=csv')

df2 = df1[df1['Title']=='Building In TensorFlow']
JupyterNotebookMarkdown(df2)
42/23: JupyterNotebookMarkdown(df1)
42/24: JupyterNotebookMarkdown(df1[df1['Title'].notnull()])
42/25:

JupyterNotebookMarkdown(df1[df1['Title'].notnull()])
42/26:
from OtherFunctions import DataFrameFromProcess

#example_dict['DataFrameFromProcess']=1
DataFrameFromProcess('Machine Learning Project')
41/40:
import os
import datetime

# Create a safe, writable log directory inside your home directory
log_dir = os.path.join(os.getcwd(), "logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

tensorboard = keras.callbacks.TensorBoard(log_dir=log_dir)
41/41:
model = get_mnist_model()
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

tensorboard = keras.callbacks.TensorBoard(
    log_dir="/full_path_to_your_log_dir",
)
model.fit(train_images, train_labels,
          epochs=10,
          validation_data=(val_images, val_labels),
          callbacks=[tensorboard])
41/42: log_dir
41/43:
model = get_mnist_model()
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])

import os
import datetime

# Create a safe, writable log directory inside your home directory
log_dir = os.path.join(os.getcwd(), "logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

tensorboard = keras.callbacks.TensorBoard(log_dir=log_dir)


model.fit(train_images, train_labels,
          epochs=10,
          validation_data=(val_images, val_labels),
          callbacks=[tensorboard])
41/44:
%load_ext tensorboard
%tensorboard --logdir /full_path_to_your_log_dir
41/45:

%load_ext tensorboard
%tensorboard --logdir logs
41/46:

%load_ext tensorboard
%tensorboard --logdir {log_dir} --reload_interval=5
41/47: !lsof -i:6006
41/48: !kill -9 PID
41/49: !kill -9 56211
41/50:

%load_ext tensorboard
%tensorboard --logdir {log_dir} --reload_interval=5
42/27:
from Visualization import JupyterNotebookMarkdown

df1  = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSQF2lNc4WPeTRQ_VzWPkqSZp4RODFkbap8AqmolWp5bKoMaslP2oRVVG21x2POu_JcbF1tGRcBgodu/pub?output=csv')
df2 = df1[df1['Title']=='Building In TensorFlow']
JupyterNotebookMarkdown(df1[df1['Title'].notnull()])
42/28:
from Visualization import JupyterNotebookMarkdown

df1  = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSQF2lNc4WPeTRQ_VzWPkqSZp4RODFkbap8AqmolWp5bKoMaslP2oRVVG21x2POu_JcbF1tGRcBgodu/pub?output=csv')
df2 = df1[df1['Title']=='TensorFlow']
JupyterNotebookMarkdown(df2)
42/29: df2
42/30:
from Visualization import JupyterNotebookMarkdown

df1  = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSQF2lNc4WPeTRQ_VzWPkqSZp4RODFkbap8AqmolWp5bKoMaslP2oRVVG21x2POu_JcbF1tGRcBgodu/pub?output=csv')
df2 = df1[df1['Process']=='TensorFlow']
JupyterNotebookMarkdown(df2)
43/1:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")

from DFProcessing import CreatePivotTableFromTimeSeries
43/2:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
43/3: from Visualization import JupyterNotebookMarkdown
43/4:
df1  = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSQF2lNc4WPeTRQ_VzWPkqSZp4RODFkbap8AqmolWp5bKoMaslP2oRVVG21x2POu_JcbF1tGRcBgodu/pub?output=csv')
df2 = df1[df1['Process']=='Time Series']
JupyterNotebookMarkdown(df2)
43/5:
df1  = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSQF2lNc4WPeTRQ_VzWPkqSZp4RODFkbap8AqmolWp5bKoMaslP2oRVVG21x2POu_JcbF1tGRcBgodu/pub?output=csv')
df2 = df1[df1['Process']=='Time Series']
JupyterNotebookMarkdown(df2)
45/1:
!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip
!unzip jena_climate_2009_2016.csv.zip
45/2: !curl -O https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip
45/3: !unzip jena_climate_2009_2016.csv.zip
45/4:
import os
fname = os.path.join("jena_climate_2009_2016.csv")

with open(fname) as f:
    data = f.read()

lines = data.split("\n")
header = lines[0].split(",")
lines = lines[1:]
print(header)
print(len(lines))
45/5:
import numpy as np
temperature = np.zeros((len(lines),))
raw_data = np.zeros((len(lines), len(header) - 1))
for i, line in enumerate(lines):
    values = [float(x) for x in line.split(",")[1:]]
    temperature[i] = values[1]
    raw_data[i, :] = values[:]
45/6:
from matplotlib import pyplot as plt
plt.plot(range(len(temperature)), temperature)
45/7: plt.plot(range(1440), temperature[:1440])
45/8:
num_train_samples = int(0.5 * len(raw_data))
num_val_samples = int(0.25 * len(raw_data))
num_test_samples = len(raw_data) - num_train_samples - num_val_samples
print("num_train_samples:", num_train_samples)
print("num_val_samples:", num_val_samples)
print("num_test_samples:", num_test_samples)
45/9:
mean = raw_data[:num_train_samples].mean(axis=0)
raw_data -= mean
std = raw_data[:num_train_samples].std(axis=0)
raw_data /= std
45/10:
import numpy as np
from tensorflow import keras
int_sequence = np.arange(10)
dummy_dataset = keras.utils.timeseries_dataset_from_array(
    data=int_sequence[:-3],
    targets=int_sequence[3:],
    sequence_length=3,
    batch_size=2,
)

for inputs, targets in dummy_dataset:
    for i in range(inputs.shape[0]):
        print([int(x) for x in inputs[i]], int(targets[i]))
45/11:
sampling_rate = 6
sequence_length = 120
delay = sampling_rate * (sequence_length + 24 - 1)
batch_size = 256

train_dataset = keras.utils.timeseries_dataset_from_array(
    raw_data[:-delay],
    targets=temperature[delay:],
    sampling_rate=sampling_rate,
    sequence_length=sequence_length,
    shuffle=True,
    batch_size=batch_size,
    start_index=0,
    end_index=num_train_samples)

val_dataset = keras.utils.timeseries_dataset_from_array(
    raw_data[:-delay],
    targets=temperature[delay:],
    sampling_rate=sampling_rate,
    sequence_length=sequence_length,
    shuffle=True,
    batch_size=batch_size,
    start_index=num_train_samples,
    end_index=num_train_samples + num_val_samples)

test_dataset = keras.utils.timeseries_dataset_from_array(
    raw_data[:-delay],
    targets=temperature[delay:],
    sampling_rate=sampling_rate,
    sequence_length=sequence_length,
    shuffle=True,
    batch_size=batch_size,
    start_index=num_train_samples + num_val_samples)
45/12:
for samples, targets in train_dataset:
    print("samples shape:", samples.shape)
    print("targets shape:", targets.shape)
    break
45/13:
def evaluate_naive_method(dataset):
    total_abs_err = 0.
    samples_seen = 0
    for samples, targets in dataset:
        preds = samples[:, -1, 1] * std[1] + mean[1]
        total_abs_err += np.sum(np.abs(preds - targets))
        samples_seen += samples.shape[0]
    return total_abs_err / samples_seen

print(f"Validation MAE: {evaluate_naive_method(val_dataset):.2f}")
print(f"Test MAE: {evaluate_naive_method(test_dataset):.2f}")
45/14:
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.Flatten()(inputs)
x = layers.Dense(16, activation="relu")(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_dense.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=10,
                    validation_data=val_dataset,
                    callbacks=callbacks)

model = keras.models.load_model("jena_dense.keras")
print(f"Test MAE: {model.evaluate(test_dataset)[1]:.2f}")
45/15:
import matplotlib.pyplot as plt
loss = history.history["mae"]
val_loss = history.history["val_mae"]
epochs = range(1, len(loss) + 1)
plt.figure()
plt.plot(epochs, loss, "bo", label="Training MAE")
plt.plot(epochs, val_loss, "b", label="Validation MAE")
plt.title("Training and validation MAE")
plt.legend()
plt.show()
45/16:
inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.Conv1D(8, 24, activation="relu")(inputs)
x = layers.MaxPooling1D(2)(x)
x = layers.Conv1D(8, 12, activation="relu")(x)
x = layers.MaxPooling1D(2)(x)
x = layers.Conv1D(8, 6, activation="relu")(x)
x = layers.GlobalAveragePooling1D()(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_conv.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=10,
                    validation_data=val_dataset,
                    callbacks=callbacks)

model = keras.models.load_model("jena_conv.keras")
print(f"Test MAE: {model.evaluate(test_dataset)[1]:.2f}")
45/17:
inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.LSTM(16)(inputs)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_lstm.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=10,
                    validation_data=val_dataset,
                    callbacks=callbacks)

model = keras.models.load_model("jena_lstm.keras")
print(f"Test MAE: {model.evaluate(test_dataset)[1]:.2f}")
46/1:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
46/2:
from Visualization import JupyterNotebookMarkdown
df1  = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSQF2lNc4WPeTRQ_VzWPkqSZp4RODFkbap8AqmolWp5bKoMaslP2oRVVG21x2POu_JcbF1tGRcBgodu/pub?output=csv')
df2 = df1[df1['Process']=='Time Series']
JupyterNotebookMarkdown(df2)
45/18:
import numpy as np
timesteps = 100
input_features = 32
output_features = 64
inputs = np.random.random((timesteps, input_features))
state_t = np.zeros((output_features,))
W = np.random.random((output_features, input_features))
U = np.random.random((output_features, output_features))
b = np.random.random((output_features,))
successive_outputs = []
for input_t in inputs:
    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)
    successive_outputs.append(output_t)
    state_t = output_t
final_output_sequence = np.stack(successive_outputs, axis=0)
45/19:
num_features = 14
inputs = keras.Input(shape=(None, num_features))
outputs = layers.SimpleRNN(16)(inputs)
45/20:
num_features = 14
steps = 120
inputs = keras.Input(shape=(steps, num_features))
outputs = layers.SimpleRNN(16, return_sequences=False)(inputs)
print(outputs.shape)
45/21:
num_features = 14
steps = 120
inputs = keras.Input(shape=(steps, num_features))
outputs = layers.SimpleRNN(16, return_sequences=True)(inputs)
print(outputs.shape)
45/22:
inputs = keras.Input(shape=(steps, num_features))
x = layers.SimpleRNN(16, return_sequences=True)(inputs)
x = layers.SimpleRNN(16, return_sequences=True)(x)
outputs = layers.SimpleRNN(16)(x)
45/23:
inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.LSTM(32, recurrent_dropout=0.25)(inputs)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_lstm_dropout.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=50,
                    validation_data=val_dataset,
                    callbacks=callbacks)
45/24:
inputs = keras.Input(shape=(sequence_length, num_features))
x = layers.LSTM(32, recurrent_dropout=0.2, unroll=True)(inputs)
45/25:
inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)
x = layers.GRU(32, recurrent_dropout=0.5)(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_stacked_gru_dropout.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=50,
                    validation_data=val_dataset,
                    callbacks=callbacks)
model = keras.models.load_model("jena_stacked_gru_dropout.keras")
print(f"Test MAE: {model.evaluate(test_dataset)[1]:.2f}")
46/3:
from Visualization import JupyterNotebookMarkdown
df1  = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSQF2lNc4WPeTRQ_VzWPkqSZp4RODFkbap8AqmolWp5bKoMaslP2oRVVG21x2POu_JcbF1tGRcBgodu/pub?output=csv')
df2 = df1[df1['Process']=='Time Series']
JupyterNotebookMarkdown(df2)
46/4:
from Visualization import JupyterNotebookMarkdown
df1  = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSQF2lNc4WPeTRQ_VzWPkqSZp4RODFkbap8AqmolWp5bKoMaslP2oRVVG21x2POu_JcbF1tGRcBgodu/pub?output=csv')
df2 = df1[df1['Process']=='Time Series']
JupyterNotebookMarkdown(df1)
45/26:
inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.Bidirectional(layers.LSTM(16))(inputs)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=10,
                    validation_data=val_dataset)
47/1:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/2:
import pandas as pd
import numpy as np


df = pd.read_excel('VCPC.xlsx')
df
47/3:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/4:
import pandas as pd
import numpy as np


df = pd.read_excel('VCPC.xlsx',sheet_name='RenewalReplacement Reserve Fund'))
df
47/5:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/6:
import pandas as pd
import numpy as np


df = pd.read_excel('VCPC.xlsx',sheet_name='RenewalReplacement Reserve Fund')
df
47/7:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/8:
import pandas as pd
import numpy as np


df = pd.read_excel('VCPC_REPLACE_LOG.xlsx')
df
47/9:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/10:
import pandas as pd
import numpy as np


df = pd.read_excel('VCPC_REPLACE_LOG.xlsx')
df = df[df['CurrentValue'].notna()]
df
47/11:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/12:
import pandas as pd
import numpy as np


df = pd.read_excel('VCPC_REPLACE_LOG.xlsx')
df = df[df['CurrentValue'].notna()]


df.sort_values('CurrentValue')
47/13:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/14:
import pandas as pd
import numpy as np


df = pd.read_excel('VCPC_REPLACE_LOG.xlsx')
df = df[df['CurrentValue'].notna()]


df.sort_values('CurrentValue',ascending=False)
47/15:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/16:
import pandas as pd
import numpy as np


df = pd.read_excel('VCPC_REPLACE_LOG.xlsx')
df = df[df['CurrentValue'].notna()]


df.sort_values('CurrentValue',ascending=False).drop_duplicates('Item')
47/17:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/18:
import pandas as pd
import numpy as np


df = pd.read_excel('VCPC_REPLACE_LOG.xlsx')
df = df[df['CurrentValue'].notna()]
df = df.sort_values('CurrentValue',ascending=False).drop_duplicates('Item')
df
47/19:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/20:
import pandas as pd
import numpy as np


df = pd.read_excel('VCPC_REPLACE_LOG.xlsx')
df = df[df['CurrentValue'].notna()]
df = df.sort_values('CurrentValue',ascending=False).drop_duplicates('Item')


for index,rows in df.iterrows():
    print(index,rows)
47/21:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/22:
import pandas as pd
import numpy as np


df = pd.read_excel('VCPC_REPLACE_LOG.xlsx')
df = df[df['CurrentValue'].notna()]
df = df.sort_values('CurrentValue',ascending=False).drop_duplicates('Item')
df
47/23:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/24: 15/5
47/25:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/26: 15^2
47/27:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/28: 15^5
47/29:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/30: 15%5
47/31:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/32: rows
47/33:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/34: rows['ReplacementLife']
47/35:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/36:

YEARLY_DICT = {}
year = 2023
count = 1

for index,rows in df.iterrows():

    replace = rows['ReplacementLife']
    value = rows['CurrentValue']
    item = rows['Item']
    
    if count%replace==0:
        if count%replace:
            YEARLY_DICT[item:value]
47/37:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/38: pd.DataFrame(YEARLY_DICT.values())
47/39:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/40:

YEARLY_DICT = {}
year = 2023
count = 1

for index,rows in df.iterrows():

    replace = rows['ReplacementLife']
    value = rows['CurrentValue']
    item = rows['Item']
    
    if count%replace==0:
        if count%replace:
            YEARLY_DICT[item:value]
    count +=1
47/41:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/42: pd.DataFrame(YEARLY_DICT.values())
47/43:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/44:

YEARLY_DICT = {}
year = 2023
count = 1

for index,rows in df.iterrows():

    replace = rows['ReplacementLife']
    value = rows['CurrentValue']
    item = rows['Item']

    print(count/replace)
    
    if count%replace==0:
        if count%replace:
            YEARLY_DICT[item:value]
    count +=1
47/45:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/46:

YEARLY_DICT = {}
year = 2023
count = 1

for index,rows in df.iterrows():

    replace = rows['ReplacementLife']
    value = rows['CurrentValue']
    item = rows['Item']

    print(count/replace)
    
    if count%replace==1:
        if count%replace:
            YEARLY_DICT[item:value]
    count +=1
47/47:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/48: item
47/49:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/50: value
47/51:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/52:

YEARLY_DICT = {}
year = 2023
count = 1

for index,rows in df.iterrows():

    replace = rows['ReplacementLife']
    value = rows['CurrentValue']
    item = rows['Item']

    print(count/replace)
    
    if count%replace==1:
        if count%replace:
            YEARLY_DICT[item]:value
    count +=1
47/53:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/54: pd.DataFrame(YEARLY_DICT.values())
47/55:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/56:

YEARLY_DICT = {}
year = 2023
count = 1

for index,rows in df.iterrows():

    replace = rows['ReplacementLife']
    value = rows['CurrentValue']
    item = rows['Item']

    print(count/replace)
    
    if count%replace==1:
        if count%replace:
            YEARLY_DICT[item] = value
    count +=1
47/57:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/58: pd.DataFrame(YEARLY_DICT.values())
47/59:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/60:

YEARLY_DICT = {}
count = 1

for year in range(2023,2053):
    for index,rows in df.iterrows():
        replace = rows['ReplacementLife']
        value = rows['CurrentValue']
        item = rows['Item']
        if count%replace==1:
            YEARLY_DICT[year] = {item:value}
47/61:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/62: YEARLY_DICT
47/63:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/64: df
47/65:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/66: df.iterrows()
47/67:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/68:

YEARLY_DICT = {}
count = 1

for year in range(2023,2053):
    for index,rows in df.iterrows():
        replace = rows['ReplacementLife']
        value = rows['CurrentValue']
        item = rows['Item']
        if count%replace==1:
            YEARLY_DICT[year] = {item:value}
    count +=1
47/69:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/70: YEARLY_DICT
47/71:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/72:

YEARLY_DICT = {}
count = 1
record = 1
for year in range(2023,2053):
    for index,rows in df.iterrows():
        replace = rows['ReplacementLife']
        value = rows['CurrentValue']
        item = rows['Item']
        if count%replace==1:
            YEARLY_DICT[record] = {year:{item:value}}
            record +=1
    count +=1
47/73:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/74: YEARLY_DICT
47/75:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/76:

YEARLY_DICT = {}
count = 1
record = 1
for year in range(2023,2053):
    for index,rows in df.iterrows():
        replace = rows['ReplacementLife']
        value = rows['CurrentValue']
        item = rows['Item']
        if count%replace==1:
            YEARLY_DICT[record] = {year:{item:value}}
            record +=1
    input()
    
    count +=1
47/77:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/78: count
47/79:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/80: replace
47/81:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/82: count%replace
47/83:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/84: 15%replace
47/85:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/86: count
47/87:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/88: replace
47/89:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/90: 10/replace
47/91:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/92: 10%replace
47/93:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/94:

YEARLY_DICT = {}
count = 1
record = 1
for year in range(2023,2053):
    for index,rows in df.iterrows():
        replace = rows['ReplacementLife']
        value = rows['CurrentValue']
        item = rows['Item']
        if count%replace==0:
            YEARLY_DICT[record] = {year:{item:value}}
            record +=1
    input()
    
    count +=1
47/95:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/96:

YEARLY_DICT = {}
count = 1
record = 1
for year in range(2023,2053):
    for index,rows in df.iterrows():
        replace = rows['ReplacementLife']
        value = rows['CurrentValue']
        item = rows['Item']
        if count%replace==0:
            YEARLY_DICT[record] = {year:{item:value}}
            record +=1    
    count +=1
47/97:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/98: YEARLY_DICT
47/99:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/100: pd.DataFrame(YEARLY_DICT)
47/101:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/102: YEARLY_DICT
47/103:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/104:
data = []
for record in YEARLY_DICT.values():
    for year, item_dict in record.items():
        for item, value in item_dict.items():
            data.append({'year': year, 'item': item, 'value': value})

# Convert to DataFrame
df_flat = pd.DataFrame(data)

# Optional: sort by year
df_flat = df_flat.sort_values('year').reset_index(drop=True)

df_flat
47/105:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/106:
data = []
for record in YEARLY_DICT.values():
    for year, item_dict in record.items():
        for item, value in item_dict.items():
            data.append({'year': year, 'item': item, 'value': value})

# Convert to DataFrame
df_flat = pd.DataFrame(data)

# Optional: sort by year
df_flat = df_flat.sort_values('year').reset_index(drop=True)

df_flat.groupby(['year']).sum()
47/107:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/108:
data = []
for record in YEARLY_DICT.values():
    for year, item_dict in record.items():
        for item, value in item_dict.items():
            data.append({'year': year, 'item': item, 'value': value})

# Convert to DataFrame
df_flat = pd.DataFrame(data)

# Optional: sort by year
df_flat = df_flat.sort_values('year').reset_index(drop=True)

df_flat
47/109:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/110: df_flat.groupby('year').sum()
47/111:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/112:  df_flat.groupby('year')['value'].sum()
47/113:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/114: df_flat[df_flat['year']==2024]
47/115:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/116: df_flat[df_flat['year']==2025]
47/117:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/118: df_flat[df_flat['year']==2026]
47/119:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/120: df_flat[df_flat['year']==2027]
47/121:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/122: df_flat[df_flat['year']==2032]
47/123:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/124:  pd.DataFrame(df_flat.groupby('year')['value'].sum())
47/125:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/126:
data = []
for record in YEARLY_DICT.values():
    for year, item_dict in record.items():
        for item, value in item_dict.items():
            data.append({'year': year, 'item': item, 'value': value})

# Convert to DataFrame
df_flat = pd.DataFrame(data)

# Optional: sort by year
df_flat = df_flat.sort_values('year').reset_index(drop=True)

df_flat.to_excel('YEARLY_EXPECTED_REPLACEMENT_COSTS.xlsx')
47/127:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/128:  pd.DataFrame(df_flat.groupby('year')['value'].sum()).to_excel("TOTAL_ANNUAL_COST.xlsx")
47/129:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/130:
data = []
for record in YEARLY_DICT.values():
    for year, item_dict in record.items():
        for item, value in item_dict.items():
            data.append({'year': year, 'item': item, 'value': value})

# Convert to DataFrame
df_flat = pd.DataFrame(data)

# Optional: sort by year
df_flat = df_flat.sort_values('year').reset_index(drop=True)

df_flat.to_excel('YEARLY_EXPECTED_REPLACEMENT_COSTS.xlsx',index=False)
47/131:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/132:  pd.DataFrame(df_flat.groupby('year')['value'].sum()).to_excel("TOTAL_ANNUAL_COST.xlsx",index=False)
47/133:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/134:  pd.DataFrame(df_flat.groupby('year')['value'].sum()).to_excel("TOTAL_ANNUAL_COST.xlsx",index=False)
47/135:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/136:
data = []
for record in YEARLY_DICT.values():
    for year, item_dict in record.items():
        for item, value in item_dict.items():
            data.append({'year': year, 'item': item, 'value': value})

# Convert to DataFrame
df_flat = pd.DataFrame(data)

# Optional: sort by year
df_flat = df_flat.sort_values('year').reset_index(drop=True)

df_flat.to_excel('YEARLY_EXPECTED_REPLACEMENT_COSTS.xlsx',index=False)
47/137:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
47/138:  pd.DataFrame(df_flat.groupby('year')['value'].sum()).to_excel("TOTAL_ANNUAL_COST.xlsx")
47/139:

import json
import getpass
import hashlib

def import_pandas_safely():
    try:
        return __import__('pandas')
    except ImportError:
        return False


__pandas = import_pandas_safely()


def is_data_frame(v: str):
    obj = eval(v)
    if  isinstance(obj, __pandas.core.frame.DataFrame) or isinstance(obj, __pandas.core.series.Series):
        return True


def dataframe_columns(var):
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return [[df.name, str(df.dtype)]]
    return list(map(lambda col: [col, str(df[col].dtype)], df.columns))


def dtypes_str(frame):
    return str(eval(frame).dtypes)

def dataframe_hash(var):
    # Return a hash including the column names and number of rows
    df = eval(var)
    if isinstance(df, __pandas.core.series.Series):
        return hashlib.sha256(f"{var}-{df.name},{len(df)}".encode('utf-8')).hexdigest()
    return hashlib.sha256(f"{var}-{','.join(df.columns)},{len(df)}".encode('utf-8')).hexdigest()

def get_dataframes():
    if __pandas is None:
        return []
    user = getpass.getuser()
    values = %who_ls
    dataframes = [
        {
            "name": var,
            "type": type(eval(var)).__name__,
            "hash": dataframe_hash(var),
            "cols": dataframe_columns(var),
            "dtypesStr": dtypes_str(var),
        }
        for var in values if is_data_frame(var)
    ]
    result = {"dataframes": dataframes, "user": user}
    return json.dumps(result, ensure_ascii=False)


get_dataframes()
48/1:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    # Ensure the most recent full month-end is used
    most_recent_month_end = pd.Timestamp.today().replace(day=1) - pd.Timedelta(days=1)

    month_list = pd.date_range(end=most_recent_month_end, periods=final_months, freq='M')

    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]

    if sort_ascending == 0:
        month_list = month_list[::-1]

    return list(month_list[month_int:final_months])

CreateMonthList(
48/2:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    # Ensure the most recent full month-end is used
    most_recent_month_end = pd.Timestamp.today().replace(day=1) - pd.Timedelta(days=1)

    month_list = pd.date_range(end=most_recent_month_end, periods=final_months, freq='M')

    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]

    if sort_ascending == 0:
        month_list = month_list[::-1]

    return list(month_list[month_int:final_months])

CreateMonthList()
48/3:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
48/4:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    # Ensure the most recent full month-end is used
    most_recent_month_end = pd.Timestamp.today().replace(day=1) - pd.Timedelta(days=1)

    month_list = pd.date_range(end=most_recent_month_end, periods=final_months, freq='M')

    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]

    if sort_ascending == 0:
        month_list = month_list[::-1]

    return list(month_list[month_int:final_months])

CreateMonthList()
48/5:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')

    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]

    if sort_ascending == 0:
        month_list = month_list[::-1]

    return list(month_list[month_int:final_months])

CreateMonthList()
48/6:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    print(month_list)
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]

    if sort_ascending == 0:
        month_list = month_list[::-1]

    return list(month_list[month_int:final_months])

CreateMonthList()
48/7:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    
    return month_list
    
    print(month_list)
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]

    if sort_ascending == 0:
        month_list = month_list[::-1]

    return list(month_list[month_int:final_months])

CreateMonthList()
48/8:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

        
    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    
    if month_int ==0:
        month_list.insert(pd.Timestamp.today())
    return month_list
    
    print(month_list)
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]

    if sort_ascending == 0:
        month_list = month_list[::-1]

    return list(month_list[month_int:final_months])

CreateMonthList()
48/9:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    month_list = list(month_list)
    
    if month_int ==0:
        month_list.insert(pd.Timestamp.today())
    return month_list
    
    print(month_list)
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]

    if sort_ascending == 0:
        month_list = month_list[::-1]

    return list(month_list[month_int:final_months])

CreateMonthList()
48/10:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    month_list = list(month_list)
    
    if month_int ==0:
        month_list.insert(0,pd.Timestamp.today())
    return month_list
    
    print(month_list)
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]

    if sort_ascending == 0:
        month_list = month_list[::-1]

    return list(month_list[month_int:final_months])

CreateMonthList()
48/11:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    month_list = list(month_list)
    
    if month_int ==0:
        month_list.insert(-1,pd.Timestamp.today())
    return month_list
    
    print(month_list)
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]

    if sort_ascending == 0:
        month_list = month_list[::-1]

    return list(month_list[month_int:final_months])

CreateMonthList()
48/12:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    month_list = list(month_list)
    
    if month_int ==0:
        month_list.append(pd.Timestamp.today()
    return month_list
    
    print(month_list)
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]

    if sort_ascending == 0:
        month_list = month_list[::-1]

    return list(month_list[month_int:final_months])

CreateMonthList()
48/13:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    month_list = list(month_list)
    
    if month_int ==0:
        month_list.append(pd.Timestamp.today())
    return month_list
    
    print(month_list)
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]

    if sort_ascending == 0:
        month_list = month_list[::-1]

    return list(month_list[month_int:final_months])

CreateMonthList()
48/14:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    month_list = list(month_list)
    
    if month_int ==0:
        month_list.append(pd.Timestamp.today())
    
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]

    if sort_ascending == 0:
        month_list.reverse()

    return month_list[month_int:final_months]

CreateMonthList()
48/15:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    month_list = list(month_list)
    
    if month_int ==0:
        month_list.append(pd.Timestamp.today())
    
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]
    elif return_value == 'date':
        month_list = [x.to_pydatetime().date() for x in month_list]

    if sort_ascending == 0:
        month_list.reverse()

    return month_list[month_int:final_months]

CreateMonthList()
48/16:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    month_list = list(month_list)
    
    if month_int ==0:
        month_list.append(pd.Timestamp.today())
    
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime() for x in month_list]
    elif return_value == 'date':
        month_list = [x.to_pydatetime().date() for x in month_list]

    if sort_ascending == 0:
        month_list.reverse()

    return month_list[month_int:final_months]

CreateMonthList(return_value='date')
48/17:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    month_list = list(month_list)
    
    if month_int ==0:
        month_list.append(pd.Timestamp.today())
    
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime().replace(hour=0,minute=0,seconds=0,microsecond=0) for x in month_list]
    elif return_value == 'date':
        month_list = [x.to_pydatetime().date() for x in month_list]

    if sort_ascending == 0:
        month_list.reverse()

    return month_list[month_int:final_months]

CreateMonthList(return_value='date')
48/18:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    month_list = list(month_list)
    
    if month_int ==0:
        month_list.append(pd.Timestamp.today())
    
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime().replace(hour=0,minute=0,seconds=0,microsecond=0) for x in month_list]
    elif return_value == 'date':
        month_list = [x.to_pydatetime().date() for x in month_list]

    if sort_ascending == 0:
        month_list.reverse()

    return month_list[month_int:final_months]

CreateMonthList(return_value='month_dt')
48/19:
def CreateMonthList(month_int=0,
                    months=36,
                    end_date_yesterday=1,
                    sort_ascending=0,
                    return_value="month_dt"):

    final_months = month_int + months

    month_list = pd.date_range(end=pd.Timestamp.today(), periods=final_months, freq='M')
    month_list = list(month_list)
    
    if month_int ==0:
        month_list.append(pd.Timestamp.today())
    
    if return_value == 'month_str':
        month_list = [x.strftime('%b-%y') for x in month_list]
    elif return_value == 'month':
        month_list = [x.strftime('%d-%b-%y') for x in month_list]
    elif return_value == 'month_dt':
        month_list = [x.to_pydatetime().replace(hour=0,minute=0,second=0,microsecond=0) for x in month_list]
    elif return_value == 'date':
        month_list = [x.to_pydatetime().date() for x in month_list]

    if sort_ascending == 0:
        month_list.reverse()

    return month_list[month_int:final_months]

CreateMonthList(return_value='month_dt')
48/20:
def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):
    
    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 
    
    Args:
        column_name (str): Name of Column
        
        partitions (int): Number of partitions to include (Decile 10)
        
        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.
        
        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 
        
        
    '''
    
    temp_dict = {}
    
    try:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
    
    except:
        pass

    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
    temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
                             
    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])
    
    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts().head(top_x_records)).reset_index()
        top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
        top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
        top_instances = top_instances.drop('count',axis=1).set_index('index')

        temp_df = pd.concat([temp_df,top_instances])
    
    try:
        temp_dict['SUM']
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})
            
        return pd.concat([temp_df,segment_df.T,seg_val_df])

    
    except:
        return temp_df
48/21:
def ColumnStatisticalReview1(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])
    
    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df
    
    try:

        # Add top X records Based on Frequency
        if top_x_records>0:
            top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index().rename(columns={column_name:'count','index':column_name})
            if len(top_instances)>0:
                top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
                top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
                top_instances = top_instances.drop('count',axis=1).set_index('index')
                temp_df = pd.concat([temp_df,top_instances])

        if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
            segment_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='')
            seg_val_df = ColumnPartitioner(df=df,
                                               column_name=column_name,
                                               partitions=partitions,
                                               exclude_blanks=exclude_blanks_from_segments,
                                               exclude_zeros=exclude_zeroes_from_segments,
                                               return_value='agg_value').rename(columns={'VALUE':column_name})
            return pd.concat([temp_df,segment_df.T,seg_val_df])
    except:
        pass
            
    return temp_df
48/22:
import inspect
import difflib

def compare_functions(func1, func2):
    f1_lines = inspect.getsource(func1).splitlines()
    f2_lines = inspect.getsource(func2).splitlines()
    diff = difflib.unified_diff(f1_lines, f2_lines, lineterm='')
    return '\n'.join(diff)



compare_functions(ColumnStatisticalReview,ColumnStatisticalReview1)
50/1:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
50/2:
from DataSets import GenerateFakeMemberDF

GenerateFakeMemberDF
50/3:
from DataSets import GenerateFakeMemberDF

GenerateFakeMemberDF,1000,2)
50/4:
from DataSets import GenerateFakeMemberDF

GenerateFakeMemberDF(1000,2)
50/5: from FeatureEngineering import CreateRandomDFColumn
52/1:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
52/2: from FeatureEngineering import CreateRandomDFColumn
52/3:
from DataSets import GenerateFakeMemberDF

GenerateFakeMemberDF(1000,2)
52/4:
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(1000,2)
52/5:
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(1000,2)
df
52/6:
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(1000,2)
df.head()
52/7:
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city = ['Burnaby','Vancouver','Kelowna','Whistler']
52/8:
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city_list = ['Burnaby','Vancouver','Kelowna','Whistler']
CreateRandomDFColumn(df,branch_list,'BRANCHNAME')
CreateRandomDFColumn(df,city_list,'CITY')
df
52/9: df['MONTH'].value_counts(
52/10: df['MONTH'].value_counts()
52/11:
df0 = df[df['MONTH']==0].copy()
df1 = df[df['MONTH']==0].copy()
52/12: df2 = df0.merge(df1,on='MEMBENBR',how='outer')
52/13: df2 = df0.merge(df1,on='MEMBERNBR',how='outer')
52/14:
df2 = df0.merge(df1,on='MEMBERNBR',how='outer')
df2.head(2)
52/15:
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(1000,2)
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city_list = ['Burnaby','Vancouver','Kelowna','Whistler']
df
52/16: df1.rename(columns={x:x for x in df1.columns}
52/17: df1.rename(columns={x:x for x in df1.columns})
52/18: df1.rename(columns={x:f'{x}_' for x in df1.columns})
52/19: df1.rename(columns={x:f'{x}_' for x in df1.columns if x not in ['MEMBERNBR'})
52/20: df1.rename(columns={x:f'{x}_' for x in df1.columns if x not in ['MEMBERNBR']})
52/21:
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(1000,2)
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city_list = ['Burnaby','Vancouver','Kelowna','Whistler']
df
52/22:
df0 = df[df['MONTH']==0].copy()
df1 = df[df['MONTH']==0].copy()
52/23: df1.rename(columns={x:f'{x}_' for x in df1.columns if x not in ['MEMBERNBR']})
52/24:
df0 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
52/25: df1.rename(columns={x:f'{x}_' for x in df1.columns if x not in ['MEMBERNBR']})
52/26:
df2 = df0.merge(df1,on='MEMBERNBR',how='outer')
df2.head(2)
52/27: df1.rename(columns={x:f'{x}_' for x in df1.columns if x not in ['MEMBERNBR']},inplace=True)
52/28:
df2 = df0.merge(df1,on='MEMBERNBR',how='outer')
df2.head(2)
52/29:

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.
    account_df = df.copy()

    if is_numeric_dtype(account_df[col]):
        account_df[col] = np.where(account_df[col] == "", 0, account_df[col])
        account_df[col1] = np.where(account_df[col1] == "", 0, account_df[col1])
    else:
        account_df[col] = np.where(account_df[col] == "", None, account_df[col])
        account_df[col1] = np.where(account_df[col1] == "", None, account_df[col1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
        
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df


RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_','MEMBERNBR')
52/30:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
from pandas.api.types import is_numeric_dtype
52/31:

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.
    account_df = df.copy()

    if is_numeric_dtype(account_df[col]):
        account_df[col] = np.where(account_df[col] == "", 0, account_df[col])
        account_df[col1] = np.where(account_df[col1] == "", 0, account_df[col1])
    else:
        account_df[col] = np.where(account_df[col] == "", None, account_df[col])
        account_df[col1] = np.where(account_df[col1] == "", None, account_df[col1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
        
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df


RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_','MEMBERNBR')
52/32:

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.
    account_df = df.copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
        
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df


RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_','MEMBERNBR')
   1:
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
from pandas.api.types import is_numeric_dtype
   2: from FeatureEngineering import CreateRandomDFColumn,BinaryComplexEquivlancey
   3:
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(1000,2)
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city_list = ['Burnaby','Vancouver','Kelowna','Whistler']
df
   4:
df0 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1.rename(columns={x:f'{x}_' for x in df1.columns if x not in ['MEMBERNBR']},inplace=True)
   5:
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(1000,2)
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city_list = ['Burnaby','Vancouver','Kelowna','Whistler']

df0 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1.rename(columns={x:f'{x}_' for x in df1.columns if x not in ['MEMBERNBR']},inplace=True)

df2 = df0.merge(df1,on='MEMBERNBR',how='outer')
df2.head(2)
   6:

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.
    account_df = df.copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
        
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_','MEMBERNBR')
   7:

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.
    account_df = df.copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1

    return account_df
    
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_','MEMBERNBR')
   8:

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.
    account_df = df.copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_',['MEMBERNBR'])
   9:

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.
    account_df = df.copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_',['MEMBERNBR'])
  10: account_df
  11: summary_df
  12: groupby_df
  13:

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_',['MEMBERNBR'])
  14: account_df
  15: summary_df
  16: groupby_df
  17: %history -g -f text.txt
