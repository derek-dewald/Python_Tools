import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
from pandas.api.types import is_numeric_dtype
from FeatureEngineering import CreateRandomDFColumn,BinaryComplexEquivlancey
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(1000,2)
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city_list = ['Burnaby','Vancouver','Kelowna','Whistler']
df
df0 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1.rename(columns={x:f'{x}_' for x in df1.columns if x not in ['MEMBERNBR']},inplace=True)
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(1000,2)
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city_list = ['Burnaby','Vancouver','Kelowna','Whistler']

df0 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1.rename(columns={x:f'{x}_' for x in df1.columns if x not in ['MEMBERNBR']},inplace=True)

df2 = df0.merge(df1,on='MEMBERNBR',how='outer')
df2.head(2)

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.
    account_df = df.copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
        
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_','MEMBERNBR')

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.
    account_df = df.copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1

    return account_df
    
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_','MEMBERNBR')

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.
    account_df = df.copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_',['MEMBERNBR'])

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.
    account_df = df.copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_',['MEMBERNBR'])
account_df
summary_df
groupby_df

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_',['MEMBERNBR'])
account_df
summary_df
groupby_df
%history -g -f text.txt
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city = ['Burnaby','Vancouver','Kelowna','Whistler']

CreateRandomDFColumn(df2,branch_list,'BRANCHNAME')
CreateRandomDFColumn(df2,city_list,'CITY')
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(1000,2)
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city_list = ['Burnaby','Vancouver','Kelowna','Whistler']

df0 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1.rename(columns={x:f'{x}_' for x in df1.columns if x not in ['MEMBERNBR']},inplace=True)

df2 = df0.merge(df1,on='MEMBERNBR',how='outer')
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city = ['Burnaby','Vancouver','Kelowna','Whistler']

CreateRandomDFColumn(df2,branch_list,'BRANCHNAME')
CreateRandomDFColumn(df2,city_list,'CITY')

df2.head(2)

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_',['MEMBERNBR','CITY'])

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_',['MEMBERNBR','CITY'])

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,'CLASSIFICATION','CLASSIFICATION_',['MEMBERNBR','CITY'])
account_df
summary_df
account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']]
account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,'LENDING','LENDING_',['MEMBERNBR','CITY'])
account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()
summary_df
groupby_df
df
df[df['MEMBERNBR']==34]
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(1000,2)
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city_list = ['Burnaby','Vancouver','Kelowna','Whistler']

df0 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1 = df[df['MONTH']==1].drop('MONTH',axis=1).copy()
df1.rename(columns={x:f'{x}_' for x in df1.columns if x not in ['MEMBERNBR']},inplace=True)

df2 = df0.merge(df1,on='MEMBERNBR',how='outer')
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city = ['Burnaby','Vancouver','Kelowna','Whistler']

CreateRandomDFColumn(df2,branch_list,'BRANCHNAME')
CreateRandomDFColumn(df2,city_list,'CITY')

df2.head(2)
df[df['MEMBERNBR']==34]

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = primary_key + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(primary_key,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,'LENDING','LENDING_',['MEMBERNBR','CITY'])
account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()
summary_df
groupby_df

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(summary_columns,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(primary_key,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(summary_columns,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = primary_key + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(summary_columns,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])
account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()
summary_df
summary_df
account_df
summary_df
summary_df

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(summary_columns,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    summary_df['COLUMN_NAME'] = primary_key
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000]):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(summary_columns,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    summary_df['COLUMN_NAME'] = column_name
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])
account_df
summary_df
groupby_df

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000],
                         groupby={'top':10,'minimum':5):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(summary_columns,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    summary_df['COLUMN_NAME'] = column_name
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1
    
    output_dict = {}
    
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000],
                         groupby_filter={'top':10,'minimum':5):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(summary_columns,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    summary_df['COLUMN_NAME'] = column_name
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1

    groupby_df = groupby_df[(groupby_df['CUM']<= groupby_filter['top'])&(groupby_df['RECORD_COUNT']>= groupby_filter['minimum'])]
        
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000],
                         groupby_filter={'top':10,'minimum':5}:
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(summary_columns,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    summary_df['COLUMN_NAME'] = column_name
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1

    groupby_df = groupby_df[(groupby_df['CUM']<= groupby_filter['top'])&(groupby_df['RECORD_COUNT']>= groupby_filter['minimum'])]
        
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000],
                         groupby_filter={'top':10,'minimum':5}:
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(summary_columns,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    summary_df['COLUMN_NAME'] = column_name
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1

    groupby_df = groupby_df[(groupby_df['CUM']<= groupby_filter['top'])&(groupby_df['RECORD_COUNT']>= groupby_filter['minimum'])]
        
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000],
                         groupby_filter={'top':10,'minimum':5}):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(summary_columns,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    summary_df['COLUMN_NAME'] = column_name
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1

    groupby_df = groupby_df[(groupby_df['CUM']<= groupby_filter['top'])&(groupby_df['RECORD_COUNT']>= groupby_filter['minimum'])]
        
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])
account_df
summary_df
groupby_df
account_df1
account_df
account_df[account_df['DF1']==100000]
summary_df
from Validation import ColumnStatisticalReview
from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'DF')
from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'LENDING')
df2
from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'LENDING')

def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])
    
    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df

    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index().rename(columns={column_name:'count','index':column_name})
        if len(top_instances)>0:
            top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
            top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
            top_instances = top_instances.drop('count',axis=1).set_index('index')
            temp_df = pd.concat([temp_df,top_instances])
        
    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})

        return pd.concat([temp_df,segment_df.T,seg_val_df])
    return temp_df

def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])

    return temp_df
    
    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df

    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index().rename(columns={column_name:'count','index':column_name})
        if len(top_instances)>0:
            top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
            top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
            top_instances = top_instances.drop('count',axis=1).set_index('index')
            temp_df = pd.concat([temp_df,top_instances])
        
    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})

        return pd.concat([temp_df,segment_df.T,seg_val_df])
    return temp_df
#from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'LENDING')
import pandas as pd
import numpy as np
import sys
sys.path.append("/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/")
from pandas.api.types import is_numeric_dtype

pd.options.display.float_format = '{:.2f}'.format
#from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'LENDING')

def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])

    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df


    return temp_df
    
    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index().rename(columns={column_name:'count','index':column_name})
        if len(top_instances)>0:
            top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
            top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
            top_instances = top_instances.drop('count',axis=1).set_index('index')
            temp_df = pd.concat([temp_df,top_instances])
        
    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})

        return pd.concat([temp_df,segment_df.T,seg_val_df])
    return temp_df
#from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'LENDING')

def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])

    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df   
    
    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index().rename(columns={column_name:'count','index':column_name})
        return top_instances
        if len(top_instances)>0:
            top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
            top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
            top_instances = top_instances.drop('count',axis=1).set_index('index')
            temp_df = pd.concat([temp_df,top_instances])
        
    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})

        return pd.concat([temp_df,segment_df.T,seg_val_df])
    return temp_df
#from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'LENDING')

def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])

    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df   
    
    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index().rename(columns={column_name:'count','index':column_name})
        
        if len(top_instances)>0:
            top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
            top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
            top_instances = top_instances.drop('count',axis=1).set_index('index')
            return top_instances
            temp_df = pd.concat([temp_df,top_instances])
            
        
    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})

        return pd.concat([temp_df,segment_df.T,seg_val_df])
    return temp_df
#from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'LENDING')

def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])

    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df   
    
    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index().rename(columns={column_name:'count','index':column_name})
        return top_instances
        if len(top_instances)>0:
            top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
            top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
            top_instances = top_instances.drop('count',axis=1).set_index('index')
            temp_df = pd.concat([temp_df,top_instances])
        
    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})

        return pd.concat([temp_df,segment_df.T,seg_val_df])
    return temp_df
#from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'LENDING')

def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])

    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df   
    
    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index()#.rename(columns={column_name:'count','index':column_name})
        return top_instances
        if len(top_instances)>0:
            top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
            top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
            top_instances = top_instances.drop('count',axis=1).set_index('index')
            temp_df = pd.concat([temp_df,top_instances])
        
    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})

        return pd.concat([temp_df,segment_df.T,seg_val_df])
    return temp_df
#from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'LENDING')

def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])

    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df   
    
    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index()#.rename(columns={column_name:'count','index':column_name})
        
        if len(top_instances)>0:
            top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
            top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
            top_instances = top_instances.drop('count',axis=1).set_index('index')
            return top_instances
            temp_df = pd.concat([temp_df,top_instances])
        
    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})

        return pd.concat([temp_df,segment_df.T,seg_val_df])
    return temp_df
#from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'LENDING')

def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])

    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df   
    
    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index()#.rename(columns={column_name:'count','index':column_name})
        
        if len(top_instances)>0:
            top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
            top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
            top_instances = top_instances.drop('count',axis=1).set_index('index')
            temp_df = pd.concat([temp_df,top_instances])
            return temp_df
        
    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})

        return pd.concat([temp_df,segment_df.T,seg_val_df])
    return temp_df
#from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'LENDING')

def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])

    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df   
    
    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index()
        
        if len(top_instances)>0:
            top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
            top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
            top_instances = top_instances.drop('count',axis=1).set_index('index')
            temp_df = pd.concat([temp_df,top_instances])
        
    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})

        return pd.concat([temp_df,segment_df.T,seg_val_df])
    return temp_df
#from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'LENDING')
from FeatureEngineering import CreateRandomDFColumn,BinaryComplexEquivlancey
from DFProcessing import ColumnPartitioner

def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 


    '''

    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])

    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df   
    
    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index()
        
        if len(top_instances)>0:
            top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
            top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
            top_instances = top_instances.drop('count',axis=1).set_index('index')
            temp_df = pd.concat([temp_df,top_instances])
        
    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})

        return pd.concat([temp_df,segment_df.T,seg_val_df])
    return temp_df
#from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'LENDING')
df2
df2.groupby(['BRANCHNAME','CITY']])
df2.groupby(['BRANCHNAME','CITY']]=)
df2.groupby(['BRANCHNAME','CITY'])
df2.groupby(['BRANCHNAME','CITY']).count()
df2.groupby(['BRANCHNAME','CITY']).count().reset_index()
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(1000,2)
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city_list = ['Burnaby','Vancouver','Kelowna','Whistler']

df0 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1 = df[df['MONTH']==1].drop('MONTH',axis=1).copy()
df1.rename(columns={x:f'{x}_' for x in df1.columns if x not in ['MEMBERNBR']},inplace=True)

df2 = df0.merge(df1,on='MEMBERNBR',how='outer')
branch_list = [f'BRANCH_{x}' for x in range(0,10)]
city = ['Burnaby','Vancouver','Kelowna','Whistler']

CreateRandomDFColumn(df2,branch_list,'BRANCHNAME')
CreateRandomDFColumn(df2,city_list,'CITY')

df2.head(2)

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000],
                         groupby_filter={'top':10,'minimum':5}):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']
    summary_df = account_df[total_columns_summary].groupby(summary_columns,dropna=False).sum().reset_index()
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    summary_df['COLUMN_NAME'] = column_name
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1

    groupby_df = groupby_df[(groupby_df['CUM']<= groupby_filter['top'])&(groupby_df['RECORD_COUNT']>= groupby_filter['minimum'])]
        
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])
account_df
account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()
summary_df
groupby_df
df2

def ColumnStatisticalReview(df,
                            column_name,
                            partitions=10,
                            top_x_records=10,
                            exclude_blanks_from_segments=1,
                            exclude_zeroes_from_segments=1):

    '''
    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution
    of values. 

    Args:
        column_name (str): Name of Column

        partitions (int): Number of partitions to include (Decile 10)

        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.
        If blank values are excluded it gives a better representation for the members of the set, however it might 
        provide a misleading representation of the population.

        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as
        such it can include both blanks and true 0 values. 

    '''
    temp_dict = {}
    
    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])
    
    if is_numeric:
        temp_dict['SUM'] = df[column_name].sum()
        temp_dict['MEAN'] = df[column_name].mean()
        temp_dict['STD_DEV'] =  df[column_name].std()
        temp_dict['MEDIAN'] = df[column_name].median()
        temp_dict['MAX'] = df[column_name].max()
        temp_dict['MIN'] = df[column_name].min()
        
    temp_dict['TOTAL_RECORDS'] = len(df)
    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))
    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])
    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])
    
    if is_numeric:
        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])
        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    

    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])

    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):
        return temp_df   
    
    # Add top X records Based on Frequency
    if top_x_records>0:
        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index()
        
        if len(top_instances)>0:
            top_instances[column_name] = top_instances.apply(lambda row: f"Value: {row[column_name]}, Frequency: {row['count']}", axis=1)
            top_instances['index'] = [f"Top {x+1}" for x in range(len(top_instances[column_name]))]
            top_instances = top_instances.drop('count',axis=1).set_index('index')
            temp_df = pd.concat([temp_df,top_instances])
        
    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):
        segment_df = ColumnPartitioner(df=df,
                                       column_name=column_name,
                                       partitions=partitions,
                                       exclude_blanks=exclude_blanks_from_segments,
                                       exclude_zeros=exclude_zeroes_from_segments,
                                       return_value='')
        
        seg_val_df = ColumnPartitioner(df=df,
                                           column_name=column_name,
                                           partitions=partitions,
                                           exclude_blanks=exclude_blanks_from_segments,
                                           exclude_zeros=exclude_zeroes_from_segments,
                                           return_value='agg_value').rename(columns={'VALUE':column_name})

        return pd.concat([temp_df,segment_df.T,seg_val_df])
    return temp_df
df2.groupby(['BRANCHNAME','CITY']).count().reset_index()
df2.groupby(['BRANCHNAME','CITY']).count().reset_index()['BRANCHNAME','CITY']
def generate_column_pairs(df, numeric_only=True, group_col_limit=50):
    """
    Generate candidate (value_column, group_column) pairs from a DataFrame.
    
    Args:
        df (pd.DataFrame): Input DataFrame
        numeric_only (bool): If True, restrict value columns to numeric
        group_col_limit (int): Max unique values for a group column (to avoid high-cardinality groups)
    
    Returns:
        List of tuples (value_col, group_col)
    """
    value_cols = df.select_dtypes(include='number').columns if numeric_only else df.columns
    group_cols = [
        col for col in df.columns 
        if df[col].nunique() <= group_col_limit and col not in value_cols
    ]
    
    # Avoid same-column pairing
    column_pairs = [
        (val_col, grp_col)
        for val_col, grp_col in product(value_cols, group_cols)
        if val_col != grp_col
    ]
    
    return column_pairs


df2
def generate_column_pairs(df, numeric_only=True, group_col_limit=50):
    """
    Generate candidate (value_column, group_column) pairs from a DataFrame.
    
    Args:
        df (pd.DataFrame): Input DataFrame
        numeric_only (bool): If True, restrict value columns to numeric
        group_col_limit (int): Max unique values for a group column (to avoid high-cardinality groups)
    
    Returns:
        List of tuples (value_col, group_col)
    """
    value_cols = df.select_dtypes(include='number').columns if numeric_only else df.columns
    group_cols = [
        col for col in df.columns 
        if df[col].nunique() <= group_col_limit and col not in value_cols
    ]
    
    # Avoid same-column pairing
    column_pairs = [
        (val_col, grp_col)
        for val_col, grp_col in product(value_cols, group_cols)
        if val_col != grp_col
    ]
    
    return column_pairs


generate_column_pairs(df2,'BRANCHNAME','CITY')
def generate_column_pairs(df, numeric_only=True, group_col_limit=50):
    """
    Generate candidate (value_column, group_column) pairs from a DataFrame.
    
    Args:
        df (pd.DataFrame): Input DataFrame
        numeric_only (bool): If True, restrict value columns to numeric
        group_col_limit (int): Max unique values for a group column (to avoid high-cardinality groups)
    
    Returns:
        List of tuples (value_col, group_col)
    """
    value_cols = df.select_dtypes(include='number').columns if numeric_only else df.columns
    group_cols = [
        col for col in df.columns 
        if df[col].nunique() <= group_col_limit and col not in value_cols
    ]
    
    # Avoid same-column pairing
    column_pairs = [
        (val_col, grp_col)
        for val_col, grp_col in product(value_cols, group_cols)
        if val_col != grp_col
    ]
    
    return column_pairs


generate_column_pairs(df2[['BRANCHNAME','CITY']])
from itertools import product
import pandas as pd

def generate_column_pairs(df, numeric_only=True, group_col_limit=50):
    """
    Generate candidate (value_column, group_column) pairs from a DataFrame.
    
    Args:
        df (pd.DataFrame): Input DataFrame
        numeric_only (bool): If True, restrict value columns to numeric
        group_col_limit (int): Max unique values for a group column (to avoid high-cardinality groups)
    
    Returns:
        List of tuples (value_col, group_col)
    """
    value_cols = df.select_dtypes(include='number').columns if numeric_only else df.columns
    group_cols = [
        col for col in df.columns 
        if df[col].nunique() <= group_col_limit and col not in value_cols
    ]
    
    # Avoid same-column pairing
    column_pairs = [
        (val_col, grp_col)
        for val_col, grp_col in product(value_cols, group_cols)
        if val_col != grp_col
    ]
    
    return column_pairs


generate_column_pairs(df2[['BRANCHNAME','CITY']])
from itertools import product
import pandas as pd

def generate_column_pairs(df, numeric_only=True, group_col_limit=50):
    """
    Generate candidate (value_column, group_column) pairs from a DataFrame.
    
    Args:
        df (pd.DataFrame): Input DataFrame
        numeric_only (bool): If True, restrict value columns to numeric
        group_col_limit (int): Max unique values for a group column (to avoid high-cardinality groups)
    
    Returns:
        List of tuples (value_col, group_col)
    """
    value_cols = df.select_dtypes(include='number').columns if numeric_only else df.columns
    group_cols = [
        col for col in df.columns 
        if df[col].nunique() <= group_col_limit and col not in value_cols
    ]
    
    # Avoid same-column pairing
    column_pairs = [
        (val_col, grp_col)
        for val_col, grp_col in product(value_cols, group_cols)
        if val_col != grp_col
    ]
    
    return column_pairs


generate_column_pairs(df2[['LENDING','LENDING_']])
#from Validation import ColumnStatisticalReview

ColumnStatisticalReview(df2,'CITY')
df2
df2['BRANCHNAME'].unique()
df2
df2['CITY']
df2['CITY'].uniquq()
df2['CITY'].unique()
df2['CITY'].unique().tolist()
df2['BRANCHNAME'].unique().tolist()
from DFProcessing import CombineLists

CombineLists
from DFProcessing import CombineLists

CombineLists([df2['CITY'].unique().tolist(),df2['BRANCHNAME'].unique().tolist()])
account_df.head()
account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum().T
pd.DataFrame(account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum())
pd.DataFrame(account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()).T

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000],
                         groupby_filter={'top':10,'minimum':5}):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']

    summary_df = pd.DataFrame(account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()).T
    summary_df['COLUMN_NAME'] = column_name
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1

    groupby_df = groupby_df[(groupby_df['CUM']<= groupby_filter['top'])&(groupby_df['RECORD_COUNT']>= groupby_filter['minimum'])]
        
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])
summary_df
summary_df
groupby_df

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000],
                         groupby_filter={'top':10,'minimum':5}):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']

    summary_df = pd.DataFrame(account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()).T
    summary_df['COLUMN_NAME'] = column_name
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1

    groupby_df = groupby_df[(groupby_df['CUM']<= groupby_filter['top'])&(groupby_df['RECORD_COUNT']>= groupby_filter['minimum'])]
    groupby_df['COLUMN_NAME'] = column_name
        
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])
account_df.head()
summary_df
groupby_df

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000],
                         groupby_filter={'top':10,'minimum':5}):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']

    summary_df = pd.DataFrame(account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()).T
    summary_df['COLUMN_NAME'] = column_name
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1

    #groupby_df = groupby_df[(groupby_df['CUM']<= groupby_filter['top'])&(groupby_df['RECORD_COUNT']>= groupby_filter['minimum'])]
    groupby_df['COLUMN_NAME'] = column_name
        
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])
groupby_df
groupby_df.head(20)

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000],
                         groupby_filter={'top':10,'minimum':5}):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']

    summary_df = pd.DataFrame(account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()).T
    summary_df['COLUMN_NAME'] = column_name
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1

    groupby_df = groupby_df[(groupby_df['CUM']<= groupby_filter['top'])&(groupby_df['RECORD_COUNT']>= groupby_filter['minimum'])]
    groupby_df['COLUMN_NAME'] = column_name
        
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])
summary_df
groupby_df.head(20)

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000],
                         groupby_filter={'top':10,'minimum':5}):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']

    summary_df = pd.DataFrame(account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()).T
    summary_df['COLUMN_NAME'] = column_name
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1

    #groupby_df = groupby_df[(groupby_df['CUM']<= groupby_filter['top'])&(groupby_df['RECORD_COUNT']>= groupby_filter['minimum'])]
    groupby_df['COLUMN_NAME'] = column_name
        
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])
groupby_df.head(20)
groupby_df[groupby_df['CUM']<=5].head(20)

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000],
                         groupby_filter={'top':10,'minimum':5}):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']

    summary_df = pd.DataFrame(account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()).T
    summary_df['COLUMN_NAME'] = column_name
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1

    groupby_df = groupby_df[(groupby_df['CUM']<= groupby_filter['top'])&(groupby_df['RECORD_COUNT']>= groupby_filter['minimum'])]
    groupby_df['COLUMN_NAME'] = column_name
        
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])
groupby_df[groupby_df['CUM']<=5]
from DFProcessing import CombineLists


def GenerateColumnCombinations(df,column_names):

    list_ = []

    for column in column_names:
        lists_.append(df[column].unique().tolist()

    return lists_

GenerateColumnCombinations(df2,['CITY','BRANCHNAME'])
from DFProcessing import CombineLists


def GenerateColumnCombinations(df,column_names):

    list_ = []

    for column in column_names:
        lists_.append(df[column].unique().tolist())

    return lists_

GenerateColumnCombinations(df2,['CITY','BRANCHNAME'])
from DFProcessing import CombineLists


def GenerateColumnCombinations(df,column_names):

    list_ = []

    for column in column_names:
        lists_.append(df[column].unique().tolist())

    return list_

GenerateColumnCombinations(df2,['CITY','BRANCHNAME'])
from DFProcessing import CombineLists


def GenerateColumnCombinations(df,column_names):

    list_ = []

    for column in column_names:
        list_.append(df[column].unique().tolist())

    return list_

GenerateColumnCombinations(df2,['CITY','BRANCHNAME'])
from DFProcessing import CombineLists


def GenerateColumnCombinations(df,column_names):

    list_ = []

    for column in column_names:
        list_.append(df[column].unique().tolist())

    return CombineLists(list_)

GenerateColumnCombinations(df2,['CITY','BRANCHNAME'])
from DFProcessing import CombineLists


def GenerateColumnCombinations(df,column_names):

    list_ = []

    for column in column_names:
        list_.append(df[column].unique().tolist())

    return CombineLists(list_)

combo = GenerateColumnCombinations(df2,['CITY','BRANCHNAME'])
combo[0[
combo[0]
from itertools import product

def GenerateColumnCombinations(df, column_names):
    """
    Generate all combinations of unique values across specified columns,
    and return them as a dictionary of dictionaries.

    Example return:
    {
        0: {'CITY': 'Toronto', 'BRANCHNAME': 'Branch A'},
        1: {'CITY': 'Vancouver', 'BRANCHNAME': 'Branch B'},
        ...
    }
    """
    # List of lists of unique values
    value_lists = [df[col].dropna().unique().tolist() for col in column_names]
    
    # Cartesian product of all value combinations
    combos = list(product(*value_lists))
    
    # Build dictionary of dictionaries
    combo_dict = {
        idx: dict(zip(column_names, values))
        for idx, values in enumerate(combos)
    }

    return combo_dict

GenerateColumnCombinations(df2,['CITY','BRANCHNAME'])
df2
from itertools import product

def GenerateColumnCombinations(df, column_names):
    """
    Generate all combinations of unique values across specified columns,
    and return them as a dictionary of dictionaries.

    Example return:
    {
        0: {'CITY': 'Toronto', 'BRANCHNAME': 'Branch A'},
        1: {'CITY': 'Vancouver', 'BRANCHNAME': 'Branch B'},
        ...
    }
    """
    # List of lists of unique values
    value_lists = [df[col].dropna().unique().tolist() for col in column_names]
    
    # Cartesian product of all value combinations
    combos = list(product(*value_lists))
    
    # Build dictionary of dictionaries
    combo_dict = {
        idx: dict(zip(column_names, values))
        for idx, values in enumerate(combos)
    }

    return combo_dict

GenerateColumnCombinations(df2,['CITY','BRANCHNAME','CLASSIFICATION])
from itertools import product

def GenerateColumnCombinations(df, column_names):
    """
    Generate all combinations of unique values across specified columns,
    and return them as a dictionary of dictionaries.

    Example return:
    {
        0: {'CITY': 'Toronto', 'BRANCHNAME': 'Branch A'},
        1: {'CITY': 'Vancouver', 'BRANCHNAME': 'Branch B'},
        ...
    }
    """
    # List of lists of unique values
    value_lists = [df[col].dropna().unique().tolist() for col in column_names]
    
    # Cartesian product of all value combinations
    combos = list(product(*value_lists))
    
    # Build dictionary of dictionaries
    combo_dict = {
        idx: dict(zip(column_names, values))
        for idx, values in enumerate(combos)
    }

    return combo_dict

GenerateColumnCombinations(df2,['CITY','BRANCHNAME','CLASSIFICATION'])
from DataSets import GenerateFakeMemberDF

df = GenerateFakeMemberDF(1000,2)
branch_list = [f'BRANCH_{x}' for x in range(0,50)]
city_list = ['Burnaby','Vancouver','Kelowna','Whistler']

df0 = df[df['MONTH']==0].drop('MONTH',axis=1).copy()
df1 = df[df['MONTH']==1].drop('MONTH',axis=1).copy()
df1.rename(columns={x:f'{x}_' for x in df1.columns if x not in ['MEMBERNBR']},inplace=True)

df2 = df0.merge(df1,on='MEMBERNBR',how='outer')
branch_list = [f'BRANCH_{x}' for x in range(0,10)]
city = ['Burnaby','Vancouver','Kelowna','Whistler']

CreateRandomDFColumn(df2,branch_list,'BRANCHNAME')
CreateRandomDFColumn(df2,city_list,'CITY')
CreateRandomDFColumn(df2,city_list,'CITY2')

df2.head(2)
from itertools import product

def GenerateColumnCombinations(df, column_names):
    """
    Generate all combinations of unique values across specified columns,
    and return them as a dictionary of dictionaries.

    Example return:
    {
        0: {'CITY': 'Toronto', 'BRANCHNAME': 'Branch A'},
        1: {'CITY': 'Vancouver', 'BRANCHNAME': 'Branch B'},
        ...
    }
    """
    # List of lists of unique values
    value_lists = [df[col].dropna().unique().tolist() for col in column_names]
    
    # Cartesian product of all value combinations
    combos = list(product(*value_lists))
    
    # Build dictionary of dictionaries
    combo_dict = {
        idx: dict(zip(column_names, values))
        for idx, values in enumerate(combos)
    }

    return combo_dict

GenerateColumnCombinations(df2,['CITY','BRANCHNAME','CITY'])
df2['CLASSIFICATION']
df2['CLASSIFICATION'][0]
from itertools import product

def GenerateColumnCombinations(df, column_names):
    """
    Generate all combinations of unique values across specified columns,
    and return them as a dictionary of dictionaries.

    Example return:
    {
        0: {'CITY': 'Toronto', 'BRANCHNAME': 'Branch A'},
        1: {'CITY': 'Vancouver', 'BRANCHNAME': 'Branch B'},
        ...
    }
    """
    # List of lists of unique values
    value_lists = [df[col].dropna().unique().tolist() for col in column_names]
    
    # Cartesian product of all value combinations
    combos = list(product(*value_lists))
    
    # Build dictionary of dictionaries
    combo_dict = {
        idx: dict(zip(column_names, values))
        for idx, values in enumerate(combos)
    }

    return combo_dict

GenerateColumnCombinations(df2,['CITY','BRANCHNAME','CITY2'])
from itertools import product

def GenerateColumnCombinations(df, column_names):
    """
    Generate all combinations of unique values across specified columns,
    and return them as a dictionary of dictionaries.

    Example return:
    {
        0: {'CITY': 'Toronto', 'BRANCHNAME': 'Branch A'},
        1: {'CITY': 'Vancouver', 'BRANCHNAME': 'Branch B'},
        ...
    }
    """
    # List of lists of unique values
    value_lists = [df[col].dropna().unique().tolist() for col in column_names]
    
    # Cartesian product of all value combinations
    combos = list(product(*value_lists))
    
    # Build dictionary of dictionaries
    combo_dict = {
        idx: dict(zip(column_names, values))
        for idx, values in enumerate(combos)
    }

    return combo_dict

a = GenerateColumnCombinations(df2,['CITY','BRANCHNAME','CITY2'])
a
a.keys()
a[0]
a[0].keys()
list(a[0].keys())
def FilterDictionary(df,CombinationDictionaries):

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]=key[col])&(df[col1]=key[col1])&(df[col2]=key[col2])]
            return temp_df
def FilterDictionary(df,CombinationDictionaries):

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
def FilterDictionary(df,CombinationDictionaries):

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = CombinationDictionaries.keys()

    return key_



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
a
from itertools import product
import pandas as pd

def generate_column_pairs(df, numeric_only=True, group_col_limit=50):
    """
    Generate candidate (value_column, group_column) pairs from a DataFrame.
    
    Args:
        df (pd.DataFrame): Input DataFrame
        numeric_only (bool): If True, restrict value columns to numeric
        group_col_limit (int): Max unique values for a group column (to avoid high-cardinality groups)
    
    Returns:
        List of tuples (value_col, group_col)
    """
    value_cols = df.select_dtypes(include='number').columns if numeric_only else df.columns
    group_cols = [
        col for col in df.columns 
        if df[col].nunique() <= group_col_limit and col not in value_cols
    ]
    # Avoid same-column pairing
    column_pairs = [
        (val_col, grp_col)
        for val_col, grp_col in product(value_cols, group_cols)
        if val_col != grp_col
    ]    
    return column_pairs

generate_column_pairs(df2[['LENDING','LENDING_']])

def RecordElementCompare(df,
                         column_name,
                         column_name1,
                         primary_key,
                         summary_columns,
                         bracketing=[-10000,-1000,-1,0,1,1000,10000],
                         groupby_filter={'top':10,'minimum':5}):
    '''
    
    Function which takes a dataframe with 2 Columns which a desired Comparison in Necessary.
    Initial Use Case Required Minimal Number Values, focused on TEXT
        
    Parameters:
    
    
    Returns:
    
    
    Example Usage:
        
        df= df[[START_BAL, START_BAL_, ACCTNBR]],
        column_name='START_BAL'

    '''
    # Make a Copy.

    start_cols = primary_key + summary_columns + [column_name] + [column_name1]
    account_df = df[start_cols].copy()

    if is_numeric_dtype(account_df[column_name]):
        account_df[column_name] = np.where(account_df[column_name] == "", 0, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", 0, account_df[column_name1])
    else:
        account_df[column_name] = np.where(account_df[column_name] == "", None, account_df[column_name])
        account_df[column_name1] = np.where(account_df[column_name1] == "", None, account_df[column_name1])

    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.
    account_df = account_df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()
    
    # Calculate DIfference
    
    BinaryComplexEquivlancey(account_df,'DF','DF1','VALUES_EQUAL')
    account_df['VALUES_NOT_EQUAL'] = np.where(account_df['VALUES_EQUAL']==0,1,0)
    account_df['NULL_RECORD_DF'] = np.where(account_df['DF'].isnull(),1,0)
    account_df['NULL_RECORD_DF1'] = np.where(account_df['DF1'].isnull(),1,0)
        
    try:
        account_df['RECORD_COUNT']
    except:
        account_df['RECORD_COUNT']=1
    
    total_columns_summary = summary_columns + ['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']

    summary_df = pd.DataFrame(account_df[['RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].sum()).T
    summary_df['COLUMN_NAME'] = column_name
    summary_df['PERC_EQUAL'] = (summary_df['VALUES_EQUAL'] /summary_df['RECORD_COUNT'])*100
    
    total_columns_groupby = total_columns_summary + ['DF','DF1']
    gb_columns = summary_columns + ['DF','DF1']
    groupby_df = account_df[total_columns_groupby].groupby(gb_columns,dropna=False).sum().reset_index().sort_values("RECORD_COUNT",ascending=False)

    groupby_df['CUM'] = groupby_df.groupby(summary_columns,dropna=False).cumcount() + 1

    groupby_df = groupby_df[(groupby_df['CUM']<= groupby_filter['top'])&(groupby_df['RECORD_COUNT']>= groupby_filter['minimum'])]
    groupby_df['COLUMN_NAME'] = column_name
        
    return account_df,summary_df,groupby_df

account_df,summary_df,groupby_df = RecordElementCompare(df2,
                                                        column_name='LENDING',
                                                        column_name1='LENDING_',
                                                        primary_key=['MEMBERNBR'],
                                                        summary_columns=['CITY'])
account_df.head()
summary_df
groupby_df[groupby_df['CUM']<=5]
def FilterDictionary(df,CombinationDictionaries):


    key_ = CombinationDictionaries.keys()

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = CombinationDictionaries.keys()

    return key_



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = CombinationDictionaries.values()

    return key_



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = CombinationDictionaries.values()[0]

    return key_



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = list(CombinationDictionaries.values()][0]

    return key_



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = list(CombinationDictionaries.values())

    return key_



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = list(CombinationDictionaries.values())

    return key_.values()



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = list(CombinationDictionaries.values())

    return key_.values



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = list(CombinationDictionaries.values())

    return key_



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = list(CombinationDictionaries.values())

    return key_[0]



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = list(CombinationDictionaries.values())

    return key_[0].keys()



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = list(CombinationDictionaries.values())

    return list(key_[0].keys())



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(CombinationDictionaries)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = list(CombinationDictionaries.values())

    return 



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(list(key_[0].keys()))==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = list(CombinationDictionaries.values())

    return len(list(key_[0].keys()))



    

    if len(CombinationDictionaries)==2:
        pass

        
    elif len(list(key_[0].keys()))==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    key_ = list(CombinationDictionaries.values())
    print(key_)
    if len(CombinationDictionaries)==2:
        pass

        
    elif len(list(key_[0].keys()))==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
list(a.values())
list(a.values())[0]
list(a.values())[0].keys()
list(list(a.values())[0].keys())
def FilterDictionary(df,CombinationDictionaries):


    dict_list = list(CombinationDictionaries.values())
    keys = list(dict_list[0].keys())

    print(keys)


    
    print(key_)
    if len(CombinationDictionaries)==2:
        pass

        
    elif len(list(key_[0].keys()))==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    dict_list = list(CombinationDictionaries.values())
    keys = list(dict_list[0].keys())

    print(keys)


    
    print(key_)
    if len(CombinationDictionaries)==2:
        pass

        
    elif len(keys)==3:
        col  = list(CombinationDictionaries[0].keys())[0]
        col1 = list(CombinationDictionaries[0].keys())[1]
        col2 = list(CombinationDictionaries[0].keys())[2]

        for key in CombinationDictionaries.keys():
            temp_df = df[(df[col]==key[col])&(df[col1]==key[col1])&(df[col2]==key[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    dict_list = list(CombinationDictionaries.values())
    keys = list(dict_list[0].keys())

    print(keys)


    
    print(key_)
    if len(CombinationDictionaries)==2:
        pass

        
    elif len(keys)==3:
        col  = keys[0]
        col1 = keys[1]
        col2 = keys[2]

        for list_ in dict_list:
            temp_df = df[(df[col]==list_[0])&(df[col1]==list_[1])&(df[col2]==list_[2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    dict_list = list(CombinationDictionaries.values())
    keys = list(dict_list[0].keys())

    if len(CombinationDictionaries)==2:
        pass
        
    elif len(keys)==3:
        col  = keys[0]
        col1 = keys[1]
        col2 = keys[2]

        for list_ in dict_list:
            temp_df = df[(df[col]==list_[0])&(df[col1]==list_[1])&(df[col2]==list_[2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    dict_list = list(CombinationDictionaries.values())
    keys = list(dict_list[0].keys())
    print(keys)
    if len(CombinationDictionaries)==2:
        pass
        
    elif len(keys)==3:
        col  = keys[0]
        col1 = keys[1]
        col2 = keys[2]

        for list_ in dict_list:
            temp_df = df[(df[col]==list_[0])&(df[col1]==list_[1])&(df[col2]==list_[2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    dict_list = list(CombinationDictionaries.values())
    keys = list(dict_list[0].keys())
    print(keys)
    if len(CombinationDictionaries)==2:
        pass
        
    elif len(keys)==3:
        
        col  = keys[0]
        col1 = keys[1]
        col2 = keys[2]

        for list_ in dict_list:
            print(list_)
            temp_df = df[(df[col]==list_[0])&(df[col1]==list_[1])&(df[col2]==list_[2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    dict_list = list(CombinationDictionaries.values())
    keys = list(dict_list[0].keys())
    print(keys)
    if len(CombinationDictionaries)==2:
        pass
        
    elif len(keys)==3:
        
        col  = keys[0]
        col1 = keys[1]
        col2 = keys[2]

        for list_ in dict_list:
            print(list_)
            temp_df = df[(df[col]==list_[col])&(df[col1]==list_[col1)&(df[col2]==list_[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):


    dict_list = list(CombinationDictionaries.values())
    keys = list(dict_list[0].keys())
    print(keys)
    if len(CombinationDictionaries)==2:
        pass
        
    elif len(keys)==3:
        
        col  = keys[0]
        col1 = keys[1]
        col2 = keys[2]

        for list_ in dict_list:
            print(list_)
            temp_df = df[(df[col]==list_[col])&(df[col1]==list_[col1])&(df[col2]==list_[col2])]
            return temp_df
        
        
FilterDictionary(df2,a)
def FilterDictionary(df,CombinationDictionaries):

    dict_list = list(CombinationDictionaries.values())
    keys = list(dict_list[0].keys())
    print(keys)
    
    if len(CombinationDictionaries)==2:
        pass
        
    elif len(keys)==3:
        
        col  = keys[0]
        col1 = keys[1]
        col2 = keys[2]

        for list_ in dict_list:
            print(list_)
            temp_df = df[(df[col]==list_[col])&(df[col1]==list_[col1])&(df[col2]==list_[col2])]
                
            return ColumnStatisticalReview(temp_df,'DEPOSIT')
        
FilterDictionary(df2,a)
%history -f text2.txt
