{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa1dd6-022a-4b08-ac59-898e96b89cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1feb677e-8d1e-4bf4-89ea-911fc7f37ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "476a8e5a-a41b-4533-8a46-05d6b8deacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary Which Stores All Files which are Run.\n",
    "example_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5783f426-f689-4395-b7d4-bc86c924dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Functions for Organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce055b-1369-4d49-b148-0f5f860f219b",
   "metadata": {},
   "source": [
    "#### 1) ReadPythonFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a17883-d44c-4fd7-9402-4a52276468f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Arguments</th>\n",
       "      <th>Return</th>\n",
       "      <th>Code</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IterateThroughListuntilText</th>\n",
       "      <td>Function which Iterates through a list of pyth...</td>\n",
       "      <td>[lines (list of str): Input lines, ]</td>\n",
       "      <td></td>\n",
       "      <td>def IterateThroughListuntilText(list_,text):\\n...</td>\n",
       "      <td>TextFunctions.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extract_doc_sections_all</th>\n",
       "      <td>Function which iterates through text, looking ...</td>\n",
       "      <td>[estimator_class: A scikit-learn class (e.g., ...</td>\n",
       "      <td></td>\n",
       "      <td>def extract_doc_sections_all(estimator_class, ...</td>\n",
       "      <td>TextFunctions.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CountWordsinDFColumn</th>\n",
       "      <td>Function which reads a Dataframe Column and re...</td>\n",
       "      <td>[df (dataframe), column_name (str): Name of Co...</td>\n",
       "      <td></td>\n",
       "      <td>def CountWordsinDFColumn(df,\\n                ...</td>\n",
       "      <td>TextFunctions.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RemovePatternFromDFColumn</th>\n",
       "      <td>Removes text matching a regex pattern from a D...</td>\n",
       "      <td>[df (pd.DataFrame): Input DataFrame., column (...</td>\n",
       "      <td></td>\n",
       "      <td>def RemovePatternFromDFColumn(df,\\n           ...</td>\n",
       "      <td>TextFunctions.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RemoveWordfromDFColumn</th>\n",
       "      <td></td>\n",
       "      <td>[df, column, word, new_column_name]</td>\n",
       "      <td>None</td>\n",
       "      <td>def RemoveWordfromDFColumn(df,\\n              ...</td>\n",
       "      <td>TextFunctions.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConvertListtoSQLText</th>\n",
       "      <td>Function to convert a python list into SQL cod...</td>\n",
       "      <td>[List_(list): Python List, return_vaule(str): ...</td>\n",
       "      <td>None</td>\n",
       "      <td>def ConvertListtoSQLText(list_, \\n            ...</td>\n",
       "      <td>SQL.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>generate_create_table_sql</th>\n",
       "      <td>Function to create a SQL Statement to Create a...</td>\n",
       "      <td>[df(df): Any DataFrame, table_name(str): desir...</td>\n",
       "      <td></td>\n",
       "      <td>def generate_create_table_sql(df, table_name, ...</td>\n",
       "      <td>SQL.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TableRecordCountByDate</th>\n",
       "      <td>Generate a SQL Server query to count records b...</td>\n",
       "      <td>[dates (List[datetime.datetime]): List of date...</td>\n",
       "      <td></td>\n",
       "      <td>def TableRecordCountByDate(table_dict,\\n      ...</td>\n",
       "      <td>SQL.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SampleDataFrame</th>\n",
       "      <td>Returns a random sample from a DataFrame based...</td>\n",
       "      <td>[df (pd.DataFrame): The dataset to sample from...</td>\n",
       "      <td></td>\n",
       "      <td>def SampleDataFrame(df, \\n                    ...</td>\n",
       "      <td>Validation.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReviewEntireDataframe</th>\n",
       "      <td>No description available</td>\n",
       "      <td>[df, file_name]</td>\n",
       "      <td>None</td>\n",
       "      <td>def ReviewEntireDataframe(df,file_name=None):\\...</td>\n",
       "      <td>Validation.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   Description  \\\n",
       "IterateThroughListuntilText  Function which Iterates through a list of pyth...   \n",
       "extract_doc_sections_all     Function which iterates through text, looking ...   \n",
       "CountWordsinDFColumn         Function which reads a Dataframe Column and re...   \n",
       "RemovePatternFromDFColumn    Removes text matching a regex pattern from a D...   \n",
       "RemoveWordfromDFColumn                                                           \n",
       "...                                                                        ...   \n",
       "ConvertListtoSQLText         Function to convert a python list into SQL cod...   \n",
       "generate_create_table_sql    Function to create a SQL Statement to Create a...   \n",
       "TableRecordCountByDate       Generate a SQL Server query to count records b...   \n",
       "SampleDataFrame              Returns a random sample from a DataFrame based...   \n",
       "ReviewEntireDataframe                                 No description available   \n",
       "\n",
       "                                                                     Arguments  \\\n",
       "IterateThroughListuntilText               [lines (list of str): Input lines, ]   \n",
       "extract_doc_sections_all     [estimator_class: A scikit-learn class (e.g., ...   \n",
       "CountWordsinDFColumn         [df (dataframe), column_name (str): Name of Co...   \n",
       "RemovePatternFromDFColumn    [df (pd.DataFrame): Input DataFrame., column (...   \n",
       "RemoveWordfromDFColumn                     [df, column, word, new_column_name]   \n",
       "...                                                                        ...   \n",
       "ConvertListtoSQLText         [List_(list): Python List, return_vaule(str): ...   \n",
       "generate_create_table_sql    [df(df): Any DataFrame, table_name(str): desir...   \n",
       "TableRecordCountByDate       [dates (List[datetime.datetime]): List of date...   \n",
       "SampleDataFrame              [df (pd.DataFrame): The dataset to sample from...   \n",
       "ReviewEntireDataframe                                          [df, file_name]   \n",
       "\n",
       "                            Return  \\\n",
       "IterateThroughListuntilText          \n",
       "extract_doc_sections_all             \n",
       "CountWordsinDFColumn                 \n",
       "RemovePatternFromDFColumn            \n",
       "RemoveWordfromDFColumn        None   \n",
       "...                            ...   \n",
       "ConvertListtoSQLText          None   \n",
       "generate_create_table_sql            \n",
       "TableRecordCountByDate               \n",
       "SampleDataFrame                      \n",
       "ReviewEntireDataframe         None   \n",
       "\n",
       "                                                                          Code  \\\n",
       "IterateThroughListuntilText  def IterateThroughListuntilText(list_,text):\\n...   \n",
       "extract_doc_sections_all     def extract_doc_sections_all(estimator_class, ...   \n",
       "CountWordsinDFColumn         def CountWordsinDFColumn(df,\\n                ...   \n",
       "RemovePatternFromDFColumn    def RemovePatternFromDFColumn(df,\\n           ...   \n",
       "RemoveWordfromDFColumn       def RemoveWordfromDFColumn(df,\\n              ...   \n",
       "...                                                                        ...   \n",
       "ConvertListtoSQLText         def ConvertListtoSQLText(list_, \\n            ...   \n",
       "generate_create_table_sql    def generate_create_table_sql(df, table_name, ...   \n",
       "TableRecordCountByDate       def TableRecordCountByDate(table_dict,\\n      ...   \n",
       "SampleDataFrame              def SampleDataFrame(df, \\n                    ...   \n",
       "ReviewEntireDataframe        def ReviewEntireDataframe(df,file_name=None):\\...   \n",
       "\n",
       "                                         File  \n",
       "IterateThroughListuntilText  TextFunctions.py  \n",
       "extract_doc_sections_all     TextFunctions.py  \n",
       "CountWordsinDFColumn         TextFunctions.py  \n",
       "RemovePatternFromDFColumn    TextFunctions.py  \n",
       "RemoveWordfromDFColumn       TextFunctions.py  \n",
       "...                                       ...  \n",
       "ConvertListtoSQLText                   SQL.py  \n",
       "generate_create_table_sql              SQL.py  \n",
       "TableRecordCountByDate                 SQL.py  \n",
       "SampleDataFrame                 Validation.py  \n",
       "ReviewEntireDataframe           Validation.py  \n",
       "\n",
       "[104 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SharedFolder import ReadPythonFiles\n",
    "example_dict = {}\n",
    "example_dict['ReadPythonFiles']=1\n",
    "PythonFunctions = ReadPythonFiles()\n",
    "PythonFunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a80b82-e351-4ba3-b662-8208c2835dd6",
   "metadata": {},
   "source": [
    "#### 2) InspectFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b6eaa05-41ff-4939-b28b-6fb852f094d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def ReadPythonFiles(location='/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/',file_list=None):\n",
      "    \n",
      "    '''\n",
      "    Function which reads a Folder, iterating through a list of Files, specifically looking for .py files utilizing the Extract Function Details AST.\n",
      "    Function Looks for Parameters, Returns, Date Created and Date Last Modified.\n",
      " \n",
      "    Parameters:\n",
      "         location (str): Folder\n",
      "         \n",
      "    Returns:\n",
      "        Dataframe of Functions with description.\n",
      "\n",
      "    Date Created: \n",
      "    Date Last Modified: 25-Aug-25\n",
      "\n",
      "    '''    \n",
      "    py_file_dict = {}\n",
      "\n",
      "    if not file_list:\n",
      "        file_list = [x for x in ReadDirectory(location) if x.find('.py')!=-1 and x.find('__')==-1]\n",
      "    \n",
      "    for file_name in file_list:\n",
      "        with open(f\"{location}{file_name}\", \"r\", encoding=\"utf-8\") as file:\n",
      "            data = file.read()\n",
      "            py_file_dict.update(PythonStringParser_AST(data,file_name))\n",
      "            \n",
      "    return pd.DataFrame(py_file_dict).T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from UtilityFunctions import InspectFunction\n",
    "example_dict['InspectFunction']=1\n",
    "InspectFunction(ReadPythonFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfa831e-0efa-4cef-a984-9d96b3771f4b",
   "metadata": {},
   "source": [
    "#### 3) CompareFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30200439-12bb-409d-907a-2c36c776a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconcilation Complete\n"
     ]
    }
   ],
   "source": [
    "from TextFunctions import CompareFunction\n",
    "example_dict['CompareFunction']=1\n",
    "CompareFunction(InspectFunction,InspectFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8dfefb-b52b-4ee2-b48e-dc643dfa8305",
   "metadata": {},
   "source": [
    "#### 4) Display Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86790ed6-e496-4a47-9a16-9000ea5f1836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def D_Notes_Reader(topic=None):\n",
      "    '''\n",
      "    Function to read Notes Files Saved in Google Docs\n",
      "\n",
      "    Parameters: \n",
      "        topic (str): Argument to enable Filtering of Returned Dataframe to a Specific Topic. If a Filter is not applied it can be difficult\n",
      "        to read the output as it's one continuous text without the Header Comments.\n",
      "\n",
      "    Returns:\n",
      "        Printed version of Notes (Powerpoint Like Format)\n",
      "\n",
      "    Date Created: August 17, 2025\n",
      "    Date Last Modified: \n",
      "    \n",
      "    '''\n",
      "    \n",
      "    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSQF2lNc4WPeTRQ_VzWPkqSZp4RODFkbap8AqmolWp5bKoMaslP2oRVVG21x2POu_JcbF1tGRcBgodu/pub?output=csv')\n",
      "    if topic:\n",
      "        temp = df[df['Process']==topic]\n",
      "        if len(temp)>0:\n",
      "            return JupyterNotebookMarkdown(temp)\n",
      "    else:\n",
      "        return JupyterNotebookMarkdown(df)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from UtilityFunctions import InspectFunction\n",
    "InspectFunction(D_Notes_Reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a418245-5b9c-4eff-a3e9-78eb4c2a8294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaad3aea-64e3-432a-ba97-07678fef55c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>1. Approaches</h4>\n",
       "<ul>\n",
       "  <ul><li>Direct User Interaction</li>\n",
       "</ul>\n",
       "  <ul><li>Agent/ Assistant Proxy</li>\n",
       "</ul>\n",
       "  <ul><li>Agent/ Assistant</li>\n",
       "</ul>\n",
       "  <ul><li>Autonomous Agent</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>2. Comparison</h4>\n",
       "<ul>\n",
       "  <ul><li>LLM</li>\n",
       "    <ul>\n",
       "      <li>Understanding and Generatic Human Like Text vs Performing a Specific Task, Autonomously while learning its environment</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>3. Story Telling</h4>\n",
       "<ul>\n",
       "  <ul><li>Armature</li>\n",
       "    <ul>\n",
       "      <li>The armature is the core message, moral, or truth your story is built to prove. </li>\n",
       "      <li>The point that your story is build around... It’s the invisible backbone of your narrative — the reason you’re telling the story. </li>\n",
       "      <li>The wire frame inside a sculpture: you don’t see it, but without it, the whole structure collapses.</li>\n",
       "      <li>To get your point across, you must understand it. Most people can’t isolate their message, they want to say something deep and profound, but end up saying nothing.</li>\n",
       "      <li>Find the thing your character would rather die than do, and make them do it.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>What is your Plot</li>\n",
       "    <ul>\n",
       "      <li>Plot: Series of events which transpire. Data Science isn’t only about Plot. Stories that matter are the stories whose plot can be rigorously be defended with logic and evidence. And those are the data science stories that can change the world</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>What is your Story</li>\n",
       "    <ul>\n",
       "      <li>What you want people to take away and feel</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Your Message</li>\n",
       "    <ul>\n",
       "      <li>The logic of discovery is not the logic of presentation.</li>\n",
       "      <li>Can others understand what you are saying? Why should they care?\n",
       "Give them the information to draw the conclusion, but also help them get to the conclusion you want them to arrive at.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Questions</li>\n",
       "    <ul>\n",
       "      <li>How are you going to ensure it is heard?</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>What is your Call to Action?</li>\n",
       "    <ul>\n",
       "      <li>What specifically are you asking? How will you know its understood.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>4. Helpful Reminders</h4>\n",
       "<ul>\n",
       "  <ul><li>Interesting and Engaging Visuals, Minimal Words.</li>\n",
       "    <ul>\n",
       "      <li>80% Salient might be better than 100% accurate</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Is it Clear?</li>\n",
       "</ul>\n",
       "  <ul><li>Is it interesting?</li>\n",
       "    <ul>\n",
       "      <li>People are easily distractable, so they might not recognize stuff that is immediately in front of them.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Am I persuaive</li>\n",
       "</ul>\n",
       "  <ul><li>Is it short?</li>\n",
       "    <ul>\n",
       "      <li>Too much information is a bad thing, people can’t remember everything.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>For More Information.</li>\n",
       "    <ul>\n",
       "      <li>Did you provide access to the necessary Information, without presenting it.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Who is the story for?</li>\n",
       "    <ul>\n",
       "      <li>You can’t fall in love with the story, or you will miss the flaws, boredom, or questions. \n",
       "Every story needs a beginning, middle and end.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>5. Presentation Tenants</h4>\n",
       "<ul>\n",
       "  <ul><li>The Message</li>\n",
       "    <ul>\n",
       "      <li>Need to have a Crystal Clear Story, Idea and Goal.</li>\n",
       "      <li>Tell them what you’re going to tell them, tell them, and then tell them what you’re told them.</li>\n",
       "      <li>Storytelling beats data 100% of the time.</li>\n",
       "      <li>Is the glass half full, half empty. If it's half full, do you have what you really want in the glass?</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Design Principles</li>\n",
       "    <ul>\n",
       "      <li>Eyes, not Mind.</li>\n",
       "      <li>Competition is not a theme. Competition is a necessary evil, is a theme.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>No No's</li>\n",
       "    <ul>\n",
       "      <li>Do not make the user think</li>\n",
       "      <li>Do not make the user remember</li>\n",
       "      <li>Do not use 3D unless absolutely necessary.</li>\n",
       "      <li>Pie Charts. They Suck.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>6. Know Your Audience</h4>\n",
       "<ul>\n",
       "  <ul><li>Who is your Audience?</li>\n",
       "    <ul>\n",
       "      <li>How to get a CEO to remove a soft drink from Japan without telling them to remove a soft drink from Japan.</li>\n",
       "      <li>The more we know about the motivations and incentives of decision makers, the more we can control the communication. </li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>What do they want and why?</li>\n",
       "</ul>\n",
       "  <ul><li>What are they expecting to hear, and why?</li>\n",
       "    <ul>\n",
       "      <li>Cuban Missle Crisis: Asymettric Informaiton, Deception, Misreading Adversarieal Strength, Attempting to disrupt competitor. Fear is the reigning notion. People don't read the data well when fear is involved.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>What Biases might they have?</li>\n",
       "    <ul>\n",
       "      <li>High Pressure Decision Making: The inverted U shape Curve, Stress vs Performance. No one knows exactly where it is for each individual.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>7. Definition</h4>\n",
       "<ul>\n",
       "  <ul><li>Definition</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>8. Classification</h4>\n",
       "<ul>\n",
       "  <ul><li>True Positive</li>\n",
       "</ul>\n",
       "  <ul><li>Falst Positive</li>\n",
       "</ul>\n",
       "  <ul><li>True Negative</li>\n",
       "</ul>\n",
       "  <ul><li>False Negative</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>9. Evaluation</h4>\n",
       "<ul>\n",
       "  <ul><li>Precision</li>\n",
       "    <ul>\n",
       "      <li>Precision is the ratio of correct positive predictions to the overall number of Positive Predictions.  It is useful when false positives are costly (e.g., spam detection, where predicting a normal email as spam is bad).</li>\n",
       "      <li>TP/ (TP + FP)</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Recall</li>\n",
       "    <ul>\n",
       "      <li>Recall is the correct positive predicitions to the overall number of positive examples. It is useful when false negatives are costly (e.g., medical diagnosis, where missing a disease case can be dangerous).</li>\n",
       "      <li>TP/ (TP + FN)</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>F1 Score</li>\n",
       "    <ul>\n",
       "      <li>F1 is harmonic mean is a type of average that is calculated by dividing the number of values by the sum of the reciprocals of the values, making it useful for situations where rates or ratios are involved, such as calculating average speeds or in situations where equal weighting is given to each data point’s contribution to the average. The F-score (or F1-score) is a measure of a model’s accuracy that considers both precision and recall. The F1-score is particularly useful when your dataset is imbalanced, and you want to ensure both precision and recall are considered. For example, if you only focus on precision, you might ignore false negatives (missed positive cases), and if you focus only on recall, you could have many false positives. The F1-score provides a balanced assessment by combining both metrics. The F Score is a Harmonic Mean, which means that it gives additional consideration to the effect of Lower Values (1/ sum (1/x)). Using travel time as an example, if you travelled 20 Miles (10 Miles at 30 MPH and 10 Miles at 60 MPH) your harmonic mean would be 36, not 45.</li>\n",
       "      <li>2 * [(Precision*Recall)/(Precision+Recall)]</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Precision/Recall Trade-off</li>\n",
       "    <ul>\n",
       "      <li>The precision–recall trade-off arises when adjusting a classifier’s decision threshold. If you lower the threshold, the model flags more positives, which increases recall but risks misclassifying more negatives as positives, reducing precision. If you raise the threshold, the model becomes stricter, so precision improves but recall drops because more true positives are missed. In essence, you can’t usually maximize both at once, so the right balance depends on whether false positives or false negatives are more costly in your context.</li>\n",
       "      <li>Not an absolute, edge cases can contradict.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>ROC Curve</li>\n",
       "    <ul>\n",
       "      <li>plot that shows how well a binary classifier distinguishes between classes at different decision thresholds. On the x-axis you have the false positive rate (FPR), and on the y-axis the true positive rate (TPR, or recall). By sweeping the threshold from 0 to 1, the curve traces out the trade-off between catching positives and accidentally flagging negatives.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>AUC Curve</li>\n",
       "    <ul>\n",
       "      <li>AUC (Area Under the Curve) measures a model’s ability to distinguish between classes by summarizing the performance of the ROC (Receiver Operating Characteristic) curve. It represents the probability that the model ranks a randomly chosen positive example higher than a randomly chosen negative one. AUC ranges from 0.5 (no better than chance) to 1.0 (perfect separation), with higher values indicating better classification performance.\n",
       "AUC is primarily a binary classification metric, so when using against a multivariate challenge must determine how to score, OVR, OVO. One Vs Rest, One vs One. Using MNist as an example, OVR evaluates 10 0 vs (1,2,3,4,5,6,7,8,9), 1 vs () ... etc, Where OVO measures 45 0 Vs 1, 0 vs 2, etc..</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>ROC vs Precision Recall Curve</li>\n",
       "    <ul>\n",
       "      <li>ROC: “Out of all the negatives, how many did I mess up?” → can hide problems when negatives vastly outnumber positives.\n",
       "PR: “Out of the people I said were positive, how often was I right?” → directly answers the question when positives are rare.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>10. Data Availability</h4>\n",
       "<ul>\n",
       "  <ul><li>Is the Data Complete?</li>\n",
       "    <ul>\n",
       "      <li>If No, Do I have enough data to correct?</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>11. Data Quality</h4>\n",
       "<ul>\n",
       "  <ul><li>Does the data appear to be Logically Consistent?</li>\n",
       "    <ul>\n",
       "      <li>Does it meet Normality, Variance</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>12. Decision Making</h4>\n",
       "<ul>\n",
       "  <ul><li>Criteria</li>\n",
       "    <ul>\n",
       "      <li>How do the criteria compare?</li>\n",
       "      <li>Remember, it is important to know what Big Decisions which occur less frequently need differnet criteria for both decisions and measurement.</li>\n",
       "      <li>What is the criteria being used other than make this decision?</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Emotions</li>\n",
       "    <ul>\n",
       "      <li>Our emotions, preconceptions, political affiliations badly wrap the way we interpret evidence.</li>\n",
       "      <li>Political decisions shape what statistics we gather and share and what we ignore and conceal.</li>\n",
       "      <li>Doubt is our product.</li>\n",
       "      <li>Don’t convince smokers cigarettes aren’t bad, just that the evidence which suggested they were dangerous was questionable.</li>\n",
       "      <li>Biases appear negatively. People can argue against what they don’t like.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Statistics</li>\n",
       "    <ul>\n",
       "      <li>Statistics show us things we can’t see in any other way.</li>\n",
       "      <li>We depend on reliable data to shape our decisions. Often only collect in the face of adversity.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Confusion</li>\n",
       "    <ul>\n",
       "      <li>Offering more choices sometimes motivates and sometimes demotivates people.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>13. Review Variable</h4>\n",
       "<ul>\n",
       "  <ul><li>Why. What am I trying to understand. Variable itself. Variables Relationship to Target, Variables Relationship to other variables. Data Quality.Correlation</li>\n",
       "    <ul>\n",
       "      <li>Historgram, Scatter Plot, Correlation Matrix, Skewness, Linearity, non linearity, Independence, normal distribution, extremes, scaling,</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>14. MLOps</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>15. CI/CD</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>16. Hypothesis Testing</h4>\n",
       "<ul>\n",
       "  <ul><li>Design</li>\n",
       "    <ul>\n",
       "      <li>HARK is an acronym for Hypothesizing After Results Known.</li>\n",
       "      <li>Standard statistical tests assume that the researcher made the choice before collecting the data, then collected data, then ran the test. If the researcher ran several tests, then made a choice, flukes are vastly more likely.</li>\n",
       "      <li>For obvious reasons, this particular flavor of survivorship bias is called “publication bias.” Interesting findings are published; non-findings, or failures to replicate previous findings, face a higher publication hurdle.</li>\n",
       "      <li>Out of all the possible studies that could have been conducted, it’s reasonable to guess that the journal was interested only in the ones that demonstrated precognition.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>17. Design Principles</h4>\n",
       "<ul>\n",
       "  <ul><li>Lessons Learnt</li>\n",
       "    <ul>\n",
       "      <li>Sample error reflects the risk that, purely by chance, a randomly chosen sample of opinions does not reflect the true views of the population. The “margin of error” reported in opinion polls reflects this risk, and the larger the sample, the smaller the margin of error. A thousand interviews is a large enough sample for many purposes, and during the 1936 election campaign Mr. Gallup is reported to have conducted three thousand interviews.</li>\n",
       "      <li>Sampling error is when a randomly chosen sample doesn’t reflect the underlying population purely by chance; sampling bias is when the sample isn’t randomly chosen at all. George Gallup took pains to find an unbiased sample because he knew that was far more important than finding a big one.</li>\n",
       "      <li>Kate Crawford, a researcher at Microsoft, has assembled many examples of when N = All assumptions have led people astray. When Hurricane Sandy hit the New York City area in 2012, researchers published an analysis of data from Twitter and a location-based search engine, Foursquare, showing that they could track a spike in grocery shopping the day before and a boom for bars and nightclubs the day after. That’s fine, as far as it goes—but those tweets about the hurricane were disproportionately from Manhattan, whereas areas such as Coney Island had been hit much harder. In fact, Coney Island had been hit so hard the electricity was out—that was why nobody there was tweeting—while densely populated and prosperous Manhattan was unusually saturated with smartphones, at least by 2012 standards, when they were less ubiquitous than today. To</li>\n",
       "      <li>who respond to Literary Digest surveys” differ in some consistent way from “people who vote in elections.” Google Flu Trends captured every Google search, but not everybody who gets the flu turns to Google. Its accuracy depended on “people with flu who consult Google about it” not being systematically different from “people with flu.”</li>\n",
       "      <li>Just like people, algorithms are neither trustworthy nor untrustworthy as a general class. Just as with people, rather than asking, “Should we trust algorithms?” we should ask, “Which algorithms can we trust, and what can we trust them to do?”\n",
       "</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>18. Data Structure</h4>\n",
       "<ul>\n",
       "  <ul><li>How do I handle the question of Imputation, Standardization, Distribution, Transformations, Feature Engineering, Is Target Available? Skewness, Linearity?</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>19. Lifecycle</h4>\n",
       "<ul>\n",
       "  <ul><li>Kubernetes Handles</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>20. Access Control</h4>\n",
       "<ul>\n",
       "  <ul><li>Role Based (IAM)</li>\n",
       "    <ul>\n",
       "      <li>The more we know about the motivations and incentives of decision makers, the more we can control the communication. </li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>21. Infrastructure</h4>\n",
       "<ul>\n",
       "  <ul><li>CPU, Memory, Disk, Network</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>22. Metrics</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>23. Centralized Logging</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>24. Exteneral DNS</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>25. SSL/ TLS Certificates</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>26. Warnings, Notifications (Metrics Based)</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>27. Cost Management</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>28. VIsualization</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>29. Load Balancer</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>30. Overview</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>31. Approaches</h4>\n",
       "<ul>\n",
       "  <ul><li>Supervised Learning</li>\n",
       "</ul>\n",
       "  <ul><li>Unsupervised Learning</li>\n",
       "</ul>\n",
       "  <ul><li>SemiSupervised Learning</li>\n",
       "</ul>\n",
       "  <ul><li>Reinforcement Learning</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>32. Optimization</h4>\n",
       "<ul>\n",
       "  <ul><li>General</li>\n",
       "    <ul>\n",
       "      <li>In 9 dimensions, might not be global minimum, a series of good solutions.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Requirements</li>\n",
       "    <ul>\n",
       "      <li>Small change hypothesis must hold.  Example Weather does not hold. Chaos.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Reminders</li>\n",
       "    <ul>\n",
       "      <li>Models will cheat. Self-driving car, multi lane highway or blur.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Approaches</li>\n",
       "    <ul>\n",
       "      <li>L1 Regularization, L2 Regularization</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Goal</li>\n",
       "    <ul>\n",
       "      <li>What are you attempting to maximize, Training Performance, or ability to Generalize.  Strive to avoid overfitting, maximize ability to Generalize.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>33. Evaluation</h4>\n",
       "<ul>\n",
       "  <ul><li>Cross Validation</li>\n",
       "    <ul>\n",
       "      <li>Splitting Data into K Folds.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Stratified Cross Validation</li>\n",
       "    <ul>\n",
       "      <li>Splitting Data into statistically represented folds. If you can assume data is IID, then generally would not be required or benefitical, however it might be determined necessary and thus can be considered.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Confusion Matrix</li>\n",
       "</ul>\n",
       "  <ul><li>Accuracy</li>\n",
       "    <ul>\n",
       "      <li>Generally not a preferred metric, Especially as it relates to Balance Datasets, where picking the underrepresented class is always a challenge.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Decision Scores</li>\n",
       "    <ul>\n",
       "      <li>Scores from the Actual Decision Boundary. Provides Opportunity to Adjust position of Precsion/ Recall Trade Off.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>predict_proba</li>\n",
       "    <ul>\n",
       "      <li>Specifcally for Decision Trees</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>34. Concepts</h4>\n",
       "<ul>\n",
       "  <ul><li>Key Terms</li>\n",
       "    <ul>\n",
       "      <li>#### Need to Automate this #####\n",
       "Baggin, Kernel Trick, Momentum, Gradient Propogation, Batch Normalization, Residual Connections, Overfit, Underfit, Bias, Variance, Bias Variance Trade. Off, Depthwise Seperable Connections, Loss Function, Polynominal Regression, Elastic Net, Early Stopping, </li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>35. Models</h4>\n",
       "<ul>\n",
       "  <ul><li>Models</li>\n",
       "    <ul>\n",
       "      <li>Also used as a regularization, sometimes only as regularization. Only added during Training. Model Weights Close to 0.L2</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Linear Regression</li>\n",
       "    <ul>\n",
       "      <li>Closed Form Solution. Python Linear Regression utilizies Pseudoinverse, which is computationally less expensive. However, it is Big O 2² to 2³, which means doubling features increases computations complexity 4 times.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Reinforcement Learning</li>\n",
       "</ul>\n",
       "  <ul><li>Convolutional Neural Network</li>\n",
       "</ul>\n",
       "  <ul><li>Large Language Models</li>\n",
       "</ul>\n",
       "  <ul><li>Recurrent Neural Network</li>\n",
       "</ul>\n",
       "  <ul><li>Transformers</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>36. Deep Learning</h4>\n",
       "<ul>\n",
       "    <ul>\n",
       "      <li>Helps to Automate Feature Engineering</li>\n",
       "</ul>\n",
       "  <ul><li>Evolution</li>\n",
       "    <ul>\n",
       "      <li>Historical Issue of gradient propogration where signal would fade through the layers was solved through, better activation functions, optimization schemes and weight initializing.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Framework</li>\n",
       "    <ul>\n",
       "      <li>Core building block is the layer, a filter which creates a representation, can chain simple layers into a complex one. </li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Model</li>\n",
       "    <ul>\n",
       "      <li>Model starts with determining the number of layers, type of connections, and activation functions. Once model is defined, to comile need to determine, Optimizer, Loss Function and Metric.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>37. Lesson Learnt</h4>\n",
       "<ul>\n",
       "  <ul><li>Always Have a Simple Baseline</li>\n",
       "    <ul>\n",
       "      <li>Need a reference to test performance of the model. If dataset is imbalanced, have simple guess. If temperature, guess trailing average, etc, find something simple, often very difficutl to beat, and juice might not be worth the squeeze.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>38. Downside</h4>\n",
       "<ul>\n",
       "  <ul><li>Simplicity Not Assumed</li>\n",
       "    <ul>\n",
       "      <li>ML can't look naively for a simple common sense solution. Unless feature engineered or hard coded, model will always just plug forward. </li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>39. Feature Selection</h4>\n",
       "<ul>\n",
       "  <ul><li>Avoid Ambigious Features</li>\n",
       "    <ul>\n",
       "      <li>Is a Banana Ripe? Who Says So.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Careful if Genuinely Random</li>\n",
       "    <ul>\n",
       "      <li>Atmospheric Change</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Data Quality Input</li>\n",
       "    <ul>\n",
       "      <li>MNIST performance when tilting Images, of adding noise.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Relevance</li>\n",
       "    <ul>\n",
       "      <li>Any data set can be optimized, even when there is NO relation. It will generalize.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>40. Tool</h4>\n",
       "<ul>\n",
       "  <ul><li>PyTorch</li>\n",
       "    <ul>\n",
       "      <li>Favored in research and academia because of its eager execution (you can run and debug step by step like normal Python)</li>\n",
       "      <li>Pythonic, intuitive, and integrates seamlessly with libraries like NumPy</li>\n",
       "      <li>Great for experimentation, quick prototyping, and custom model development.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>TensorFlow</li>\n",
       "    <ul>\n",
       "      <li>Stronger in production/industry, especially with enterprise systems.</li>\n",
       "      <li>TensorFlow Serving, TensorFlow Lite (for mobile/edge), and TensorFlow.js (for web) make it great for deployment</li>\n",
       "      <li>High-level, user-friendly API for building models quickly.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>41. Derivative</h4>\n",
       "<ul>\n",
       "  <ul><li>Rate of Change</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>42. Derivative (second order)</h4>\n",
       "<ul>\n",
       "  <ul><li>Acceleration of Rate of Change.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>43. Instantiate the Model</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>44. Calculate the Loss</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>45. Calculate the Gradient</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>46. Change Loss</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>47. Measure Change.</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>48. Confirm Problem</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>49. Identify ML Approach</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>50. Obtain Data</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>51. Process Data to ML Model</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>52. Standardize Data</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>53. Train Model</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>54. Test Model</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>55. Tweak Model</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>56. Validate Model</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>57. Regression</h4>\n",
       "<ul>\n",
       "  <ul><li>Description</li>\n",
       "    <ul>\n",
       "      <li>DESCRIPTION</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>58. Classification</h4>\n",
       "<ul>\n",
       "  <ul><li>Binary</li>\n",
       "    <ul>\n",
       "      <li>DESCRIPTION</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Multiclass</li>\n",
       "    <ul>\n",
       "      <li>Binary vs Multiclass.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Evaluation</li>\n",
       "    <ul>\n",
       "      <li>Must consider differences in Binary Vs Multiclass. \n",
       "Certain Models are only designed for Binary Classification, they can be extended, but must understand model functionality and purpose.</li>\n",
       "      <li>One vs One. Create a series of Classifiers for every Binary Comparison. Results in best overall performance, however number of classifiers quickly increases,  N × (N – 1) / 2. In MNIST, this means 45 Classifers.</li>\n",
       "      <li>One Vs Rest. Create a One Vs series of possible One vs All Classifiers for known possible Classes, run each time, pick the highest score.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>59. Dimensionality Reduction</h4>\n",
       "<ul>\n",
       "  <ul><li>Dimensionality Reduction</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>60. Evaluate Bias</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>61. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Learning Type, Problem Type, Explanability, In- Out Memory, Training Speed, PRedicition Speed, Alog, Loss Function, Loss Objective.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>62. Convolutional Neural Networds</h4>\n",
       "<ul>\n",
       "  <ul><li>General Description</li>\n",
       "    <ul>\n",
       "      <li>Convolutional Neural Networks, or CNNs, are a type of machine learning model designed to automatically detect meaningful patterns in data by applying filters that scan across the input. While they are most commonly associated with image processing, CNNs can also be applied effectively to structured data with spatial or temporal patterns, such as time series. In time series data, CNNs act like pattern detectors that can identify trends, spikes, or repeated behaviors over short intervals of time. These models use one-dimensional convolutions, which apply filters along the time axis, allowing the model to capture localized patterns regardless of where they occur in the sequence.\n",
       "\n",
       "For classification tasks, a CNN can learn to recognize short-term patterns in the time series that are associated with specific labels, such as detecting equipment faults or classifying sensor behavior. For regression tasks, CNNs can extract features from recent data windows and use those to predict future values, like forecasting temperature, stock prices, or energy usage. CNNs are often preferred over recurrent models (like LSTMs) when the relationships in the data are more local or when training speed is important, as they tend to be faster and require fewer parameters.\n",
       "\n",
       "In summary, convolutional models are a flexible and efficient tool for analyzing time-based data. They are particularly useful when you want to detect local patterns in a sequence and use those to classify or predict outcomes, even in non-image domains.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>63. Neural Networks</h4>\n",
       "<ul>\n",
       "  <ul><li>As Features</li>\n",
       "    <ul>\n",
       "      <li>When a CNN processes input (like a time series), it transforms the raw data into higher-level feature representations through its convolutional layers. These representations capture meaningful patterns — like trends, spikes, or periodicities — that are often more informative than the raw data itself.\n",
       "\n",
       "You can extract these intermediate outputs and use them as features for other models</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Translation invariance</li>\n",
       "    <ul>\n",
       "      <li>Translation invariance means that a model, like a CNN, can recognize a pattern regardless of where it appears in the input—for example, detecting a spike in a time series whether it occurs early or late in the sequence</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Max Pooling</li>\n",
       "    <ul>\n",
       "      <li>A max pooling layer in machine learning reduces the size of the input by sliding a window over it and keeping only the maximum value in each window, which helps the model focus on the most important features while reducing computation and increasing robustness to small shifts or noise.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Long Short Term Memory</li>\n",
       "    <ul>\n",
       "      <li>An LSTM (Long Short-Term Memory) is a special kind of neural network used for sequential data, like time series or text, that can learn and remember long-term patterns by using a memory cell and gates that control the flow of information—this allows it to keep important information from earlier in the sequence and forget what’s not relevant, making it well-suited for tasks like prediction, translation, or anomaly detection over time.\n",
       "\n",
       "Traditional recurrent neural networks (RNNs) struggle to retain information over long sequences—they remember the recent (short-term) past but tend to \"forget\" earlier (long-term) information. LSTMs were designed to fix this by introducing a memory cell that can store information over long durations, while also using gates to manage short-term updates</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Feedforward</li>\n",
       "    <ul>\n",
       "      <li>Feedforward refers to a type of neural network where information moves in one direction only—from the input layer, through any hidden layers, to the output layer—without any loops or feedback connections. Each layer passes its output to the next, and the model does not retain memory of previous inputs, making it well-suited for tasks like classification or regression on fixed-size data.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Vanishing Gradient</li>\n",
       "    <ul>\n",
       "      <li>The vanishing gradient problem occurs during the training of deep neural networks when gradients used to update the model’s weights become very small as they are backpropagated through many layers. This causes earlier layers to learn very slowly or not at all, because their weights are barely updated, making it difficult for the model to capture long-range dependencies—especially in deep networks and recurrent architectures like RNNs.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Recurrent Dropout</li>\n",
       "    <ul>\n",
       "      <li>Recurrent dropout is a form of dropout applied within the recurrent connections of RNNs or LSTMs, where the same dropout mask is used at each time step to prevent overfitting without disrupting the sequence memory.\n",
       "\n",
       "In regular dropout, random neurons are \"dropped\" independently at each time step, which can break the model’s ability to learn time-based patterns. Recurrent dropout fixes this by applying the same mask consistently across time, helping the model remain stable while still benefiting from regularization.\n",
       "\n",
       "This technique is especially useful for LSTM and GRU networks working on small or noisy sequential datasets.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Stacking Recurrent Layers</li>\n",
       "</ul>\n",
       "  <ul><li>Bidirectional Recurrent Layers</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>64. Natural Language Processing</h4>\n",
       "<ul>\n",
       "  <ul><li>Definition</li>\n",
       "    <ul>\n",
       "      <li>Natural Language - Opposed to Created Language, backwards looking, usage determines structure and format. Differs from created languages, such as computer languages.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Background</li>\n",
       "    <ul>\n",
       "      <li>Originally expert systems, morphed into RNN and parametric models, towards Transformers.</li>\n",
       "      <li>Models do not understand language, they simply predict based on what has been done or said before and how statitically similiar it is likely to be.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>65. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Reciprocity</li>\n",
       "    <ul>\n",
       "      <li>People say yes to those who said yes to them. Invest in those you want to invest in you. Be the first to give, and do it personalized and unexpected.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>66. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Scarcity</li>\n",
       "    <ul>\n",
       "      <li>People want what is in short supply. Tell them what they gain, what is unique and what they stand to lose.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>67. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Authority</li>\n",
       "    <ul>\n",
       "      <li>Make sure people know your expertise. Don’t be arrogant. Specifically if they don’t know you, get a third party to introduce you.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>68. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Consistency</li>\n",
       "    <ul>\n",
       "      <li>People like to be consistent when they take a public opinion. Get them to take a small step first, then they will be willing to take a big step in the future.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>69. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Liking</li>\n",
       "    <ul>\n",
       "      <li>People want to be liked and work with people who like them. We like people who are similar to us, compliment us, cooperate with us</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>70. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Consensus</li>\n",
       "    <ul>\n",
       "      <li>People follow the leads of others.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>71. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Ethos – Character</li>\n",
       "    <ul>\n",
       "      <li>You are the character in your story. You need to establish your trustworthiness. Someone else always introduces the speaker.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>72. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Logos – Reason</li>\n",
       "    <ul>\n",
       "      <li>Articulate why your audience should care</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>73. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Pathos – Emotion</li>\n",
       "    <ul>\n",
       "      <li>Tell stories, make people feel it.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>74. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Metaphor</li>\n",
       "    <ul>\n",
       "      <li>Can help people understand. Need to take care, as they don’t always work. Specifically as they’re culturally specific.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>75. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Brevity</li>\n",
       "    <ul>\n",
       "      <li>Keep it brief, Bottom Line Up Front. Bluf.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>76. Story Telling</h4>\n",
       "<ul>\n",
       "  <ul><li>Steps to a Better Story</li>\n",
       "    <ul>\n",
       "      <li>1) Once Upon a Time..</li>\n",
       "      <li>2) And Every Day..</li>\n",
       "      <li>3) Until One Day..</li>\n",
       "      <li>4) And Because of This..</li>\n",
       "      <li>5) And Because of This..</li>\n",
       "      <li>6) Until Finally..</li>\n",
       "      <li>7) And Ever Since That Day..</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Fundamental Tenants</li>\n",
       "    <ul>\n",
       "      <li>What is your character good at; comfortable with? Throw the polar opposite at them. Challenge them. How do they deal?</li>\n",
       "      <li>Come up with your ending before you figure out your middle. Endings are hard; get yours working up front.</li>\n",
       "      <li>Finish your story; let go of it even if it's not perfect. Move on, and do better next time.</li>\n",
       "      <li>When you're stuck, make a list of what wouldn't happen next; often the material to get you unstuck will show up.</li>\n",
       "      <li>Pull apart the stories you like. What you like in them is a part of you; you've got to recognize it before you can use it.</li>\n",
       "      <li>Putting it on paper lets you start fixing it. If it stays in your head a perfect idea, you'll never share it with anyone.</li>\n",
       "      <li>Discount the first thing that comes to mind, and the second, third, fourth, fifth. Get the obvious out of the way, and surprise yourself.</li>\n",
       "      <li>Give your characters opinions. Passive/malleable might seem likable to you as you write, but it's poison to the audience.</li>\n",
       "      <li>Why must you tell this story? What's the belief that your story feeds off of? That's the heart of it.</li>\n",
       "      <li>If you were your character, in this situation, how would you feel? Honesty lends credibility to unbelievable situations.</li>\n",
       "      <li>What are the stakes? Give us reason to root for the character. What happens if they don't succeed? Stack the odds against the character.</li>\n",
       "      <li>No work is ever wasted. If it's not working, let go and move on; it'll come back around to be useful later.</li>\n",
       "      <li>You have to know yourself and know the difference between doing your best and fussing (story is testing, not refining).</li>\n",
       "      <li>Coincidences to get characters into trouble are great; coincidences to get them out of it are cheating.</li>\n",
       "      <li>Exercise: Take the building blocks of a movie you dislike. How would you rearrange them into something you do like?</li>\n",
       "      <li>You have to identify with your situation/characters; you can't just write something \"cool.\" What would make you act that way?</li>\n",
       "      <li>What's the essence of your story? The most economical telling of it? If you know that, you can build out from there.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>77. Command</h4>\n",
       "<ul>\n",
       "  <ul><li>Assert Statement</li>\n",
       "    <ul>\n",
       "      <li>Detemine if something is true, if not generate Error.</li>\n",
       "      <li>assert sys.version_info >= (3, 7)</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>78. General Principles</h4>\n",
       "<ul>\n",
       "  <ul><li>Nature of Individuals</li>\n",
       "    <ul>\n",
       "      <li>Need to contend with the strategic nature of people - A person might try and \"fool\" you to think they are someone or something that they are not. A piece of steel is not going to if you're looking to understand how it performs under various types of stress.</li>\n",
       "      <li>The ability to explain, describe, or predict the behavior of a system based on its underlying structure, mechanisms, or causal relationships. Involves forming mental models, theories, or hypotheses about how things work. Often grounded in observation, reasoning, or data analysis. Example: Understanding that increasing interest rates tends to reduce inflation through decreased spending.\n",
       "\n",
       "We can can decide without understanding. Decision-making can be pragmatic, not always rooted in deep understanding\n",
       "We can Understand, without control - Tornado.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>79. Definition</h4>\n",
       "<ul>\n",
       "  <ul><li>Definition</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>80. Provisioning Data</h4>\n",
       "<ul>\n",
       "  <ul><li>fetch_*</li>\n",
       "    <ul>\n",
       "      <li>Download Real Datasets (fetch_openml())</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>load_*</li>\n",
       "    <ul>\n",
       "      <li>Download Toy Dataset</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>make_*</li>\n",
       "    <ul>\n",
       "      <li>Generate Fake Datasets for Testing</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>81. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Identify your Characters</li>\n",
       "    <ul>\n",
       "      <li>You admire a character for trying more than for their successes.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>82. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Identify your Plot</li>\n",
       "    <ul>\n",
       "      <li>Keep in mind what's interesting to you as an audience member, not what's fun to do as a writer (they can be very different).</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>83. nan</h4>\n",
       "<ul>\n",
       "  <ul><li>Identify your Theme</li>\n",
       "    <ul>\n",
       "      <li>Trying for theme is important, but you won't see what the story is actually about until you're at the end of it; now rewrite the story.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>84. Tensor</h4>\n",
       "<ul>\n",
       "  <ul><li>Properties</li>\n",
       "    <ul>\n",
       "      <li>Number of Dimensions (ndim)</li>\n",
       "      <li>Shape</li>\n",
       "      <li>Data Type</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Dimensions</li>\n",
       "    <ul>\n",
       "      <li>Rank 2: Vector (Sample, Features). A grouping of examples, with same features as values, basic Linear Regression Data.</li>\n",
       "      <li>Rank 3: Matrix(Sample, Timestamp, Features). Traditional Time Series Data (Also grey scale images). A grouping of examples with a sequence and features.</li>\n",
       "      <li>Rank 4. (Sample, Height, Width, Channel). Grid of 2D pixels, where each pixel is represented by a vector of values (channel).</li>\n",
       "      <li>Rank 5. Video (Sample, Frame, Height, Width, Channel) where each sample is a sequence (of length frames) of images.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Definition</li>\n",
       "    <ul>\n",
       "      <li>Container for Data</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Scalar</li>\n",
       "    <ul>\n",
       "      <li>Rank 0 Tensor</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Vector</li>\n",
       "    <ul>\n",
       "      <li>Rank 1 Tensor</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Matrix</li>\n",
       "    <ul>\n",
       "      <li>Rank 2 Tensor, array of vectors</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>[0,1,2,3,4]</li>\n",
       "    <ul>\n",
       "      <li>5D Tensor.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>85. Base Properties</h4>\n",
       "<ul>\n",
       "  <ul><li>Tensors (Data)</li>\n",
       "</ul>\n",
       "  <ul><li>Tensor Operations (Relu, Matmul, etc)</li>\n",
       "</ul>\n",
       "  <ul><li>Backpropagation</li>\n",
       "</ul>\n",
       "  <ul><li>Layers</li>\n",
       "</ul>\n",
       "  <ul><li>Loss Function</li>\n",
       "</ul>\n",
       "  <ul><li>Optimizer</li>\n",
       "</ul>\n",
       "  <ul><li>Metrics</li>\n",
       "</ul>\n",
       "  <ul><li>Training Loop</li>\n",
       "</ul>\n",
       "  <ul><li>Transfer Learning</li>\n",
       "    <ul>\n",
       "      <li>Using .layers from trained model can, utilize previously trained Neurons/Nodes as inputs for different models. </li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>86. Structures</h4>\n",
       "<ul>\n",
       "  <ul><li>Densely (Fully) Connected Layers</li>\n",
       "</ul>\n",
       "  <ul><li>Recurrent Layers</li>\n",
       "</ul>\n",
       "  <ul><li>Convolutional Layers</li>\n",
       "</ul>\n",
       "  <ul><li>Sequential</li>\n",
       "</ul>\n",
       "  <ul><li>Two Branch Networks</li>\n",
       "</ul>\n",
       "  <ul><li>Multihead Networks</li>\n",
       "</ul>\n",
       "  <ul><li>Residual Connections</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>87. Optimizer</h4>\n",
       "<ul>\n",
       "  <ul><li>SGD (With and without Momentum)</li>\n",
       "</ul>\n",
       "  <ul><li>RMSProp</li>\n",
       "</ul>\n",
       "  <ul><li>Adam</li>\n",
       "</ul>\n",
       "  <ul><li>Adagrad</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>88. Loss</h4>\n",
       "<ul>\n",
       "  <ul><li>CategoricalCrossentropy</li>\n",
       "</ul>\n",
       "  <ul><li>SparseCategoricalCrossentropy</li>\n",
       "</ul>\n",
       "  <ul><li>BinaryCrossentropy</li>\n",
       "</ul>\n",
       "  <ul><li>MeanSquaredError</li>\n",
       "</ul>\n",
       "  <ul><li>KLDivergence</li>\n",
       "</ul>\n",
       "  <ul><li>CosineSimilarity</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>89. Metrics</h4>\n",
       "<ul>\n",
       "  <ul><li>CategoricalAccuracy</li>\n",
       "</ul>\n",
       "  <ul><li>SparseCategoricalAccuracy</li>\n",
       "</ul>\n",
       "  <ul><li>BinaryAccuracy</li>\n",
       "</ul>\n",
       "  <ul><li>AUC</li>\n",
       "</ul>\n",
       "  <ul><li>Precision</li>\n",
       "</ul>\n",
       "  <ul><li>Recall</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>90. Parameters</h4>\n",
       "<ul>\n",
       "  <ul><li>Epochs</li>\n",
       "</ul>\n",
       "  <ul><li>Batch Size</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>91. Model Building</h4>\n",
       "<ul>\n",
       "  <ul><li>Sequential</li>\n",
       "    <ul>\n",
       "      <li>Simplest Method. Define Each Layer Directly, or via .add()</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Functional API</li>\n",
       "</ul>\n",
       "  <ul><li>Model Subclassing</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>92. Callbacks</h4>\n",
       "<ul>\n",
       "  <ul><li>Definition</li>\n",
       "    <ul>\n",
       "      <li>Object that is passed into the model in the .fit() which provides additional control.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Model Checkpointing</li>\n",
       "</ul>\n",
       "  <ul><li>Early Stopping</li>\n",
       "</ul>\n",
       "  <ul><li>Learning Rate Scheduler</li>\n",
       "</ul>\n",
       "  <ul><li>Dynamically Adjust Parameters</li>\n",
       "</ul>\n",
       "  <ul><li>Promote and Articulate training metrics during training.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>93. TF-Agents</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>94. TFX</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>95. TensorFlow Serving</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>96. TensorFlow Hub</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>97. Time Series</h4>\n",
       "<ul>\n",
       "  <ul><li>Predicition</li>\n",
       "    <ul>\n",
       "      <li>It is important to remember, what you are trying to predict and what scale.</li>\n",
       "      <li>Much Easier to predict over longer time periods. Months Vs Days Vs Hours Vs Minute.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Classification</li>\n",
       "</ul>\n",
       "  <ul><li>Event Detection</li>\n",
       "</ul>\n",
       "  <ul><li>Anomoly Detection</li>\n",
       "</ul>\n",
       "  <ul><li>Data Preperation</li>\n",
       "    <ul>\n",
       "      <li>It is important to use Newer Validation and Testing data opposed to training, as you're trying to predict the future (not the past).</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>98. MySQL</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>99. PostgreSQL</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>100. GCP</h4>\n",
       "<ul>\n",
       "  <ul><li>BigQuery</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>101. AWS</h4>\n",
       "<ul>\n",
       "  <ul><li>S3</li>\n",
       "</ul>\n",
       "  <ul><li>Redshift</li>\n",
       "</ul>\n",
       "  <ul><li>Lambda</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>102. Power BI</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>103. Tableau</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>104. GCP</h4>\n",
       "<ul>\n",
       "  <ul><li>Looker</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>105. Python</h4>\n",
       "<ul>\n",
       "  <ul><li>Matplotlib</li>\n",
       "</ul>\n",
       "  <ul><li>Plotly</li>\n",
       "</ul>\n",
       "  <ul><li>Pandas</li>\n",
       "</ul>\n",
       "  <ul><li>Numpy</li>\n",
       "</ul>\n",
       "  <ul><li>Poetry</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>106. Docker</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>107. Kubernetes</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>108. Airflow</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>109. FastApi</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>110. Definition</h4>\n",
       "<ul>\n",
       "  <ul><li>Common Type of Problems</li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>111. Common Types of Problems</h4>\n",
       "<ul>\n",
       "  <ul><li>Literal</li>\n",
       "    <ul>\n",
       "      <li>The Answer is Directly Stated somewhere.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Inferential</li>\n",
       "    <ul>\n",
       "      <li>The Answer is Implied by looking at multiple facts</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Evaluative</li>\n",
       "    <ul>\n",
       "      <li>The Answer requires judgement, reasoning or evaluation. Specifically, where data is not available and someone is going to make a decision based on their interpretation.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Analytical</li>\n",
       "    <ul>\n",
       "      <li>Based on Data, Fact and Causation.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>Creative</li>\n",
       "    <ul>\n",
       "      <li>Innovation, Future Design, where Data might suggest something, but ultimately the Correct solution does not currently exist.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>112. Visualizations</h4>\n",
       "<ul>\n",
       "  <ul><li>How people Interpret</li>\n",
       "    <ul>\n",
       "      <li>Christ at Emmaus - Feelings can trump expertise. We find ways to dismiss evidence we don’t like, when it supports our preconceptions we are less likely to look for flaws.</li>\n",
       "</ul>\n",
       "</ul>\n",
       "  <ul><li>What Am I looking at.</li>\n",
       "    <ul>\n",
       "      <li>The bigger display with a wider array of jams attracted more customers but very few of them actually bought jam. The display that offered fewer choices inspired more sales.[1] The counterintuitive result went viral—it hit a sweet spot. People respond better to fewer choices!</li>\n",
       "      <li>Has the performance of the algorithm been assessed rigorously—for example, by running a randomized trial to see if people make better decisions with or without algorithmic advice? Have independent experts been given a chance to evaluate the algorithm? What have they concluded? We should not simply trust that algorithms are doing a better job than humans, nor should we assume that if the algorithms are flawed, the humans would be flawless.</li>\n",
       "      <li>When someone tells a joke poorly it is more likely than not that they have forgotten to convey an important piece of information in the set up that makes the punch line funny. So it seems the joke is in the set up and not the punch line.</li>\n",
       "    </ul>\n",
       "  </ul>\n",
       "</ul>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from Organization import D_Notes_Reader\n",
    "\n",
    "D_Notes_Reader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892e3ce-f23f-4746-98e1-0d75642208b7",
   "metadata": {},
   "source": [
    "#### 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9f99c-0afe-4385-877a-4f45ccfaa004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c4e052e-109f-43e3-bd69-8021ad5f9843",
   "metadata": {},
   "source": [
    "#### 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248b630-6bab-480d-9870-c390c7e58dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b079f028-ce5f-4d98-a868-2e6d56cd59b6",
   "metadata": {},
   "source": [
    "#### 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b47cc-a4b1-4e50-ac5c-59b568eeb474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "360280b3-cfbd-4957-ad11-0329b8e48426",
   "metadata": {},
   "source": [
    "#### 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229f106-4d2f-42f2-af58-f6eba2ffe9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aa5bc5b-959d-4e0a-a67c-97cad64b843f",
   "metadata": {},
   "source": [
    "#### 9) BackUpGoogleSheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e6a5078-0f3f-4f6d-abc9-6c462eabfcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back Up Saved, /Users/derekdewald/Documents/Python/Github_Repo/CSV Backup Files/ProcessSheet\n",
      "Back Up Saved, /Users/derekdewald/Documents/Python/Github_Repo/CSV Backup Files/DataDashboardSheet\n",
      "Back Up Saved, /Users/derekdewald/Documents/Python/Github_Repo/CSV Backup Files/CodingDDashboard\n",
      "Counld Not Print Record 3\n",
      "Back Up Saved, /Users/derekdewald/Documents/Python/Github_Repo/CSV Backup Files/Mapping Sheet\n",
      "Back Up Saved, /Users/derekdewald/Documents/Python/Github_Repo/CSV Backup Files/Notes\n",
      "Counld Not Print Record 6\n",
      "Counld Not Print Record 7\n",
      "Counld Not Print Record 8\n",
      "Counld Not Print Record 9\n",
      "Counld Not Print Record 10\n",
      "Counld Not Print Record 11\n",
      "Counld Not Print Record 12\n",
      "Counld Not Print Record 13\n",
      "Counld Not Print Record 14\n",
      "Counld Not Print Record 15\n",
      "Counld Not Print Record 16\n",
      "Back Up Saved, /Users/derekdewald/Documents/Python/Github_Repo/CSV Backup Files/SKlearn Model Parameters\n",
      "Back Up Saved, /Users/derekdewald/Documents/Python/Github_Repo/CSV Backup Files/Sklearn Models\n",
      "Counld Not Print Record 19\n",
      "Counld Not Print Record 20\n"
     ]
    }
   ],
   "source": [
    "from Connections import BackUpGoogleSheets\n",
    "example_dict['BackUpGoogleSheets']=1\n",
    "BackUpGoogleSheets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac5648a-631f-4a45-99e1-da72b3688f25",
   "metadata": {},
   "source": [
    "#### 10) DownloadFilesFromGit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36658a52-052b-4bcd-a727-70d18e69d204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Connections import DownloadFilesFromGit\n",
    "example_dict['DownloadFilesFromGit']=1\n",
    "DownloadFilesFromGit(output_folder='/Users/derekdewald/Documents/Python/Github_Repo/JupyterNotebooks/ArchivePyFunctions/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ee66db0-bf27-476e-9027-828bece2173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Work Through All File Sequentially To Document Correclty, Set Up structure. Etc\n",
    "\n",
    "# Review Files and Ensure Each is In Dict, \n",
    "PythonFunctions['File'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac103a09-b38a-4a00-a350-f16e6e6af362",
   "metadata": {},
   "outputs": [],
   "source": [
    "PythonFunctions[PythonFunctions['File']=='Connections.py']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e9924-6a07-45c9-b6a1-96845233571f",
   "metadata": {},
   "source": [
    "### Iterate through Individual Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c473686f-e893-41a3-a38f-84025ede4aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatch Identified at Record Number: 4\n",
      "['which', 'Compares', '2', 'Functions', 'and', 'determines', 'if', 'they', 'are', 'different.', 'Specifically,', 'it', 'can', 'help', 'to', 'easily', 'Manage', 'Version', 'control', 'of']\n",
      "['to', 'Compare', '2', 'Functions,', 'looking', 'for', 'the', 'first', 'point', 'of', 'differences.', 'Parameters:', 'Returns:', 'Date', 'Created:', 'August', '17,', '2025', 'Date', 'Last']\n",
      "Reconcilation Complete\n"
     ]
    }
   ],
   "source": [
    "from TextFunctions import CompareFunction\n",
    "\n",
    "CompareFunction(CompareFunction,CompareFunctions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680caab2-3519-475b-bbbb-b5b6964fad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompareFunction(func1,func2,additional_records=20):\n",
    "    \n",
    "    '''\n",
    "    Function takes 2 Python Function, uses a thrid function FunctiontoStr to convert them into a List of Values and then iterates through the list\n",
    "    to compare Item by Item Equality. If there is a different it will return the next N variables to help compare where the error is and lead to \n",
    "    reconcilation.\n",
    "    \n",
    "    Parameters:\n",
    "        func1 (Function)\n",
    "        func2 (Function)\n",
    "        additional_records (int)\n",
    "    \n",
    "    Returns:\n",
    "        List with Difference or confirmation via Print output that items reconcile.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    list1 = FunctionToSTR(func1)\n",
    "    list2 = FunctionToSTR(func2)\n",
    "    \n",
    "    length = max(len(list1),len(list2))\n",
    "    \n",
    "    for record in range(0,length):\n",
    "        # Functions should not have the same name. Pass first\n",
    "        if record==1:\n",
    "            pass\n",
    "        elif list1[record]!=list2[record]:\n",
    "            print(f'Mismatch Identified at Record Number: {record}')\n",
    "            print(list1[record:record+additional_records])\n",
    "            print(list2[record:record+additional_records])\n",
    "            \n",
    "            break\n",
    "\n",
    "    print('Reconcilation Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e6c2c-4004-4ed5-b9a2-a932cc74b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ReadTextFile(file_name,encoding='utf-8',print_=None):\n",
    "    \n",
    "    from pathlib import Path\n",
    "    \n",
    "    text = Path(file_name).read_text(encoding=encoding)\n",
    "    \n",
    "    if print_:\n",
    "        for i, line in enumerate(text.splitlines(), 1):\n",
    "            print(f\"{line}\")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "from SharedFolder import ExtractPythonFiles\n",
    "\n",
    "from Organization import JupyterNotebookMarkdown\n",
    "example_dict['D_Notes_Reader'] = 1\n",
    "D_Notes_Reader()\n",
    "\n",
    "from Organization import D_Notes_Reader\n",
    "example_dict['D_Notes_Reader'] = 1\n",
    "D_Notes_Reader()\n",
    "\n",
    "df = ExtractPythonFiles()\n",
    "df.head()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/\")\n",
    "\n",
    "\n",
    "v1 ='/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/'\n",
    "d1 = ReadDirectory(v1)\n",
    "d1\n",
    "\n",
    "def CreateProcessCheckList(process_name=None):\n",
    "\n",
    "    from Connections import GoogleProcessSheetLinks\n",
    "    \n",
    "    df = pd.read_csv(GoogleProcessSheetLinks('link_dict')['ProcessSheet'])\n",
    "\n",
    "    if process_name:\n",
    "        # List of processes which are Aggregated Processes, such that it will iterate through TITLE and return the processes included\n",
    "        # opposed ot the PROCESS.\n",
    "        if process_name in ['D Project Partcipation Guidelines']:\n",
    "            process_name = df[df['Process']=='D Project Partcipation Guidelines']['Title'].tolist()        \n",
    "        else:\n",
    "            process_name = [process_name]\n",
    "        \n",
    "        df = df[df['Process'].isin(process_name)]\n",
    "        \n",
    "    df['Owner'] = \"\"\n",
    "    df['Date Required'] = ''\n",
    "\n",
    "    return df\n",
    "\n",
    "d = CreateProcessCheckList('D Project Partcipation Guidelines')\n",
    "d\n",
    "\n",
    "from SharedFolder import ReadDirectory\n",
    "\n",
    "v2 = '/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/v2'\n",
    "d2 = ReadDirectory(v2)\n",
    "d2\n",
    "\n",
    "from TextFunctions import ReadPythonFiles\n",
    "ReadPythonFiles(file_list=['Connections.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218475e-be32-4e8a-ac39-f621937f054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "example_dict = {}\n",
    "\n",
    "from SharedFolder import ExtractPythonFiles\n",
    "example_dict['ExtractPythonFiles']=1\n",
    "ExtractPythonFiles()\n",
    "\n",
    "\n",
    "from Organization import JupyterNotebookMarkdown\n",
    "\n",
    "df = pd.read_excel('/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/D_Python_Functions.xlsx')\n",
    "\n",
    "example_dict['JupyterNotebookMarkdown']=1\n",
    "JupyterNotebookMarkdown(df,['File','Function Name','Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f5586-8ecf-4c73-ab9c-1d72c7295e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996d8488-950e-4181-8384-90339af75a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e3b0fe-9f75-4860-876b-11bea18bae89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a08ad-06d6-49f0-85b1-f30a35c3cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Connections import NavigateUsingDMap, ParamterMapping\n",
    "\n",
    "example_dict['ProcessSheet']=1\n",
    "ParamterMapping('ProcessSheet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25573094-fd38-4f7c-b17e-b3180b2769e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['NavigateUsingDMap']=1\n",
    "NavigateUsingDMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a2301-0315-41c7-9121-8c5bbe0f54cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a73ee64-2013-4c8f-9654-d38496957c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97553989",
   "metadata": {},
   "source": [
    "### Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3160b98f-a4a5-499e-9e7a-406d53a30b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSets import diabetes_df,iris_df\n",
    "example_dict['diabetes_df']=1\n",
    "example_dict['iris_df']=1\n",
    "\n",
    "display(diabetes_df.head())\n",
    "display(iris_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0a4b9-1379-45b3-8c00-3380041272ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSets import MNIST_SKLEARN\n",
    "example_dict['MNIST_SKLEARN']=1\n",
    "df,z = MNIST_SKLEARN(normalize=False,flatten=True,return_value='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4599fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSets import GenerateFakeMemberDF\n",
    "example_dict['GenerateFakeMemberDF']=1\n",
    "df = GenerateFakeMemberDF(10000,18)\n",
    "df_ = GenerateFakeMemberDF(10000,18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46708939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSets import FakeBaseballstats \n",
    "\n",
    "fake_bb_df = FakeBaseballstats()\n",
    "example_dict['FakeBaseballstats']=1\n",
    "fake_bb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a026af-9f96-4786-965a-fdd2565341a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSets import GenerateSKModelDoc\n",
    "example_dict['GenerateSKModelDoc']=1\n",
    "sklearn_model_df,param_df = GenerateSKModelDoc()\n",
    "display(sklearn_model_df.head()) \n",
    "display(param_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291bf609",
   "metadata": {},
   "source": [
    "## Import Data For Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2692b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSets import diabetes_df\n",
    "example_dict['diabetes_df']=1\n",
    "diabetes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb7f3d9-27d0-48b8-8b2f-cbf5d7a2d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSets import GenerateFakeMemberDF\n",
    "example_dict['GenerateFakeMemberDF']=1\n",
    "d = GenerateFakeMemberDF(2000,18)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb363f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c1c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Connections import DownloadFilesFromGit,BackUpGoogleSheets,ParamterMapping\n",
    "example_dict['ParamterMapping']=1\n",
    "ParamterMapping()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff6d05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Date Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A Number of Function related to Date Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DateFunctions import MonthSelector,CreateMonthList,generate_day_list\n",
    "example_dict['MonthSelector']=1\n",
    "MonthSelector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['CreateMonthList']=1\n",
    "CreateMonthList(0,3,sort_ascending=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['generate_day_list']=1\n",
    "generate_day_list(datetime.datetime(2025,4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041b2d2",
   "metadata": {},
   "source": [
    "## DF Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d569076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DFProcessing import DataFrameColumnObservations,CombineLists,FilterDataframe,TransposePivotTable\n",
    "example_dict['DataFrameColumnObservations']=1\n",
    "DataFrameColumnObservations(fake_bb_df,['Player','Opponent'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ebf078",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['CombineLists']=1\n",
    "CombineLists(fake_bb_df['Player'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f56565",
   "metadata": {},
   "outputs": [],
   "source": [
    "CombineLists([fake_bb_df['Player'].unique().tolist(),fake_bb_df['Opponent'].unique().tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e399f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['FilterDataframe']=1\n",
    "FilterDataframe(fake_bb_df,\n",
    "               binary_exclude={'Player':\"Roger Dorn\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c981ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterDataframe(fake_bb_df,\n",
    "               binary_include={'Player':\"Roger Dorn\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f483379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['TransposePivotTable']=1\n",
    "e = TransposePivotTable(diabetes_df.fillna(0).corr(),value=\"CorrelationCoefficient\")\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DFProcessing import CreatePivotTableFromTimeSeries,ColumnElementalChange,DFStructureReview,ColumnPartitioner,CalculateColumnWiseCorrelation,MissingCartesianProducts,ColumnStatisticalCompare,ColumnStatisticalReview,CountBlanksZeroes,CalculateColumnWiseCorrelation,GenerateBinaryChange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b22dc33",
   "metadata": {},
   "source": [
    "\n",
    "## CreatePivotTableFromTimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e54594",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['CreatePivotTableFromTimeSeries']=1\n",
    "\n",
    "df1 = CreatePivotTableFromTimeSeries(df,index='MEMBERNBR',columns='MONTH',values='DEPOSIT').reset_index()\n",
    "df_1 = CreatePivotTableFromTimeSeries(df_,index='MEMBERNBR',columns='MONTH',values='DEPOSIT').reset_index()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946958a8",
   "metadata": {},
   "source": [
    "## ColumnElementalChange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['ColumnElementalChange']=1\n",
    "ColumnElementalChange(df1,df_1,\"MEAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f0348",
   "metadata": {},
   "source": [
    "## DFStructureReview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb0754",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['DFStructureReview']=1\n",
    "DFStructureReview(df1.reset_index(),primary_key = 'MEMBERNBR',df1=df_1.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18a9eb",
   "metadata": {},
   "source": [
    "## ColumnPartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3bc404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_dict['ColumnPartitioner']=1\n",
    "ColumnPartitioner(df1,'MEAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6707a75",
   "metadata": {},
   "source": [
    "## CalculateColumnWiseCorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746c06d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_dict['CalculateColumnWiseCorrelation']=1\n",
    "all_cols_cor,top_10_corr = CalculateColumnWiseCorrelation(df1[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]])\n",
    "top_10_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb16571",
   "metadata": {},
   "source": [
    "## Generate Binary Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54534c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['GenerateBinaryChange']=1\n",
    "GenerateBinaryChange(df1,'CHG_DF')\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16a9b20",
   "metadata": {},
   "source": [
    "## Generate Statistical Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e670f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['ColumnStatisticalReview']=1\n",
    "ColumnStatisticalReview(df1,'MEAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bf31fa",
   "metadata": {},
   "source": [
    "## ColumnStatisticalCompare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b410f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['ColumnStatisticalCompare']=1\n",
    "ColumnStatisticalCompare(df1,df_1,'MEAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12987258-f5dd-4782-a027-c4b70e22cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['MissingCartesianProducts']=1\n",
    "test_df = pd.DataFrame([['Derek','Boston Red Sox','MLB'],['Derek','Toronto Maple Leafs','NHL'],['Derek','Green Bay Packers','NFL']],columns=['ME',\"Favorite Team\",'League'])\n",
    "\n",
    "\n",
    "\n",
    "def MissingCartesianProducts(list1_,\n",
    "                             list2_,\n",
    "                             columns,\n",
    "                             merge_df=None,\n",
    "                             remove_values=['0',\"\",'N/A']):\n",
    "    '''\n",
    "    Function which Looks at the Combination of Two Lists and explores all possible Combinations. \n",
    "    Developed for the purpose of understanding how many combinations exist and generating a list of Values which do not\n",
    "    exist, this list can be valueable for pending to previously Aggregagted Datasets to insure all possible records have\n",
    "    representation\n",
    "    \n",
    "    Parameters:\n",
    "        list1_ (list)\n",
    "        list2_ (list)\n",
    "        columns (Columns to be included, should represent the expected Column Name of list1_ and list2_)\n",
    "        merge_df (Dataframe): To be used to Validate the number of missing records, if not included, then \n",
    "        returns only combination.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    from DFProcessing import CombineLists\n",
    "    \n",
    "    list1_ = [x for x in list1_ if x not in remove_values]\n",
    "    list2_ = [x for x in list2_ if x not in remove_values]\n",
    "    \n",
    "    required_records = CombineLists([list1_,list2_])\n",
    "    df = pd.DataFrame(required_records,columns=columns)\n",
    "\n",
    "    \n",
    "    if len(merge_df)==0:\n",
    "        return df\n",
    "    \n",
    "    else:\n",
    "        df = df.merge(merge_df[columns].drop_duplicates(),on=columns,how='left',indicator=True)\n",
    "        print(f\"Distribution of Records and Missing Records:\\n{df['_merge'].value_counts()}\")\n",
    "        df = df[df['_merge']=='left_only'].drop('_merge',axis=1)\n",
    "        return df\n",
    "\n",
    "\n",
    "MissingCartesianProducts(list1_=['Derek'],\n",
    "                         list2_=['Boston Red Sox','Toronto Maple Leafs','Test'],\n",
    "                         columns=['ME','Favorite Team'],\n",
    "                         merge_df=test_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b2f319-7ac6-4834-b54d-363cdf2acdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict['CountBlanksZeroes']=1\n",
    "CountBlanksZeroes(df1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0babf08e",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a5b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EDA import analyze_distribution\n",
    "\n",
    "d = analyze_distribution(diabetes_df)\n",
    "example_dict['analyze_distribution']=1\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48af98",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c40011",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d14d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FeatureEngineering import BracketColumn\n",
    "example_dict['BracketColumn']=1\n",
    "BracketColumn(e,'CorrelationCoefficient','Segment',[-1,-.5,0,.5,1])\n",
    "e.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240461d",
   "metadata": {},
   "source": [
    "## Other Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3c467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from OtherFunctions import DataFrameFromProcess\n",
    "\n",
    "#example_dict['DataFrameFromProcess']=1\n",
    "DataFrameFromProcess('Machine Learning Project')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8f1cf3-1268-45f6-aced-8c3b59b316ed",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473d969-8f42-4abb-94e9-a229d3a74c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acabaeb8-37d4-4d94-a024-672aa6ebe542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8d9ce-4d0e-4eb3-920a-8c0726166ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Visualization import plot_histograms, plot_scatter_matrix,ConvertAxisValue\n",
    "example_dict['plot_histograms']=1\n",
    "\n",
    "plot_histograms(diabetes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd437a-ff5d-463e-8ef3-3e0a022c5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Visualization import plot_scatter_matrix\n",
    "example_dict['plot_scatter_matrix']=1\n",
    "\n",
    "plot_scatter_matrix(diabetes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da53fb0-e762-4547-bbeb-06780c062c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Visualization import SimpleBar\n",
    "example_dict['SimpleBar']=1\n",
    "SimpleBar(diabetes_df,'Target','bp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d8ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Visualization import Heatmap\n",
    "example_dict['Heatmap']=1\n",
    "Heatmap(diabetes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e0dc1-6eae-4418-8933-5f7b042b1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Visualization import visualize_hex_color, hex_color_list\n",
    "\n",
    "visualize_hex_color(hex_color_list[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f930a-73e7-4551-9de4-50aff8303f7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Visualization import Scatter\n",
    "\n",
    "Scatter(diabetes_df,X='bmi',y='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6983737-ce8e-415e-9e7d-11251e97c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Visualization import JupyterNotebookMarkdown\n",
    "\n",
    "df1  = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSQF2lNc4WPeTRQ_VzWPkqSZp4RODFkbap8AqmolWp5bKoMaslP2oRVVG21x2POu_JcbF1tGRcBgodu/pub?output=csv')\n",
    "df2 = df1[df1['Process']=='TensorFlow']\n",
    "JupyterNotebookMarkdown(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04029935-3bf5-4139-b60e-1852b27ada60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6401870b-a6d4-42f3-9f23-f68cbe5141ed",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f5086-a0fc-4e00-a4aa-3323561336de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from UtilityFunctions import InspectFunction\n",
    "\n",
    "InspectFunction(GoogleProcessSheetLinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ef778",
   "metadata": {},
   "source": [
    "## Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9519d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Organization import ReadPythonFiles\n",
    "\n",
    "example_dict['ReadPythonFiles'] = 1\n",
    "\n",
    "d = ReadPythonFiles()\n",
    "d = d.reset_index()\n",
    "\n",
    "d[d['index'].str.contains('plot',case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406bc52",
   "metadata": {},
   "source": [
    "## Missing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eade1c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Organization import ReadPythonFiles\n",
    "\n",
    "d = ReadPythonFiles().reset_index().rename(columns={'index':'Function'})[['Function','File']].merge(pd.DataFrame(list(example_dict.items()), columns=['Function', 'Count']),on='Function',how='left').fillna(0)\n",
    "f = d[d['Count']==0]\n",
    "print(f'Number of Missing Functions: {len(f)}')\n",
    "fj\n",
    "\n",
    "f[f['File']=='DFProcessing.py']['Function'].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
