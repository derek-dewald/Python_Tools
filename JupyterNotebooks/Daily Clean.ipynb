{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9398275a-f194-452c-847f-87bec354f750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store', 'daily_shell_job.sh']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('/Users/derekdewald/scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd26d97-f124-4bf0-ac6c-fa227039b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a8f3040-ebb6-4d58-9faa-27dad976349b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>TEXT_SEGMENT</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Historical_Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4) GTE $-3.00, LT $0.00</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>20</td>\n",
       "      <td>-2</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              TEXT_SEGMENT Historical_Score              \n",
       "                                       mean count max min\n",
       "0  4) GTE $-3.00, LT $0.00            -2.75    20  -2  -3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: Data Engineering\n",
      "Classification: Definition\n",
      "Word: Cache\n",
      "\n",
      "#############################################################################################################################\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter for Answer. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition: Cache takes load away from critical systems. Databases. Things that are looked up every second, minute. Can Cache at every layer.  Memory Buffers exist everywhere. Authentication is terrible caching. Don't Cache purchases and refunds. Search Queries, too broad. Can't cache transaction.\n",
      "Notes: nan\n",
      "Link: nan\n",
      "Markdown: nan\n",
      "Dataset Size: nan\n",
      "Learning Type: nan\n",
      "Alogrithm Class: nan\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Did you Pass or Fail? F\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: ML Model\n",
      "Classification: Algorithm\n",
      "Word: RandomTreesEmbedding\n",
      "\n",
      "#############################################################################################################################\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter for Answer. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition: nan\n",
      "Notes: nan\n",
      "Link: nan\n",
      "Markdown: nan\n",
      "Dataset Size: Large\n",
      "Learning Type: nan\n",
      "Alogrithm Class: nan\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdaily_processes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m review_test_results\n\u001b[0;32m----> 3\u001b[0m review_test_results()\n",
      "File \u001b[0;32m~/Documents/Python/Github_Repo/d_py_functions/daily_processes.py:440\u001b[0m, in \u001b[0;36mreview_test_results\u001b[0;34m(file_location, sample_records)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefinition: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNotes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLink: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMarkdown: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDataset Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLearning Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAlogrithm Class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mac\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;66;03m# Record Result\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDid you Pass or Fail?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    441\u001b[0m     results_dict[word] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m0\u001b[39m] \n\u001b[1;32m    443\u001b[0m results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTemp_Score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m results_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(results_dict)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/D2/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1287\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/D2/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from daily_processes import review_test_results\n",
    "\n",
    "review_test_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57bb0d9b-c492-4f54-994a-85c017d812f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31adb1a0-8182-4e90-aee2-f6a68d55cfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29dc5a-6535-4cd8-99a4-76da7c9b6a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd4bc6-29e5-4c69-91a4-854b26df599f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "872a2636-f5f6-4e05-b0f4-3ea430934075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'Datasets.ipynb',\n",
       " 'Pokemon',\n",
       " '.ipynb_checkpoints',\n",
       " 'daily_test_results.csv']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec0a188f-89dc-462b-8871-e7ed88a7e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Process_Count\"] = df.groupby(\"Process\")[\"Process\"].transform(\"count\")\n",
    "df[\"CAT_Count\"] = df.groupby(\"Categorization\")[\"Categorization\"].transform(\"count\")\n",
    "df[\"Word_Count\"] = df.groupby(\"Word\")[\"Word\"].transform(\"count\")\n",
    "df['ProcessCAT_Count'] = df.groupby(['Process','Categorization'])[\"Word\"].transform(\"count\")\n",
    "\n",
    "df.to_csv('/Users/derekdewald/Documents/Python/Github_Repo/Data/Streamlit_DefinitionSummary.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61771aa3-e087-4de6-970a-ea7bb6a87100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1519ae6a-f0c7-4054-b7c3-50ed12cbad5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following words, can you please provide me for each word a definition which is 4-5 sentences highlighting the definition,\n",
      "its origin, its importance and application. If the word is a ML Model, can you also please define the following in point form,\n",
      "type of ML learning, type of ML problem, size of data set it works best on, and anything else which might be appropriate.\n",
      "Unique Words: Davies–Bouldin, Calinski–Harabasz\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from data_d_strings import gpt_question\n",
    "\n",
    "gpt_question(['Davies–Bouldin','Calinski–Harabasz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35022a0-3996-447f-bbb5-9026de2178cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1. Correlation & Redundancy Analysis\n",
    "    1. If Greater than .9 then redudanant, as a rule of thumb, newest is best, but can be based on decision\n",
    "2. Variance & Stability Contribution\n",
    "    1. Variance Inflation Factor\n",
    "3. Clustering Sensitivity / Stability Tests\n",
    "    1. Testing Results of Model\n",
    "    2. Adjusted Rand Index (ARI)\n",
    "    3. Silhouette/ DB Index\n",
    "    4. Run Now and Overtime\n",
    "4. Incremental Value via Feature Importance in Unsupervised Models\n",
    "    1. Reconstruction Error in Autoencoders\n",
    "    2. Mutual Information with Latent Cluster Assignment\n",
    "    3. Permutation for Clustering.\n",
    "\n",
    "\n",
    "Mechanism\n",
    "Structure\n",
    "Action and Learning.\n",
    "\n",
    "Purpose and Contraints.\n",
    "Stabiltiy and Change. \n",
    "\n",
    "Causation and Correlation. We're just looking at Correlation. Not Causation\n",
    "\n",
    "Clustering utilizes mathematical precision to identify \n",
    "\n",
    "## Need Process for Clustering to Document decision on inclusion as it tests and score tests results somewhere central with rationale\n",
    "\n",
    "1. Identify and validate the behavioural attributes, transactions, and usage patterns that define similarity among members\n",
    "2. Understand the relative contribution and sensitivity of features in forming and separating clusters\n",
    "3. Identify and define distinct, stable member archetypes based on observed behavioural patterns\n",
    "4. Characterize each archetype using clear, interpretable behavioural and financial characteristics\n",
    "5. Define the intended use, boundaries, and decision domains for archetype-based strategies\n",
    "6. Design and apply differentiated engagement strategies aligned to each archetype’s characteristics and needs\n",
    "7. Monitor archetype-level outcomes, migration, and engagement effectiveness over time\n",
    "\n",
    "If this project succeeds, you should be able to answer:\n",
    "* “Which archetype are we intentionally trying to grow?”\n",
    "* “Which archetype are we comfortable shrinking?”\n",
    "* “Which engagement changed member behavior within a cluster?”\n",
    "* “Which engagement caused members to migrate to a better archetype?”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80becb5-44ad-4257-9109-0594a67db0fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b35e25-764e-4165-b001-1fe03426339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Jan 7\n",
    "Kmeans Clustering, Parameter = n_init='auto' KMeans is sensitive to initial centroid positions. A bad initialization can lead to poor clustering (high inertia). To mitigate this, KMeans runs multiple times with different random initializations and picks the best run (lowest inertia)\n",
    "\n",
    "Interia\n",
    "\n",
    "The Davies–Bouldin Index measures clustering quality by evaluating the average similarity between each cluster and its most similar cluster, based on the ratio of within-cluster scatter to between-cluster separation. Lower values indicate better clustering because clusters are compact and well-separated. It is useful when comparing different clustering models or parameter settings, especially for algorithms like K-Means. Expected values: closer to 0 is better, and there is no fixed upper bound—higher values suggest overlapping or poorly separated clusters.\n",
    "\n",
    "The Calinski–Harabasz Index (Variance Ratio Criterion) assesses clustering quality by comparing the dispersion between clusters to the dispersion within clusters. Higher values indicate better-defined clusters because they are more separated and internally cohesive. It is commonly used for model selection, such as choosing the optimal number of clusters in K-Means or hierarchical clustering. Expected values: larger is better, and the score tends to increase with more distinct clusters and decrease when clusters overlap\n",
    "\n",
    "It calculates the ratio of the sum of squared distances between cluster centers (between-group variance) to the sum of squared distances within clusters (within-group variance), scaled by the number of clusters and points.\n",
    "\n",
    "\n",
    "< 1.0 → Generally considered good clustering.\n",
    "1–2 → Acceptable but could be improved.\n",
    "> 2 → Often suggests poor separation or overlapping clusters.\n",
    "\n",
    "\n",
    "# Wall Time Versus Process Time\n",
    "time.process_time()\n",
    "time.perf_counter()\n",
    "\n",
    "##########################################################################\n",
    "Jan 6\n",
    "- Clarify what Clustering is. \n",
    "    - Which Features Create a Stable, Interpretable and Useful Segment. \n",
    "    - Not which Features are most important.\n",
    "- Pipeline - Drop Characteristics.\n",
    "- Change Cluster Size.\n",
    "- Articulate Meaning of Clusters and Explain Grouping. \n",
    "- How do Edge Cases Impact.\n",
    "- Do I need to break up my Definitions based on Time. TO include Shorter and Longer Term.\n",
    "- How can I quantify, measure and explain Clusters.\n",
    "\n",
    "The right way to think about “sweet spot”\n",
    "The “sweet spot” is not:\n",
    "* maximum variance explained\n",
    "* maximum number of features\n",
    "* maximum complexity\n",
    "It is:\n",
    "* stable clusters\n",
    "* interpretable differences\n",
    "* consistent behavior across runs\n",
    "* sensitivity that matches your use case (short-term vs long-term behavior)\n",
    "\n",
    "Step 1 — Group features into families (critical)\n",
    "Never drop features one-by-one randomly. ",
    "Drop feature families together.\n",
    "\n",
    "Ratios are:\n",
    "* high signal\n",
    "* high risk\n",
    "They:\n",
    "* amplify noise\n",
    "* can dominate PCA\n",
    "* are hard to interpret\n",
    "Rule:\n",
    "* Try models with and without ratios\n",
    "* Prefer simpler models unless ratios clearly add value\n",
    "Ratios must earn their place.\n",
    "\n",
    "Balanced behavioral model\n",
    "* 3M avg\n",
    "* 6M growth\n",
    "* engagement\n",
    "* transaction mix\n",
    "Long-term structure model\n",
    "* 6–12M avg\n",
    "* 12M growth\n",
    "* low short-term noise\n",
    "Short-term sensitivity model\n",
    "* 1–3M dynamics\n",
    "* volatility\n",
    "* no long-term averages\n",
    "\n",
    "\n",
    "Silhouette: Is this point closer to points in its own cluster than to points in the nearest other cluster ",
    "\n",
    "Cohesion (how tight your cluster is)\n",
    "Separation (how far apart clusters are)\n",
    "\n",
    "For K Means. \n",
    "Does this variable differ meaningfully between clusters?\n",
    "\n",
    "Fit Predict, You get one thing only:\n",
    "The index of the nearest centroid for each observation\n",
    "That’s it.\n",
    "Important clarifications:\n",
    "* It does not tell you how confident the assignment is\n",
    "* It does not tell you how far a point is from its centroid\n",
    "* It does not tell you how ambiguous the assignment was\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c9b2aa-960e-499b-b803-617851411d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# 1) Keep only numeric columns\n",
    "num_cols = cc_model_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# 2) (Recommended) Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(cc_model_df[num_cols]), columns=num_cols, index=cc_model_df.index)\n",
    "\n",
    "# 3) Compute VIF\n",
    "vif_table = compute_vif(X_scaled)\n",
    "print(vif_table)\n",
    "\n",
    "### Reviewing Variables for Model Inclusion\n",
    "\n",
    "def set_manual_column_order(df,column_name,column_order):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df[column_name] = pd.Categorical(df[column_name], categories=column_order, ordered=True)\n",
    "\n",
    "import timeit\n",
    "from UtilityFunctions import PauseProcess\n",
    "from eda_dq import column_statistical_review\n",
    "\n",
    "def data_preparation_checklist(df=pd.DataFrame(),word_list=[]):\n",
    "    if len(df)==0:\n",
    "        df = pd.read_csv(links['d_learning_notes_url'])\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for word in word_list:\n",
    "        temp_df = df[(df['Definition'].fillna(\"\").str.contains(word,case=False))|(df['Categorization']==word)]\n",
    "        final_df = pd.concat([final_df,temp_df])\n",
    "\n",
    "    final_df = final_df.drop(['Source','Process','Categorization'],axis=1)\n",
    "    display(final_df)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def build_member_snapshot(final_mbr_df: pd.DataFrame,\n",
    "                          final_txn_df: pd.DataFrame,\n",
    "                          snapshot_month=None,\n",
    "                          window_3m=3,\n",
    "                          window_6m=6) -> pd.DataFrame:\n",
    "    m = final_mbr_df.copy()\n",
    "    t = final_txn_df.copy()\n",
    "\n",
    "    # normalize month to month-start timestamp\n",
    "    m[\"MONTH\"] = _month_floor(m[\"MONTH\"])\n",
    "    t[\"MONTH\"] = _month_floor(t[\"MONTH\"])\n",
    "\n",
    "    if snapshot_month is None:\n",
    "        snapshot_month = m[\"MONTH\"].max()\n",
    "    snapshot_month = pd.to_datetime(snapshot_month).to_period(\"M\").to_timestamp()\n",
    "\n",
    "    # restrict to history up to snapshot month\n",
    "    m = m[m[\"MONTH\"] <= snapshot_month]\n",
    "    t = t[t[\"MONTH\"] <= snapshot_month]\n",
    "\n",
    "    # ----- Member-month features windows -----\n",
    "    # Keep a lean set of signals from your schema\n",
    "    base_cols = [\n",
    "        \"MEMBERNBR\",\"MONTH\",\"AGE\",\"CITY\",\"BRANCHNAME\",\"PRIMARY_IS_BEEM\",\"HOME_OWNER\",\n",
    "        \"ANNUAL_INCOME\",\"DEPOSIT_BALANCE\",\"MORTGAGE_BALANCE\",\"LIQUID_ASSETS\",\n",
    "        \"ENGAGEMENT_SCORE\",\"PERC_DEPOSIT\",\"PERC_MORTGAGE\",\n",
    "        \"POS_TXN_DEBIT\",\"BILL_PAY_DEBIT\",\"PAYROLL_DEPOSIT\",\"OTHER_DEPOSIT\"\n",
    "    ]\n",
    "    base_cols = [c for c in base_cols if c in m.columns]\n",
    "    m = m[base_cols].sort_values([\"MEMBERNBR\",\"MONTH\"])\n",
    "\n",
    "    # helper: window filter\n",
    "    start_3m = snapshot_month - pd.DateOffset(months=window_3m-1)\n",
    "    start_6m = snapshot_month - pd.DateOffset(months=window_6m-1)\n",
    "\n",
    "    m3 = m[m[\"MONTH\"].between(start_3m, snapshot_month)]\n",
    "    m6 = m[m[\"MONTH\"].between(start_6m, snapshot_month)]\n",
    "\n",
    "    # 3m aggregates (means/sums)\n",
    "    agg_3m = m3.groupby(\"MEMBERNBR\").agg(\n",
    "        age=(\"AGE\",\"last\"),\n",
    "        city=(\"CITY\",\"last\"),\n",
    "        branch=(\"BRANCHNAME\",\"last\"),\n",
    "        primary=(\"PRIMARY_IS_BEEM\",\"last\"),\n",
    "        home_owner=(\"HOME_OWNER\",\"last\"),\n",
    "        income_mean_3m=(\"ANNUAL_INCOME\",\"mean\"),\n",
    "        deposit_mean_3m=(\"DEPOSIT_BALANCE\",\"mean\"),\n",
    "        loan_mean_3m=(\"MORTGAGE_BALANCE\",\"mean\"),\n",
    "        engagement_mean_3m=(\"ENGAGEMENT_SCORE\",\"mean\"),\n",
    "        pos_sum_3m=(\"POS_TXN_DEBIT\",\"sum\"),\n",
    "        bill_sum_3m=(\"BILL_PAY_DEBIT\",\"sum\"),\n",
    "        payroll_sum_3m=(\"PAYROLL_DEPOSIT\",\"sum\"),\n",
    "        otherdep_sum_3m=(\"OTHER_DEPOSIT\",\"sum\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # 6m dynamics: volatility + trend (slope)\n",
    "    def dyn_features(df, col):\n",
    "        return df.groupby(\"MEMBERNBR\")[col].agg(\n",
    "            **{f\"{col.lower()}_std_6m\":\"std\",\n",
    "               f\"{col.lower()}_slope_6m\":_slope}\n",
    "        )\n",
    "\n",
    "    dep_dyn = m6.groupby(\"MEMBERNBR\")[\"DEPOSIT_BALANCE\"].agg(\n",
    "        deposit_std_6m=\"std\",\n",
    "        deposit_slope_6m=_slope\n",
    "    )\n",
    "    loan_dyn = m6.groupby(\"MEMBERNBR\")[\"MORTGAGE_BALANCE\"].agg(\n",
    "        loan_std_6m=\"std\",\n",
    "        loan_slope_6m=_slope\n",
    "    )\n",
    "    eng_dyn = m6.groupby(\"MEMBERNBR\")[\"ENGAGEMENT_SCORE\"].agg(\n",
    "        engagement_std_6m=\"std\",\n",
    "        engagement_slope_6m=_slope\n",
    "    )\n",
    "\n",
    "    dyn = pd.concat([dep_dyn, loan_dyn, eng_dyn], axis=1).reset_index()\n",
    "\n",
    "    # ----- Transaction mix features (from your long txn table) -----\n",
    "    # Build 3m spend mix + entropy (behavioral fingerprint)\n",
    "    t3 = t[t[\"MONTH\"].between(start_3m, snapshot_month)].copy()\n",
    "    # aggregate category totals per member\n",
    "    cat = t3.groupby([\"MEMBERNBR\",\"variable\"])[\"value\"].sum().reset_index()\n",
    "    total = cat.groupby(\"MEMBERNBR\")[\"value\"].sum().rename(\"txn_total_3m\").reset_index()\n",
    "    cat = cat.merge(total, on=\"MEMBERNBR\", how=\"left\")\n",
    "    cat[\"share\"] = np.where(cat[\"txn_total_3m\"] > 0, cat[\"value\"] / cat[\"txn_total_3m\"], 0.0)\n",
    "\n",
    "    # pivot shares wide: share_<category>\n",
    "    mix = cat.pivot_table(index=\"MEMBERNBR\", columns=\"variable\", values=\"share\", fill_value=0.0)\n",
    "    mix.columns = [f\"share_{c}\" for c in mix.columns]\n",
    "    mix = mix.reset_index()\n",
    "\n",
    "    # entropy: higher = more diverse mix\n",
    "    # (avoid log(0) by masking)\n",
    "    share_mat = mix.drop(columns=[\"MEMBERNBR\"]).to_numpy(dtype=float)\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        ent = -(share_mat * np.log(np.where(share_mat > 0, share_mat, 1.0))).sum(axis=1)\n",
    "    mix[\"mix_entropy_3m\"] = ent\n",
    "\n",
    "    # ----- Combine -----\n",
    "    snap = agg_3m.merge(dyn, on=\"MEMBERNBR\", how=\"left\").merge(mix, on=\"MEMBERNBR\", how=\"left\")\n",
    "\n",
    "    # convenience derived features\n",
    "    snap[\"net_worth_mean_3m\"] = snap[\"deposit_mean_3m\"].fillna(0) - snap[\"loan_mean_3m\"].fillna(0)\n",
    "    snap[\"snapshot_month\"] = snapshot_month\n",
    "\n",
    "    # fill missing mix features with 0 (e.g., no txns in window)\n",
    "    share_cols = [c for c in snap.columns if c.startswith(\"share_\")]\n",
    "    snap[share_cols] = snap[share_cols].fillna(0.0)\n",
    "    for c in [\"deposit_std_6m\",\"deposit_slope_6m\",\"loan_std_6m\",\"loan_slope_6m\",\n",
    "              \"engagement_std_6m\",\"engagement_slope_6m\",\"mix_entropy_3m\"]:\n",
    "        if c in snap.columns:\n",
    "            snap[c] = snap[c].fillna(0.0)\n",
    "\n",
    "    return snap\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def make_preprocess_pipeline(feature_df: pd.DataFrame,\n",
    "                             cat_cols=(\"city\",\"branch\"),\n",
    "                             force_drop=(\"MEMBERNBR\",\"snapshot_month\")):\n",
    "    df = feature_df.copy()\n",
    "\n",
    "    # numeric candidates: everything number-ish except ids\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_cols = [c for c in num_cols if c not in force_drop]\n",
    "\n",
    "    cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "\n",
    "    # log-transform for heavy-tailed money-ish cols\n",
    "    log_cols = [c for c in num_cols if any(k in c for k in [\"deposit\",\"loan\",\"income\",\"pos_\",\"bill_\",\"payroll\",\"otherdep\",\"net_worth\"])]\n",
    "    passthrough_cols = [c for c in num_cols if c not in log_cols]\n",
    "\n",
    "    log_pipe = Pipeline([\n",
    "        (\"log1p\", FunctionTransformer(lambda x: np.log1p(np.clip(x, 0, None)))),\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ])\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"log_num\", log_pipe, log_cols),\n",
    "            (\"num\", num_pipe, passthrough_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "# final_mbr_df, final_txn_df = generate_synthetic_dataset(...)\n",
    "\n",
    "snap = build_member_snapshot(final_mbr_df, final_txn_df, snapshot_month=None, window_3m=3, window_6m=6)\n",
    "\n",
    "feature_snap = snap.drop(columns=['MEMBERNBR','snapshot_month'],errors='ignore')\n",
    "member_ids = snap['MEMBERNBR'].to_numpy()\n",
    "\n",
    "pre = make_preprocess_pipeline(feature_snap)\n",
    "X = pre.fit_transform(feature_snap)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# Cant run because it errors out, would be 22 Billion . Need to Understand this\n",
    "#S = cosine_similarity(X)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=51,metric='cosine',algorithm='auto')\n",
    "\n",
    "nn.fit(X)\n",
    "\n",
    "member_index = {mid: i for i, mid in enumerate(member_ids)}\n",
    "\n",
    "def test_knn(X,nn,k=10,sample_size=100,seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idxs = rng.choice(X.shape[0],size=sample_size,replace=False)\n",
    "\n",
    "    # returns the distance and Index\n",
    "    nn_dist,nn_index = nn.kneighbors(X[idxs],n_neighbors=k+1)\n",
    "    nn_dist = nn_dist[:,1:].mean(axis=1)\n",
    "    \n",
    "    rand_idxs = rng.choice(X.shape[0],size=(sample_size,k),replace=True)\n",
    "    rand_dist = np.linalg.norm(X[idxs][:,None,:]-X[rand_idxs],axis=2).mean(axis=1)\n",
    "    \n",
    "    return nn_dist,rand_dist,idxs\n",
    "\n",
    "\n",
    "a,b,c = test_knn(X,nn)\n",
    "\n",
    "mid = int(member_ids[0])\n",
    "neighbors, sim = knn_topk(nn, member_index, member_ids, X, mid, k=10)\n",
    "list(zip(neighbors.tolist(), sim.tolist()))\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# What does StandardScaler do. Transforms data to Mean 0 and Standard Dev = 1\\\n",
    "\n",
    "def Heatmap(df,\n",
    "            correlation=True,\n",
    "            column_list=[],\n",
    "            title='Heat Map of Correlation',\n",
    "            cmap='coolwarm',\n",
    "            annotate=True,\n",
    "            x_rotate=0,\n",
    "            y_rotate=0,\n",
    "            cbar=True,\n",
    "            set_center=0,\n",
    "            figsize=(10,10)):\n",
    "    \n",
    "    '''\n",
    "    Function Which Generates a Heatmap\n",
    "    \n",
    "    Parameters:\n",
    "        Dataframe\n",
    "        column_name (list): If included, will only show certain columns on the Horizontal Axis.\n",
    "    \n",
    "    Returns:\n",
    "        matlplot plot.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    sns.set(style='white')\n",
    "    \n",
    "    # View column with Abbreviated title or full. Abbreviated displays nicer.\n",
    "    if correlation:\n",
    "        corr = df.corr()\n",
    "    else:\n",
    "        corr = df.copy()\n",
    "    \n",
    "    if len(column_list)!=0:\n",
    "        corr = corr[column_list]\n",
    "    \n",
    "    mask= np.zeros_like(corr,dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)]=True\n",
    "    f,ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    if len(str(set_center))!=0:\n",
    "        sns.heatmap(corr,mask=mask,cmap=cmap,center=set_center,square=True,linewidths=1,annot=annotate,cbar=cbar)\n",
    "    else:\n",
    "        sns.heatmap(corr,mask=mask,cmap=cmap,square=True,linewidths=1,annot=annotate,cbar=cbar)\n",
    "    \n",
    "    \n",
    "    plt.title(title)\n",
    "    if y_rotate !=0:\n",
    "        for tick in ax.get_yticklabels():\n",
    "            tick.set_rotation(0)\n",
    "            tick.set_horizontalalignment('right')\n",
    "    if x_rotate !=0:\n",
    "        plt.xticks(rotation=x_rotate,ha='center', va='top')\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
