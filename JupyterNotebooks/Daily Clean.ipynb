{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "958f9b4d-1736-4534-b3d1-c8339491c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location=None\n",
    "\n",
    "if not file_location:\n",
    "    file_location= '/Users/derekdewald/Documents/Python/Github_Repo/Data/daily_test_results.csv'\n",
    "    \n",
    "primary_key = ['Process','Categorization','Word']\n",
    "    \n",
    "results_df = pd.read_csv(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e649b96a-6e12-4215-9e12-158b0f5ca97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Temp_Score</th>\n",
       "      <th>Historical_Score</th>\n",
       "      <th>Word</th>\n",
       "      <th>Process</th>\n",
       "      <th>Categorization</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Link</th>\n",
       "      <th>Image</th>\n",
       "      <th>Markdown Equation</th>\n",
       "      <th>Dataset Size</th>\n",
       "      <th>Learning Type</th>\n",
       "      <th>Algorithm Class</th>\n",
       "      <th>Model</th>\n",
       "      <th>Ensemble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-07</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>SGDClassifier is a linear classification algor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Large</td>\n",
       "      <td>Supervised</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Linear</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-07</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>VotingClassifier</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>VotingClassifier is an ensemble method that co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medium, Large</td>\n",
       "      <td>Supervised</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Meta-Model</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-07</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>Quantile Random Forest</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>Quantile Random Forest is an extension of rand...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Supervised</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Tree</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-07</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>RandomTreesEmbedding</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Large</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-07</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>Area Under the Curve</td>\n",
       "      <td>Model Evaluation</td>\n",
       "      <td>Feature Selection</td>\n",
       "      <td>AUC (Area Under the Curve) measures a model’s ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>Perceptron</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>The Perceptron is one of the earliest linear c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Large</td>\n",
       "      <td>Supervised</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Linear</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>RidgeCV</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>RidgeCV is a regression model that combines ri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medium, Large</td>\n",
       "      <td>Supervised</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Linear</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>Regularization</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Technique used to prevent overfitting by addin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>SelectFpr</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>SelectFpr is a feature selection method that s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Large</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2026-01-08</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>AdaBoostClassifier is an ensemble boosting alg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Small, Medium</td>\n",
       "      <td>Supervised</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Linear</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2026-01-13</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>Training</td>\n",
       "      <td>Optimizer</td>\n",
       "      <td>Variation of Gradient Descent, which adapts th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$$\\n\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2026-01-13</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>NMF</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medium, Large</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2026-01-13</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>Cache</td>\n",
       "      <td>Data Engineering</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Cache takes load away from critical systems. D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2026-01-13</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>Diagnostic</td>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Semantic Type</td>\n",
       "      <td>Representing metrics of directionality, moment...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2026-01-13</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>Grapth Theory</td>\n",
       "      <td>Graph Theory</td>\n",
       "      <td>Definition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2026-01-14</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Small</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2026-01-14</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medium, Large</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2026-01-14</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>DBSCAN</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>DBSCAN is a density-based clustering algorithm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Small, Medium</td>\n",
       "      <td>Unsupervised</td>\n",
       "      <td>Clustering</td>\n",
       "      <td>Denisty Based Clustering</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2026-01-14</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>RFE</td>\n",
       "      <td>ML Model</td>\n",
       "      <td>Algorithm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Small, Medium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2026-01-14</td>\n",
       "      <td>0</td>\n",
       "      <td>-3</td>\n",
       "      <td>Core Variables</td>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Functional Role</td>\n",
       "      <td>Primary drivers of cluster formation and direc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Temp_Score  Historical_Score                    Word  \\\n",
       "0   2026-01-07           0                -3           SGDClassifier   \n",
       "1   2026-01-07           0                -3        VotingClassifier   \n",
       "2   2026-01-07           0                -3  Quantile Random Forest   \n",
       "3   2026-01-07           0                -3    RandomTreesEmbedding   \n",
       "4   2026-01-07           0                -3    Area Under the Curve   \n",
       "5   2026-01-08           0                -3              Perceptron   \n",
       "6   2026-01-08           0                -3                 RidgeCV   \n",
       "7   2026-01-08           0                -3          Regularization   \n",
       "8   2026-01-08           0                -3               SelectFpr   \n",
       "9   2026-01-08           0                -3      AdaBoostClassifier   \n",
       "10  2026-01-13           0                -3                 Adagrad   \n",
       "11  2026-01-13           0                -2                     NMF   \n",
       "12  2026-01-13           0                -3                   Cache   \n",
       "13  2026-01-13           0                -3             Diagnostic    \n",
       "14  2026-01-13           0                -2           Grapth Theory   \n",
       "15  2026-01-14           0                -2          HuberRegressor   \n",
       "16  2026-01-14           0                -2  RandomForestClassifier   \n",
       "17  2026-01-14           0                -3                  DBSCAN   \n",
       "18  2026-01-14           0                -2                     RFE   \n",
       "19  2026-01-14           0                -3          Core Variables   \n",
       "\n",
       "             Process     Categorization  \\\n",
       "0           ML Model          Algorithm   \n",
       "1           ML Model          Algorithm   \n",
       "2           ML Model          Algorithm   \n",
       "3           ML Model          Algorithm   \n",
       "4   Model Evaluation  Feature Selection   \n",
       "5           ML Model          Algorithm   \n",
       "6           ML Model          Algorithm   \n",
       "7           ML Model         Definition   \n",
       "8           ML Model          Algorithm   \n",
       "9           ML Model          Algorithm   \n",
       "10          Training          Optimizer   \n",
       "11          ML Model          Algorithm   \n",
       "12  Data Engineering         Definition   \n",
       "13  Data Preparation     Semantic Type    \n",
       "14      Graph Theory         Definition   \n",
       "15          ML Model          Algorithm   \n",
       "16          ML Model          Algorithm   \n",
       "17          ML Model          Algorithm   \n",
       "18          ML Model          Algorithm   \n",
       "19  Data Preparation   Functional Role    \n",
       "\n",
       "                                           Definition  Notes  Link  Image  \\\n",
       "0   SGDClassifier is a linear classification algor...    NaN   NaN    NaN   \n",
       "1   VotingClassifier is an ensemble method that co...    NaN   NaN    NaN   \n",
       "2   Quantile Random Forest is an extension of rand...    NaN   NaN    NaN   \n",
       "3                                                 NaN    NaN   NaN    NaN   \n",
       "4   AUC (Area Under the Curve) measures a model’s ...    NaN   NaN    NaN   \n",
       "5   The Perceptron is one of the earliest linear c...    NaN   NaN    NaN   \n",
       "6   RidgeCV is a regression model that combines ri...    NaN   NaN    NaN   \n",
       "7   Technique used to prevent overfitting by addin...    NaN   NaN    NaN   \n",
       "8   SelectFpr is a feature selection method that s...    NaN   NaN    NaN   \n",
       "9   AdaBoostClassifier is an ensemble boosting alg...    NaN   NaN    NaN   \n",
       "10  Variation of Gradient Descent, which adapts th...    NaN   NaN    NaN   \n",
       "11                                                NaN    NaN   NaN    NaN   \n",
       "12  Cache takes load away from critical systems. D...    NaN   NaN    NaN   \n",
       "13  Representing metrics of directionality, moment...    NaN   NaN    NaN   \n",
       "14                                                NaN    NaN   NaN    NaN   \n",
       "15                                                NaN    NaN   NaN    NaN   \n",
       "16                                                NaN    NaN   NaN    NaN   \n",
       "17  DBSCAN is a density-based clustering algorithm...    NaN   NaN    NaN   \n",
       "18                                                NaN    NaN   NaN    NaN   \n",
       "19  Primary drivers of cluster formation and direc...    NaN   NaN    NaN   \n",
       "\n",
       "                                    Markdown Equation   Dataset Size  \\\n",
       "0                                                 NaN          Large   \n",
       "1                                                 NaN  Medium, Large   \n",
       "2                                                 NaN            NaN   \n",
       "3                                                 NaN          Large   \n",
       "4                                                 NaN            NaN   \n",
       "5                                                 NaN          Large   \n",
       "6                                                 NaN  Medium, Large   \n",
       "7                                                 NaN            NaN   \n",
       "8                                                 NaN          Large   \n",
       "9                                                 NaN  Small, Medium   \n",
       "10  $$\\n\\theta_{t+1} = \\theta_t - \\frac{\\alpha}{\\s...            NaN   \n",
       "11                                                NaN  Medium, Large   \n",
       "12                                                NaN            NaN   \n",
       "13                                                NaN            NaN   \n",
       "14                                                NaN            NaN   \n",
       "15                                                NaN          Small   \n",
       "16                                                NaN  Medium, Large   \n",
       "17                                                NaN  Small, Medium   \n",
       "18                                                NaN  Small, Medium   \n",
       "19                                                NaN            NaN   \n",
       "\n",
       "   Learning Type Algorithm Class                     Model  Ensemble  \n",
       "0     Supervised  Classification                    Linear       NaN  \n",
       "1     Supervised  Classification                Meta-Model       1.0  \n",
       "2     Supervised      Regression                      Tree       NaN  \n",
       "3            NaN             NaN                       NaN       NaN  \n",
       "4            NaN             NaN                       NaN       NaN  \n",
       "5     Supervised  Classification                    Linear       NaN  \n",
       "6     Supervised      Regression                    Linear       NaN  \n",
       "7            NaN             NaN                       NaN       NaN  \n",
       "8            NaN             NaN                       NaN       NaN  \n",
       "9     Supervised  Classification                    Linear       1.0  \n",
       "10           NaN             NaN                       NaN       NaN  \n",
       "11           NaN             NaN                       NaN       NaN  \n",
       "12           NaN             NaN                       NaN       NaN  \n",
       "13           NaN             NaN                       NaN       NaN  \n",
       "14           NaN             NaN                       NaN       NaN  \n",
       "15           NaN             NaN                       NaN       NaN  \n",
       "16           NaN             NaN                       NaN       NaN  \n",
       "17  Unsupervised      Clustering  Denisty Based Clustering       NaN  \n",
       "18           NaN             NaN                       NaN       NaN  \n",
       "19           NaN             NaN                       NaN       NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[['Word','Historical_Score']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57bb0d9b-c492-4f54-994a-85c017d812f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_test_results(file_location=None):\n",
    "\n",
    "    '''\n",
    "    Function to Facilitate a Daily Review of Historically Created Words. \n",
    "    Function has a scoring Component, a Correct Answer (Pass is worth 1 Point), A Incorrect Answer (Fail is worth -2 points), if cummulative score is not postive\n",
    "    then user is expected to Answer, with expectation of making score positive, if less than 10 examples with a negative score, it randomly samples from \n",
    "    positive scores.\n",
    "\n",
    "    \n",
    "    '''\n",
    "\n",
    "    if not file_location:\n",
    "        file_location= '/Users/derekdewald/Documents/Python/Github_Repo/Data/daily_test_results.csv'\n",
    "    \n",
    "    primary_key = ['Process','Categorization','Word']\n",
    "    \n",
    "    results_df = pd.read_csv(file_location)\n",
    "    \n",
    "    results_df[\"Date\"] = pd.to_datetime(results_df[\"Date\"], format='%Y-%m-%d')\n",
    "    results_df[\"Date\"] = results_df[\"Date\"].apply(lambda x:x.date())\n",
    "    \n",
    "    # Generate Data Set to Test. \n",
    "    # Test Everything where Review_Score is 0.\n",
    "    review_df = results_df[(results_df['Historical_Score']<0)]\n",
    "\n",
    "    # If Less than 10 Items to review then sample historical items \n",
    "    if len(review_df)<10:\n",
    "        review_df = pd.concat([\n",
    "            review_df,\n",
    "            results_df[(results_df['Historical_Score']>0)].sample(10-len(review_df))\n",
    "        ])\n",
    "        \n",
    "    review_df = review_df.reset_index(drop=True)\n",
    "\n",
    "    results_dict = {}\n",
    "    for count in range(len(review_df)):\n",
    "        cat,cat1,word = review_df[primary_key].iloc[count]    \n",
    "        print(f\"Process: {cat}\\nClassification: {cat1}\\nWord: {word}\\n\")\n",
    "        print(\"#############################################################################################################################\")\n",
    "        result = input('Did you Pass or Fail?')\n",
    "        results_dict[word] = [1 if result.lower() =='p' else -2][0] \n",
    "        df, nt,lk,md,ds, lt,ac = review_df.iloc[count][['Definition','Notes','Link','Markdown Equation','Dataset Size','Learning Type',\"Algorithm Class\"]]\n",
    "        print(f\"Definition: {df}\\nNotes: {nt}\\nLink: {lk}\\nMarkdown: {md}\\nDataset Size: {ds}\\nLearning Type: {lt}\\nAlogrithm Class: {ac}\\n\")\n",
    "\n",
    "    results_df['Temp_Score'] = results_df['Word'].map(results_dict).fillna(0)\n",
    "    results_df['Historical_Score'] = results_df['Historical_Score'] + results_df['Temp_Score'] \n",
    "    results_df['Temp_Score'] = 0\n",
    "\n",
    "    results_df.to_csv(file_location,index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31adb1a0-8182-4e90-aee2-f6a68d55cfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29dc5a-6535-4cd8-99a4-76da7c9b6a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd4bc6-29e5-4c69-91a4-854b26df599f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "872a2636-f5f6-4e05-b0f4-3ea430934075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'Datasets.ipynb',\n",
       " 'Pokemon',\n",
       " '.ipynb_checkpoints',\n",
       " 'daily_test_results.csv']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec0a188f-89dc-462b-8871-e7ed88a7e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Process_Count\"] = df.groupby(\"Process\")[\"Process\"].transform(\"count\")\n",
    "df[\"CAT_Count\"] = df.groupby(\"Categorization\")[\"Categorization\"].transform(\"count\")\n",
    "df[\"Word_Count\"] = df.groupby(\"Word\")[\"Word\"].transform(\"count\")\n",
    "df['ProcessCAT_Count'] = df.groupby(['Process','Categorization'])[\"Word\"].transform(\"count\")\n",
    "\n",
    "df.to_csv('/Users/derekdewald/Documents/Python/Github_Repo/Data/Streamlit_DefinitionSummary.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61771aa3-e087-4de6-970a-ea7bb6a87100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1519ae6a-f0c7-4054-b7c3-50ed12cbad5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following words, can you please provide me for each word a definition which is 4-5 sentences highlighting the definition,\n",
      "its origin, its importance and application. If the word is a ML Model, can you also please define the following in point form,\n",
      "type of ML learning, type of ML problem, size of data set it works best on, and anything else which might be appropriate.\n",
      "Unique Words: Davies–Bouldin, Calinski–Harabasz\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from data_d_strings import gpt_question\n",
    "\n",
    "gpt_question(['Davies–Bouldin','Calinski–Harabasz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35022a0-3996-447f-bbb5-9026de2178cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1. Correlation & Redundancy Analysis\n",
    "    1. If Greater than .9 then redudanant, as a rule of thumb, newest is best, but can be based on decision\n",
    "2. Variance & Stability Contribution\n",
    "    1. Variance Inflation Factor\n",
    "3. Clustering Sensitivity / Stability Tests\n",
    "    1. Testing Results of Model\n",
    "    2. Adjusted Rand Index (ARI)\n",
    "    3. Silhouette/ DB Index\n",
    "    4. Run Now and Overtime\n",
    "4. Incremental Value via Feature Importance in Unsupervised Models\n",
    "    1. Reconstruction Error in Autoencoders\n",
    "    2. Mutual Information with Latent Cluster Assignment\n",
    "    3. Permutation for Clustering.\n",
    "\n",
    "\n",
    "Mechanism\n",
    "Structure\n",
    "Action and Learning.\n",
    "\n",
    "Purpose and Contraints.\n",
    "Stabiltiy and Change. \n",
    "\n",
    "Causation and Correlation. We're just looking at Correlation. Not Causation\n",
    "\n",
    "Clustering utilizes mathematical precision to identify \n",
    "\n",
    "## Need Process for Clustering to Document decision on inclusion as it tests and score tests results somewhere central with rationale\n",
    "\n",
    "1. Identify and validate the behavioural attributes, transactions, and usage patterns that define similarity among members\n",
    "2. Understand the relative contribution and sensitivity of features in forming and separating clusters\n",
    "3. Identify and define distinct, stable member archetypes based on observed behavioural patterns\n",
    "4. Characterize each archetype using clear, interpretable behavioural and financial characteristics\n",
    "5. Define the intended use, boundaries, and decision domains for archetype-based strategies\n",
    "6. Design and apply differentiated engagement strategies aligned to each archetype’s characteristics and needs\n",
    "7. Monitor archetype-level outcomes, migration, and engagement effectiveness over time\n",
    "\n",
    "If this project succeeds, you should be able to answer:\n",
    "* “Which archetype are we intentionally trying to grow?”\n",
    "* “Which archetype are we comfortable shrinking?”\n",
    "* “Which engagement changed member behavior within a cluster?”\n",
    "* “Which engagement caused members to migrate to a better archetype?”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80becb5-44ad-4257-9109-0594a67db0fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b35e25-764e-4165-b001-1fe03426339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Jan 7\n",
    "Kmeans Clustering, Parameter = n_init='auto' KMeans is sensitive to initial centroid positions. A bad initialization can lead to poor clustering (high inertia). To mitigate this, KMeans runs multiple times with different random initializations and picks the best run (lowest inertia)\n",
    "\n",
    "Interia\n",
    "\n",
    "The Davies–Bouldin Index measures clustering quality by evaluating the average similarity between each cluster and its most similar cluster, based on the ratio of within-cluster scatter to between-cluster separation. Lower values indicate better clustering because clusters are compact and well-separated. It is useful when comparing different clustering models or parameter settings, especially for algorithms like K-Means. Expected values: closer to 0 is better, and there is no fixed upper bound—higher values suggest overlapping or poorly separated clusters.\n",
    "\n",
    "The Calinski–Harabasz Index (Variance Ratio Criterion) assesses clustering quality by comparing the dispersion between clusters to the dispersion within clusters. Higher values indicate better-defined clusters because they are more separated and internally cohesive. It is commonly used for model selection, such as choosing the optimal number of clusters in K-Means or hierarchical clustering. Expected values: larger is better, and the score tends to increase with more distinct clusters and decrease when clusters overlap\n",
    "\n",
    "It calculates the ratio of the sum of squared distances between cluster centers (between-group variance) to the sum of squared distances within clusters (within-group variance), scaled by the number of clusters and points.\n",
    "\n",
    "\n",
    "< 1.0 → Generally considered good clustering.\n",
    "1–2 → Acceptable but could be improved.\n",
    "> 2 → Often suggests poor separation or overlapping clusters.\n",
    "\n",
    "\n",
    "# Wall Time Versus Process Time\n",
    "time.process_time()\n",
    "time.perf_counter()\n",
    "\n",
    "##########################################################################\n",
    "Jan 6\n",
    "- Clarify what Clustering is. \n",
    "    - Which Features Create a Stable, Interpretable and Useful Segment. \n",
    "    - Not which Features are most important.\n",
    "- Pipeline - Drop Characteristics.\n",
    "- Change Cluster Size.\n",
    "- Articulate Meaning of Clusters and Explain Grouping. \n",
    "- How do Edge Cases Impact.\n",
    "- Do I need to break up my Definitions based on Time. TO include Shorter and Longer Term.\n",
    "- How can I quantify, measure and explain Clusters.\n",
    "\n",
    "The right way to think about “sweet spot”\n",
    "The “sweet spot” is not:\n",
    "* maximum variance explained\n",
    "* maximum number of features\n",
    "* maximum complexity\n",
    "It is:\n",
    "* stable clusters\n",
    "* interpretable differences\n",
    "* consistent behavior across runs\n",
    "* sensitivity that matches your use case (short-term vs long-term behavior)\n",
    "\n",
    "Step 1 — Group features into families (critical)\n",
    "Never drop features one-by-one randomly. ",
    "Drop feature families together.\n",
    "\n",
    "Ratios are:\n",
    "* high signal\n",
    "* high risk\n",
    "They:\n",
    "* amplify noise\n",
    "* can dominate PCA\n",
    "* are hard to interpret\n",
    "Rule:\n",
    "* Try models with and without ratios\n",
    "* Prefer simpler models unless ratios clearly add value\n",
    "Ratios must earn their place.\n",
    "\n",
    "Balanced behavioral model\n",
    "* 3M avg\n",
    "* 6M growth\n",
    "* engagement\n",
    "* transaction mix\n",
    "Long-term structure model\n",
    "* 6–12M avg\n",
    "* 12M growth\n",
    "* low short-term noise\n",
    "Short-term sensitivity model\n",
    "* 1–3M dynamics\n",
    "* volatility\n",
    "* no long-term averages\n",
    "\n",
    "\n",
    "Silhouette: Is this point closer to points in its own cluster than to points in the nearest other cluster ",
    "\n",
    "Cohesion (how tight your cluster is)\n",
    "Separation (how far apart clusters are)\n",
    "\n",
    "For K Means. \n",
    "Does this variable differ meaningfully between clusters?\n",
    "\n",
    "Fit Predict, You get one thing only:\n",
    "The index of the nearest centroid for each observation\n",
    "That’s it.\n",
    "Important clarifications:\n",
    "* It does not tell you how confident the assignment is\n",
    "* It does not tell you how far a point is from its centroid\n",
    "* It does not tell you how ambiguous the assignment was\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c9b2aa-960e-499b-b803-617851411d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# 1) Keep only numeric columns\n",
    "num_cols = cc_model_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# 2) (Recommended) Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(cc_model_df[num_cols]), columns=num_cols, index=cc_model_df.index)\n",
    "\n",
    "# 3) Compute VIF\n",
    "vif_table = compute_vif(X_scaled)\n",
    "print(vif_table)\n",
    "\n",
    "### Reviewing Variables for Model Inclusion\n",
    "\n",
    "def set_manual_column_order(df,column_name,column_order):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df[column_name] = pd.Categorical(df[column_name], categories=column_order, ordered=True)\n",
    "\n",
    "import timeit\n",
    "from UtilityFunctions import PauseProcess\n",
    "from eda_dq import column_statistical_review\n",
    "\n",
    "def data_preparation_checklist(df=pd.DataFrame(),word_list=[]):\n",
    "    if len(df)==0:\n",
    "        df = pd.read_csv(links['d_learning_notes_url'])\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for word in word_list:\n",
    "        temp_df = df[(df['Definition'].fillna(\"\").str.contains(word,case=False))|(df['Categorization']==word)]\n",
    "        final_df = pd.concat([final_df,temp_df])\n",
    "\n",
    "    final_df = final_df.drop(['Source','Process','Categorization'],axis=1)\n",
    "    display(final_df)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def build_member_snapshot(final_mbr_df: pd.DataFrame,\n",
    "                          final_txn_df: pd.DataFrame,\n",
    "                          snapshot_month=None,\n",
    "                          window_3m=3,\n",
    "                          window_6m=6) -> pd.DataFrame:\n",
    "    m = final_mbr_df.copy()\n",
    "    t = final_txn_df.copy()\n",
    "\n",
    "    # normalize month to month-start timestamp\n",
    "    m[\"MONTH\"] = _month_floor(m[\"MONTH\"])\n",
    "    t[\"MONTH\"] = _month_floor(t[\"MONTH\"])\n",
    "\n",
    "    if snapshot_month is None:\n",
    "        snapshot_month = m[\"MONTH\"].max()\n",
    "    snapshot_month = pd.to_datetime(snapshot_month).to_period(\"M\").to_timestamp()\n",
    "\n",
    "    # restrict to history up to snapshot month\n",
    "    m = m[m[\"MONTH\"] <= snapshot_month]\n",
    "    t = t[t[\"MONTH\"] <= snapshot_month]\n",
    "\n",
    "    # ----- Member-month features windows -----\n",
    "    # Keep a lean set of signals from your schema\n",
    "    base_cols = [\n",
    "        \"MEMBERNBR\",\"MONTH\",\"AGE\",\"CITY\",\"BRANCHNAME\",\"PRIMARY_IS_BEEM\",\"HOME_OWNER\",\n",
    "        \"ANNUAL_INCOME\",\"DEPOSIT_BALANCE\",\"MORTGAGE_BALANCE\",\"LIQUID_ASSETS\",\n",
    "        \"ENGAGEMENT_SCORE\",\"PERC_DEPOSIT\",\"PERC_MORTGAGE\",\n",
    "        \"POS_TXN_DEBIT\",\"BILL_PAY_DEBIT\",\"PAYROLL_DEPOSIT\",\"OTHER_DEPOSIT\"\n",
    "    ]\n",
    "    base_cols = [c for c in base_cols if c in m.columns]\n",
    "    m = m[base_cols].sort_values([\"MEMBERNBR\",\"MONTH\"])\n",
    "\n",
    "    # helper: window filter\n",
    "    start_3m = snapshot_month - pd.DateOffset(months=window_3m-1)\n",
    "    start_6m = snapshot_month - pd.DateOffset(months=window_6m-1)\n",
    "\n",
    "    m3 = m[m[\"MONTH\"].between(start_3m, snapshot_month)]\n",
    "    m6 = m[m[\"MONTH\"].between(start_6m, snapshot_month)]\n",
    "\n",
    "    # 3m aggregates (means/sums)\n",
    "    agg_3m = m3.groupby(\"MEMBERNBR\").agg(\n",
    "        age=(\"AGE\",\"last\"),\n",
    "        city=(\"CITY\",\"last\"),\n",
    "        branch=(\"BRANCHNAME\",\"last\"),\n",
    "        primary=(\"PRIMARY_IS_BEEM\",\"last\"),\n",
    "        home_owner=(\"HOME_OWNER\",\"last\"),\n",
    "        income_mean_3m=(\"ANNUAL_INCOME\",\"mean\"),\n",
    "        deposit_mean_3m=(\"DEPOSIT_BALANCE\",\"mean\"),\n",
    "        loan_mean_3m=(\"MORTGAGE_BALANCE\",\"mean\"),\n",
    "        engagement_mean_3m=(\"ENGAGEMENT_SCORE\",\"mean\"),\n",
    "        pos_sum_3m=(\"POS_TXN_DEBIT\",\"sum\"),\n",
    "        bill_sum_3m=(\"BILL_PAY_DEBIT\",\"sum\"),\n",
    "        payroll_sum_3m=(\"PAYROLL_DEPOSIT\",\"sum\"),\n",
    "        otherdep_sum_3m=(\"OTHER_DEPOSIT\",\"sum\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # 6m dynamics: volatility + trend (slope)\n",
    "    def dyn_features(df, col):\n",
    "        return df.groupby(\"MEMBERNBR\")[col].agg(\n",
    "            **{f\"{col.lower()}_std_6m\":\"std\",\n",
    "               f\"{col.lower()}_slope_6m\":_slope}\n",
    "        )\n",
    "\n",
    "    dep_dyn = m6.groupby(\"MEMBERNBR\")[\"DEPOSIT_BALANCE\"].agg(\n",
    "        deposit_std_6m=\"std\",\n",
    "        deposit_slope_6m=_slope\n",
    "    )\n",
    "    loan_dyn = m6.groupby(\"MEMBERNBR\")[\"MORTGAGE_BALANCE\"].agg(\n",
    "        loan_std_6m=\"std\",\n",
    "        loan_slope_6m=_slope\n",
    "    )\n",
    "    eng_dyn = m6.groupby(\"MEMBERNBR\")[\"ENGAGEMENT_SCORE\"].agg(\n",
    "        engagement_std_6m=\"std\",\n",
    "        engagement_slope_6m=_slope\n",
    "    )\n",
    "\n",
    "    dyn = pd.concat([dep_dyn, loan_dyn, eng_dyn], axis=1).reset_index()\n",
    "\n",
    "    # ----- Transaction mix features (from your long txn table) -----\n",
    "    # Build 3m spend mix + entropy (behavioral fingerprint)\n",
    "    t3 = t[t[\"MONTH\"].between(start_3m, snapshot_month)].copy()\n",
    "    # aggregate category totals per member\n",
    "    cat = t3.groupby([\"MEMBERNBR\",\"variable\"])[\"value\"].sum().reset_index()\n",
    "    total = cat.groupby(\"MEMBERNBR\")[\"value\"].sum().rename(\"txn_total_3m\").reset_index()\n",
    "    cat = cat.merge(total, on=\"MEMBERNBR\", how=\"left\")\n",
    "    cat[\"share\"] = np.where(cat[\"txn_total_3m\"] > 0, cat[\"value\"] / cat[\"txn_total_3m\"], 0.0)\n",
    "\n",
    "    # pivot shares wide: share_<category>\n",
    "    mix = cat.pivot_table(index=\"MEMBERNBR\", columns=\"variable\", values=\"share\", fill_value=0.0)\n",
    "    mix.columns = [f\"share_{c}\" for c in mix.columns]\n",
    "    mix = mix.reset_index()\n",
    "\n",
    "    # entropy: higher = more diverse mix\n",
    "    # (avoid log(0) by masking)\n",
    "    share_mat = mix.drop(columns=[\"MEMBERNBR\"]).to_numpy(dtype=float)\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        ent = -(share_mat * np.log(np.where(share_mat > 0, share_mat, 1.0))).sum(axis=1)\n",
    "    mix[\"mix_entropy_3m\"] = ent\n",
    "\n",
    "    # ----- Combine -----\n",
    "    snap = agg_3m.merge(dyn, on=\"MEMBERNBR\", how=\"left\").merge(mix, on=\"MEMBERNBR\", how=\"left\")\n",
    "\n",
    "    # convenience derived features\n",
    "    snap[\"net_worth_mean_3m\"] = snap[\"deposit_mean_3m\"].fillna(0) - snap[\"loan_mean_3m\"].fillna(0)\n",
    "    snap[\"snapshot_month\"] = snapshot_month\n",
    "\n",
    "    # fill missing mix features with 0 (e.g., no txns in window)\n",
    "    share_cols = [c for c in snap.columns if c.startswith(\"share_\")]\n",
    "    snap[share_cols] = snap[share_cols].fillna(0.0)\n",
    "    for c in [\"deposit_std_6m\",\"deposit_slope_6m\",\"loan_std_6m\",\"loan_slope_6m\",\n",
    "              \"engagement_std_6m\",\"engagement_slope_6m\",\"mix_entropy_3m\"]:\n",
    "        if c in snap.columns:\n",
    "            snap[c] = snap[c].fillna(0.0)\n",
    "\n",
    "    return snap\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def make_preprocess_pipeline(feature_df: pd.DataFrame,\n",
    "                             cat_cols=(\"city\",\"branch\"),\n",
    "                             force_drop=(\"MEMBERNBR\",\"snapshot_month\")):\n",
    "    df = feature_df.copy()\n",
    "\n",
    "    # numeric candidates: everything number-ish except ids\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_cols = [c for c in num_cols if c not in force_drop]\n",
    "\n",
    "    cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "\n",
    "    # log-transform for heavy-tailed money-ish cols\n",
    "    log_cols = [c for c in num_cols if any(k in c for k in [\"deposit\",\"loan\",\"income\",\"pos_\",\"bill_\",\"payroll\",\"otherdep\",\"net_worth\"])]\n",
    "    passthrough_cols = [c for c in num_cols if c not in log_cols]\n",
    "\n",
    "    log_pipe = Pipeline([\n",
    "        (\"log1p\", FunctionTransformer(lambda x: np.log1p(np.clip(x, 0, None)))),\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ])\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"log_num\", log_pipe, log_cols),\n",
    "            (\"num\", num_pipe, passthrough_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "# final_mbr_df, final_txn_df = generate_synthetic_dataset(...)\n",
    "\n",
    "snap = build_member_snapshot(final_mbr_df, final_txn_df, snapshot_month=None, window_3m=3, window_6m=6)\n",
    "\n",
    "feature_snap = snap.drop(columns=['MEMBERNBR','snapshot_month'],errors='ignore')\n",
    "member_ids = snap['MEMBERNBR'].to_numpy()\n",
    "\n",
    "pre = make_preprocess_pipeline(feature_snap)\n",
    "X = pre.fit_transform(feature_snap)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# Cant run because it errors out, would be 22 Billion . Need to Understand this\n",
    "#S = cosine_similarity(X)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=51,metric='cosine',algorithm='auto')\n",
    "\n",
    "nn.fit(X)\n",
    "\n",
    "member_index = {mid: i for i, mid in enumerate(member_ids)}\n",
    "\n",
    "def test_knn(X,nn,k=10,sample_size=100,seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idxs = rng.choice(X.shape[0],size=sample_size,replace=False)\n",
    "\n",
    "    # returns the distance and Index\n",
    "    nn_dist,nn_index = nn.kneighbors(X[idxs],n_neighbors=k+1)\n",
    "    nn_dist = nn_dist[:,1:].mean(axis=1)\n",
    "    \n",
    "    rand_idxs = rng.choice(X.shape[0],size=(sample_size,k),replace=True)\n",
    "    rand_dist = np.linalg.norm(X[idxs][:,None,:]-X[rand_idxs],axis=2).mean(axis=1)\n",
    "    \n",
    "    return nn_dist,rand_dist,idxs\n",
    "\n",
    "\n",
    "a,b,c = test_knn(X,nn)\n",
    "\n",
    "mid = int(member_ids[0])\n",
    "neighbors, sim = knn_topk(nn, member_index, member_ids, X, mid, k=10)\n",
    "list(zip(neighbors.tolist(), sim.tolist()))\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# What does StandardScaler do. Transforms data to Mean 0 and Standard Dev = 1\\\n",
    "\n",
    "def Heatmap(df,\n",
    "            correlation=True,\n",
    "            column_list=[],\n",
    "            title='Heat Map of Correlation',\n",
    "            cmap='coolwarm',\n",
    "            annotate=True,\n",
    "            x_rotate=0,\n",
    "            y_rotate=0,\n",
    "            cbar=True,\n",
    "            set_center=0,\n",
    "            figsize=(10,10)):\n",
    "    \n",
    "    '''\n",
    "    Function Which Generates a Heatmap\n",
    "    \n",
    "    Parameters:\n",
    "        Dataframe\n",
    "        column_name (list): If included, will only show certain columns on the Horizontal Axis.\n",
    "    \n",
    "    Returns:\n",
    "        matlplot plot.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    sns.set(style='white')\n",
    "    \n",
    "    # View column with Abbreviated title or full. Abbreviated displays nicer.\n",
    "    if correlation:\n",
    "        corr = df.corr()\n",
    "    else:\n",
    "        corr = df.copy()\n",
    "    \n",
    "    if len(column_list)!=0:\n",
    "        corr = corr[column_list]\n",
    "    \n",
    "    mask= np.zeros_like(corr,dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)]=True\n",
    "    f,ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    if len(str(set_center))!=0:\n",
    "        sns.heatmap(corr,mask=mask,cmap=cmap,center=set_center,square=True,linewidths=1,annot=annotate,cbar=cbar)\n",
    "    else:\n",
    "        sns.heatmap(corr,mask=mask,cmap=cmap,square=True,linewidths=1,annot=annotate,cbar=cbar)\n",
    "    \n",
    "    \n",
    "    plt.title(title)\n",
    "    if y_rotate !=0:\n",
    "        for tick in ax.get_yticklabels():\n",
    "            tick.set_rotation(0)\n",
    "            tick.set_horizontalalignment('right')\n",
    "    if x_rotate !=0:\n",
    "        plt.xticks(rotation=x_rotate,ha='center', va='top')\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
