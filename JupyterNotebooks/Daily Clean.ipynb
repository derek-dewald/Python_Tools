{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663ab9a9-969c-4f53-8312-70ae0c0946ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662dfd4d-0774-4c43-8edf-69e646c61d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = {\n",
    "    'Clustering':[\n",
    "        'How can I mathematically Explain a Cluster.',\n",
    "        'What is important.',\n",
    "        'How can I determine what to drop, and when.',\n",
    "        'How do I understand time components in Clusters.'\n",
    "        ' The right way to think about “sweet spot”. The “sweet spot” is not: maximum variance explained, maximum number of features, maximum complexity It is: stable clusters, interpretable differences, consistent behavior across runs, sensitivity that matches your use case (short-term vs long-term behavior)',\n",
    "        'Why ratios can be problematic in clustering.',\n",
    "        'Ratios are: high signal, high risk. They:amplify noise, can dominate PCA, are hard to interpret. Rule: Try models with and without ratios Prefer simpler models unless ratios clearly add value. Ratios must earn their place.'\n",
    "        \n",
    "    ]\n",
    "\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c9b2aa-960e-499b-b803-617851411d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# 1) Keep only numeric columns\n",
    "num_cols = cc_model_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# 2) (Recommended) Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(cc_model_df[num_cols]), columns=num_cols, index=cc_model_df.index)\n",
    "\n",
    "# 3) Compute VIF\n",
    "vif_table = compute_vif(X_scaled)\n",
    "print(vif_table)\n",
    "\n",
    "### Reviewing Variables for Model Inclusion\n",
    "\n",
    "def set_manual_column_order(df,column_name,column_order):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df[column_name] = pd.Categorical(df[column_name], categories=column_order, ordered=True)\n",
    "\n",
    "import timeit\n",
    "from UtilityFunctions import PauseProcess\n",
    "from eda_dq import column_statistical_review\n",
    "\n",
    "def data_preparation_checklist(df=pd.DataFrame(),word_list=[]):\n",
    "    if len(df)==0:\n",
    "        df = pd.read_csv(links['d_learning_notes_url'])\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for word in word_list:\n",
    "        temp_df = df[(df['Definition'].fillna(\"\").str.contains(word,case=False))|(df['Categorization']==word)]\n",
    "        final_df = pd.concat([final_df,temp_df])\n",
    "\n",
    "    final_df = final_df.drop(['Source','Process','Categorization'],axis=1)\n",
    "    display(final_df)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def build_member_snapshot(final_mbr_df: pd.DataFrame,\n",
    "                          final_txn_df: pd.DataFrame,\n",
    "                          snapshot_month=None,\n",
    "                          window_3m=3,\n",
    "                          window_6m=6) -> pd.DataFrame:\n",
    "    m = final_mbr_df.copy()\n",
    "    t = final_txn_df.copy()\n",
    "\n",
    "    # normalize month to month-start timestamp\n",
    "    m[\"MONTH\"] = _month_floor(m[\"MONTH\"])\n",
    "    t[\"MONTH\"] = _month_floor(t[\"MONTH\"])\n",
    "\n",
    "    if snapshot_month is None:\n",
    "        snapshot_month = m[\"MONTH\"].max()\n",
    "    snapshot_month = pd.to_datetime(snapshot_month).to_period(\"M\").to_timestamp()\n",
    "\n",
    "    # restrict to history up to snapshot month\n",
    "    m = m[m[\"MONTH\"] <= snapshot_month]\n",
    "    t = t[t[\"MONTH\"] <= snapshot_month]\n",
    "\n",
    "    # ----- Member-month features windows -----\n",
    "    # Keep a lean set of signals from your schema\n",
    "    base_cols = [\n",
    "        \"MEMBERNBR\",\"MONTH\",\"AGE\",\"CITY\",\"BRANCHNAME\",\"PRIMARY_IS_BEEM\",\"HOME_OWNER\",\n",
    "        \"ANNUAL_INCOME\",\"DEPOSIT_BALANCE\",\"MORTGAGE_BALANCE\",\"LIQUID_ASSETS\",\n",
    "        \"ENGAGEMENT_SCORE\",\"PERC_DEPOSIT\",\"PERC_MORTGAGE\",\n",
    "        \"POS_TXN_DEBIT\",\"BILL_PAY_DEBIT\",\"PAYROLL_DEPOSIT\",\"OTHER_DEPOSIT\"\n",
    "    ]\n",
    "    base_cols = [c for c in base_cols if c in m.columns]\n",
    "    m = m[base_cols].sort_values([\"MEMBERNBR\",\"MONTH\"])\n",
    "\n",
    "    # helper: window filter\n",
    "    start_3m = snapshot_month - pd.DateOffset(months=window_3m-1)\n",
    "    start_6m = snapshot_month - pd.DateOffset(months=window_6m-1)\n",
    "\n",
    "    m3 = m[m[\"MONTH\"].between(start_3m, snapshot_month)]\n",
    "    m6 = m[m[\"MONTH\"].between(start_6m, snapshot_month)]\n",
    "\n",
    "    # 3m aggregates (means/sums)\n",
    "    agg_3m = m3.groupby(\"MEMBERNBR\").agg(\n",
    "        age=(\"AGE\",\"last\"),\n",
    "        city=(\"CITY\",\"last\"),\n",
    "        branch=(\"BRANCHNAME\",\"last\"),\n",
    "        primary=(\"PRIMARY_IS_BEEM\",\"last\"),\n",
    "        home_owner=(\"HOME_OWNER\",\"last\"),\n",
    "        income_mean_3m=(\"ANNUAL_INCOME\",\"mean\"),\n",
    "        deposit_mean_3m=(\"DEPOSIT_BALANCE\",\"mean\"),\n",
    "        loan_mean_3m=(\"MORTGAGE_BALANCE\",\"mean\"),\n",
    "        engagement_mean_3m=(\"ENGAGEMENT_SCORE\",\"mean\"),\n",
    "        pos_sum_3m=(\"POS_TXN_DEBIT\",\"sum\"),\n",
    "        bill_sum_3m=(\"BILL_PAY_DEBIT\",\"sum\"),\n",
    "        payroll_sum_3m=(\"PAYROLL_DEPOSIT\",\"sum\"),\n",
    "        otherdep_sum_3m=(\"OTHER_DEPOSIT\",\"sum\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # 6m dynamics: volatility + trend (slope)\n",
    "    def dyn_features(df, col):\n",
    "        return df.groupby(\"MEMBERNBR\")[col].agg(\n",
    "            **{f\"{col.lower()}_std_6m\":\"std\",\n",
    "               f\"{col.lower()}_slope_6m\":_slope}\n",
    "        )\n",
    "\n",
    "    dep_dyn = m6.groupby(\"MEMBERNBR\")[\"DEPOSIT_BALANCE\"].agg(\n",
    "        deposit_std_6m=\"std\",\n",
    "        deposit_slope_6m=_slope\n",
    "    )\n",
    "    loan_dyn = m6.groupby(\"MEMBERNBR\")[\"MORTGAGE_BALANCE\"].agg(\n",
    "        loan_std_6m=\"std\",\n",
    "        loan_slope_6m=_slope\n",
    "    )\n",
    "    eng_dyn = m6.groupby(\"MEMBERNBR\")[\"ENGAGEMENT_SCORE\"].agg(\n",
    "        engagement_std_6m=\"std\",\n",
    "        engagement_slope_6m=_slope\n",
    "    )\n",
    "\n",
    "    dyn = pd.concat([dep_dyn, loan_dyn, eng_dyn], axis=1).reset_index()\n",
    "\n",
    "    # ----- Transaction mix features (from your long txn table) -----\n",
    "    # Build 3m spend mix + entropy (behavioral fingerprint)\n",
    "    t3 = t[t[\"MONTH\"].between(start_3m, snapshot_month)].copy()\n",
    "    # aggregate category totals per member\n",
    "    cat = t3.groupby([\"MEMBERNBR\",\"variable\"])[\"value\"].sum().reset_index()\n",
    "    total = cat.groupby(\"MEMBERNBR\")[\"value\"].sum().rename(\"txn_total_3m\").reset_index()\n",
    "    cat = cat.merge(total, on=\"MEMBERNBR\", how=\"left\")\n",
    "    cat[\"share\"] = np.where(cat[\"txn_total_3m\"] > 0, cat[\"value\"] / cat[\"txn_total_3m\"], 0.0)\n",
    "\n",
    "    # pivot shares wide: share_<category>\n",
    "    mix = cat.pivot_table(index=\"MEMBERNBR\", columns=\"variable\", values=\"share\", fill_value=0.0)\n",
    "    mix.columns = [f\"share_{c}\" for c in mix.columns]\n",
    "    mix = mix.reset_index()\n",
    "\n",
    "    # entropy: higher = more diverse mix\n",
    "    # (avoid log(0) by masking)\n",
    "    share_mat = mix.drop(columns=[\"MEMBERNBR\"]).to_numpy(dtype=float)\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        ent = -(share_mat * np.log(np.where(share_mat > 0, share_mat, 1.0))).sum(axis=1)\n",
    "    mix[\"mix_entropy_3m\"] = ent\n",
    "\n",
    "    # ----- Combine -----\n",
    "    snap = agg_3m.merge(dyn, on=\"MEMBERNBR\", how=\"left\").merge(mix, on=\"MEMBERNBR\", how=\"left\")\n",
    "\n",
    "    # convenience derived features\n",
    "    snap[\"net_worth_mean_3m\"] = snap[\"deposit_mean_3m\"].fillna(0) - snap[\"loan_mean_3m\"].fillna(0)\n",
    "    snap[\"snapshot_month\"] = snapshot_month\n",
    "\n",
    "    # fill missing mix features with 0 (e.g., no txns in window)\n",
    "    share_cols = [c for c in snap.columns if c.startswith(\"share_\")]\n",
    "    snap[share_cols] = snap[share_cols].fillna(0.0)\n",
    "    for c in [\"deposit_std_6m\",\"deposit_slope_6m\",\"loan_std_6m\",\"loan_slope_6m\",\n",
    "              \"engagement_std_6m\",\"engagement_slope_6m\",\"mix_entropy_3m\"]:\n",
    "        if c in snap.columns:\n",
    "            snap[c] = snap[c].fillna(0.0)\n",
    "\n",
    "    return snap\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def make_preprocess_pipeline(feature_df: pd.DataFrame,\n",
    "                             cat_cols=(\"city\",\"branch\"),\n",
    "                             force_drop=(\"MEMBERNBR\",\"snapshot_month\")):\n",
    "    df = feature_df.copy()\n",
    "\n",
    "    # numeric candidates: everything number-ish except ids\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_cols = [c for c in num_cols if c not in force_drop]\n",
    "\n",
    "    cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "\n",
    "    # log-transform for heavy-tailed money-ish cols\n",
    "    log_cols = [c for c in num_cols if any(k in c for k in [\"deposit\",\"loan\",\"income\",\"pos_\",\"bill_\",\"payroll\",\"otherdep\",\"net_worth\"])]\n",
    "    passthrough_cols = [c for c in num_cols if c not in log_cols]\n",
    "\n",
    "    log_pipe = Pipeline([\n",
    "        (\"log1p\", FunctionTransformer(lambda x: np.log1p(np.clip(x, 0, None)))),\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ])\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"log_num\", log_pipe, log_cols),\n",
    "            (\"num\", num_pipe, passthrough_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "# final_mbr_df, final_txn_df = generate_synthetic_dataset(...)\n",
    "\n",
    "snap = build_member_snapshot(final_mbr_df, final_txn_df, snapshot_month=None, window_3m=3, window_6m=6)\n",
    "\n",
    "feature_snap = snap.drop(columns=['MEMBERNBR','snapshot_month'],errors='ignore')\n",
    "member_ids = snap['MEMBERNBR'].to_numpy()\n",
    "\n",
    "pre = make_preprocess_pipeline(feature_snap)\n",
    "X = pre.fit_transform(feature_snap)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# Cant run because it errors out, would be 22 Billion . Need to Understand this\n",
    "#S = cosine_similarity(X)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=51,metric='cosine',algorithm='auto')\n",
    "\n",
    "nn.fit(X)\n",
    "\n",
    "member_index = {mid: i for i, mid in enumerate(member_ids)}\n",
    "\n",
    "def test_knn(X,nn,k=10,sample_size=100,seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idxs = rng.choice(X.shape[0],size=sample_size,replace=False)\n",
    "\n",
    "    # returns the distance and Index\n",
    "    nn_dist,nn_index = nn.kneighbors(X[idxs],n_neighbors=k+1)\n",
    "    nn_dist = nn_dist[:,1:].mean(axis=1)\n",
    "    \n",
    "    rand_idxs = rng.choice(X.shape[0],size=(sample_size,k),replace=True)\n",
    "    rand_dist = np.linalg.norm(X[idxs][:,None,:]-X[rand_idxs],axis=2).mean(axis=1)\n",
    "    \n",
    "    return nn_dist,rand_dist,idxs\n",
    "\n",
    "\n",
    "a,b,c = test_knn(X,nn)\n",
    "\n",
    "mid = int(member_ids[0])\n",
    "neighbors, sim = knn_topk(nn, member_index, member_ids, X, mid, k=10)\n",
    "list(zip(neighbors.tolist(), sim.tolist()))\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# What does StandardScaler do. Transforms data to Mean 0 and Standard Dev = 1\\\n",
    "\n",
    "def Heatmap(df,\n",
    "            correlation=True,\n",
    "            column_list=[],\n",
    "            title='Heat Map of Correlation',\n",
    "            cmap='coolwarm',\n",
    "            annotate=True,\n",
    "            x_rotate=0,\n",
    "            y_rotate=0,\n",
    "            cbar=True,\n",
    "            set_center=0,\n",
    "            figsize=(10,10)):\n",
    "    \n",
    "    '''\n",
    "    Function Which Generates a Heatmap\n",
    "    \n",
    "    Parameters:\n",
    "        Dataframe\n",
    "        column_name (list): If included, will only show certain columns on the Horizontal Axis.\n",
    "    \n",
    "    Returns:\n",
    "        matlplot plot.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    sns.set(style='white')\n",
    "    \n",
    "    # View column with Abbreviated title or full. Abbreviated displays nicer.\n",
    "    if correlation:\n",
    "        corr = df.corr()\n",
    "    else:\n",
    "        corr = df.copy()\n",
    "    \n",
    "    if len(column_list)!=0:\n",
    "        corr = corr[column_list]\n",
    "    \n",
    "    mask= np.zeros_like(corr,dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)]=True\n",
    "    f,ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    if len(str(set_center))!=0:\n",
    "        sns.heatmap(corr,mask=mask,cmap=cmap,center=set_center,square=True,linewidths=1,annot=annotate,cbar=cbar)\n",
    "    else:\n",
    "        sns.heatmap(corr,mask=mask,cmap=cmap,square=True,linewidths=1,annot=annotate,cbar=cbar)\n",
    "    \n",
    "    \n",
    "    plt.title(title)\n",
    "    if y_rotate !=0:\n",
    "        for tick in ax.get_yticklabels():\n",
    "            tick.set_rotation(0)\n",
    "            tick.set_horizontalalignment('right')\n",
    "    if x_rotate !=0:\n",
    "        plt.xticks(rotation=x_rotate,ha='center', va='top')\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
