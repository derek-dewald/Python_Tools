{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9df0276-f143-4f0e-a7be-7f144b7f482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_d_dicts import links\n",
    "\n",
    "df = pd.read_csv(links['google_definition_csv'])[['Process','Categorization','Word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77b97365-651f-4544-8d88-cc0974211fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      8.0\n",
       "1      8.0\n",
       "2      8.0\n",
       "3      8.0\n",
       "4      8.0\n",
       "      ... \n",
       "443    NaN\n",
       "444    NaN\n",
       "445    NaN\n",
       "446    NaN\n",
       "447    NaN\n",
       "Name: Word, Length: 448, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18deb0ce-c100-4f9c-a437-de475d999ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Categorization</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Dimension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Implementation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Momentum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Parameter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Volatility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Regularization</td>\n",
       "      <td>ElasticNet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Regularization</td>\n",
       "      <td>Lasso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Regularization</td>\n",
       "      <td>Ridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Model Evaluation</td>\n",
       "      <td>Feature Selection</td>\n",
       "      <td>Area Under the Curve</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Process     Categorization                  Word\n",
       "0  Data Preparation         Definition              Baseline\n",
       "1  Data Preparation         Definition             Dimension\n",
       "2  Data Preparation         Definition        Implementation\n",
       "3  Data Preparation         Definition              Momentum\n",
       "4  Data Preparation         Definition             Parameter\n",
       "5  Data Preparation         Definition            Volatility\n",
       "6  Data Preparation     Regularization            ElasticNet\n",
       "7  Data Preparation     Regularization                 Lasso\n",
       "8  Data Preparation     Regularization                 Ridge\n",
       "9  Model Evaluation  Feature Selection  Area Under the Curve"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "872a2636-f5f6-4e05-b0f4-3ea430934075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'Datasets.ipynb',\n",
       " 'Pokemon',\n",
       " '.ipynb_checkpoints',\n",
       " 'daily_test_results.csv']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec0a188f-89dc-462b-8871-e7ed88a7e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Process_Count\"] = df.groupby(\"Process\")[\"Process\"].transform(\"count\")\n",
    "df[\"CAT_Count\"] = df.groupby(\"Categorization\")[\"Categorization\"].transform(\"count\")\n",
    "df[\"Word_Count\"] = df.groupby(\"Word\")[\"Word\"].transform(\"count\")\n",
    "df['ProcessCAT_Count'] = df.groupby(['Process','Categorization'])[\"Word\"].transform(\"count\")\n",
    "\n",
    "df.to_csv('/Users/derekdewald/Documents/Python/Github_Repo/Data/Streamlit_DefinitionSummary.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61771aa3-e087-4de6-970a-ea7bb6a87100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1519ae6a-f0c7-4054-b7c3-50ed12cbad5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following words, can you please provide me for each word a definition which is 4-5 sentences highlighting the definition,\n",
      "its origin, its importance and application. If the word is a ML Model, can you also please define the following in point form,\n",
      "type of ML learning, type of ML problem, size of data set it works best on, and anything else which might be appropriate.\n",
      "Unique Words: Davies–Bouldin, Calinski–Harabasz\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from data_d_strings import gpt_question\n",
    "\n",
    "gpt_question(['Davies–Bouldin','Calinski–Harabasz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35022a0-3996-447f-bbb5-9026de2178cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "1. Correlation & Redundancy Analysis\n",
    "    1. If Greater than .9 then redudanant, as a rule of thumb, newest is best, but can be based on decision\n",
    "2. Variance & Stability Contribution\n",
    "    1. Variance Inflation Factor\n",
    "3. Clustering Sensitivity / Stability Tests\n",
    "    1. Testing Results of Model\n",
    "    2. Adjusted Rand Index (ARI)\n",
    "    3. Silhouette/ DB Index\n",
    "    4. Run Now and Overtime\n",
    "4. Incremental Value via Feature Importance in Unsupervised Models\n",
    "    1. Reconstruction Error in Autoencoders\n",
    "    2. Mutual Information with Latent Cluster Assignment\n",
    "    3. Permutation for Clustering.\n",
    "\n",
    "\n",
    "Mechanism\n",
    "Structure\n",
    "Action and Learning.\n",
    "\n",
    "Purpose and Contraints.\n",
    "Stabiltiy and Change. \n",
    "\n",
    "Causation and Correlation. We're just looking at Correlation. Not Causation\n",
    "\n",
    "Clustering utilizes mathematical precision to identify \n",
    "\n",
    "## Need Process for Clustering to Document decision on inclusion as it tests and score tests results somewhere central with rationale\n",
    "\n",
    "1. Identify and validate the behavioural attributes, transactions, and usage patterns that define similarity among members\n",
    "2. Understand the relative contribution and sensitivity of features in forming and separating clusters\n",
    "3. Identify and define distinct, stable member archetypes based on observed behavioural patterns\n",
    "4. Characterize each archetype using clear, interpretable behavioural and financial characteristics\n",
    "5. Define the intended use, boundaries, and decision domains for archetype-based strategies\n",
    "6. Design and apply differentiated engagement strategies aligned to each archetype’s characteristics and needs\n",
    "7. Monitor archetype-level outcomes, migration, and engagement effectiveness over time\n",
    "\n",
    "If this project succeeds, you should be able to answer:\n",
    "* “Which archetype are we intentionally trying to grow?”\n",
    "* “Which archetype are we comfortable shrinking?”\n",
    "* “Which engagement changed member behavior within a cluster?”\n",
    "* “Which engagement caused members to migrate to a better archetype?”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80becb5-44ad-4257-9109-0594a67db0fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b35e25-764e-4165-b001-1fe03426339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Jan 7\n",
    "Kmeans Clustering, Parameter = n_init='auto' KMeans is sensitive to initial centroid positions. A bad initialization can lead to poor clustering (high inertia). To mitigate this, KMeans runs multiple times with different random initializations and picks the best run (lowest inertia)\n",
    "\n",
    "Interia\n",
    "\n",
    "The Davies–Bouldin Index measures clustering quality by evaluating the average similarity between each cluster and its most similar cluster, based on the ratio of within-cluster scatter to between-cluster separation. Lower values indicate better clustering because clusters are compact and well-separated. It is useful when comparing different clustering models or parameter settings, especially for algorithms like K-Means. Expected values: closer to 0 is better, and there is no fixed upper bound—higher values suggest overlapping or poorly separated clusters.\n",
    "\n",
    "The Calinski–Harabasz Index (Variance Ratio Criterion) assesses clustering quality by comparing the dispersion between clusters to the dispersion within clusters. Higher values indicate better-defined clusters because they are more separated and internally cohesive. It is commonly used for model selection, such as choosing the optimal number of clusters in K-Means or hierarchical clustering. Expected values: larger is better, and the score tends to increase with more distinct clusters and decrease when clusters overlap\n",
    "\n",
    "It calculates the ratio of the sum of squared distances between cluster centers (between-group variance) to the sum of squared distances within clusters (within-group variance), scaled by the number of clusters and points.\n",
    "\n",
    "\n",
    "< 1.0 → Generally considered good clustering.\n",
    "1–2 → Acceptable but could be improved.\n",
    "> 2 → Often suggests poor separation or overlapping clusters.\n",
    "\n",
    "\n",
    "# Wall Time Versus Process Time\n",
    "time.process_time()\n",
    "time.perf_counter()\n",
    "\n",
    "##########################################################################\n",
    "Jan 6\n",
    "- Clarify what Clustering is. \n",
    "    - Which Features Create a Stable, Interpretable and Useful Segment. \n",
    "    - Not which Features are most important.\n",
    "- Pipeline - Drop Characteristics.\n",
    "- Change Cluster Size.\n",
    "- Articulate Meaning of Clusters and Explain Grouping. \n",
    "- How do Edge Cases Impact.\n",
    "- Do I need to break up my Definitions based on Time. TO include Shorter and Longer Term.\n",
    "- How can I quantify, measure and explain Clusters.\n",
    "\n",
    "The right way to think about “sweet spot”\n",
    "The “sweet spot” is not:\n",
    "* maximum variance explained\n",
    "* maximum number of features\n",
    "* maximum complexity\n",
    "It is:\n",
    "* stable clusters\n",
    "* interpretable differences\n",
    "* consistent behavior across runs\n",
    "* sensitivity that matches your use case (short-term vs long-term behavior)\n",
    "\n",
    "Step 1 — Group features into families (critical)\n",
    "Never drop features one-by-one randomly. ",
    "Drop feature families together.\n",
    "\n",
    "Ratios are:\n",
    "* high signal\n",
    "* high risk\n",
    "They:\n",
    "* amplify noise\n",
    "* can dominate PCA\n",
    "* are hard to interpret\n",
    "Rule:\n",
    "* Try models with and without ratios\n",
    "* Prefer simpler models unless ratios clearly add value\n",
    "Ratios must earn their place.\n",
    "\n",
    "Balanced behavioral model\n",
    "* 3M avg\n",
    "* 6M growth\n",
    "* engagement\n",
    "* transaction mix\n",
    "Long-term structure model\n",
    "* 6–12M avg\n",
    "* 12M growth\n",
    "* low short-term noise\n",
    "Short-term sensitivity model\n",
    "* 1–3M dynamics\n",
    "* volatility\n",
    "* no long-term averages\n",
    "\n",
    "\n",
    "Silhouette: Is this point closer to points in its own cluster than to points in the nearest other cluster ",
    "\n",
    "Cohesion (how tight your cluster is)\n",
    "Separation (how far apart clusters are)\n",
    "\n",
    "For K Means. \n",
    "Does this variable differ meaningfully between clusters?\n",
    "\n",
    "Fit Predict, You get one thing only:\n",
    "The index of the nearest centroid for each observation\n",
    "That’s it.\n",
    "Important clarifications:\n",
    "* It does not tell you how confident the assignment is\n",
    "* It does not tell you how far a point is from its centroid\n",
    "* It does not tell you how ambiguous the assignment was\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c9b2aa-960e-499b-b803-617851411d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Example usage ---\n",
    "# 1) Keep only numeric columns\n",
    "num_cols = cc_model_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# 2) (Recommended) Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(cc_model_df[num_cols]), columns=num_cols, index=cc_model_df.index)\n",
    "\n",
    "# 3) Compute VIF\n",
    "vif_table = compute_vif(X_scaled)\n",
    "print(vif_table)\n",
    "\n",
    "### Reviewing Variables for Model Inclusion\n",
    "\n",
    "def set_manual_column_order(df,column_name,column_order):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df[column_name] = pd.Categorical(df[column_name], categories=column_order, ordered=True)\n",
    "\n",
    "import timeit\n",
    "from UtilityFunctions import PauseProcess\n",
    "from eda_dq import column_statistical_review\n",
    "\n",
    "def data_preparation_checklist(df=pd.DataFrame(),word_list=[]):\n",
    "    if len(df)==0:\n",
    "        df = pd.read_csv(links['d_learning_notes_url'])\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for word in word_list:\n",
    "        temp_df = df[(df['Definition'].fillna(\"\").str.contains(word,case=False))|(df['Categorization']==word)]\n",
    "        final_df = pd.concat([final_df,temp_df])\n",
    "\n",
    "    final_df = final_df.drop(['Source','Process','Categorization'],axis=1)\n",
    "    display(final_df)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def build_member_snapshot(final_mbr_df: pd.DataFrame,\n",
    "                          final_txn_df: pd.DataFrame,\n",
    "                          snapshot_month=None,\n",
    "                          window_3m=3,\n",
    "                          window_6m=6) -> pd.DataFrame:\n",
    "    m = final_mbr_df.copy()\n",
    "    t = final_txn_df.copy()\n",
    "\n",
    "    # normalize month to month-start timestamp\n",
    "    m[\"MONTH\"] = _month_floor(m[\"MONTH\"])\n",
    "    t[\"MONTH\"] = _month_floor(t[\"MONTH\"])\n",
    "\n",
    "    if snapshot_month is None:\n",
    "        snapshot_month = m[\"MONTH\"].max()\n",
    "    snapshot_month = pd.to_datetime(snapshot_month).to_period(\"M\").to_timestamp()\n",
    "\n",
    "    # restrict to history up to snapshot month\n",
    "    m = m[m[\"MONTH\"] <= snapshot_month]\n",
    "    t = t[t[\"MONTH\"] <= snapshot_month]\n",
    "\n",
    "    # ----- Member-month features windows -----\n",
    "    # Keep a lean set of signals from your schema\n",
    "    base_cols = [\n",
    "        \"MEMBERNBR\",\"MONTH\",\"AGE\",\"CITY\",\"BRANCHNAME\",\"PRIMARY_IS_BEEM\",\"HOME_OWNER\",\n",
    "        \"ANNUAL_INCOME\",\"DEPOSIT_BALANCE\",\"MORTGAGE_BALANCE\",\"LIQUID_ASSETS\",\n",
    "        \"ENGAGEMENT_SCORE\",\"PERC_DEPOSIT\",\"PERC_MORTGAGE\",\n",
    "        \"POS_TXN_DEBIT\",\"BILL_PAY_DEBIT\",\"PAYROLL_DEPOSIT\",\"OTHER_DEPOSIT\"\n",
    "    ]\n",
    "    base_cols = [c for c in base_cols if c in m.columns]\n",
    "    m = m[base_cols].sort_values([\"MEMBERNBR\",\"MONTH\"])\n",
    "\n",
    "    # helper: window filter\n",
    "    start_3m = snapshot_month - pd.DateOffset(months=window_3m-1)\n",
    "    start_6m = snapshot_month - pd.DateOffset(months=window_6m-1)\n",
    "\n",
    "    m3 = m[m[\"MONTH\"].between(start_3m, snapshot_month)]\n",
    "    m6 = m[m[\"MONTH\"].between(start_6m, snapshot_month)]\n",
    "\n",
    "    # 3m aggregates (means/sums)\n",
    "    agg_3m = m3.groupby(\"MEMBERNBR\").agg(\n",
    "        age=(\"AGE\",\"last\"),\n",
    "        city=(\"CITY\",\"last\"),\n",
    "        branch=(\"BRANCHNAME\",\"last\"),\n",
    "        primary=(\"PRIMARY_IS_BEEM\",\"last\"),\n",
    "        home_owner=(\"HOME_OWNER\",\"last\"),\n",
    "        income_mean_3m=(\"ANNUAL_INCOME\",\"mean\"),\n",
    "        deposit_mean_3m=(\"DEPOSIT_BALANCE\",\"mean\"),\n",
    "        loan_mean_3m=(\"MORTGAGE_BALANCE\",\"mean\"),\n",
    "        engagement_mean_3m=(\"ENGAGEMENT_SCORE\",\"mean\"),\n",
    "        pos_sum_3m=(\"POS_TXN_DEBIT\",\"sum\"),\n",
    "        bill_sum_3m=(\"BILL_PAY_DEBIT\",\"sum\"),\n",
    "        payroll_sum_3m=(\"PAYROLL_DEPOSIT\",\"sum\"),\n",
    "        otherdep_sum_3m=(\"OTHER_DEPOSIT\",\"sum\"),\n",
    "    ).reset_index()\n",
    "\n",
    "    # 6m dynamics: volatility + trend (slope)\n",
    "    def dyn_features(df, col):\n",
    "        return df.groupby(\"MEMBERNBR\")[col].agg(\n",
    "            **{f\"{col.lower()}_std_6m\":\"std\",\n",
    "               f\"{col.lower()}_slope_6m\":_slope}\n",
    "        )\n",
    "\n",
    "    dep_dyn = m6.groupby(\"MEMBERNBR\")[\"DEPOSIT_BALANCE\"].agg(\n",
    "        deposit_std_6m=\"std\",\n",
    "        deposit_slope_6m=_slope\n",
    "    )\n",
    "    loan_dyn = m6.groupby(\"MEMBERNBR\")[\"MORTGAGE_BALANCE\"].agg(\n",
    "        loan_std_6m=\"std\",\n",
    "        loan_slope_6m=_slope\n",
    "    )\n",
    "    eng_dyn = m6.groupby(\"MEMBERNBR\")[\"ENGAGEMENT_SCORE\"].agg(\n",
    "        engagement_std_6m=\"std\",\n",
    "        engagement_slope_6m=_slope\n",
    "    )\n",
    "\n",
    "    dyn = pd.concat([dep_dyn, loan_dyn, eng_dyn], axis=1).reset_index()\n",
    "\n",
    "    # ----- Transaction mix features (from your long txn table) -----\n",
    "    # Build 3m spend mix + entropy (behavioral fingerprint)\n",
    "    t3 = t[t[\"MONTH\"].between(start_3m, snapshot_month)].copy()\n",
    "    # aggregate category totals per member\n",
    "    cat = t3.groupby([\"MEMBERNBR\",\"variable\"])[\"value\"].sum().reset_index()\n",
    "    total = cat.groupby(\"MEMBERNBR\")[\"value\"].sum().rename(\"txn_total_3m\").reset_index()\n",
    "    cat = cat.merge(total, on=\"MEMBERNBR\", how=\"left\")\n",
    "    cat[\"share\"] = np.where(cat[\"txn_total_3m\"] > 0, cat[\"value\"] / cat[\"txn_total_3m\"], 0.0)\n",
    "\n",
    "    # pivot shares wide: share_<category>\n",
    "    mix = cat.pivot_table(index=\"MEMBERNBR\", columns=\"variable\", values=\"share\", fill_value=0.0)\n",
    "    mix.columns = [f\"share_{c}\" for c in mix.columns]\n",
    "    mix = mix.reset_index()\n",
    "\n",
    "    # entropy: higher = more diverse mix\n",
    "    # (avoid log(0) by masking)\n",
    "    share_mat = mix.drop(columns=[\"MEMBERNBR\"]).to_numpy(dtype=float)\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        ent = -(share_mat * np.log(np.where(share_mat > 0, share_mat, 1.0))).sum(axis=1)\n",
    "    mix[\"mix_entropy_3m\"] = ent\n",
    "\n",
    "    # ----- Combine -----\n",
    "    snap = agg_3m.merge(dyn, on=\"MEMBERNBR\", how=\"left\").merge(mix, on=\"MEMBERNBR\", how=\"left\")\n",
    "\n",
    "    # convenience derived features\n",
    "    snap[\"net_worth_mean_3m\"] = snap[\"deposit_mean_3m\"].fillna(0) - snap[\"loan_mean_3m\"].fillna(0)\n",
    "    snap[\"snapshot_month\"] = snapshot_month\n",
    "\n",
    "    # fill missing mix features with 0 (e.g., no txns in window)\n",
    "    share_cols = [c for c in snap.columns if c.startswith(\"share_\")]\n",
    "    snap[share_cols] = snap[share_cols].fillna(0.0)\n",
    "    for c in [\"deposit_std_6m\",\"deposit_slope_6m\",\"loan_std_6m\",\"loan_slope_6m\",\n",
    "              \"engagement_std_6m\",\"engagement_slope_6m\",\"mix_entropy_3m\"]:\n",
    "        if c in snap.columns:\n",
    "            snap[c] = snap[c].fillna(0.0)\n",
    "\n",
    "    return snap\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def make_preprocess_pipeline(feature_df: pd.DataFrame,\n",
    "                             cat_cols=(\"city\",\"branch\"),\n",
    "                             force_drop=(\"MEMBERNBR\",\"snapshot_month\")):\n",
    "    df = feature_df.copy()\n",
    "\n",
    "    # numeric candidates: everything number-ish except ids\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_cols = [c for c in num_cols if c not in force_drop]\n",
    "\n",
    "    cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "\n",
    "    # log-transform for heavy-tailed money-ish cols\n",
    "    log_cols = [c for c in num_cols if any(k in c for k in [\"deposit\",\"loan\",\"income\",\"pos_\",\"bill_\",\"payroll\",\"otherdep\",\"net_worth\"])]\n",
    "    passthrough_cols = [c for c in num_cols if c not in log_cols]\n",
    "\n",
    "    log_pipe = Pipeline([\n",
    "        (\"log1p\", FunctionTransformer(lambda x: np.log1p(np.clip(x, 0, None)))),\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ])\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        (\"scaler\", RobustScaler())\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"log_num\", log_pipe, log_cols),\n",
    "            (\"num\", num_pipe, passthrough_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "# final_mbr_df, final_txn_df = generate_synthetic_dataset(...)\n",
    "\n",
    "snap = build_member_snapshot(final_mbr_df, final_txn_df, snapshot_month=None, window_3m=3, window_6m=6)\n",
    "\n",
    "feature_snap = snap.drop(columns=['MEMBERNBR','snapshot_month'],errors='ignore')\n",
    "member_ids = snap['MEMBERNBR'].to_numpy()\n",
    "\n",
    "pre = make_preprocess_pipeline(feature_snap)\n",
    "X = pre.fit_transform(feature_snap)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# Cant run because it errors out, would be 22 Billion . Need to Understand this\n",
    "#S = cosine_similarity(X)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=51,metric='cosine',algorithm='auto')\n",
    "\n",
    "nn.fit(X)\n",
    "\n",
    "member_index = {mid: i for i, mid in enumerate(member_ids)}\n",
    "\n",
    "def test_knn(X,nn,k=10,sample_size=100,seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idxs = rng.choice(X.shape[0],size=sample_size,replace=False)\n",
    "\n",
    "    # returns the distance and Index\n",
    "    nn_dist,nn_index = nn.kneighbors(X[idxs],n_neighbors=k+1)\n",
    "    nn_dist = nn_dist[:,1:].mean(axis=1)\n",
    "    \n",
    "    rand_idxs = rng.choice(X.shape[0],size=(sample_size,k),replace=True)\n",
    "    rand_dist = np.linalg.norm(X[idxs][:,None,:]-X[rand_idxs],axis=2).mean(axis=1)\n",
    "    \n",
    "    return nn_dist,rand_dist,idxs\n",
    "\n",
    "\n",
    "a,b,c = test_knn(X,nn)\n",
    "\n",
    "mid = int(member_ids[0])\n",
    "neighbors, sim = knn_topk(nn, member_index, member_ids, X, mid, k=10)\n",
    "list(zip(neighbors.tolist(), sim.tolist()))\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "# What does StandardScaler do. Transforms data to Mean 0 and Standard Dev = 1\\\n",
    "\n",
    "def Heatmap(df,\n",
    "            correlation=True,\n",
    "            column_list=[],\n",
    "            title='Heat Map of Correlation',\n",
    "            cmap='coolwarm',\n",
    "            annotate=True,\n",
    "            x_rotate=0,\n",
    "            y_rotate=0,\n",
    "            cbar=True,\n",
    "            set_center=0,\n",
    "            figsize=(10,10)):\n",
    "    \n",
    "    '''\n",
    "    Function Which Generates a Heatmap\n",
    "    \n",
    "    Parameters:\n",
    "        Dataframe\n",
    "        column_name (list): If included, will only show certain columns on the Horizontal Axis.\n",
    "    \n",
    "    Returns:\n",
    "        matlplot plot.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    sns.set(style='white')\n",
    "    \n",
    "    # View column with Abbreviated title or full. Abbreviated displays nicer.\n",
    "    if correlation:\n",
    "        corr = df.corr()\n",
    "    else:\n",
    "        corr = df.copy()\n",
    "    \n",
    "    if len(column_list)!=0:\n",
    "        corr = corr[column_list]\n",
    "    \n",
    "    mask= np.zeros_like(corr,dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)]=True\n",
    "    f,ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    if len(str(set_center))!=0:\n",
    "        sns.heatmap(corr,mask=mask,cmap=cmap,center=set_center,square=True,linewidths=1,annot=annotate,cbar=cbar)\n",
    "    else:\n",
    "        sns.heatmap(corr,mask=mask,cmap=cmap,square=True,linewidths=1,annot=annotate,cbar=cbar)\n",
    "    \n",
    "    \n",
    "    plt.title(title)\n",
    "    if y_rotate !=0:\n",
    "        for tick in ax.get_yticklabels():\n",
    "            tick.set_rotation(0)\n",
    "            tick.set_horizontalalignment('right')\n",
    "    if x_rotate !=0:\n",
    "        plt.xticks(rotation=x_rotate,ha='center', va='top')\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
