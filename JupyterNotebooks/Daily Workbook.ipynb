{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec11205d-65c9-42d9-830a-647a3645ac62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product_A</th>\n",
       "      <th>Product_B</th>\n",
       "      <th>Product_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>West</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>East</td>\n",
       "      <td>200</td>\n",
       "      <td>250</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID Region  Product_A  Product_B  Product_C\n",
       "0           1   West        100        150        200\n",
       "1           2   East        200        250        300"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Region</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>West</td>\n",
       "      <td>Product_A</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>East</td>\n",
       "      <td>Product_A</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>West</td>\n",
       "      <td>Product_B</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>East</td>\n",
       "      <td>Product_B</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>West</td>\n",
       "      <td>Product_C</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>East</td>\n",
       "      <td>Product_C</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID Region   variable  value\n",
       "0           1   West  Product_A    100\n",
       "1           2   East  Product_A    200\n",
       "2           1   West  Product_B    150\n",
       "3           2   East  Product_B    250\n",
       "4           1   West  Product_C    200\n",
       "5           2   East  Product_C    300"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32e99f5f-396d-411a-9c70-9faa445f18b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Score_1</th>\n",
       "      <th>Weight_1</th>\n",
       "      <th>Score_2</th>\n",
       "      <th>Weight_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>30</td>\n",
       "      <td>F</td>\n",
       "      <td>90</td>\n",
       "      <td>0.9</td>\n",
       "      <td>88</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>25</td>\n",
       "      <td>M</td>\n",
       "      <td>85</td>\n",
       "      <td>0.8</td>\n",
       "      <td>82</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   Name  Age Gender  Score_1  Weight_1  Score_2  Weight_2\n",
       "0   1  Alice   30      F       90       0.9       88      0.85\n",
       "1   2    Bob   25      M       85       0.8       82      0.75"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID   Name  Age Gender  Metric  Score  Weight\n",
      "0   1  Alice   30      F       1     90    0.90\n",
      "1   1  Alice   30      F       2     88    0.85\n",
      "2   2    Bob   25      M       1     85    0.80\n",
      "3   2    Bob   25      M       2     82    0.75\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'ID': [1, 2],\n",
    "    'Name': ['Alice', 'Bob'],\n",
    "    'Age': [30, 25],\n",
    "    'Gender': ['F', 'M'],\n",
    "    'Score1': [90, 85],\n",
    "    'Weight1': [0.9, 0.8],\n",
    "    'Score2': [88, 82],\n",
    "    'Weight2': [0.85, 0.75]\n",
    "})\n",
    "\n",
    "# Rename to a common pattern\n",
    "df.columns = ['ID', 'Name', 'Age', 'Gender', 'Score_1', 'Weight_1', 'Score_2', 'Weight_2']\n",
    "\n",
    "display(df)\n",
    "\n",
    "# Convert to long format\n",
    "df_long = pd.wide_to_long(df,\n",
    "                          stubnames=['Score', 'Weight'],\n",
    "                          i=['ID', 'Name', 'Age', 'Gender'],\n",
    "                          j='Metric',\n",
    "                          sep='_',\n",
    "                          suffix='\\\\d+').reset_index()\n",
    "\n",
    "print(df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b24421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f206589-2443-4566-aa65-9e44486ea13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Definition</th>\n",
       "      <th>Description</th>\n",
       "      <th>KEY</th>\n",
       "      <th>Link</th>\n",
       "      <th>CSV</th>\n",
       "      <th>Streamlit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ProcessSheet</td>\n",
       "      <td>Google Sheet, storing standard processes used ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/1iXlm49...</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/e/2PACX...</td>\n",
       "      <td>https://process-powerpoint-presenetation.strea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DataDashboardSheet</td>\n",
       "      <td>Google Sheet, Used to store information relate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/1tZ-_5V...</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/e/2PACX...</td>\n",
       "      <td>https://derek-dewald-datadashboard.streamlit.app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CodingDDashboard</td>\n",
       "      <td>Data Dictionary nformation related to Coding.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/1FpYYq4...</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/e/2PACX...</td>\n",
       "      <td>https://derekdewald-codedboard.streamlit.app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ML_Pipeline</td>\n",
       "      <td>Information related to ML Functions (Might be ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/e/2PACX...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mapping Sheet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/1Wfr7Ds...</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/e/2PACX...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SKLearn Models</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/1NgKYNm...</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/e/2PACX...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Notes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/1jddkkF...</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/e/2PACX...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d_py_functions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://d-python-domain.streamlit.app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JobSearchNotes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://docs.google.com/spreadsheets/d/1sMdgmp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BerkeleyEmail</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://sites.google.com/berkeley.edu/alumni-e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OurWorldData</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>www.ourworlddata.org</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>KaggelDatasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.kaggle.com/datasets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>StLouisFed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://fred.stlouisfed.org</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GoggleResearch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://datasetsearch.research.google.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UCI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://archive.ics.uci.edu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AWS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://registry.opendata.aws/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AWSDataExchange</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://aws.amazon.com/data-exchange/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AWSPublicData</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://aws.amazon.com/public-datasets/</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Definition                                        Description  \\\n",
       "0         ProcessSheet  Google Sheet, storing standard processes used ...   \n",
       "1   DataDashboardSheet  Google Sheet, Used to store information relate...   \n",
       "2     CodingDDashboard      Data Dictionary nformation related to Coding.   \n",
       "3          ML_Pipeline  Information related to ML Functions (Might be ...   \n",
       "4        Mapping Sheet                                                NaN   \n",
       "5       SKLearn Models                                                NaN   \n",
       "6                Notes                                                NaN   \n",
       "7       d_py_functions                                                NaN   \n",
       "8       JobSearchNotes                                                NaN   \n",
       "9        BerkeleyEmail                                                NaN   \n",
       "10        OurWorldData                                                NaN   \n",
       "11      KaggelDatasets                                                NaN   \n",
       "12          StLouisFed                                                NaN   \n",
       "13      GoggleResearch                                                NaN   \n",
       "14                 UCI                                                NaN   \n",
       "15                 AWS                                                NaN   \n",
       "16     AWSDataExchange                                                NaN   \n",
       "17       AWSPublicData                                                NaN   \n",
       "\n",
       "    KEY                                               Link  \\\n",
       "0   NaN  https://docs.google.com/spreadsheets/d/1iXlm49...   \n",
       "1   NaN  https://docs.google.com/spreadsheets/d/1tZ-_5V...   \n",
       "2   NaN  https://docs.google.com/spreadsheets/d/1FpYYq4...   \n",
       "3   NaN                                                NaN   \n",
       "4   NaN  https://docs.google.com/spreadsheets/d/1Wfr7Ds...   \n",
       "5   NaN  https://docs.google.com/spreadsheets/d/1NgKYNm...   \n",
       "6   NaN  https://docs.google.com/spreadsheets/d/1jddkkF...   \n",
       "7   NaN                                                NaN   \n",
       "8   NaN  https://docs.google.com/spreadsheets/d/1sMdgmp...   \n",
       "9   NaN  https://sites.google.com/berkeley.edu/alumni-e...   \n",
       "10  NaN                               www.ourworlddata.org   \n",
       "11  NaN                    https://www.kaggle.com/datasets   \n",
       "12  NaN                        https://fred.stlouisfed.org   \n",
       "13  NaN          https://datasetsearch.research.google.com   \n",
       "14  NaN                        https://archive.ics.uci.edu   \n",
       "15  NaN                     https://registry.opendata.aws/   \n",
       "16  NaN              https://aws.amazon.com/data-exchange/   \n",
       "17  NaN            https://aws.amazon.com/public-datasets/   \n",
       "\n",
       "                                                  CSV  \\\n",
       "0   https://docs.google.com/spreadsheets/d/e/2PACX...   \n",
       "1   https://docs.google.com/spreadsheets/d/e/2PACX...   \n",
       "2   https://docs.google.com/spreadsheets/d/e/2PACX...   \n",
       "3   https://docs.google.com/spreadsheets/d/e/2PACX...   \n",
       "4   https://docs.google.com/spreadsheets/d/e/2PACX...   \n",
       "5   https://docs.google.com/spreadsheets/d/e/2PACX...   \n",
       "6   https://docs.google.com/spreadsheets/d/e/2PACX...   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13                                                NaN   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                                NaN   \n",
       "\n",
       "                                            Streamlit  \n",
       "0   https://process-powerpoint-presenetation.strea...  \n",
       "1    https://derek-dewald-datadashboard.streamlit.app  \n",
       "2        https://derekdewald-codedboard.streamlit.app  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "5                                                 NaN  \n",
       "6                                                 NaN  \n",
       "7               https://d-python-domain.streamlit.app  \n",
       "8                                                 NaN  \n",
       "9                                                 NaN  \n",
       "10                                                NaN  \n",
       "11                                                NaN  \n",
       "12                                                NaN  \n",
       "13                                                NaN  \n",
       "14                                                NaN  \n",
       "15                                                NaN  \n",
       "16                                                NaN  \n",
       "17                                                NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Which Process would You like to review? d_py_functions\n",
      "What would you like to return? Streamlit\n"
     ]
    }
   ],
   "source": [
    "from Connections import NavigateUsingDMap\n",
    "\n",
    "\n",
    "NavigateUsingDMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ec933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from D_Testing import DailyTest\n",
    "\n",
    "DailyTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import all_estimators\n",
    "import mlflow\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# ML Pipeline Function\n",
    "def MLPipeline(df, project_name, scaler=None, ml_model_type='regressor', target_column='Target', test_size=0.2):\n",
    "    mlflow.set_experiment(project_name)\n",
    "\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    X_train, X_test = apply_scaling(X_train, X_test, scaler=scaler)\n",
    "\n",
    "    model_list = all_estimators(type_filter=ml_model_type)\n",
    "    results = []\n",
    "\n",
    "    for name, model_class in model_list:\n",
    "        try:\n",
    "            model = model_class()\n",
    "            start_time = time.time()\n",
    "\n",
    "            with mlflow.start_run(run_name=name):\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                if ml_model_type == \"classifier\":\n",
    "                    metric = accuracy_score(y_test, y_pred)\n",
    "                    mlflow.log_metric(\"Accuracy\", metric)\n",
    "                else:\n",
    "                    # Compute RMSE manually (fix for 'squared' error)\n",
    "                    metric = mean_squared_error(y_test, y_pred) ** 0.5  \n",
    "                    mlflow.log_metric(\"RMSE\", metric)\n",
    "\n",
    "                mlflow.sklearn.log_model(model, name)\n",
    "                mlflow.log_param(\"Model\", name)\n",
    "                mlflow.log_param(\"Training Time\", round(time.time() - start_time, 2))\n",
    "\n",
    "                results.append({\n",
    "                    \"Model\": name,\n",
    "                    \"Metric\": metric,\n",
    "                    \"Time (s)\": round(time.time() - start_time, 2)\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ {name} failed: {str(e)}\")  # Handle model failures but continue\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(by=\"Metric\", ascending=(ml_model_type == \"regressor\"))\n",
    "\n",
    "    # Display DataFrame\n",
    "    import ace_tools as tools\n",
    "    tools.display_dataframe_to_user(name=\"ML Model Performance\", dataframe=results_df)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def calculate_trends(data, id_column, time_columns):\n",
    "    \"\"\"\n",
    "    Calculate trends (slopes) for entities over time using linear regression.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Input DataFrame containing entity IDs and time-based columns.\n",
    "        id_column (str): Column name for the unique entity identifier (e.g., member_id).\n",
    "        time_columns (list of str): List of column names representing the time periods (e.g., months).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing entity IDs, slopes, and trends.\n",
    "    \"\"\"\n",
    "    # Extract unique entity IDs and the time-series data\n",
    "    entity_ids = data[id_column].unique()\n",
    "    time_data = data[time_columns].values\n",
    "    \n",
    "    # Prepare the X (time) matrix\n",
    "    time_indices = np.arange(1, len(time_columns) + 1)  # Generate time indices\n",
    "    X = np.vstack([time_indices, np.ones(len(time_indices))]).T  # Add a bias term for the intercept\n",
    "\n",
    "    # Perform linear regression for all entities\n",
    "    slopes = np.linalg.lstsq(X, time_data.T, rcond=None)[0][0]  # Extract slopes\n",
    "    \n",
    "    # Prepare results DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        id_column: entity_ids,\n",
    "        \"slope\": slopes\n",
    "    })\n",
    "    \n",
    "    # Classify trends\n",
    "    results[\"trend\"] = np.where(\n",
    "        results[\"slope\"] > 0, \"increasing\",\n",
    "        np.where(results[\"slope\"] < 0, \"decreasing\", \"stable\")\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "test_dict = {}\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for model in models:\n",
    "    for metric in ['F1','AUC','Accuracy']:   \n",
    "        test_dict.setdefault('Top 5 Performing Models',{})[metric] = np.mean(value_list[:4])\n",
    "        test_dict.setdefault('Top Performing Model',{})[metric] = value_list[0]\n",
    "        test_dict.setdefault('All Models Mean',{})[metric] = np.mean(value_list)\n",
    "        \n",
    "        temp_df = pd.DataFrame(test_dict).T.reset_index().rename(columns={'index':'Type'})\n",
    "        temp_df['Observed Value'] = model\n",
    "    \n",
    "    final_df = pd.concat([final_df,temp_df])\n",
    "\n",
    "\n",
    "    'Extra Tree': {\n",
    "        'Type': 'Ensemble',\n",
    "        'Description': 'Similar to Random Forest but selects splits randomly rather than optimizing them.',\n",
    "        'UseCase': [\"Classification\", \"Regression\"],\n",
    "        'ModelStrengths': \"Reduces overfitting further, computationally efficient compared to Random Forest.\",\n",
    "        'Model Weaknesses': \"Slightly less accurate in some cases, still computationally heavy.\",\n",
    "        'lowerbound_feature_threshold':.05,\n",
    "        'upperbound_feature_threshold':.1,\n",
    "        'ReferenceWorkbooks': \"\",\n",
    "        'SKlearn': ExtraTreesClassifier(random_state=random_state)\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'Type': 'Ensemble',\n",
    "        'Description': 'Builds models sequentially, correcting errors from previous models in an additive fashion.',\n",
    "        'UseCase': [\"Classification\", \"Regression\"],\n",
    "        'ModelStrengths': \"Highly accurate, handles non-linear relationships well.\",\n",
    "        'Model Weaknesses': \"Sensitive to hyperparameters, prone to overfitting on noisy data.\",\n",
    "        'ReferenceWorkbooks': \"\",\n",
    "        'lowerbound_feature_threshold':.05,\n",
    "        'upperbound_feature_threshold':.1,\n",
    "        'SKlearn': GradientBoostingClassifier(random_state=random_state)\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'Type': 'Ensemble',\n",
    "        'Description': 'An optimized gradient boosting model with regularization to prevent overfitting.',\n",
    "        'UseCase': [\"Classification\", \"Regression\"],\n",
    "        'ModelStrengths': \"Fast, scalable, handles sparse data, regularization prevents overfitting.\",\n",
    "        'Model Weaknesses': \"Requires careful tuning of hyperparameters, computationally expensive.\",\n",
    "        'lowerbound_feature_threshold':.05,\n",
    "        'upperbound_feature_threshold':.1,\n",
    "        'ReferenceWorkbooks': \"\",\n",
    "        'SKlearn': xgb.XGBClassifier(objective=xgb_objective, eval_metric=xgb_eval_metric, use_label_encoder=False, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd38665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Connections import ParamterMapping\n",
    "\n",
    "dd = pd.read_csv(ParamterMapping('DataDashboardSheet')['CSV'].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "328f9c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub Categorization</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Link</th>\n",
       "      <th>Image</th>\n",
       "      <th>Markdown Equation</th>\n",
       "      <th>Is Model</th>\n",
       "      <th>Learning Type</th>\n",
       "      <th>Algorithm Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Evaluation</td>\n",
       "      <td>Number of correctly classified examples divide...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>F1 Score</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F1 is harmonic mean is a type of average that ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Precision</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Evaluation</td>\n",
       "      <td>Precision is the ratio of correct positive pre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$$ P = \\frac{TP}{TP + FP} $$</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Recall</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Evaluation</td>\n",
       "      <td>Recall is the correct positive predicitions to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$$ R = \\frac{TP}{TP + FN} $$</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word          Category Sub Categorization  \\\n",
       "0     Accuracy  Machine Learning         Evaluation   \n",
       "118   F1 Score  Machine Learning                NaN   \n",
       "248  Precision  Machine Learning         Evaluation   \n",
       "269     Recall  Machine Learning         Evaluation   \n",
       "\n",
       "                                            Definition Notes Link Image  \\\n",
       "0    Number of correctly classified examples divide...   NaN  NaN   NaN   \n",
       "118  F1 is harmonic mean is a type of average that ...   NaN  NaN   NaN   \n",
       "248  Precision is the ratio of correct positive pre...   NaN  NaN   NaN   \n",
       "269  Recall is the correct positive predicitions to...   NaN  NaN   NaN   \n",
       "\n",
       "                                     Markdown Equation  Is Model  \\\n",
       "0    $$ Accuracy = \\frac{TP + TN}{TP + TN + FP + FN...       NaN   \n",
       "118                                                NaN       NaN   \n",
       "248                       $$ P = \\frac{TP}{TP + FP} $$       NaN   \n",
       "269                       $$ R = \\frac{TP}{TP + FN} $$       NaN   \n",
       "\n",
       "    Learning Type Algorithm Class  \n",
       "0             NaN             NaN  \n",
       "118           NaN             NaN  \n",
       "248           NaN             NaN  \n",
       "269           NaN             NaN  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[dd['Word'].isin(['Accuracy','Precision','Recall','F1 Score'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638dc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateMLModels(df,\n",
    "                     Features,\n",
    "                     Target,\n",
    "                     model_list,\n",
    "                     scaler_list,\n",
    "                     balance_list,\n",
    "                     log_max_it=1000,\n",
    "                     rf_estimators=50,\n",
    "                     include_narrative=0):\n",
    "    \n",
    "    model_dict = {}\n",
    "    final_result_df = pd.DataFrame()\n",
    "    \n",
    "    param_list = list(product(model_list,scaler_list,balance_list))\n",
    "    \n",
    "    for count,params in enumerate(param_list):\n",
    "        print(count,params)\n",
    "        \n",
    "        model = params[0]\n",
    "        scaler = params[1]\n",
    "        if params[2]!=0:\n",
    "            balance_target_observations = [1,params[2]]\n",
    "        else:\n",
    "            balance_target_observations = [0,0]\n",
    "        \n",
    "        run_dict,result = MLManualPipeline(df=df,\n",
    "                                           Features=Features,\n",
    "                                           Target=Target,\n",
    "                                           model=model,\n",
    "                                           scaler_=scaler,\n",
    "                                           balance_target_observations=balance_target_observations,\n",
    "                                           include_narrative=include_narrative,\n",
    "                                           rf_estimators=rf_estimators,\n",
    "                                           SequenceNumber=count,\n",
    "                                           log_max_it=log_max_it)\n",
    "        \n",
    "        model_dict[count] = run_dict\n",
    "        final_result_df = pd.concat([final_result_df,result])\n",
    "        \n",
    "    return model_dict,final_result_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ModelPerformanceByCategory(df,\n",
    "                               primary_key='SequenceNumber',\n",
    "                               ml_parameters=['Model','Scaler','Training Set Balance Perc'],\n",
    "                               observations=20,\n",
    "                               metrics=['AUC','F1','Accuracy']):\n",
    "    \n",
    "    # Create List of Models Based on Original Input Dataframe\n",
    "    models = list(df['Model'].unique())\n",
    "    \n",
    "    # Create Values for Particular Metric under review\n",
    "    for metric in metrics:\n",
    "        # Sort Value based on Metric\n",
    "        metric_df = df.sort_values(metric,ascending=False)\n",
    "        # Iterate through Categories as defined by Function\n",
    "        temp = pd.DataFrame()\n",
    "        for ml_ in ml_parameters:\n",
    "            _ = pd.DataFrame(metric_df[ml_].fillna(\"\").head(observations).value_counts()).reset_index().rename(columns={'index':'Observed Value',ml_:metric})\n",
    "            _['Type'] = ml_\n",
    "            \n",
    "            temp = pd.concat([temp,_])\n",
    "            \n",
    "        # Generate Metric Summary Value \n",
    "        # Create Segments\n",
    "        brackets_v3(metric_df,metric,f\"{metric}_segment\",[0,.25,.5,.6,.75,.9])\n",
    "        perc = pd.DataFrame(metric_df[f\"{metric}_segment\"].value_counts()).reset_index().rename(columns={'index':\"Observed Value\",f\"{metric}_segment\":metric})\n",
    "        perc['Type'] = 'Model Performance'\n",
    "        \n",
    "        temp = pd.concat([temp,perc[perc[metric]!=0]])\n",
    "        \n",
    "        try:\n",
    "            final_df = final_df.merge(temp,on=['Observed Value','Type'],how='outer')\n",
    "        except:\n",
    "            final_df = temp.copy()\n",
    "            \n",
    "        test_dict = {}\n",
    "        perf_df = pd.DataFrame()\n",
    "        for model in models:\n",
    "            for metric in ['F1','AUC','Accuracy']:\n",
    "        \n",
    "                value_list = df[df['Model']==model].sort_values(metric,ascending=False)[metric].tolist()\n",
    "                test_dict.setdefault('Model Best Five',{})[metric] = np.mean(value_list[:4])\n",
    "                test_dict.setdefault('Model Best Performing',{})[metric] = value_list[0]\n",
    "                test_dict.setdefault('Model Mean Performance',{})[metric] = np.mean(value_list)\n",
    "\n",
    "                temp_df = pd.DataFrame(test_dict).T.reset_index().rename(columns={'index':'Type'})\n",
    "                temp_df['Observed Value'] = model\n",
    "            \n",
    "            perf_df = pd.concat([perf_df,temp_df])\n",
    "\n",
    "    return pd.concat([final_df,perf_df]).sort_values(['Type',metrics[0]],ascending=[True,False]).set_index(['Observed Value','Type']).fillna(0)\n",
    "    \n",
    "\n",
    "\n",
    "def MLManualPipeline(df,\n",
    "                     Features,\n",
    "                     Target,\n",
    "                     model,\n",
    "                     balance_target_observations=[0,.25],\n",
    "                     scaler_=None,\n",
    "                     test_size=.3,\n",
    "                     random_state=42,\n",
    "                     include_narrative=1,\n",
    "                     rf_estimators=50,\n",
    "                     log_max_it=100,\n",
    "                     SequenceNumber=0,\n",
    "                     time_model=1,\n",
    "                     xgb_objective='binary:logistic',\n",
    "                     xgb_eval_metric='logloss'):\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "        \n",
    "    generated_models = {}\n",
    "    \n",
    "    # Create Copy to ensure original data is not altered\n",
    "    df = df.copy()\n",
    "\n",
    "    if balance_target_observations[0]==1:\n",
    "        df = BalanceTargetDistribution(df=df,\n",
    "                                       Target=Target,\n",
    "                                       desired_percentage=balance_target_observations[1])\n",
    "        balance_target_observations = f'{balance_target_observations[1]}'\n",
    "    else:\n",
    "        balance_target_observations = '0'\n",
    "        \n",
    "    # If features not specified, entire Dataset to be processed (less Target), otherwise include only features\n",
    "    if len(Features) == 0:\n",
    "        Features = list(df.drop(Target,axis=1).columns)\n",
    "        X = np.array(df[Features].copy())\n",
    "        df_len = len(df)\n",
    "        feature_count = len(X[0])\n",
    "    else:\n",
    "        X = np.array(df[Features])\n",
    "        df_len = len(df)\n",
    "        feature_count = len(X[0])\n",
    "        \n",
    "    # Create Target Array - Target already provided as a List\n",
    "    y = np.array(df[Target].squeeze())\n",
    "    target_vc = df[Target].value_counts()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    if scaler_ in scaler_dict.keys():\n",
    "        scaler = scaler_dict[scaler_]\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        x_test = scaler.transform(X_test)\n",
    "    else:\n",
    "        scaler = None\n",
    "        \n",
    "    if include_narrative==1:\n",
    "        print(f\"Total Number of Records in Dataset: {df_len}\\ntotal features: {feature_count}\\nTarget Distribution:\\n{target_vc}\\n\")\n",
    "        print(f\"Training Data Set:\\nTraining Features:{len(X_train)}, Training Target:{len(y_train)}\\nTraining Target Distribution:\\n{pd.DataFrame(y_train).value_counts()}\\n\")\n",
    "        print(f\"Testing Data Set:\\nTest Features:{len(X_test)}, Test Target:{len(y_test)}Training Target Distribution:\\n{pd.DataFrame(y_test).value_counts()}\\n\")\n",
    "        \n",
    "    if model=='Logistic Regression':\n",
    "        \n",
    "        # Create Model\n",
    "        logreg=LogisticRegression(max_iter=log_max_it)\n",
    "        \n",
    "        # Train Model\n",
    "        logreg.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on Text\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        \n",
    "        # Generate Test DataFrame\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred,columns=['PREDICTION']),pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        \n",
    "        # Create Feature Importance DataFrame\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':np.abs(logreg.coef_[0]),}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        \n",
    "        # Generate Results and Dataframe\n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        \n",
    "        performance['RunTime'] = stop_time\n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':logreg,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}\n",
    "        \n",
    "        try:\n",
    "            # Generate a Balanced Variant of the Logistic Regression Where \n",
    "            perc_dist = len(df[df[Target]==0])/len(df)\n",
    "            if perc_dist>.6:\n",
    "                lr0=LogisticRegression(max_iter=log_max_it,class_weight='balanced')\n",
    "                lr0.fit(X_train, y_train)\n",
    "                y_pred_rf = lr0.predict(X_test)\n",
    "                result_df = pd.concat([pd.DataFrame(y_pred_rf,columns=['PREDICTION']),\n",
    "                                       pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "                feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                                   'IMPORTANCE':rf.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "                result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                               model='Logistic Regression - Balanced',\n",
    "                                                               scaler=scaler_,\n",
    "                                                               balance=balance_target_observations,\n",
    "                                                               sequence_num=SequenceNumber+1000000)\n",
    "                stop_time = timeit.default_timer() - start_time\n",
    "                performance['RunTime'] = stop_time\n",
    "                generated_models[SequenceNumber+1000000] = {'ModelType':'Logistic Regression - Balanced',\n",
    "                                                        'Model':lr0,\n",
    "                                                        'Scaler':scaler,\n",
    "                                                        'FeatureImportance':feature_importance,\n",
    "                                                        'BinaryResultDF':result_df,\n",
    "                                                        'ModelPerformance':performance}\n",
    "        except:\n",
    "            print('Attempted to Review Random Forrest - Balanced, could not')\n",
    "\n",
    "    elif model =='Decision Tree':    \n",
    "        ############################################ ESTIMATORS\n",
    "        dt = DecisionTreeClassifier(random_state=random_state)\n",
    "        dt.fit(X_train, y_train)\n",
    "        y_pred_dt = dt.predict(X_test)\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred_dt,columns=['PREDICTION']),pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':dt.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        performance['RunTime'] = stop_time\n",
    "        \n",
    "        \n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':dt,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}\n",
    "\n",
    "    elif model =='Random Forest':    \n",
    "        ############################################ ESTIMATORS\n",
    "        rf = RandomForestClassifier(random_state=random_state, n_estimators=rf_estimators)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred_rf = rf.predict(X_test)\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred_rf,columns=['PREDICTION']),\n",
    "                               pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':rf.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        performance['RunTime'] = stop_time\n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':rf,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}\n",
    "        \n",
    "        try:\n",
    "            # Generate a Balanced Variant of the Random Forest Where \n",
    "            perc_dist = len(df[df[Target]==0])/len(df)\n",
    "            if perc_dist>.6:\n",
    "                rf0 = RandomForestClassifier(random_state=random_state, n_estimators=rf_estimators,class_weight='balanced')\n",
    "                rf0.fit(X_train, y_train)\n",
    "                y_pred_rf = rf0.predict(X_test)\n",
    "                result_df = pd.concat([pd.DataFrame(y_pred_rf,columns=['PREDICTION']),\n",
    "                                       pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "                feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                                   'IMPORTANCE':rf.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "                result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                               model='Random Forest - Balanced',\n",
    "                                                               scaler=scaler_,\n",
    "                                                               balance=balance_target_observations,\n",
    "                                                               sequence_num=SequenceNumber+1000000)\n",
    "                stop_time = timeit.default_timer() - start_time\n",
    "                performance['RunTime'] = stop_time\n",
    "                generated_models[SequenceNumber+1000000] = {'ModelType':'Random Forest - Balanced',\n",
    "                                                        'Model':rf0,\n",
    "                                                        'Scaler':scaler,\n",
    "                                                        'FeatureImportance':feature_importance,\n",
    "                                                        'BinaryResultDF':result_df,\n",
    "                                                        'ModelPerformance':performance}\n",
    "        except:\n",
    "            print('Attempted to Review Random Forrest - Balanced, could not')\n",
    "                \n",
    "    elif model == 'Gradient Boosting':\n",
    "        gb = GradientBoostingClassifier()\n",
    "        gb.fit(X_train, y_train)\n",
    "        y_pred_gb = gb.predict(X_test)\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred_gb,columns=['PREDICTION']),pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':gb.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        \n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        performance['RunTime'] = stop_time\n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':gb,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}\n",
    "    elif model == 'Ada':\n",
    "        ada = AdaBoostClassifier()\n",
    "        ada.fit(X_train, y_train)\n",
    "        y_pred_ada = ada.predict(X_test)\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred_ada,columns=['PREDICTION']),pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':ada.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        \n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        performance['RunTime'] = stop_time\n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':ada,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}\n",
    "        \n",
    "    elif model == 'Extra Tree':\n",
    "        et = ExtraTreesClassifier()\n",
    "        et.fit(X_train, y_train)\n",
    "        y_pred_et = et.predict(X_test)\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred_et,columns=['PREDICTION']),pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':et.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        \n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        performance['RunTime'] = stop_time\n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':et,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}\n",
    "        \n",
    "    elif model == 'XGBoost':\n",
    "        xgb_ = xgb.XGBClassifier(objective=xgb_objective,\n",
    "                                eval_metric=xgb_eval_metric,\n",
    "                                use_label_encoder=False,\n",
    "                                random_state=random_state)\n",
    "        xgb_.fit(X_train, y_train)\n",
    "        y_pred_et = xgb_.predict(X_test)\n",
    "        result_df = pd.concat([pd.DataFrame(y_pred_et,columns=['PREDICTION']),\n",
    "                               pd.DataFrame(y_test,columns=['ACTUAL'])],axis=1)\n",
    "        feature_importance = pd.DataFrame({'FEATURE':Features,\n",
    "                                           'IMPORTANCE':xgb_.feature_importances_}).sort_values(by='IMPORTANCE',ascending=False)\n",
    "        \n",
    "        result_df, performance = ClassificationMetrics(df=result_df,\n",
    "                                                       model=model,\n",
    "                                                       scaler=scaler_,\n",
    "                                                       balance=balance_target_observations,\n",
    "                                                       sequence_num=SequenceNumber)\n",
    "        stop_time = timeit.default_timer() - start_time\n",
    "        performance['RunTime'] = stop_time\n",
    "        \n",
    "        generated_models[SequenceNumber] = {'ModelType':model,\n",
    "                                            'Model':xgb,\n",
    "                                            'Scaler':scaler,\n",
    "                                            'FeatureImportance':feature_importance,\n",
    "                                            'BinaryResultDF':result_df,\n",
    "                                            'ModelPerformance':performance}    \n",
    "    else:\n",
    "        print('Model Not Defined, please update function')\n",
    "        \n",
    "    performance_metrics = pd.DataFrame()\n",
    "    for key in generated_models.keys():\n",
    "        performance_metrics = pd.concat([performance_metrics,generated_models[key]['ModelPerformance']])\n",
    "        \n",
    "    return generated_models,performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71228ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "adef train_neural_network(X,\n",
    "                y,\n",
    "                input_dim,\n",
    "                metrics,\n",
    "                hidden_layer_sizes,\n",
    "                activation, \n",
    "                optimizer,\n",
    "                learning_rate,\n",
    "                batch_size,\n",
    "                num_epochs,\n",
    "                validation_split,\n",
    "                verbose=0):\n",
    "                       \n",
    "\n",
    "    # Build the model.\n",
    "    model = build_binary_classification_model(input_dim=input_dim,\n",
    "                                              hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                              activation=activation, \n",
    "                                              optimizer=optimizer,\n",
    "                                              learning_rate=learning_rate,\n",
    "                                              metrics=metrics)\n",
    "    \n",
    "    print(model.summary())     \n",
    "                        \n",
    "    # Train the model.\n",
    "    history = model.fit(x=X,\n",
    "                        y=y,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=num_epochs,\n",
    "                        validation_split=validation_split,\n",
    "                        verbose=verbose)\n",
    "\n",
    "    # Retrieve the training metrics (after each train epoch) and the final test\n",
    "    # accuracy.\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(train_accuracy, label='train_accuracy')\n",
    "    plt.plot(val_accuracy, label='validation accuracy')\n",
    "    plt.xticks(range(num_epochs))\n",
    "    plt.xlabel('Train epochs')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    " \n",
    "    return history,model\n",
    "\n",
    "    \n",
    "def neural_network(X_df,\n",
    "                   y_df,\n",
    "                   input_dim, \n",
    "                   hidden_layer_sizes,\n",
    "                   activation,\n",
    "                   optimizer,\n",
    "                   learning_rate,\n",
    "                   metrics,\n",
    "                   verbose=0):\n",
    "\n",
    "    \"\"\"Build a binary classification model using Keras.\n",
    "\n",
    "      Args:\n",
    "        input_dim: Number of features in the input data.\n",
    "        hidden_layer_sizes: A list with the number of units in each hidden layer.\n",
    "        activation: The activation function to use for the hidden layers.\n",
    "        optimizer: The optimizer\n",
    "        learning_rate: The desired learning rate for the optimizer.\n",
    "\n",
    "      Returns:\n",
    "        model: A tf.keras model.\n",
    "    \"\"\"\n",
    "    # Instantiate Model\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # Add Input Layer\n",
    "    model.add(layers.InputLayer(input_shape=(input_dim,)))\n",
    "\n",
    "    # Add Hidden Layers\n",
    "    for nodes in hidden_layer_sizes:\n",
    "        model.add(layers.Dense(units=nodes, activation=activation))\n",
    "\n",
    "    # Add Output Layer\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Configure optimizer and compile the model\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
