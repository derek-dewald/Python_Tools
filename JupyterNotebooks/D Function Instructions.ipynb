{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c072a9-8e88-47e1-94ec-1925d183ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "elif page == \"D Definitions\":\n",
    "    st.title(\"D Definitions\")\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode, DataReturnMode\n",
    "\n",
    "    df_base = data_dict[\"google_definition_df\"].copy()\n",
    "\n",
    "    # ✅ Only convert actual NaN/None to \"\"\n",
    "    df_base = df_base.fillna(\"\")\n",
    "\n",
    "    required = [\"Category\", \"Categorization\", \"Word\", \"Definition\"]\n",
    "    missing = [c for c in required if c not in df_base.columns]\n",
    "    if missing:\n",
    "        st.error(f\"google_definition_df is missing required columns: {missing}\")\n",
    "        st.stop()\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1) Slicers\n",
    "    # ----------------------------\n",
    "    c1, c2, c3 = st.columns([1, 1, 1])\n",
    "\n",
    "    with c1:\n",
    "        cat_vals = df_base[\"Category\"].astype(str)\n",
    "        opts1 = [\"(All)\"] + sorted([x for x in cat_vals.unique() if x.strip()])\n",
    "        sel1 = st.selectbox(\"Category\", opts1, index=0)\n",
    "\n",
    "    df1 = df_base if sel1 == \"(All)\" else df_base[df_base[\"Category\"].astype(str) == str(sel1)]\n",
    "\n",
    "    with c2:\n",
    "        categ_vals = df1[\"Categorization\"].astype(str)\n",
    "        opts2 = [\"(All)\"] + sorted([x for x in categ_vals.unique() if x.strip()])\n",
    "        sel2 = st.selectbox(\"Categorization\", opts2, index=0)\n",
    "\n",
    "    df2 = df1 if sel2 == \"(All)\" else df1[df1[\"Categorization\"].astype(str) == str(sel2)]\n",
    "\n",
    "    with c3:\n",
    "        word_vals = df2[\"Word\"].astype(str)\n",
    "        opts3 = [\"(All)\"] + sorted([x for x in word_vals.unique() if x.strip()])\n",
    "        sel3 = st.selectbox(\"Word\", opts3, index=0)\n",
    "\n",
    "    df_view_full = df2 if sel3 == \"(All)\" else df2[df2[\"Word\"].astype(str) == str(sel3)]\n",
    "\n",
    "    st.caption(f\"Rows: {len(df_view_full)}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2) Grid (ONLY 4 visual columns) + hidden _row_id for mapping\n",
    "    # ----------------------------\n",
    "    df_view_full = df_view_full.copy().reset_index(drop=False)\n",
    "\n",
    "    # Use the original index as a stable id; name it _row_id\n",
    "    df_view_full = df_view_full.rename(columns={\"index\": \"_row_id\"})\n",
    "\n",
    "    visible_cols = [\"Category\", \"Categorization\", \"Word\", \"Definition\"]\n",
    "    grid_df = df_view_full[[\"_row_id\"] + visible_cols].copy()\n",
    "\n",
    "    gb = GridOptionsBuilder.from_dataframe(grid_df)\n",
    "    gb.configure_default_column(resizable=True, sortable=True, filter=True, wrapText=True, autoHeight=True)\n",
    "    gb.configure_selection(\"single\", use_checkbox=False)\n",
    "    gb.configure_column(\"_row_id\", hide=True)\n",
    "\n",
    "    gb.configure_column(\"Category\", width=120)\n",
    "    gb.configure_column(\"Categorization\", width=160)\n",
    "    gb.configure_column(\"Word\", width=160)\n",
    "    gb.configure_column(\"Definition\", width=520)\n",
    "\n",
    "    grid_resp = AgGrid(\n",
    "        grid_df,\n",
    "        gridOptions=gb.build(),\n",
    "        height=320,\n",
    "        fit_columns_on_grid_load=True,\n",
    "        allow_unsafe_jscode=True,\n",
    "        update_mode=GridUpdateMode.SELECTION_CHANGED,\n",
    "        data_return_mode=DataReturnMode.FILTERED_AND_SORTED,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b7c25-b909-40cd-8952-1fe7c6ab73e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3241329-6202-4c8f-8819-577d114d89e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addaeb94-d17b-4053-aa49-1cc38017680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed Multithread Processing from Read Directory. Need to have displine and Principles related to \n",
    "# Creation of Functions, Create Individual functions which do components, opposed to long ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7809e9d-3de0-4544-a3d3-66d032360591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "239d1677-52bf-4a03-9f01-8f42433137eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Categorization</th>\n",
       "      <th>Word</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Link</th>\n",
       "      <th>Image</th>\n",
       "      <th>Markdown Equation</th>\n",
       "      <th>Dataset Size</th>\n",
       "      <th>Learning Type</th>\n",
       "      <th>Algorithm Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best Linear Unbiased Estimator</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Homoscedasticity</td>\n",
       "      <td>Variance of the residuals should be constant.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Best Linear Unbiased Estimator</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Independence</td>\n",
       "      <td>Observations should be independent of each oth...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Best Linear Unbiased Estimator</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Linearity</td>\n",
       "      <td>Relationship between Independent and Dependent...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Best Linear Unbiased Estimator</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>No Perfect Collinearity</td>\n",
       "      <td>No perfect linear relationship between residuals</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Best Linear Unbiased Estimator</td>\n",
       "      <td>Requirements</td>\n",
       "      <td>Normality of Residuals</td>\n",
       "      <td>Residuals should be normally distributed.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Functional Role</td>\n",
       "      <td>Contextual Data</td>\n",
       "      <td>Provides situational or external information t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Functional Role</td>\n",
       "      <td>Structural Data</td>\n",
       "      <td>Represents relatively stable attributes that d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Semantic Type</td>\n",
       "      <td>Describes what kind of real-world concept a va...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Functional Role</td>\n",
       "      <td>Describes how a variable is used within a spec...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Monitoring</td>\n",
       "      <td>Clustering</td>\n",
       "      <td>This is a Test</td>\n",
       "      <td>Does this Show???</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Category    Categorization  \\\n",
       "0    Best Linear Unbiased Estimator      Requirements   \n",
       "1    Best Linear Unbiased Estimator      Requirements   \n",
       "2    Best Linear Unbiased Estimator      Requirements   \n",
       "3    Best Linear Unbiased Estimator      Requirements   \n",
       "4    Best Linear Unbiased Estimator      Requirements   \n",
       "..                              ...               ...   \n",
       "304                Data Preparation  Functional Role    \n",
       "305                Data Preparation  Functional Role    \n",
       "306                Machine Learning        Definition   \n",
       "307                Machine Learning        Definition   \n",
       "308                      Monitoring        Clustering   \n",
       "\n",
       "                        Word  \\\n",
       "0           Homoscedasticity   \n",
       "1               Independence   \n",
       "2                  Linearity   \n",
       "3    No Perfect Collinearity   \n",
       "4     Normality of Residuals   \n",
       "..                       ...   \n",
       "304          Contextual Data   \n",
       "305          Structural Data   \n",
       "306           Semantic Type    \n",
       "307         Functional Role    \n",
       "308           This is a Test   \n",
       "\n",
       "                                            Definition Notes  Link Image  \\\n",
       "0        Variance of the residuals should be constant.   NaN   NaN   NaN   \n",
       "1    Observations should be independent of each oth...   NaN   NaN   NaN   \n",
       "2    Relationship between Independent and Dependent...   NaN   NaN   NaN   \n",
       "3     No perfect linear relationship between residuals   NaN   NaN   NaN   \n",
       "4            Residuals should be normally distributed.   NaN   NaN   NaN   \n",
       "..                                                 ...   ...   ...   ...   \n",
       "304  Provides situational or external information t...   NaN   NaN   NaN   \n",
       "305  Represents relatively stable attributes that d...   NaN   NaN   NaN   \n",
       "306  Describes what kind of real-world concept a va...   NaN   NaN   NaN   \n",
       "307  Describes how a variable is used within a spec...   NaN   NaN   NaN   \n",
       "308                                  Does this Show???   NaN   NaN   NaN   \n",
       "\n",
       "    Markdown Equation Dataset Size  Learning Type  Algorithm Class  \n",
       "0                 NaN          NaN            NaN              NaN  \n",
       "1                 NaN          NaN            NaN              NaN  \n",
       "2                 NaN          NaN            NaN              NaN  \n",
       "3                 NaN          NaN            NaN              NaN  \n",
       "4                 NaN          NaN            NaN              NaN  \n",
       "..                ...          ...            ...              ...  \n",
       "304               NaN          NaN            NaN              NaN  \n",
       "305               NaN          NaN            NaN              NaN  \n",
       "306               NaN          NaN            NaN              NaN  \n",
       "307               NaN          NaN            NaN              NaN  \n",
       "308               NaN          NaN            NaN              NaN  \n",
       "\n",
       "[309 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_definition_csv = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQq1-3cTas8DCWBa2NKYhVFXpl8kLaFDohg0zMfNTAU_Fiw6aIFLWfA5zRem4eSaGPa7UiQvkz05loW/pub?output=csv'\n",
    "\n",
    "df = pd.read_csv(google_definition_csv)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7e1be-9a49-447d-b4c4-81fba48a2686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038dafbb-1907-4ee2-aa52-48fcad615dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    Definition of Function\n",
      "\n",
      "    Parameters:\n",
      "        List of Parameters\n",
      "\n",
      "    Returns:\n",
      "        Object Type\n",
      "\n",
      "    date_created:30-Dec-25\n",
      "    date_last_modified: 30-Dec-25\n",
      "    classification:TBD\n",
      "    sub_classification:TBD\n",
      "    usage:\n",
      "        Example Function Call\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_d_strings import template_doc_string_print\n",
    "\n",
    "template_doc_string_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e4883ce-79ab-4b5f-b7fb-6c55de64cc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['utility_functions.py',\n",
       " '.DS_Store',\n",
       " 'dict_processing.py',\n",
       " 'Archive',\n",
       " 'data_d_lists.py',\n",
       " 'shared_folder.py',\n",
       " 'list_processing.py',\n",
       " 'connections.py',\n",
       " '__init__.py',\n",
       " '__pycache__',\n",
       " 'string_processing.py',\n",
       " 'V2',\n",
       " 'input_functions_ignore.py',\n",
       " 'sql_.py',\n",
       " '.ipynb_checkpoints',\n",
       " 'data_d_strings.py',\n",
       " 'data_d_dicts.py',\n",
       " 'df_processing.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shared_folder import read_directory\n",
    "d_py_function =  '/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/'\n",
    "read_directory(d_py_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e33665-a40f-4bdb-a2d0-52dc7bf6a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import a .TXT or .PY File\n",
    "location = '/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/shared_folder.py'\n",
    "\n",
    "from shared_folder import text_file_import\n",
    "file = text_file_import(location)\n",
    "\n",
    "from shared_folder import parse_dot_py_file\n",
    "function_list, function_parameters = parse_dot_py_file(file)\n",
    "\n",
    "## Generate a Summary File for a Folder.\n",
    "from shared_folder import parse_dot_py_folder\n",
    "function_list, function_parameters = parse_dot_py_folder()\n",
    "\n",
    "from shared_folder import create_py_table_dict\n",
    "create_py_table_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b910ee8-bd2d-4da0-ac9b-6de5dc29535c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "DownloadFilesFromGit()\n",
    "\n",
    "BackUpGoogleSheets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db4215-d821-4838-88af-6c7a729d7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def TranposeNonTimeSeriesDF(df, index, columns=None):\n",
    "    '''\n",
    "    Transposes a non-time-series DataFrame from wide to long format by melting specified columns.\n",
    "\n",
    "    This is especially useful for flattening columns into a single column to support tools \n",
    "    like Power BI, where long format enables dynamic pivoting and aggregation.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input pandas DataFrame.\n",
    "        index (list): Columns to retain as identifiers (will remain unchanged).\n",
    "        columns (list): Columns to unpivot into key-value pairs.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A long-format DataFrame with 'variable' and 'value' columns.\n",
    "    '''\n",
    "    if not columns:\n",
    "        columns = [col for col in final_df1.columns if (isinstance(col, pd.Timestamp))|(isinstance(col, datetime.datetime))]\n",
    "    \n",
    "    return df.melt(id_vars=index, value_vars=columns)\n",
    "\n",
    "def CreatePivotTableFromTimeSeries(df,\n",
    "                                   index,\n",
    "                                   columns,\n",
    "                                   values,\n",
    "                                   aggfunc='sum',\n",
    "                                   skipna=True):\n",
    "    \n",
    "    '''\n",
    "    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 1. Pivot\n",
    "    if index==None:\n",
    "        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)\n",
    "    else:\n",
    "        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)\n",
    "\n",
    "    # 2. Capture original month columns IMMEDIATELY after pivot\n",
    "    month_cols = df1.columns.tolist()\n",
    " \n",
    "    # 3. Add rolling window stats\n",
    "    if len(month_cols) >= 3:\n",
    "        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]\n",
    "        try:\n",
    "            df1['PERC_CHG_3M'] = df1['CHG_3M']/df1[month_cols[-3]]\n",
    "        except:\n",
    "            df1['PERC_CHG_3M'] = 0\n",
    "    \n",
    "    if len(month_cols) >= 6:\n",
    "        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]\n",
    "        try:\n",
    "            df1['PERC_CHG_6M'] = df1['CHG_6M']/df1[month_cols[-6]]\n",
    "        except:\n",
    "            df1['PERC_CHG_6M'] = 0\n",
    "            \n",
    "    if len(month_cols) >= 12:\n",
    "        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]\n",
    "        try:\n",
    "            df1['PERC_CHG_12M'] = df1['CHG_12M']/df1[month_cols[-12]]\n",
    "        except:\n",
    "            df1['PERC_CHG_12M'] = 0\n",
    "\n",
    "    df1['CHG_DF']  = df1[month_cols[-1]]-df1[month_cols[0]]\n",
    "    df1['AVG_DF'] = df1[month_cols[-1:]].mean(axis=1, skipna=skipna)\n",
    "    df1['PERC_CHG_DF'] = df1['AVG_DF']/df1[month_cols[-1]]\n",
    "\n",
    "    \n",
    "    # 4. Now calculate global stats **only using the original month columns**\n",
    "    stats = pd.DataFrame({\n",
    "        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),\n",
    "        'STD': df1[month_cols].std(axis=1, skipna=skipna),\n",
    "        'MAX': df1[month_cols].max(axis=1, skipna=skipna),\n",
    "        'MIN': df1[month_cols].min(axis=1, skipna=skipna),\n",
    "        'COUNT': df1[month_cols].count(axis=1)\n",
    "    })\n",
    "\n",
    "    # 5. Merge the stats\n",
    "    df1 = pd.concat([df1, stats], axis=1)\n",
    "    \n",
    "    return df1.fillna(0)\n",
    "\n",
    "\n",
    "def CreateMultiplePivotTableFromTimeSeries(df,\n",
    "                                           index_list,\n",
    "                                           metric_list,\n",
    "                                           column):\n",
    "    '''\n",
    "    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through all Possible Metrics Selected.\n",
    "    \n",
    "    for metric in metric_list:\n",
    "        print(f'Attempting to Process:{metric}')\n",
    "        try:\n",
    "            all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') \n",
    "            cols = list(all_df.columns)\n",
    "            all_df = all_df.reset_index(drop=True)\n",
    "            all_df['METRIC'] = metric\n",
    "            cols.insert(0,'METRIC')\n",
    "\n",
    "            for key in index_list:\n",
    "                cols.insert(0,key)\n",
    "                all_df[key] = 'All'\n",
    "\n",
    "            final_df = pd.concat([final_df,all_df[cols]])\n",
    "            # Iterate through all Index Items Individually\n",
    "            for key in index_list:\n",
    "                temp = CreatePivotTableFromTimeSeries(df,\n",
    "                                                      index=key,\n",
    "                                                      values=metric,\n",
    "                                                      columns=column).reset_index() \n",
    "                for missing in [x for x in index_list if x != key]:\n",
    "                    temp[missing] = 'All'\n",
    "                temp['METRIC'] = metric\n",
    "                final_df = pd.concat([final_df,temp])\n",
    "\n",
    "            # Add Value for Metric with Entire Index Combination\n",
    "            temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()\n",
    "            temp['METRIC'] = metric\n",
    "            final_df = pd.concat([final_df,temp])\n",
    "        except:\n",
    "            print(f'Could Not Process Metric:{metric}.')\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):\n",
    "    '''\n",
    "    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through all Possible Metrics Selected.\n",
    "    for metric in metric_list:\n",
    "        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') \n",
    "        cols = list(all_df.columns)\n",
    "        all_df = all_df.reset_index(drop=True)\n",
    "        all_df['METRIC'] = metric\n",
    "        cols.insert(0,'METRIC')\n",
    "\n",
    "        for key in index:\n",
    "            cols.insert(0,key)\n",
    "            all_df[key] = 'All'\n",
    "\n",
    "        final_df = pd.concat([final_df,all_df[cols]])\n",
    "\n",
    "        # Iterate through all Index Items Individually\n",
    "        for key in index_list:\n",
    "            temp = CreatePivotTableFromTimeSeries(df,index=key,\n",
    "                                                  values=metric,\n",
    "                                                  columns=column).reset_index() \n",
    "            for missing in [x for x in index if x != key]:\n",
    "                temp[missing] = 'All'\n",
    "            temp['METRIC'] = metric\n",
    "            final_df = pd.concat([final_df,temp])\n",
    "        \n",
    "        # Add Value for Metric with Entire Index Combination\n",
    "        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()\n",
    "        temp['METRIC'] = metric\n",
    "        final_df = pd.concat([final_df,temp])\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def SummarizeTimeSeriesDf(df,\n",
    "                          summary_cols,\n",
    "                          primary_key_list):\n",
    "    '''\n",
    "    Function to Summarize a Time Series dataframe based on a finite number of identified Columns.\n",
    "    \n",
    "    Parameters\n",
    "        df (Dataframe): TimeSeries in Nature\n",
    "        summary_cols (List): List of Columns which are to be included in SUmmary\n",
    "        primary_key_list (list): Primary Key of Dataframe\n",
    "    \n",
    "    Returns\n",
    "        temp_df1: Raw Data of SUmmary Cols with a Count of Observations. If include Month Variable Easy to add to Pivot Table\n",
    "        summary: Summary (Excluding Primary Key). including Total Observations, MEan, Max, Min.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    temp_df = df[summary_cols].copy()\n",
    "    temp_df['COUNT'] = 1\n",
    "    \n",
    "    # Unique Occurances by Pivot Criteria. Important to Include Month\n",
    "    temp_df1 = temp_df.groupby(summary_cols).sum().reset_index().rename(columns={'COUNT':'TOTAL_DAYS'})\n",
    "    \n",
    "    pivot_columns1 = [x for x in summary_cols if x not in primary_key_list]\n",
    "    \n",
    "    summary = temp_df1.groupby(pivot_columns1).agg(\n",
    "        TOTAL=('TOTAL_DAYS', 'count'),\n",
    "        AVG_DAYS_OPEN=('TOTAL_DAYS', 'mean'),\n",
    "        MAX_OBS=('TOTAL_DAYS', 'min'),\n",
    "        MIN_OBS=('TOTAL_DAYS', 'max')).reset_index()\n",
    "    \n",
    "    return temp_df1,summary\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def SampleDataFrame(df, \n",
    "                    conf=.95, \n",
    "                    me=0.05,\n",
    "                    mv=0.5,\n",
    "                    print_=0,\n",
    "                    new_column_name=\"\"):\n",
    "    \"\"\"\n",
    "    Returns a random sample from a DataFrame based on confidence level and margin of error.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataset to sample from.\n",
    "        conf(float): Desired Confidence Percentage Level (e.g., 90, 95, 99).\n",
    "        me (float): Margin of Error, (default is 5%).\n",
    "        mv (float): Maximum Variability (Expected Level of Default)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A random sample of the required size.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if not 0 <= mv <= 1:\n",
    "        raise ValueError(\"mv (failure rate) must be between 0 and 1.\")\n",
    "\n",
    "    N = len(df)\n",
    "    if N == 0:\n",
    "        raise ValueError(\"DataFrame is empty\")\n",
    "\n",
    "    # Calculate the Z-score based on the confidence level\n",
    "    z = norm.ppf(1 - (1 - conf) / 2)\n",
    "    \n",
    "\n",
    "    # Calculate the initial sample size (without finite population correction)\n",
    "    n0 = (z**2 * mv * (1 - mv)) / (me**2)\n",
    "    \n",
    "    # Apply finite population correction if the population is smaller than 100,000\n",
    "    if N >= 10000:  # For large populations, skip the correction\n",
    "        n = int(n0)\n",
    "    else:\n",
    "        n = int((n0 * N) / (n0 + N - 1))\n",
    "\n",
    "    if print_==1:\n",
    "        print(f\"Z-score: {z}\")  # Debug Z-score\n",
    "        print(f\"Initial sample size (n0): {n0}\")  # Debug n0\n",
    "        print(f\"Sample size with FPC: {n}\")  # Debug final sample size\n",
    "    \n",
    "    sample = df.sample(n=n, random_state=42)\n",
    "    \n",
    "    if len(new_column_name)==0:\n",
    "        return sample \n",
    "\n",
    "    else:\n",
    "        sample_index = sample.index\n",
    "        df[new_column_name] = 0\n",
    "        df.loc[sample_index, new_column_name] = 1\n",
    "        return df\n",
    "\n",
    "def ReviewEntireDataframe(df,file_name=None):\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for column in df.columns:\n",
    "        start_time = timeit.default_timer()\n",
    "        temp_df = ColumnStatisticalReview(df,column)\n",
    "        print(f'Elapsed time to process {column}:{timeit.default_timer() - start_time:,.2f}')\n",
    "        final_df = pd.concat([final_df,temp_df],axis=1)\n",
    "    if file_name:\n",
    "        final_df.to_csv(f\"{file_name}.csv\")\n",
    "        \n",
    "    return final_df\n",
    "\n",
    "def ColumnStatisticalReview(df,\n",
    "                            column_name,\n",
    "                            partitions=10,\n",
    "                            top_x_records=10,\n",
    "                            exclude_blanks_from_segments=1,\n",
    "                            exclude_zeroes_from_segments=1):\n",
    "\n",
    "    '''\n",
    "    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution\n",
    "    of values. \n",
    "\n",
    "    Args:\n",
    "        column_name (str): Name of Column\n",
    "\n",
    "        partitions (int): Number of partitions to include (Decile 10)\n",
    "\n",
    "        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.\n",
    "        If blank values are excluded it gives a better representation for the members of the set, however it might \n",
    "        provide a misleading representation of the population.\n",
    "\n",
    "        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as\n",
    "        such it can include both blanks and true 0 values. \n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    temp_dict = {}\n",
    "    \n",
    "    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])\n",
    "    \n",
    "    if is_numeric:\n",
    "        temp_dict['SUM'] = df[column_name].sum()\n",
    "        temp_dict['MEAN'] = df[column_name].mean()\n",
    "        temp_dict['STD_DEV'] =  df[column_name].std()\n",
    "        temp_dict['MEDIAN'] = df[column_name].median()\n",
    "        temp_dict['MAX'] = df[column_name].max()\n",
    "        temp_dict['MIN'] = df[column_name].min()\n",
    "        \n",
    "    temp_dict['TOTAL_RECORDS'] = len(df)\n",
    "    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))\n",
    "    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])\n",
    "    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])\n",
    "    \n",
    "    if is_numeric:\n",
    "        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])\n",
    "        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    \n",
    "\n",
    "    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])\n",
    "    \n",
    "    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):\n",
    "        return temp_df\n",
    "\n",
    "    # Add top X records Based on Frequency\n",
    "    if top_x_records>0:\n",
    "        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index().rename(columns={column_name:'count','index':column_name})\n",
    "        if len(top_instances)>0:\n",
    "            top_instances[column_name] = top_instances.apply(lambda row: f\"Value: {row[column_name]}, Frequency: {row['count']}\", axis=1)\n",
    "            top_instances['index'] = [f\"Top {x+1}\" for x in range(len(top_instances[column_name]))]\n",
    "            top_instances = top_instances.drop('count',axis=1).set_index('index')\n",
    "            temp_df = pd.concat([temp_df,top_instances])\n",
    "        \n",
    "    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):\n",
    "        segment_df = ColumnPartitioner(df=df,\n",
    "                                       column_name=column_name,\n",
    "                                       partitions=partitions,\n",
    "                                       exclude_blanks=exclude_blanks_from_segments,\n",
    "                                       exclude_zeros=exclude_zeroes_from_segments,\n",
    "                                       return_value='')\n",
    "        \n",
    "        seg_val_df = ColumnPartitioner(df=df,\n",
    "                                           column_name=column_name,\n",
    "                                           partitions=partitions,\n",
    "                                           exclude_blanks=exclude_blanks_from_segments,\n",
    "                                           exclude_zeros=exclude_zeroes_from_segments,\n",
    "                                           return_value='agg_value').rename(columns={'VALUE':column_name})\n",
    "\n",
    "        return pd.concat([temp_df,segment_df.T,seg_val_df])\n",
    "    return temp_df\n",
    "\n",
    "def CompareFunction(func1,func2,additional_records=20):\n",
    "    \n",
    "    '''\n",
    "    Function which Compares 2 Functions and determines if they are different. Specifically, it can help to easily\n",
    "    Manage Version control of Functions outside of a More robust environment such as GIT.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    list1 = FunctionToSTR(func1)\n",
    "    list2 = FunctionToSTR(func2)\n",
    "    \n",
    "    length = max(len(list1),len(list2))\n",
    "    \n",
    "    for record in range(0,length):\n",
    "        if list1[record]==list2[record]:\n",
    "            if record == (length-1):\n",
    "                print(\"All Records Reconcile\")\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                print(list1[record:record+additional_records])\n",
    "                print(list2[record:record+additional_records])\n",
    "            except:\n",
    "                print(list1[record:record:])\n",
    "                print(list2[record:record:])\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f03f7-6c88-43bd-85d0-05dfb969eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "def DownloadFilesFromGit(user='derek-dewald',\n",
    "                        repo='Python_Tools',\n",
    "                        folder='d_py_functions',\n",
    "                        output_folder=\"\"):\n",
    "    '''\n",
    "    Function to Download Files from Github to a dedicated folder. Specifically used when i DO NOT want to formally link to Github.\n",
    "    \n",
    "    Parameters:\n",
    "        User:\n",
    "        Repo:\n",
    "        folder:\n",
    "        output_folder:\n",
    "        \n",
    "    Returns:\n",
    "        Saves files to Output Folder.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    if len(output_folder) == 0:\n",
    "        output_folder = os.getcwd()\n",
    "    \n",
    "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents/{folder}\"\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        files = response.json()\n",
    "        py_files = [file for file in files if file['name'].endswith('.py')]\n",
    "\n",
    "        for file in py_files:\n",
    "            file_url = file['download_url']\n",
    "            file_name = file['name']\n",
    "            file_response = requests.get(file_url)\n",
    "\n",
    "            if file_response.status_code == 200:\n",
    "                with open(os.path.join(output_folder, file_name), \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(file_response.text)\n",
    "                    \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def ParamterMapping(Definition=\"\"):\n",
    "    \n",
    "    '''\n",
    "    Function to Google Mapping Sheet, which is used to store Mappings, Links, etc.\n",
    "    For both simplicity and Organization\n",
    "    \n",
    "    Args:\n",
    "        Definition (Str): Key word used to Access individual elements\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe, unless Definition is defined, in which case it might be Str.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSwDznLz-GKgWFT1uN0XZYm3bsos899I9MS-pSvEoDC-Cjqo9CWeEuSdjitxjqzF3O39LmjJB0_Fg-B/pub?output=csv')\n",
    "    \n",
    "    # If user has not included a definition, the return entire DF\n",
    "    if len(Definition)==0:\n",
    "        return df\n",
    "    else:\n",
    "        try:\n",
    "            df1 = df[df['Definition']==Definition]\n",
    "            if len(df1)==1:\n",
    "                if df1['TYPE'].item()=='csv':\n",
    "                    return pd.read_csv(df1['VALUE'].item())\n",
    "                else:\n",
    "                    return df1['VALUE'].item()\n",
    "        except:\n",
    "            return df[df['Definition']==Definition] \n",
    "\n",
    "\n",
    "def BackUpGoogleSheets(location='/Users/derekdewald/Documents/Python/Github_Repo/CSV Backup Files/'):\n",
    "    '''\n",
    "    Function to Create a Backup of Information Stored in Google Sheets.\n",
    "    \n",
    "    Parameters:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        CSV Files \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = ParamterMapping()\n",
    "    \n",
    "    for row in range(len(df)):\n",
    "        try:\n",
    "            file_name = df['Definition'][row]\n",
    "            file_location = df['CSV'][row]\n",
    "            month = datetime.datetime.now().strftime('%b-%y')\n",
    "            \n",
    "            temp_df = pd.read_csv(file_location)\n",
    "            temp_df.to_csv(f'{location}{file_name}_{month}.csv',index=False)\n",
    "            print(f'Back Up Saved, {location}{file_name}')\n",
    "        except:\n",
    "            print(f'Counld Not Print Record {row}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def NavigateUsingDMap():\n",
    "     \n",
    "    '''\n",
    "    Function to Google Mapping Sheet, Navigate to Specific Sites.\n",
    "    Provides Options, Enable Selection based on inputs.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSwDznLz-GKgWFT1uN0XZYm3bsos899I9MS-pSvEoDC-Cjqo9CWeEuSdjitxjqzF3O39LmjJB0_Fg-B/pub?output=csv')\n",
    "    \n",
    "    display(df)\n",
    "    \n",
    "    p = input('Which Process would You like to review?')\n",
    "    v = input('What would you like to return?')\n",
    "    \n",
    "    df1 = df[df['Definition']==p]\n",
    "    \n",
    "    if v.lower() =='link':\n",
    "        webbrowser.open(df1['Link'].item())\n",
    "    elif v.lower() == 'csv':\n",
    "        return pd.read_csv(df1['CSV'].item())\n",
    "    elif v.lower()=='streamlit':\n",
    "        webbrowser.open(df1['Streamlit'].item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
