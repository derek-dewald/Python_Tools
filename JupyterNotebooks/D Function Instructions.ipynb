{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addaeb94-d17b-4053-aa49-1cc38017680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed Multithread Processing from Read Directory. Need to have displine and Principles related to \n",
    "# Creation of Functions, Create Individual functions which do components, opposed to long ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612910a5-d2bd-49cf-a21b-1769505aafaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Item</th>\n",
       "      <th>Text</th>\n",
       "      <th>Source</th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Verse(s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15-Dec-25</td>\n",
       "      <td>Scripture</td>\n",
       "      <td>I am not saying this because I am in need, for...</td>\n",
       "      <td>Philippians</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15-Dec-25</td>\n",
       "      <td>Proverb</td>\n",
       "      <td>My son, if sinful men entice you, do not give ...</td>\n",
       "      <td>Proverbs</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15-Dec-25</td>\n",
       "      <td>Psalm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15-Dec-25</td>\n",
       "      <td>Word</td>\n",
       "      <td>Take No Ones Word For it.</td>\n",
       "      <td>Nullius in verba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15-Dec-25</td>\n",
       "      <td>Quote</td>\n",
       "      <td>You are what your record says you are.</td>\n",
       "      <td>Bill Parcells</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>16-Jul-26</td>\n",
       "      <td>Quote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>17-Jul-26</td>\n",
       "      <td>Scripture</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>17-Jul-26</td>\n",
       "      <td>Proverb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>17-Jul-26</td>\n",
       "      <td>Psalm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>17-Jul-26</td>\n",
       "      <td>Word</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date       Item                                               Text  \\\n",
       "0    15-Dec-25  Scripture  I am not saying this because I am in need, for...   \n",
       "1    15-Dec-25    Proverb  My son, if sinful men entice you, do not give ...   \n",
       "2    15-Dec-25      Psalm                                                NaN   \n",
       "3    15-Dec-25       Word                          Take No Ones Word For it.   \n",
       "4    15-Dec-25      Quote             You are what your record says you are.   \n",
       "..         ...        ...                                                ...   \n",
       "994  16-Jul-26      Quote                                                NaN   \n",
       "995  17-Jul-26  Scripture                                                NaN   \n",
       "996  17-Jul-26    Proverb                                                NaN   \n",
       "997  17-Jul-26      Psalm                                                NaN   \n",
       "998  17-Jul-26       Word                                                NaN   \n",
       "\n",
       "               Source  Chapter Verse(s)  \n",
       "0         Philippians      4.0    11:13  \n",
       "1            Proverbs      1.0       10  \n",
       "2                 NaN      NaN      NaN  \n",
       "3    Nullius in verba      NaN      NaN  \n",
       "4       Bill Parcells      NaN      NaN  \n",
       "..                ...      ...      ...  \n",
       "994               NaN      NaN      NaN  \n",
       "995               NaN      NaN      NaN  \n",
       "996               NaN      NaN      NaN  \n",
       "997               NaN      NaN      NaN  \n",
       "998               NaN      NaN      NaN  \n",
       "\n",
       "[999 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_word_quote = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTjXiFjpGgyqWDg9RImj1HR_BeriXs4c5-NSJVwQFn2eRKksitY46oJT0GvVX366LO-m1GM8znXDcBp/pub?gid=1117793378&single=true&output=csv'\n",
    "pd.read_csv(google_word_quote)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7809e9d-3de0-4544-a3d3-66d032360591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038dafbb-1907-4ee2-aa52-48fcad615dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    Definition of Function\n",
      "\n",
      "    Parameters:\n",
      "        List of Parameters\n",
      "\n",
      "    Returns:\n",
      "        Object Type\n",
      "\n",
      "    date_created:30-Dec-25\n",
      "    date_last_modified: 30-Dec-25\n",
      "    classification:TBD\n",
      "    sub_classification:TBD\n",
      "    usage:\n",
      "        Example Function Call\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_d_strings import template_doc_string_print\n",
    "\n",
    "template_doc_string_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e4883ce-79ab-4b5f-b7fb-6c55de64cc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['utility_functions.py',\n",
       " '.DS_Store',\n",
       " 'dict_processing.py',\n",
       " 'Archive',\n",
       " 'data_d_lists.py',\n",
       " 'shared_folder.py',\n",
       " 'list_processing.py',\n",
       " 'connections.py',\n",
       " '__init__.py',\n",
       " '__pycache__',\n",
       " 'string_processing.py',\n",
       " 'V2',\n",
       " 'input_functions_ignore.py',\n",
       " 'sql_.py',\n",
       " '.ipynb_checkpoints',\n",
       " 'data_d_strings.py',\n",
       " 'data_d_dicts.py',\n",
       " 'df_processing.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shared_folder import read_directory\n",
    "d_py_function =  '/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/'\n",
    "read_directory(d_py_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e33665-a40f-4bdb-a2d0-52dc7bf6a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import a .TXT or .PY File\n",
    "location = '/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/shared_folder.py'\n",
    "\n",
    "from shared_folder import text_file_import\n",
    "file = text_file_import(location)\n",
    "\n",
    "from shared_folder import parse_dot_py_file\n",
    "function_list, function_parameters = parse_dot_py_file(file)\n",
    "\n",
    "## Generate a Summary File for a Folder.\n",
    "from shared_folder import parse_dot_py_folder\n",
    "function_list, function_parameters = parse_dot_py_folder()\n",
    "\n",
    "from shared_folder import create_py_table_dict\n",
    "create_py_table_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b910ee8-bd2d-4da0-ac9b-6de5dc29535c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "DownloadFilesFromGit()\n",
    "\n",
    "BackUpGoogleSheets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db4215-d821-4838-88af-6c7a729d7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def TranposeNonTimeSeriesDF(df, index, columns=None):\n",
    "    '''\n",
    "    Transposes a non-time-series DataFrame from wide to long format by melting specified columns.\n",
    "\n",
    "    This is especially useful for flattening columns into a single column to support tools \n",
    "    like Power BI, where long format enables dynamic pivoting and aggregation.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input pandas DataFrame.\n",
    "        index (list): Columns to retain as identifiers (will remain unchanged).\n",
    "        columns (list): Columns to unpivot into key-value pairs.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A long-format DataFrame with 'variable' and 'value' columns.\n",
    "    '''\n",
    "    if not columns:\n",
    "        columns = [col for col in final_df1.columns if (isinstance(col, pd.Timestamp))|(isinstance(col, datetime.datetime))]\n",
    "    \n",
    "    return df.melt(id_vars=index, value_vars=columns)\n",
    "\n",
    "def CreatePivotTableFromTimeSeries(df,\n",
    "                                   index,\n",
    "                                   columns,\n",
    "                                   values,\n",
    "                                   aggfunc='sum',\n",
    "                                   skipna=True):\n",
    "    \n",
    "    '''\n",
    "    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 1. Pivot\n",
    "    if index==None:\n",
    "        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)\n",
    "    else:\n",
    "        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)\n",
    "\n",
    "    # 2. Capture original month columns IMMEDIATELY after pivot\n",
    "    month_cols = df1.columns.tolist()\n",
    " \n",
    "    # 3. Add rolling window stats\n",
    "    if len(month_cols) >= 3:\n",
    "        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]\n",
    "        try:\n",
    "            df1['PERC_CHG_3M'] = df1['CHG_3M']/df1[month_cols[-3]]\n",
    "        except:\n",
    "            df1['PERC_CHG_3M'] = 0\n",
    "    \n",
    "    if len(month_cols) >= 6:\n",
    "        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]\n",
    "        try:\n",
    "            df1['PERC_CHG_6M'] = df1['CHG_6M']/df1[month_cols[-6]]\n",
    "        except:\n",
    "            df1['PERC_CHG_6M'] = 0\n",
    "            \n",
    "    if len(month_cols) >= 12:\n",
    "        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]\n",
    "        try:\n",
    "            df1['PERC_CHG_12M'] = df1['CHG_12M']/df1[month_cols[-12]]\n",
    "        except:\n",
    "            df1['PERC_CHG_12M'] = 0\n",
    "\n",
    "    df1['CHG_DF']  = df1[month_cols[-1]]-df1[month_cols[0]]\n",
    "    df1['AVG_DF'] = df1[month_cols[-1:]].mean(axis=1, skipna=skipna)\n",
    "    df1['PERC_CHG_DF'] = df1['AVG_DF']/df1[month_cols[-1]]\n",
    "\n",
    "    \n",
    "    # 4. Now calculate global stats **only using the original month columns**\n",
    "    stats = pd.DataFrame({\n",
    "        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),\n",
    "        'STD': df1[month_cols].std(axis=1, skipna=skipna),\n",
    "        'MAX': df1[month_cols].max(axis=1, skipna=skipna),\n",
    "        'MIN': df1[month_cols].min(axis=1, skipna=skipna),\n",
    "        'COUNT': df1[month_cols].count(axis=1)\n",
    "    })\n",
    "\n",
    "    # 5. Merge the stats\n",
    "    df1 = pd.concat([df1, stats], axis=1)\n",
    "    \n",
    "    return df1.fillna(0)\n",
    "\n",
    "\n",
    "def CreateMultiplePivotTableFromTimeSeries(df,\n",
    "                                           index_list,\n",
    "                                           metric_list,\n",
    "                                           column):\n",
    "    '''\n",
    "    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through all Possible Metrics Selected.\n",
    "    \n",
    "    for metric in metric_list:\n",
    "        print(f'Attempting to Process:{metric}')\n",
    "        try:\n",
    "            all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') \n",
    "            cols = list(all_df.columns)\n",
    "            all_df = all_df.reset_index(drop=True)\n",
    "            all_df['METRIC'] = metric\n",
    "            cols.insert(0,'METRIC')\n",
    "\n",
    "            for key in index_list:\n",
    "                cols.insert(0,key)\n",
    "                all_df[key] = 'All'\n",
    "\n",
    "            final_df = pd.concat([final_df,all_df[cols]])\n",
    "            # Iterate through all Index Items Individually\n",
    "            for key in index_list:\n",
    "                temp = CreatePivotTableFromTimeSeries(df,\n",
    "                                                      index=key,\n",
    "                                                      values=metric,\n",
    "                                                      columns=column).reset_index() \n",
    "                for missing in [x for x in index_list if x != key]:\n",
    "                    temp[missing] = 'All'\n",
    "                temp['METRIC'] = metric\n",
    "                final_df = pd.concat([final_df,temp])\n",
    "\n",
    "            # Add Value for Metric with Entire Index Combination\n",
    "            temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()\n",
    "            temp['METRIC'] = metric\n",
    "            final_df = pd.concat([final_df,temp])\n",
    "        except:\n",
    "            print(f'Could Not Process Metric:{metric}.')\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):\n",
    "    '''\n",
    "    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through all Possible Metrics Selected.\n",
    "    for metric in metric_list:\n",
    "        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') \n",
    "        cols = list(all_df.columns)\n",
    "        all_df = all_df.reset_index(drop=True)\n",
    "        all_df['METRIC'] = metric\n",
    "        cols.insert(0,'METRIC')\n",
    "\n",
    "        for key in index:\n",
    "            cols.insert(0,key)\n",
    "            all_df[key] = 'All'\n",
    "\n",
    "        final_df = pd.concat([final_df,all_df[cols]])\n",
    "\n",
    "        # Iterate through all Index Items Individually\n",
    "        for key in index_list:\n",
    "            temp = CreatePivotTableFromTimeSeries(df,index=key,\n",
    "                                                  values=metric,\n",
    "                                                  columns=column).reset_index() \n",
    "            for missing in [x for x in index if x != key]:\n",
    "                temp[missing] = 'All'\n",
    "            temp['METRIC'] = metric\n",
    "            final_df = pd.concat([final_df,temp])\n",
    "        \n",
    "        # Add Value for Metric with Entire Index Combination\n",
    "        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()\n",
    "        temp['METRIC'] = metric\n",
    "        final_df = pd.concat([final_df,temp])\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def SummarizeTimeSeriesDf(df,\n",
    "                          summary_cols,\n",
    "                          primary_key_list):\n",
    "    '''\n",
    "    Function to Summarize a Time Series dataframe based on a finite number of identified Columns.\n",
    "    \n",
    "    Parameters\n",
    "        df (Dataframe): TimeSeries in Nature\n",
    "        summary_cols (List): List of Columns which are to be included in SUmmary\n",
    "        primary_key_list (list): Primary Key of Dataframe\n",
    "    \n",
    "    Returns\n",
    "        temp_df1: Raw Data of SUmmary Cols with a Count of Observations. If include Month Variable Easy to add to Pivot Table\n",
    "        summary: Summary (Excluding Primary Key). including Total Observations, MEan, Max, Min.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    temp_df = df[summary_cols].copy()\n",
    "    temp_df['COUNT'] = 1\n",
    "    \n",
    "    # Unique Occurances by Pivot Criteria. Important to Include Month\n",
    "    temp_df1 = temp_df.groupby(summary_cols).sum().reset_index().rename(columns={'COUNT':'TOTAL_DAYS'})\n",
    "    \n",
    "    pivot_columns1 = [x for x in summary_cols if x not in primary_key_list]\n",
    "    \n",
    "    summary = temp_df1.groupby(pivot_columns1).agg(\n",
    "        TOTAL=('TOTAL_DAYS', 'count'),\n",
    "        AVG_DAYS_OPEN=('TOTAL_DAYS', 'mean'),\n",
    "        MAX_OBS=('TOTAL_DAYS', 'min'),\n",
    "        MIN_OBS=('TOTAL_DAYS', 'max')).reset_index()\n",
    "    \n",
    "    return temp_df1,summary\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def SampleDataFrame(df, \n",
    "                    conf=.95, \n",
    "                    me=0.05,\n",
    "                    mv=0.5,\n",
    "                    print_=0,\n",
    "                    new_column_name=\"\"):\n",
    "    \"\"\"\n",
    "    Returns a random sample from a DataFrame based on confidence level and margin of error.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataset to sample from.\n",
    "        conf(float): Desired Confidence Percentage Level (e.g., 90, 95, 99).\n",
    "        me (float): Margin of Error, (default is 5%).\n",
    "        mv (float): Maximum Variability (Expected Level of Default)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A random sample of the required size.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if not 0 <= mv <= 1:\n",
    "        raise ValueError(\"mv (failure rate) must be between 0 and 1.\")\n",
    "\n",
    "    N = len(df)\n",
    "    if N == 0:\n",
    "        raise ValueError(\"DataFrame is empty\")\n",
    "\n",
    "    # Calculate the Z-score based on the confidence level\n",
    "    z = norm.ppf(1 - (1 - conf) / 2)\n",
    "    \n",
    "\n",
    "    # Calculate the initial sample size (without finite population correction)\n",
    "    n0 = (z**2 * mv * (1 - mv)) / (me**2)\n",
    "    \n",
    "    # Apply finite population correction if the population is smaller than 100,000\n",
    "    if N >= 10000:  # For large populations, skip the correction\n",
    "        n = int(n0)\n",
    "    else:\n",
    "        n = int((n0 * N) / (n0 + N - 1))\n",
    "\n",
    "    if print_==1:\n",
    "        print(f\"Z-score: {z}\")  # Debug Z-score\n",
    "        print(f\"Initial sample size (n0): {n0}\")  # Debug n0\n",
    "        print(f\"Sample size with FPC: {n}\")  # Debug final sample size\n",
    "    \n",
    "    sample = df.sample(n=n, random_state=42)\n",
    "    \n",
    "    if len(new_column_name)==0:\n",
    "        return sample \n",
    "\n",
    "    else:\n",
    "        sample_index = sample.index\n",
    "        df[new_column_name] = 0\n",
    "        df.loc[sample_index, new_column_name] = 1\n",
    "        return df\n",
    "\n",
    "def ReviewEntireDataframe(df,file_name=None):\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for column in df.columns:\n",
    "        start_time = timeit.default_timer()\n",
    "        temp_df = ColumnStatisticalReview(df,column)\n",
    "        print(f'Elapsed time to process {column}:{timeit.default_timer() - start_time:,.2f}')\n",
    "        final_df = pd.concat([final_df,temp_df],axis=1)\n",
    "    if file_name:\n",
    "        final_df.to_csv(f\"{file_name}.csv\")\n",
    "        \n",
    "    return final_df\n",
    "\n",
    "def ColumnStatisticalReview(df,\n",
    "                            column_name,\n",
    "                            partitions=10,\n",
    "                            top_x_records=10,\n",
    "                            exclude_blanks_from_segments=1,\n",
    "                            exclude_zeroes_from_segments=1):\n",
    "\n",
    "    '''\n",
    "    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution\n",
    "    of values. \n",
    "\n",
    "    Args:\n",
    "        column_name (str): Name of Column\n",
    "\n",
    "        partitions (int): Number of partitions to include (Decile 10)\n",
    "\n",
    "        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.\n",
    "        If blank values are excluded it gives a better representation for the members of the set, however it might \n",
    "        provide a misleading representation of the population.\n",
    "\n",
    "        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as\n",
    "        such it can include both blanks and true 0 values. \n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    temp_dict = {}\n",
    "    \n",
    "    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])\n",
    "    \n",
    "    if is_numeric:\n",
    "        temp_dict['SUM'] = df[column_name].sum()\n",
    "        temp_dict['MEAN'] = df[column_name].mean()\n",
    "        temp_dict['STD_DEV'] =  df[column_name].std()\n",
    "        temp_dict['MEDIAN'] = df[column_name].median()\n",
    "        temp_dict['MAX'] = df[column_name].max()\n",
    "        temp_dict['MIN'] = df[column_name].min()\n",
    "        \n",
    "    temp_dict['TOTAL_RECORDS'] = len(df)\n",
    "    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))\n",
    "    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])\n",
    "    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])\n",
    "    \n",
    "    if is_numeric:\n",
    "        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])\n",
    "        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    \n",
    "\n",
    "    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])\n",
    "    \n",
    "    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):\n",
    "        return temp_df\n",
    "\n",
    "    # Add top X records Based on Frequency\n",
    "    if top_x_records>0:\n",
    "        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index().rename(columns={column_name:'count','index':column_name})\n",
    "        if len(top_instances)>0:\n",
    "            top_instances[column_name] = top_instances.apply(lambda row: f\"Value: {row[column_name]}, Frequency: {row['count']}\", axis=1)\n",
    "            top_instances['index'] = [f\"Top {x+1}\" for x in range(len(top_instances[column_name]))]\n",
    "            top_instances = top_instances.drop('count',axis=1).set_index('index')\n",
    "            temp_df = pd.concat([temp_df,top_instances])\n",
    "        \n",
    "    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):\n",
    "        segment_df = ColumnPartitioner(df=df,\n",
    "                                       column_name=column_name,\n",
    "                                       partitions=partitions,\n",
    "                                       exclude_blanks=exclude_blanks_from_segments,\n",
    "                                       exclude_zeros=exclude_zeroes_from_segments,\n",
    "                                       return_value='')\n",
    "        \n",
    "        seg_val_df = ColumnPartitioner(df=df,\n",
    "                                           column_name=column_name,\n",
    "                                           partitions=partitions,\n",
    "                                           exclude_blanks=exclude_blanks_from_segments,\n",
    "                                           exclude_zeros=exclude_zeroes_from_segments,\n",
    "                                           return_value='agg_value').rename(columns={'VALUE':column_name})\n",
    "\n",
    "        return pd.concat([temp_df,segment_df.T,seg_val_df])\n",
    "    return temp_df\n",
    "\n",
    "def CompareFunction(func1,func2,additional_records=20):\n",
    "    \n",
    "    '''\n",
    "    Function which Compares 2 Functions and determines if they are different. Specifically, it can help to easily\n",
    "    Manage Version control of Functions outside of a More robust environment such as GIT.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    list1 = FunctionToSTR(func1)\n",
    "    list2 = FunctionToSTR(func2)\n",
    "    \n",
    "    length = max(len(list1),len(list2))\n",
    "    \n",
    "    for record in range(0,length):\n",
    "        if list1[record]==list2[record]:\n",
    "            if record == (length-1):\n",
    "                print(\"All Records Reconcile\")\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                print(list1[record:record+additional_records])\n",
    "                print(list2[record:record+additional_records])\n",
    "            except:\n",
    "                print(list1[record:record:])\n",
    "                print(list2[record:record:])\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f03f7-6c88-43bd-85d0-05dfb969eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "def DownloadFilesFromGit(user='derek-dewald',\n",
    "                        repo='Python_Tools',\n",
    "                        folder='d_py_functions',\n",
    "                        output_folder=\"\"):\n",
    "    '''\n",
    "    Function to Download Files from Github to a dedicated folder. Specifically used when i DO NOT want to formally link to Github.\n",
    "    \n",
    "    Parameters:\n",
    "        User:\n",
    "        Repo:\n",
    "        folder:\n",
    "        output_folder:\n",
    "        \n",
    "    Returns:\n",
    "        Saves files to Output Folder.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    if len(output_folder) == 0:\n",
    "        output_folder = os.getcwd()\n",
    "    \n",
    "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents/{folder}\"\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        files = response.json()\n",
    "        py_files = [file for file in files if file['name'].endswith('.py')]\n",
    "\n",
    "        for file in py_files:\n",
    "            file_url = file['download_url']\n",
    "            file_name = file['name']\n",
    "            file_response = requests.get(file_url)\n",
    "\n",
    "            if file_response.status_code == 200:\n",
    "                with open(os.path.join(output_folder, file_name), \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(file_response.text)\n",
    "                    \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def ParamterMapping(Definition=\"\"):\n",
    "    \n",
    "    '''\n",
    "    Function to Google Mapping Sheet, which is used to store Mappings, Links, etc.\n",
    "    For both simplicity and Organization\n",
    "    \n",
    "    Args:\n",
    "        Definition (Str): Key word used to Access individual elements\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe, unless Definition is defined, in which case it might be Str.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSwDznLz-GKgWFT1uN0XZYm3bsos899I9MS-pSvEoDC-Cjqo9CWeEuSdjitxjqzF3O39LmjJB0_Fg-B/pub?output=csv')\n",
    "    \n",
    "    # If user has not included a definition, the return entire DF\n",
    "    if len(Definition)==0:\n",
    "        return df\n",
    "    else:\n",
    "        try:\n",
    "            df1 = df[df['Definition']==Definition]\n",
    "            if len(df1)==1:\n",
    "                if df1['TYPE'].item()=='csv':\n",
    "                    return pd.read_csv(df1['VALUE'].item())\n",
    "                else:\n",
    "                    return df1['VALUE'].item()\n",
    "        except:\n",
    "            return df[df['Definition']==Definition] \n",
    "\n",
    "\n",
    "def BackUpGoogleSheets(location='/Users/derekdewald/Documents/Python/Github_Repo/CSV Backup Files/'):\n",
    "    '''\n",
    "    Function to Create a Backup of Information Stored in Google Sheets.\n",
    "    \n",
    "    Parameters:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        CSV Files \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = ParamterMapping()\n",
    "    \n",
    "    for row in range(len(df)):\n",
    "        try:\n",
    "            file_name = df['Definition'][row]\n",
    "            file_location = df['CSV'][row]\n",
    "            month = datetime.datetime.now().strftime('%b-%y')\n",
    "            \n",
    "            temp_df = pd.read_csv(file_location)\n",
    "            temp_df.to_csv(f'{location}{file_name}_{month}.csv',index=False)\n",
    "            print(f'Back Up Saved, {location}{file_name}')\n",
    "        except:\n",
    "            print(f'Counld Not Print Record {row}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def NavigateUsingDMap():\n",
    "     \n",
    "    '''\n",
    "    Function to Google Mapping Sheet, Navigate to Specific Sites.\n",
    "    Provides Options, Enable Selection based on inputs.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSwDznLz-GKgWFT1uN0XZYm3bsos899I9MS-pSvEoDC-Cjqo9CWeEuSdjitxjqzF3O39LmjJB0_Fg-B/pub?output=csv')\n",
    "    \n",
    "    display(df)\n",
    "    \n",
    "    p = input('Which Process would You like to review?')\n",
    "    v = input('What would you like to return?')\n",
    "    \n",
    "    df1 = df[df['Definition']==p]\n",
    "    \n",
    "    if v.lower() =='link':\n",
    "        webbrowser.open(df1['Link'].item())\n",
    "    elif v.lower() == 'csv':\n",
    "        return pd.read_csv(df1['CSV'].item())\n",
    "    elif v.lower()=='streamlit':\n",
    "        webbrowser.open(df1['Streamlit'].item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
