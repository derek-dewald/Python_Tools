{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7577b67-ecbd-4402-ba4a-c1b2a1776de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addaeb94-d17b-4053-aa49-1cc38017680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed Multithread Processing from Read Directory. Need to have displine and Principles related to \n",
    "# Creation of Functions, Create Individual functions which do components, opposed to long ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486b38d-d580-459a-be58-6ee90436c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "\n",
    "## MONTHLY  ?\n",
    "\n",
    "\n",
    "# Once a Week update Historocal Files\n",
    "if datetime.datetime.now().day==14:\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7809e9d-3de0-4544-a3d3-66d032360591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.date(2026,1,12).weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9549d3c7-6465-46c1-a617-e64f3047d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_definition_csv = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQq1-3cTas8DCWBa2NKYhVFXpl8kLaFDohg0zMfNTAU_Fiw6aIFLWfA5zRem4eSaGPa7UiQvkz05loW/pub?output=csv'\n",
    "df = pd.read_csv(google_definition_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0096a942-e912-45a5-9873-052e34384370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model\n",
       "Linear        4\n",
       "Tree          1\n",
       "Meta-Model    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef430a2-bcb2-4255-a20e-16856d172f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Algorithm Class\n",
       "Classification              4\n",
       "Regression                  2\n",
       "Density-based clustering    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Algorithm Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f701b6c-b5af-42a1-8ec7-963be2453918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learning Type\n",
       "Supervised    6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Learning Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3306c3e3-e4a0-423c-9a4c-c683d43a5841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categorization\n",
       "Algorithm                    201\n",
       "Procedure                     22\n",
       "Operational Definition        18\n",
       "Pipeline Stage                13\n",
       "Definition                    13\n",
       "Optimizer                      7\n",
       "Semantic Type                  6\n",
       "Function                       4\n",
       "Model Type                     4\n",
       "Concept                        4\n",
       "Feature Selection              4\n",
       "Centroid-based clustering      3\n",
       "Regularization                 3\n",
       "General Principles             3\n",
       "Functional Role                3\n",
       "Clustering                     3\n",
       "Theorm                         2\n",
       "Model Architecture             2\n",
       "Density-based clustering       2\n",
       "Theorem                        1\n",
       "Property                       1\n",
       "Evaluation                     1\n",
       "Probabilistic clustering       1\n",
       "Graph-based clustering         1\n",
       "Insance Based Learning         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Categorization'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038dafbb-1907-4ee2-aa52-48fcad615dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    Definition of Function\n",
      "\n",
      "    Parameters:\n",
      "        List of Parameters\n",
      "\n",
      "    Returns:\n",
      "        Object Type\n",
      "\n",
      "    date_created:30-Dec-25\n",
      "    date_last_modified: 30-Dec-25\n",
      "    classification:TBD\n",
      "    sub_classification:TBD\n",
      "    usage:\n",
      "        Example Function Call\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_d_strings import template_doc_string_print\\\n",
    "template_doc_string_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e4883ce-79ab-4b5f-b7fb-6c55de64cc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['utility_functions.py',\n",
       " '.DS_Store',\n",
       " 'dict_processing.py',\n",
       " 'Archive',\n",
       " 'data_d_lists.py',\n",
       " 'shared_folder.py',\n",
       " 'list_processing.py',\n",
       " 'connections.py',\n",
       " '__init__.py',\n",
       " '__pycache__',\n",
       " 'string_processing.py',\n",
       " 'V2',\n",
       " 'input_functions_ignore.py',\n",
       " 'sql_.py',\n",
       " '.ipynb_checkpoints',\n",
       " 'data_d_strings.py',\n",
       " 'data_d_dicts.py',\n",
       " 'df_processing.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shared_folder import read_directory\n",
    "d_py_function =  '/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/'\n",
    "read_directory(d_py_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e33665-a40f-4bdb-a2d0-52dc7bf6a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import a .TXT or .PY File\n",
    "location = '/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/shared_folder.py'\n",
    "\n",
    "from shared_folder import text_file_import\n",
    "file = text_file_import(location)\n",
    "\n",
    "from shared_folder import parse_dot_py_file\n",
    "function_list, function_parameters = parse_dot_py_file(file)\n",
    "\n",
    "## Generate a Summary File for a Folder.\n",
    "\n",
    "from shared_folder import create_py_table_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1730d21-8b04-49a5-999f-3837551b45d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python_function_list Saved to /Users/derekdewald/Documents/Python/Github_Repo/Streamlit/DataDictionary/\n",
      "python_function_parameters Saved to /Users/derekdewald/Documents/Python/Github_Repo/Streamlit/DataDictionary/\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'export_locaiton' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshared_folder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_dot_py_folder\n\u001b[0;32m----> 2\u001b[0m function_list, function_parameters \u001b[38;5;241m=\u001b[39m parse_dot_py_folder()\n",
      "File \u001b[0;32m~/Documents/Python/Github_Repo/d_py_functions/shared_folder.py:320\u001b[0m, in \u001b[0;36mparse_dot_py_folder\u001b[0;34m(location, export_location)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython_function_parameters Saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexport_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    319\u001b[0m     function_list\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexport_location\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mpython_function_list.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 320\u001b[0m     function_parameters\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexport_locaiton\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mpython_function_parameters.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function_list,function_parameters\n",
      "\u001b[0;31mNameError\u001b[0m: name 'export_locaiton' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbe42169-9091-4cd2-8fd0-e4c289eacc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def create_py_table_dict(base_location= '/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/',\n",
      "                         export_location='/Users/derekdewald/Documents/Python/Github_Repo/Streamlit/DataDictionary/folder_listing.csv'):\n",
      "    \n",
      "    '''\n",
      "    Function which Generates a Dataframe representing a Function Dictionary, sourcing the Functions from a Shared Folder Location, and\n",
      "    using the definitions sourced from a Python Dictionary\n",
      "\n",
      "    Parameters:\n",
      "        base_location (str): Location of Windows Directory containing .py Files.\n",
      "\n",
      "    Returns:\n",
      "        DataFrame\n",
      "\n",
      "    date_created:4-Dec-25\n",
      "    date_last_modified:4-Dec-25\n",
      "    classification:TBD\n",
      "    sub_classification:TBD\n",
      "    usage:\n",
      "        python_function_dict_df = create_py_table_dict()\n",
      "    '''\n",
      "    from data_d_dicts import function_table_dictionary\n",
      "\n",
      "    # Get Defined Functions from Dictionary Reference Listing\n",
      "    temp_ = dict_to_dataframe( function_table_dictionary,key_name='Function Name',value_name='Definition')\n",
      "    temp_['Type'] = 'Definition'\n",
      "\n",
      "    py_functions = list_to_dataframe([x for x in read_directory(base_location,file_type='.py') if (x.find('init')==-1)],column_name_list=['File Name'])\n",
      "    py_functions['Source'] = 'PY File'\n",
      "    py_functions['Function Name'] = py_functions['File Name'].apply(lambda x:x.replace('.py',''))\n",
      "\n",
      "    final_df = py_functions.merge(temp_,on='Function Name',how='outer')\n",
      "\n",
      "\n",
      "    if export_location:\n",
      "        print(f'folder_listing Saved to {export_location}')\n",
      "        final_df.to_csv(export_location,index=False)\n",
      "\n",
      "    return final_df\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect_function(create_py_table_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66ac12de-e9cf-4b92-9bff-891976d410ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder_listing Saved to /Users/derekdewald/Documents/Python/Github_Repo/Streamlit/DataDictionary/folder_listing.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Source</th>\n",
       "      <th>Function Name</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>connections.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>connections</td>\n",
       "      <td>Functions Connecting to External Data Sources</td>\n",
       "      <td>Definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daily_processes.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>daily_processes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_d_dicts.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>data_d_dicts</td>\n",
       "      <td>Repository of Dictionaries which have been sav...</td>\n",
       "      <td>Definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_d_lists.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>data_d_lists</td>\n",
       "      <td>Repository of Lists which have been saved for ...</td>\n",
       "      <td>Definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_d_strings.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>data_d_strings</td>\n",
       "      <td>Repository of Strings which have been saved fo...</td>\n",
       "      <td>Definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data_sets.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>data_sets</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>date_functions.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>date_functions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>df_processing.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>df_processing</td>\n",
       "      <td>Functions related to dataframe Transformations...</td>\n",
       "      <td>Definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dict_processing.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>dict_processing</td>\n",
       "      <td>Functions related to Manipulating, Transformin...</td>\n",
       "      <td>Definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>input_functions_ignore.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>input_functions_ignore</td>\n",
       "      <td>Functions which are Created for Single Use, or...</td>\n",
       "      <td>Definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>list_processing.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>list_processing</td>\n",
       "      <td>Functions related to Manipulating, Transformin...</td>\n",
       "      <td>Definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>shared_folder.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>shared_folder</td>\n",
       "      <td>Functions related to Management, Maintenance a...</td>\n",
       "      <td>Definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sql_.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>sql_</td>\n",
       "      <td>Functions related to Processing of SQL</td>\n",
       "      <td>Definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>string_processing.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>string_processing</td>\n",
       "      <td>Functions related to Manipulating, Transformin...</td>\n",
       "      <td>Definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>synthetic_mbr.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>synthetic_mbr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>utility_functions.py</td>\n",
       "      <td>PY File</td>\n",
       "      <td>utility_functions</td>\n",
       "      <td>Functions which are in development, do not nic...</td>\n",
       "      <td>Definition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    File Name   Source           Function Name  \\\n",
       "0              connections.py  PY File             connections   \n",
       "1          daily_processes.py  PY File         daily_processes   \n",
       "2             data_d_dicts.py  PY File            data_d_dicts   \n",
       "3             data_d_lists.py  PY File            data_d_lists   \n",
       "4           data_d_strings.py  PY File          data_d_strings   \n",
       "5                data_sets.py  PY File               data_sets   \n",
       "6           date_functions.py  PY File          date_functions   \n",
       "7            df_processing.py  PY File           df_processing   \n",
       "8          dict_processing.py  PY File         dict_processing   \n",
       "9   input_functions_ignore.py  PY File  input_functions_ignore   \n",
       "10         list_processing.py  PY File         list_processing   \n",
       "11           shared_folder.py  PY File           shared_folder   \n",
       "12                    sql_.py  PY File                    sql_   \n",
       "13       string_processing.py  PY File       string_processing   \n",
       "14           synthetic_mbr.py  PY File           synthetic_mbr   \n",
       "15       utility_functions.py  PY File       utility_functions   \n",
       "\n",
       "                                           Definition        Type  \n",
       "0       Functions Connecting to External Data Sources  Definition  \n",
       "1                                                 NaN         NaN  \n",
       "2   Repository of Dictionaries which have been sav...  Definition  \n",
       "3   Repository of Lists which have been saved for ...  Definition  \n",
       "4   Repository of Strings which have been saved fo...  Definition  \n",
       "5                                                 NaN         NaN  \n",
       "6                                                 NaN         NaN  \n",
       "7   Functions related to dataframe Transformations...  Definition  \n",
       "8   Functions related to Manipulating, Transformin...  Definition  \n",
       "9   Functions which are Created for Single Use, or...  Definition  \n",
       "10  Functions related to Manipulating, Transformin...  Definition  \n",
       "11  Functions related to Management, Maintenance a...  Definition  \n",
       "12             Functions related to Processing of SQL  Definition  \n",
       "13  Functions related to Manipulating, Transformin...  Definition  \n",
       "14                                                NaN         NaN  \n",
       "15  Functions which are in development, do not nic...  Definition  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_py_table_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e0964ed-74c7-4a29-b1fb-f1007cc19bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def create_py_table_dict(base_location= '/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/',\n",
      "                         export_location='/Users/derekdewald/Documents/Python/Github_Repo/Streamlit/DataDictionary/folder_listing.csv'):\n",
      "    \n",
      "    '''\n",
      "    Function which Generates a Dataframe representing a Function Dictionary, sourcing the Functions from a Shared Folder Location, and\n",
      "    using the definitions sourced from a Python Dictionary\n",
      "\n",
      "    Parameters:\n",
      "        base_location (str): Location of Windows Directory containing .py Files.\n",
      "\n",
      "    Returns:\n",
      "        DataFrame\n",
      "\n",
      "    date_created:4-Dec-25\n",
      "    date_last_modified:4-Dec-25\n",
      "    classification:TBD\n",
      "    sub_classification:TBD\n",
      "    usage:\n",
      "        python_function_dict_df = create_py_table_dict()\n",
      "    '''\n",
      "    from data_d_dicts import function_table_dictionary\n",
      "\n",
      "    # Get Defined Functions from Dictionary Reference Listing\n",
      "    temp_ = dict_to_dataframe( function_table_dictionary,key_name='Function Name',value_name='Definition')\n",
      "    temp_['Type'] = 'Definition'\n",
      "\n",
      "    py_functions = list_to_dataframe([x for x in read_directory(base_location,file_type='.py') if (x.find('init')==-1)],column_name_list=['File Name'])\n",
      "    py_functions['Source'] = 'PY File'\n",
      "    py_functions['Function Name'] = py_functions['File Name'].apply(lambda x:x.replace('.py',''))\n",
      "\n",
      "    final_df = py_functions.merge(temp_,on='Function Name',how='outer')\n",
      "\n",
      "\n",
      "    if export_location:\n",
      "        print(f'folder_listing Saved to {export_location}')\n",
      "        final_df.to_csv(export_location,index=False)\n",
      "\n",
      "    return final_df\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utility_functions import inspect_function\n",
    "\n",
    "inspect_function(create_py_table_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7223e4b-00aa-4263-8014-c0d9780ae362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b910ee8-bd2d-4da0-ac9b-6de5dc29535c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "DownloadFilesFromGit()\n",
    "BackUpGoogleSheets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db4215-d821-4838-88af-6c7a729d7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def TranposeNonTimeSeriesDF(df, index, columns=None):\n",
    "    '''\n",
    "    Transposes a non-time-series DataFrame from wide to long format by melting specified columns.\n",
    "\n",
    "    This is especially useful for flattening columns into a single column to support tools \n",
    "    like Power BI, where long format enables dynamic pivoting and aggregation.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input pandas DataFrame.\n",
    "        index (list): Columns to retain as identifiers (will remain unchanged).\n",
    "        columns (list): Columns to unpivot into key-value pairs.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A long-format DataFrame with 'variable' and 'value' columns.\n",
    "    '''\n",
    "    if not columns:\n",
    "        columns = [col for col in final_df1.columns if (isinstance(col, pd.Timestamp))|(isinstance(col, datetime.datetime))]\n",
    "    \n",
    "    return df.melt(id_vars=index, value_vars=columns)\n",
    "\n",
    "def CreatePivotTableFromTimeSeries(df,\n",
    "                                   index,\n",
    "                                   columns,\n",
    "                                   values,\n",
    "                                   aggfunc='sum',\n",
    "                                   skipna=True):\n",
    "    \n",
    "    '''\n",
    "    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 1. Pivot\n",
    "    if index==None:\n",
    "        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)\n",
    "    else:\n",
    "        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)\n",
    "\n",
    "    # 2. Capture original month columns IMMEDIATELY after pivot\n",
    "    month_cols = df1.columns.tolist()\n",
    " \n",
    "    # 3. Add rolling window stats\n",
    "    if len(month_cols) >= 3:\n",
    "        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]\n",
    "        try:\n",
    "            df1['PERC_CHG_3M'] = df1['CHG_3M']/df1[month_cols[-3]]\n",
    "        except:\n",
    "            df1['PERC_CHG_3M'] = 0\n",
    "    \n",
    "    if len(month_cols) >= 6:\n",
    "        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]\n",
    "        try:\n",
    "            df1['PERC_CHG_6M'] = df1['CHG_6M']/df1[month_cols[-6]]\n",
    "        except:\n",
    "            df1['PERC_CHG_6M'] = 0\n",
    "            \n",
    "    if len(month_cols) >= 12:\n",
    "        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]\n",
    "        try:\n",
    "            df1['PERC_CHG_12M'] = df1['CHG_12M']/df1[month_cols[-12]]\n",
    "        except:\n",
    "            df1['PERC_CHG_12M'] = 0\n",
    "\n",
    "    df1['CHG_DF']  = df1[month_cols[-1]]-df1[month_cols[0]]\n",
    "    df1['AVG_DF'] = df1[month_cols[-1:]].mean(axis=1, skipna=skipna)\n",
    "    df1['PERC_CHG_DF'] = df1['AVG_DF']/df1[month_cols[-1]]\n",
    "\n",
    "    \n",
    "    # 4. Now calculate global stats **only using the original month columns**\n",
    "    stats = pd.DataFrame({\n",
    "        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),\n",
    "        'STD': df1[month_cols].std(axis=1, skipna=skipna),\n",
    "        'MAX': df1[month_cols].max(axis=1, skipna=skipna),\n",
    "        'MIN': df1[month_cols].min(axis=1, skipna=skipna),\n",
    "        'COUNT': df1[month_cols].count(axis=1)\n",
    "    })\n",
    "\n",
    "    # 5. Merge the stats\n",
    "    df1 = pd.concat([df1, stats], axis=1)\n",
    "    \n",
    "    return df1.fillna(0)\n",
    "\n",
    "\n",
    "def CreateMultiplePivotTableFromTimeSeries(df,\n",
    "                                           index_list,\n",
    "                                           metric_list,\n",
    "                                           column):\n",
    "    '''\n",
    "    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through all Possible Metrics Selected.\n",
    "    \n",
    "    for metric in metric_list:\n",
    "        print(f'Attempting to Process:{metric}')\n",
    "        try:\n",
    "            all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') \n",
    "            cols = list(all_df.columns)\n",
    "            all_df = all_df.reset_index(drop=True)\n",
    "            all_df['METRIC'] = metric\n",
    "            cols.insert(0,'METRIC')\n",
    "\n",
    "            for key in index_list:\n",
    "                cols.insert(0,key)\n",
    "                all_df[key] = 'All'\n",
    "\n",
    "            final_df = pd.concat([final_df,all_df[cols]])\n",
    "            # Iterate through all Index Items Individually\n",
    "            for key in index_list:\n",
    "                temp = CreatePivotTableFromTimeSeries(df,\n",
    "                                                      index=key,\n",
    "                                                      values=metric,\n",
    "                                                      columns=column).reset_index() \n",
    "                for missing in [x for x in index_list if x != key]:\n",
    "                    temp[missing] = 'All'\n",
    "                temp['METRIC'] = metric\n",
    "                final_df = pd.concat([final_df,temp])\n",
    "\n",
    "            # Add Value for Metric with Entire Index Combination\n",
    "            temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()\n",
    "            temp['METRIC'] = metric\n",
    "            final_df = pd.concat([final_df,temp])\n",
    "        except:\n",
    "            print(f'Could Not Process Metric:{metric}.')\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):\n",
    "    '''\n",
    "    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through all Possible Metrics Selected.\n",
    "    for metric in metric_list:\n",
    "        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') \n",
    "        cols = list(all_df.columns)\n",
    "        all_df = all_df.reset_index(drop=True)\n",
    "        all_df['METRIC'] = metric\n",
    "        cols.insert(0,'METRIC')\n",
    "\n",
    "        for key in index:\n",
    "            cols.insert(0,key)\n",
    "            all_df[key] = 'All'\n",
    "\n",
    "        final_df = pd.concat([final_df,all_df[cols]])\n",
    "\n",
    "        # Iterate through all Index Items Individually\n",
    "        for key in index_list:\n",
    "            temp = CreatePivotTableFromTimeSeries(df,index=key,\n",
    "                                                  values=metric,\n",
    "                                                  columns=column).reset_index() \n",
    "            for missing in [x for x in index if x != key]:\n",
    "                temp[missing] = 'All'\n",
    "            temp['METRIC'] = metric\n",
    "            final_df = pd.concat([final_df,temp])\n",
    "        \n",
    "        # Add Value for Metric with Entire Index Combination\n",
    "        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()\n",
    "        temp['METRIC'] = metric\n",
    "        final_df = pd.concat([final_df,temp])\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def SummarizeTimeSeriesDf(df,\n",
    "                          summary_cols,\n",
    "                          primary_key_list):\n",
    "    '''\n",
    "    Function to Summarize a Time Series dataframe based on a finite number of identified Columns.\n",
    "    \n",
    "    Parameters\n",
    "        df (Dataframe): TimeSeries in Nature\n",
    "        summary_cols (List): List of Columns which are to be included in SUmmary\n",
    "        primary_key_list (list): Primary Key of Dataframe\n",
    "    \n",
    "    Returns\n",
    "        temp_df1: Raw Data of SUmmary Cols with a Count of Observations. If include Month Variable Easy to add to Pivot Table\n",
    "        summary: Summary (Excluding Primary Key). including Total Observations, MEan, Max, Min.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    temp_df = df[summary_cols].copy()\n",
    "    temp_df['COUNT'] = 1\n",
    "    \n",
    "    # Unique Occurances by Pivot Criteria. Important to Include Month\n",
    "    temp_df1 = temp_df.groupby(summary_cols).sum().reset_index().rename(columns={'COUNT':'TOTAL_DAYS'})\n",
    "    \n",
    "    pivot_columns1 = [x for x in summary_cols if x not in primary_key_list]\n",
    "    \n",
    "    summary = temp_df1.groupby(pivot_columns1).agg(\n",
    "        TOTAL=('TOTAL_DAYS', 'count'),\n",
    "        AVG_DAYS_OPEN=('TOTAL_DAYS', 'mean'),\n",
    "        MAX_OBS=('TOTAL_DAYS', 'min'),\n",
    "        MIN_OBS=('TOTAL_DAYS', 'max')).reset_index()\n",
    "    \n",
    "    return temp_df1,summary\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def SampleDataFrame(df, \n",
    "                    conf=.95, \n",
    "                    me=0.05,\n",
    "                    mv=0.5,\n",
    "                    print_=0,\n",
    "                    new_column_name=\"\"):\n",
    "    \"\"\"\n",
    "    Returns a random sample from a DataFrame based on confidence level and margin of error.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataset to sample from.\n",
    "        conf(float): Desired Confidence Percentage Level (e.g., 90, 95, 99).\n",
    "        me (float): Margin of Error, (default is 5%).\n",
    "        mv (float): Maximum Variability (Expected Level of Default)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A random sample of the required size.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if not 0 <= mv <= 1:\n",
    "        raise ValueError(\"mv (failure rate) must be between 0 and 1.\")\n",
    "\n",
    "    N = len(df)\n",
    "    if N == 0:\n",
    "        raise ValueError(\"DataFrame is empty\")\n",
    "\n",
    "    # Calculate the Z-score based on the confidence level\n",
    "    z = norm.ppf(1 - (1 - conf) / 2)\n",
    "    \n",
    "\n",
    "    # Calculate the initial sample size (without finite population correction)\n",
    "    n0 = (z**2 * mv * (1 - mv)) / (me**2)\n",
    "    \n",
    "    # Apply finite population correction if the population is smaller than 100,000\n",
    "    if N >= 10000:  # For large populations, skip the correction\n",
    "        n = int(n0)\n",
    "    else:\n",
    "        n = int((n0 * N) / (n0 + N - 1))\n",
    "\n",
    "    if print_==1:\n",
    "        print(f\"Z-score: {z}\")  # Debug Z-score\n",
    "        print(f\"Initial sample size (n0): {n0}\")  # Debug n0\n",
    "        print(f\"Sample size with FPC: {n}\")  # Debug final sample size\n",
    "    \n",
    "    sample = df.sample(n=n, random_state=42)\n",
    "    \n",
    "    if len(new_column_name)==0:\n",
    "        return sample \n",
    "\n",
    "    else:\n",
    "        sample_index = sample.index\n",
    "        df[new_column_name] = 0\n",
    "        df.loc[sample_index, new_column_name] = 1\n",
    "        return df\n",
    "\n",
    "def ReviewEntireDataframe(df,file_name=None):\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for column in df.columns:\n",
    "        start_time = timeit.default_timer()\n",
    "        temp_df = ColumnStatisticalReview(df,column)\n",
    "        print(f'Elapsed time to process {column}:{timeit.default_timer() - start_time:,.2f}')\n",
    "        final_df = pd.concat([final_df,temp_df],axis=1)\n",
    "    if file_name:\n",
    "        final_df.to_csv(f\"{file_name}.csv\")\n",
    "        \n",
    "    return final_df\n",
    "\n",
    "def ColumnStatisticalReview(df,\n",
    "                            column_name,\n",
    "                            partitions=10,\n",
    "                            top_x_records=10,\n",
    "                            exclude_blanks_from_segments=1,\n",
    "                            exclude_zeroes_from_segments=1):\n",
    "\n",
    "    '''\n",
    "    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution\n",
    "    of values. \n",
    "\n",
    "    Args:\n",
    "        column_name (str): Name of Column\n",
    "\n",
    "        partitions (int): Number of partitions to include (Decile 10)\n",
    "\n",
    "        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.\n",
    "        If blank values are excluded it gives a better representation for the members of the set, however it might \n",
    "        provide a misleading representation of the population.\n",
    "\n",
    "        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as\n",
    "        such it can include both blanks and true 0 values. \n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    temp_dict = {}\n",
    "    \n",
    "    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])\n",
    "    \n",
    "    if is_numeric:\n",
    "        temp_dict['SUM'] = df[column_name].sum()\n",
    "        temp_dict['MEAN'] = df[column_name].mean()\n",
    "        temp_dict['STD_DEV'] =  df[column_name].std()\n",
    "        temp_dict['MEDIAN'] = df[column_name].median()\n",
    "        temp_dict['MAX'] = df[column_name].max()\n",
    "        temp_dict['MIN'] = df[column_name].min()\n",
    "        \n",
    "    temp_dict['TOTAL_RECORDS'] = len(df)\n",
    "    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))\n",
    "    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])\n",
    "    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])\n",
    "    \n",
    "    if is_numeric:\n",
    "        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])\n",
    "        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    \n",
    "\n",
    "    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])\n",
    "    \n",
    "    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):\n",
    "        return temp_df\n",
    "\n",
    "    # Add top X records Based on Frequency\n",
    "    if top_x_records>0:\n",
    "        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index().rename(columns={column_name:'count','index':column_name})\n",
    "        if len(top_instances)>0:\n",
    "            top_instances[column_name] = top_instances.apply(lambda row: f\"Value: {row[column_name]}, Frequency: {row['count']}\", axis=1)\n",
    "            top_instances['index'] = [f\"Top {x+1}\" for x in range(len(top_instances[column_name]))]\n",
    "            top_instances = top_instances.drop('count',axis=1).set_index('index')\n",
    "            temp_df = pd.concat([temp_df,top_instances])\n",
    "        \n",
    "    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):\n",
    "        segment_df = ColumnPartitioner(df=df,\n",
    "                                       column_name=column_name,\n",
    "                                       partitions=partitions,\n",
    "                                       exclude_blanks=exclude_blanks_from_segments,\n",
    "                                       exclude_zeros=exclude_zeroes_from_segments,\n",
    "                                       return_value='')\n",
    "        \n",
    "        seg_val_df = ColumnPartitioner(df=df,\n",
    "                                           column_name=column_name,\n",
    "                                           partitions=partitions,\n",
    "                                           exclude_blanks=exclude_blanks_from_segments,\n",
    "                                           exclude_zeros=exclude_zeroes_from_segments,\n",
    "                                           return_value='agg_value').rename(columns={'VALUE':column_name})\n",
    "\n",
    "        return pd.concat([temp_df,segment_df.T,seg_val_df])\n",
    "    return temp_df\n",
    "\n",
    "def CompareFunction(func1,func2,additional_records=20):\n",
    "    \n",
    "    '''\n",
    "    Function which Compares 2 Functions and determines if they are different. Specifically, it can help to easily\n",
    "    Manage Version control of Functions outside of a More robust environment such as GIT.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    list1 = FunctionToSTR(func1)\n",
    "    list2 = FunctionToSTR(func2)\n",
    "    \n",
    "    length = max(len(list1),len(list2))\n",
    "    \n",
    "    for record in range(0,length):\n",
    "        if list1[record]==list2[record]:\n",
    "            if record == (length-1):\n",
    "                print(\"All Records Reconcile\")\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                print(list1[record:record+additional_records])\n",
    "                print(list2[record:record+additional_records])\n",
    "            except:\n",
    "                print(list1[record:record:])\n",
    "                print(list2[record:record:])\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f03f7-6c88-43bd-85d0-05dfb969eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "def DownloadFilesFromGit(user='derek-dewald',\n",
    "                        repo='Python_Tools',\n",
    "                        folder='d_py_functions',\n",
    "                        output_folder=\"\"):\n",
    "    '''\n",
    "    Function to Download Files from Github to a dedicated folder. Specifically used when i DO NOT want to formally link to Github.\n",
    "    \n",
    "    Parameters:\n",
    "        User:\n",
    "        Repo:\n",
    "        folder:\n",
    "        output_folder:\n",
    "        \n",
    "    Returns:\n",
    "        Saves files to Output Folder.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    if len(output_folder) == 0:\n",
    "        output_folder = os.getcwd()\n",
    "    \n",
    "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents/{folder}\"\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        files = response.json()\n",
    "        py_files = [file for file in files if file['name'].endswith('.py')]\n",
    "\n",
    "        for file in py_files:\n",
    "            file_url = file['download_url']\n",
    "            file_name = file['name']\n",
    "            file_response = requests.get(file_url)\n",
    "\n",
    "            if file_response.status_code == 200:\n",
    "                with open(os.path.join(output_folder, file_name), \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(file_response.text)\n",
    "                    \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def ParamterMapping(Definition=\"\"):\n",
    "    \n",
    "    '''\n",
    "    Function to Google Mapping Sheet, which is used to store Mappings, Links, etc.\n",
    "    For both simplicity and Organization\n",
    "    \n",
    "    Args:\n",
    "        Definition (Str): Key word used to Access individual elements\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe, unless Definition is defined, in which case it might be Str.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSwDznLz-GKgWFT1uN0XZYm3bsos899I9MS-pSvEoDC-Cjqo9CWeEuSdjitxjqzF3O39LmjJB0_Fg-B/pub?output=csv')\n",
    "    \n",
    "    # If user has not included a definition, the return entire DF\n",
    "    if len(Definition)==0:\n",
    "        return df\n",
    "    else:\n",
    "        try:\n",
    "            df1 = df[df['Definition']==Definition]\n",
    "            if len(df1)==1:\n",
    "                if df1['TYPE'].item()=='csv':\n",
    "                    return pd.read_csv(df1['VALUE'].item())\n",
    "                else:\n",
    "                    return df1['VALUE'].item()\n",
    "        except:\n",
    "            return df[df['Definition']==Definition] \n",
    "\n",
    "\n",
    "def BackUpGoogleSheets(location='/Users/derekdewald/Documents/Python/Github_Repo/CSV Backup Files/'):\n",
    "    '''\n",
    "    Function to Create a Backup of Information Stored in Google Sheets.\n",
    "    \n",
    "    Parameters:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        CSV Files \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = ParamterMapping()\n",
    "    \n",
    "    for row in range(len(df)):\n",
    "        try:\n",
    "            file_name = df['Definition'][row]\n",
    "            file_location = df['CSV'][row]\n",
    "            month = datetime.datetime.now().strftime('%b-%y')\n",
    "            \n",
    "            temp_df = pd.read_csv(file_location)\n",
    "            temp_df.to_csv(f'{location}{file_name}_{month}.csv',index=False)\n",
    "            print(f'Back Up Saved, {location}{file_name}')\n",
    "        except:\n",
    "            print(f'Counld Not Print Record {row}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def NavigateUsingDMap():\n",
    "     \n",
    "    '''\n",
    "    Function to Google Mapping Sheet, Navigate to Specific Sites.\n",
    "    Provides Options, Enable Selection based on inputs.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSwDznLz-GKgWFT1uN0XZYm3bsos899I9MS-pSvEoDC-Cjqo9CWeEuSdjitxjqzF3O39LmjJB0_Fg-B/pub?output=csv')\n",
    "    \n",
    "    display(df)\n",
    "    \n",
    "    p = input('Which Process would You like to review?')\n",
    "    v = input('What would you like to return?')\n",
    "    \n",
    "    df1 = df[df['Definition']==p]\n",
    "    \n",
    "    if v.lower() =='link':\n",
    "        webbrowser.open(df1['Link'].item())\n",
    "    elif v.lower() == 'csv':\n",
    "        return pd.read_csv(df1['CSV'].item())\n",
    "    elif v.lower()=='streamlit':\n",
    "        webbrowser.open(df1['Streamlit'].item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
