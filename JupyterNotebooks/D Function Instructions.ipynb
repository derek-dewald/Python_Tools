{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d035c3b-7fc8-4446-93c5-0acc681fef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segments = pd.DataFrame({\n",
    "            \"SEGMENT\": segments,\n",
    "            column_name: [f\"Decile {x}\" for x in range(len(segments))]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4a4921-7506-4459-bca2-c2b0dc8f59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1aba48-f2d6-4e87-8ce2-8882b58ba785",
   "metadata": {},
   "source": [
    "## Need a process for Looking at a .py File and then ensuring I have a reference usage here so I can remember.\n",
    "## Need to also Classify the models based on usage and Dates so I can see changes.\n",
    "## Need to determine a plan to migrate to a single usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adffe88-6f38-47f5-9a94-c3c62ddd8345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_d_strings import gpt_question\n",
    "gpt_question('HuberRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44133440-786b-43c1-ab69-6f9f8fc66fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c53f0e9f-2d2c-431d-8e7c-7410aceb5ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For the definition HuberRegressor can you please provide me a 4-5 sentences highlighting the definition, its origin, its importance and its application.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feef9a0-35f3-4528-ab32-e28b3035d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION TO STR\n",
    "# Column Staistical Review Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12762840-14c1-40c4-9318-2912e488251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    Definition of Function\n",
      "\n",
      "    Parameters:\n",
      "        List of Parameters\n",
      "\n",
      "    Returns:\n",
      "        Object Type\n",
      "\n",
      "    date_created:14-Jan-26\n",
      "    date_last_modified: 14-Jan-26\n",
      "    classification:TBD\n",
      "    sub_classification:TBD\n",
      "    usage:\n",
      "        Example Function Call\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_d_strings import template_doc_string_print\n",
    "\n",
    "template_doc_string_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d1bc8-62ab-499d-9a79-ed77a779639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PartitionDF(df, index_, func,include_all=True,*args,**kwargs):\n",
    "    '''\n",
    "    Function to automate the task of Applying a Specific Function on a dataframe which has been Segregatted based on \n",
    "    a combination of Columns as defined in the argument index_\n",
    "    \n",
    "    An example, where the function was originally designed: PartitionDF(temp_df1,['ENTITY','MEMBER_DURATION'],DFStatisticalReview) \n",
    "    \n",
    "    Noting that to generate 32 distinct subsets on 50K Records and 124 Columns 6.25M records is 60 Seconds.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): \n",
    "        index_ (list): List of Columns which will be included in Group By \n",
    "        func (Function): Function which will be applied to the function\n",
    "        include_all (Bol): Option to include a Aggregated Calculation in Dataset, represented of calculation for All\n",
    "        *args: Arguements for inclusion in Function\n",
    "        **kwargs: Keyword Arguments for inclusion in Function (if required)\n",
    "        \n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with Unique DataFrame and Indicataors for the record place and [FUNC] output.\n",
    "        \n",
    "    Date Created: August 20, 2025\n",
    "    Date Last Modified: August 22, 2025. Added ValueError to Stop when DF missing Index Values, which could not be processed\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Can not Partion Null Values. Dangerous to Assume. Return Error, force User to Solve Data Problems\n",
    "    \n",
    "    for record in index_:\n",
    "        if len(df[df[record].isnull()])>0:\n",
    "            raise ValueError(f'Null Value found in Index {record}, please review data. Fill or Remove Value')\n",
    "    \n",
    "    partitioned_list = []\n",
    "    \n",
    "    # Full dataset (All for all index columns)\n",
    "    \n",
    "    if include_all:\n",
    "    \n",
    "        temp_ = {value: 'All' for value in index_}\n",
    "        temp_['DF'] = df\n",
    "        temp_func = func(df).reset_index().rename(columns={'index': \"COLUMN_NAME\"})\n",
    "        for column in index_:\n",
    "            temp_func[column] = 'All'\n",
    "        temp_['FUNC'] = temp_func\n",
    "        partitioned_list.append(temp_)\n",
    "    else:\n",
    "        temp_ = dict()\n",
    "\n",
    "    # Generate all partial combinations of index columns\n",
    "    for i in range(1, len(index_)):\n",
    "        for combo in itertools.combinations(index_, i):\n",
    "            gb = df.groupby(list(combo))\n",
    "            for selection in gb.groups.keys():\n",
    "                temp_df = gb.get_group(selection).copy()\n",
    "                if include_all:\n",
    "                    temp_ = {col: 'All' for col in index_}\n",
    "                if isinstance(selection, tuple):\n",
    "                    for col, val in zip(combo, selection):\n",
    "                        temp_[col] = val\n",
    "                else:\n",
    "                    temp_[combo[0]] = selection\n",
    "\n",
    "                temp_['DF'] = temp_df\n",
    "                temp_func = func(temp_df).reset_index().rename(columns={'index': \"COLUMN_NAME\"})\n",
    "                for col in index_:\n",
    "                    temp_func[col] = temp_[col]\n",
    "                temp_['FUNC'] = temp_func\n",
    "                partitioned_list.append(temp_)\n",
    "    \n",
    "    # Fully segmented combinations\n",
    "    gb = df.groupby(index_)\n",
    "    for selection in gb.groups.keys():\n",
    "        temp_df = gb.get_group(selection).copy()\n",
    "        temp_ = dict(zip(index_, selection))\n",
    "        temp_['DF'] = temp_df\n",
    "        temp_func = fun                  \n",
    "\n",
    "def SummarizedDataSetforBITool(df, dimensions, metrics,apply_tranpose=False):\n",
    "    \"\"\"\n",
    "    Builds a DataFrame with all combinations of ALL-level rollups \n",
    "    across the specified dimensions and metrics.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        dimensions (list of str): Dimension column names.\n",
    "        metrics (list of str): Metric column names to aggregate.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated DataFrame with 'ALL' rollups.\n",
    "        \n",
    "        \n",
    "    Date Created:\n",
    "    Date Last Maintained: 26-Sep-25\n",
    "    Updated Apply Transpose, to include Option to Transpose DF.\n",
    "        \n",
    "    \"\"\"\n",
    "    result_frames = []\n",
    "    \n",
    "    available_metrics = [x for x in metrics if x in df.columns]\n",
    "\n",
    "    for r in range(len(dimensions) + 1):\n",
    "        for dims in itertools.combinations(dimensions, r):\n",
    "            group_cols = list(dims)\n",
    "            \n",
    "            # Aggregate metrics with or without groupby\n",
    "            if group_cols:\n",
    "                agg_df = df.groupby(group_cols, dropna=False)[available_metrics].sum().reset_index()\n",
    "            else:\n",
    "                # Grand total (ALL for all dims)\n",
    "                sums = df[available_metrics].sum().to_frame().T\n",
    "                agg_df = sums\n",
    "                for col in dimensions:\n",
    "                    agg_df[col] = 'ALL'\n",
    "\n",
    "            # Fill missing dimension columns with 'ALL'\n",
    "            for col in dimensions:\n",
    "                if col not in group_cols:\n",
    "                    agg_df[col] = 'ALL'\n",
    "\n",
    "            # Ensure consistent column order\n",
    "            agg_df = agg_df[dimensions + available_metrics]\n",
    "            result_frames.append(agg_df)\n",
    "\n",
    "    final_df = pd.concat(result_frames, ignore_index=True)\n",
    "    \n",
    "    if apply_tranpose:\n",
    "        return TranposeDF(final_df,dimensions).rename(columns={'variable':'METRIC','value':\"VALUE\"})\n",
    "    else:\n",
    "        return final_df\n",
    "\n",
    "\n",
    "def DFColumnManualSortOrder(df,column_name,column_order):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df[column_name] = pd.Categorical(df[column_name], categories=column_order, ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7577b67-ecbd-4402-ba4a-c1b2a1776de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addaeb94-d17b-4053-aa49-1cc38017680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed Multithread Processing from Read Directory. Need to have displine and Principles related to \n",
    "# Creation of Functions, Create Individual functions which do components, opposed to long ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9549d3c7-6465-46c1-a617-e64f3047d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_definition_csv = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQq1-3cTas8DCWBa2NKYhVFXpl8kLaFDohg0zMfNTAU_Fiw6aIFLWfA5zRem4eSaGPa7UiQvkz05loW/pub?output=csv'\n",
    "df = pd.read_csv(google_definition_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0096a942-e912-45a5-9873-052e34384370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model\n",
       "Linear                      4\n",
       "Denisty Based Clustering    2\n",
       "Graph-based clustering      2\n",
       "Instance Based Learning     1\n",
       "Tree                        1\n",
       "Meta-Model                  1\n",
       "Probabilistic clustering    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef430a2-bcb2-4255-a20e-16856d172f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Algorithm Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f701b6c-b5af-42a1-8ec7-963be2453918",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Learning Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306c3e3-e4a0-423c-9a4c-c683d43a5841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Categorization'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b784709-02d7-4e95-b34f-1e03e935bf56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db4215-d821-4838-88af-6c7a729d7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ReviewEntireDataframe(df,file_name=None):\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for column in df.columns:\n",
    "        start_time = timeit.default_timer()\n",
    "        temp_df = ColumnStatisticalReview(df,column)\n",
    "        print(f'Elapsed time to process {column}:{timeit.default_timer() - start_time:,.2f}')\n",
    "        final_df = pd.concat([final_df,temp_df],axis=1)\n",
    "    if file_name:\n",
    "        final_df.to_csv(f\"{file_name}.csv\")\n",
    "        \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac57f4df-81b3-484a-8907-24aae0809fb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FunctionToSTR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(list2[record:record:])\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m CompareFunction(test,TranposeNonTimeSeriesDF)\n",
      "Cell \u001b[0;32mIn[19], line 10\u001b[0m, in \u001b[0;36mCompareFunction\u001b[0;34m(func1, func2, additional_records)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mCompareFunction\u001b[39m(func1,func2,additional_records\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Function which Compares 2 Functions and determines if they are different. Specifically, it can help to easily\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Manage Version control of Functions outside of a More robust environment such as GIT.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     list1 \u001b[38;5;241m=\u001b[39m FunctionToSTR(func1)\n\u001b[1;32m     11\u001b[0m     list2 \u001b[38;5;241m=\u001b[39m FunctionToSTR(func2)\n\u001b[1;32m     13\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(list1),\u001b[38;5;28mlen\u001b[39m(list2))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FunctionToSTR' is not defined"
     ]
    }
   ],
   "source": [
    "def CompareFunction(func1,func2,additional_records=20):\n",
    "    \n",
    "    '''\n",
    "    Function which Compares 2 Functions and determines if they are different. Specifically, it can help to easily\n",
    "    Manage Version control of Functions outside of a More robust environment such as GIT.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    list1 = FunctionToSTR(func1)\n",
    "    list2 = FunctionToSTR(func2)\n",
    "    \n",
    "    length = max(len(list1),len(list2))\n",
    "    \n",
    "    for record in range(0,length):\n",
    "        if list1[record]==list2[record]:\n",
    "            if record == (length-1):\n",
    "                print(\"All Records Reconcile\")\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                print(list1[record:record+additional_records])\n",
    "                print(list2[record:record+additional_records])\n",
    "            except:\n",
    "                print(list1[record:record:])\n",
    "                print(list2[record:record:])\n",
    "            break\n",
    "\n",
    "\n",
    "CompareFunction(test,TranposeNonTimeSeriesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df274efd-2087-483b-8b54-a99aaac1f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(df, index, columns=None):\n",
    "    '''\n",
    "    Transposes a non-time-series DataFrame from wide to long format by melting specified columns.\n",
    "\n",
    "    This is especially useful for flattening columns into a single column to support tools \n",
    "    like Power BI, where long format enables dynamic pivoting and aggregation.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input pandas DataFrame.\n",
    "        index (list): Columns to retain as identifiers (will remain unchanged).\n",
    "        columns (list): Columns to unpivot into key-value pairs.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A long-format DataFrame with 'variable' and 'value' columns.\n",
    "    '''\n",
    "    if not columns:\n",
    "        columns = [col for col in final_df1.columns if (isinstance(col, pd.Timestamp))|(isinstance(col, datetime.datetime))]\n",
    "    \n",
    "    return df.melt(id_vars=index, value_vars=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a179850b-7e12-4041-a00c-100d76bbb658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TranposeNonTimeSeriesDF(df, index, columns=None):\n",
    "    '''\n",
    "    Transposes a non-time-series DataFrame from wide to long format by melting specified columns.\n",
    "\n",
    "    This is especially useful for flattening columns into a single column to support tools \n",
    "    like Power BI, where long format enables dynamic pivoting and aggregation.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input pandas DataFrame.\n",
    "        index (list): Columns to retain as identifiers (will remain unchanged).\n",
    "        columns (list): Columns to unpivot into key-value pairs.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A long-format DataFrame with 'variable' and 'value' columns.\n",
    "    '''\n",
    "    if not columns:\n",
    "        columns = [col for col in final_df1.columns if (isinstance(col, pd.Timestamp))|(isinstance(col, datetime.datetime))]\n",
    "    \n",
    "    return df.melt(id_vars=index, value_vars=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f03f7-6c88-43bd-85d0-05dfb969eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f52914a-b2df-4ae2-8f53-6850909f322a",
   "metadata": {},
   "source": [
    "## TIME SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145806b3-7a45-4272-aa8f-f589f08e9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def TranposeNonTimeSeriesDF(df, index, columns=None):\n",
    "    '''\n",
    "    Transposes a non-time-series DataFrame from wide to long format by melting specified columns.\n",
    "\n",
    "    This is especially useful for flattening columns into a single column to support tools \n",
    "    like Power BI, where long format enables dynamic pivoting and aggregation.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input pandas DataFrame.\n",
    "        index (list): Columns to retain as identifiers (will remain unchanged).\n",
    "        columns (list): Columns to unpivot into key-value pairs.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A long-format DataFrame with 'variable' and 'value' columns.\n",
    "    '''\n",
    "    if not columns:\n",
    "        columns = [col for col in final_df1.columns if (isinstance(col, pd.Timestamp))|(isinstance(col, datetime.datetime))]\n",
    "    \n",
    "    return df.melt(id_vars=index, value_vars=columns)\n",
    "\n",
    "def CreatePivotTableFromTimeSeries(df,\n",
    "                                   index,\n",
    "                                   columns,\n",
    "                                   values,\n",
    "                                   aggfunc='sum',\n",
    "                                   skipna=True):\n",
    "    \n",
    "    '''\n",
    "    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 1. Pivot\n",
    "    if index==None:\n",
    "        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)\n",
    "    else:\n",
    "        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)\n",
    "\n",
    "    # 2. Capture original month columns IMMEDIATELY after pivot\n",
    "    month_cols = df1.columns.tolist()\n",
    " \n",
    "    # 3. Add rolling window stats\n",
    "    if len(month_cols) >= 3:\n",
    "        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]\n",
    "        try:\n",
    "            df1['PERC_CHG_3M'] = df1['CHG_3M']/df1[month_cols[-3]]\n",
    "        except:\n",
    "            df1['PERC_CHG_3M'] = 0\n",
    "    \n",
    "    if len(month_cols) >= 6:\n",
    "        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]\n",
    "        try:\n",
    "            df1['PERC_CHG_6M'] = df1['CHG_6M']/df1[month_cols[-6]]\n",
    "        except:\n",
    "            df1['PERC_CHG_6M'] = 0\n",
    "            \n",
    "    if len(month_cols) >= 12:\n",
    "        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]\n",
    "        try:\n",
    "            df1['PERC_CHG_12M'] = df1['CHG_12M']/df1[month_cols[-12]]\n",
    "        except:\n",
    "            df1['PERC_CHG_12M'] = 0\n",
    "\n",
    "    df1['CHG_DF']  = df1[month_cols[-1]]-df1[month_cols[0]]\n",
    "    df1['AVG_DF'] = df1[month_cols[-1:]].mean(axis=1, skipna=skipna)\n",
    "    df1['PERC_CHG_DF'] = df1['AVG_DF']/df1[month_cols[-1]]\n",
    "\n",
    "    \n",
    "    # 4. Now calculate global stats **only using the original month columns**\n",
    "    stats = pd.DataFrame({\n",
    "        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),\n",
    "        'STD': df1[month_cols].std(axis=1, skipna=skipna),\n",
    "        'MAX': df1[month_cols].max(axis=1, skipna=skipna),\n",
    "        'MIN': df1[month_cols].min(axis=1, skipna=skipna),\n",
    "        'COUNT': df1[month_cols].count(axis=1)\n",
    "    })\n",
    "\n",
    "    # 5. Merge the stats\n",
    "    df1 = pd.concat([df1, stats], axis=1)\n",
    "    \n",
    "    return df1.fillna(0)\n",
    "\n",
    "\n",
    "def CreateMultiplePivotTableFromTimeSeries(df,\n",
    "                                           index_list,\n",
    "                                           metric_list,\n",
    "                                           column):\n",
    "    '''\n",
    "    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through all Possible Metrics Selected.\n",
    "    \n",
    "    for metric in metric_list:\n",
    "        print(f'Attempting to Process:{metric}')\n",
    "        try:\n",
    "            all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') \n",
    "            cols = list(all_df.columns)\n",
    "            all_df = all_df.reset_index(drop=True)\n",
    "            all_df['METRIC'] = metric\n",
    "            cols.insert(0,'METRIC')\n",
    "\n",
    "            for key in index_list:\n",
    "                cols.insert(0,key)\n",
    "                all_df[key] = 'All'\n",
    "\n",
    "            final_df = pd.concat([final_df,all_df[cols]])\n",
    "            # Iterate through all Index Items Individually\n",
    "            for key in index_list:\n",
    "                temp = CreatePivotTableFromTimeSeries(df,\n",
    "                                                      index=key,\n",
    "                                                      values=metric,\n",
    "                                                      columns=column).reset_index() \n",
    "                for missing in [x for x in index_list if x != key]:\n",
    "                    temp[missing] = 'All'\n",
    "                temp['METRIC'] = metric\n",
    "                final_df = pd.concat([final_df,temp])\n",
    "\n",
    "            # Add Value for Metric with Entire Index Combination\n",
    "            temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()\n",
    "            temp['METRIC'] = metric\n",
    "            final_df = pd.concat([final_df,temp])\n",
    "        except:\n",
    "            print(f'Could Not Process Metric:{metric}.')\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):\n",
    "    '''\n",
    "    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through all Possible Metrics Selected.\n",
    "    for metric in metric_list:\n",
    "        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') \n",
    "        cols = list(all_df.columns)\n",
    "        all_df = all_df.reset_index(drop=True)\n",
    "        all_df['METRIC'] = metric\n",
    "        cols.insert(0,'METRIC')\n",
    "\n",
    "        for key in index:\n",
    "            cols.insert(0,key)\n",
    "            all_df[key] = 'All'\n",
    "\n",
    "        final_df = pd.concat([final_df,all_df[cols]])\n",
    "\n",
    "        # Iterate through all Index Items Individually\n",
    "        for key in index_list:\n",
    "            temp = CreatePivotTableFromTimeSeries(df,index=key,\n",
    "                                                  values=metric,\n",
    "                                                  columns=column).reset_index() \n",
    "            for missing in [x for x in index if x != key]:\n",
    "                temp[missing] = 'All'\n",
    "            temp['METRIC'] = metric\n",
    "            final_df = pd.concat([final_df,temp])\n",
    "        \n",
    "        # Add Value for Metric with Entire Index Combination\n",
    "        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()\n",
    "        temp['METRIC'] = metric\n",
    "        final_df = pd.concat([final_df,temp])\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def SummarizeTimeSeriesDf(df,\n",
    "                          summary_cols,\n",
    "                          primary_key_list):\n",
    "    '''\n",
    "    Function to Summarize a Time Series dataframe based on a finite number of identified Columns.\n",
    "    \n",
    "    Parameters\n",
    "        df (Dataframe): TimeSeries in Nature\n",
    "        summary_cols (List): List of Columns which are to be included in SUmmary\n",
    "        primary_key_list (list): Primary Key of Dataframe\n",
    "    \n",
    "    Returns\n",
    "        temp_df1: Raw Data of SUmmary Cols with a Count of Observations. If include Month Variable Easy to add to Pivot Table\n",
    "        summary: Summary (Excluding Primary Key). including Total Observations, MEan, Max, Min.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    temp_df = df[summary_cols].copy()\n",
    "    temp_df['COUNT'] = 1\n",
    "    \n",
    "    # Unique Occurances by Pivot Criteria. Important to Include Month\n",
    "    temp_df1 = temp_df.groupby(summary_cols).sum().reset_index().rename(columns={'COUNT':'TOTAL_DAYS'})\n",
    "    \n",
    "    pivot_columns1 = [x for x in summary_cols if x not in primary_key_list]\n",
    "    \n",
    "    summary = temp_df1.groupby(pivot_columns1).agg(\n",
    "        TOTAL=('TOTAL_DAYS', 'count'),\n",
    "        AVG_DAYS_OPEN=('TOTAL_DAYS', 'mean'),\n",
    "        MAX_OBS=('TOTAL_DAYS', 'min'),\n",
    "        MIN_OBS=('TOTAL_DAYS', 'max')).reset_index()\n",
    "    \n",
    "    return temp_df1,summary\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ca483-8576-4bc2-8aba-4ce465788404",
   "metadata": {},
   "source": [
    "## Recently Created Perhaps a Better Function. Review What I did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d8894-156a-4cf9-82ea-0c2004da1188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_column_compare(df,\n",
    "                          column_name,\n",
    "                          column_name1,\n",
    "                          additional_filter=None,\n",
    "                          bracketing=[-10000,-1000,-100,-1,0,1,100,1000,10000]):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Function which takes a dataframe with 2 Columns which are identical and attempts to Compare.\n",
    "    Function df_column_compare applies this function to an Entire Dataframe\n",
    "    \n",
    "    Parameters:\n",
    "        column_name (str):\n",
    "        column_name1 (str): For One Off Use, to compare different name\n",
    "        additional_filter (str): Default parameter to distinguish combined dataframes, also used in MergeAndRenameColumnsDf\n",
    "        bracketing(list): Value to create Distintion when Calculated Difference between columns is numeric.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of 3 Dataframes, Account, Summary and Group By.\n",
    "    \n",
    "    Values:\n",
    "    \n",
    "    \n",
    "    Date Created: August 21, 2025\n",
    "    Date Last Modified:\n",
    "    \n",
    "\n",
    "\n",
    "    '''\n",
    "    if not column_name1:\n",
    "        column_name1 = f\"{column_name}{column_distinction}\"    \n",
    "    \n",
    "    # Change Names of Individual Columns to Something Generic so datasets can be Concatenated.\n",
    "    temp_df = df.rename(columns={column_name:'DF',column_name1:'DF1'}).copy()\n",
    "    \n",
    "    output_dict = {}\n",
    "    \n",
    "    temp_df['COLUMN_NAME'] = column_name\n",
    "    \n",
    "    BinaryComplexEquivlancey(temp_df,'DF','DF1','VALUES_EQUAL')\n",
    "    \n",
    "    temp_df['VALUES_NOT_EQUAL'] = np.where(temp_df['VALUES_EQUAL']==0,1,0)\n",
    "    temp_df['NULL_RECORD_DF'] = np.where(temp_df['DF'].isnull(),1,0)\n",
    "    temp_df['NULL_RECORD_DF1'] = np.where(temp_df['DF1'].isnull(),1,0)\n",
    "    \n",
    "    try:\n",
    "        temp_df['DIFFERENCE'] = temp_df['DF'].fillna(0)-temp_df['DF1'].fillna(0)\n",
    "    except:        \n",
    "        temp_df['DIFFERENCE'] = 0\n",
    "        \n",
    "    try:\n",
    "        BracketColumn(temp_df,'DIFFERENCE','DIFF_SEGMENT',bracketing)\n",
    "    except:\n",
    "        temp_df['DIFF_SEGMENT'] = 'Could Not Calculate'\n",
    "    \n",
    "    # Removed Column Partitioner as it wasn't being Used.\n",
    "    \n",
    "    temp_df1 = temp_df.copy()\n",
    "    temp_df1['RECORD_COUNT']=1\n",
    "    \n",
    "    if additional_filter:\n",
    "        output_dict['groupby_df'] = temp_df1[[additional_filter,'COLUMN_NAME','DF','DF1','RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].groupby([additional_filter,'COLUMN_NAME','DF','DF1'],dropna=False).sum().sort_values('VALUES_EQUAL',ascending=False).head(20).reset_index()\n",
    "        \n",
    "    else:\n",
    "        output_dict['groupby_df'] = temp_df1[['COLUMN_NAME','DF','DF1','RECORD_COUNT','VALUES_EQUAL','VALUES_NOT_EQUAL','NULL_RECORD_DF','NULL_RECORD_DF1']].groupby(['COLUMN_NAME','DF','DF1'],dropna=False).sum().sort_values('VALUES_EQUAL',ascending=False).head(20).reset_index()\n",
    "    \n",
    "    if additional_filter:\n",
    "        summary_df = pd.DataFrame()\n",
    "        for value in temp_df[additional_filter].unique():\n",
    "            temp = temp_df[temp_df[additional_filter]==value]\n",
    "            value_dict = {\n",
    "                'Total Combined Records':len(temp),\n",
    "                'Values Equal':temp['VALUES_EQUAL'].sum(),\n",
    "                'Values Not Equal':len(temp[temp['VALUES_EQUAL']==0]),\n",
    "                'Percent Values Equal': (temp['VALUES_EQUAL'].sum()/len(temp))*100,\n",
    "                'Null Records DF':temp['NULL_RECORD_DF'].sum(),\n",
    "                'Null Records DF1':temp['NULL_RECORD_DF1'].sum()}\n",
    "            \n",
    "            try:\n",
    "                value_dict['Total Difference']=temp['DIFFERENCE'].sum()\n",
    "            except:\n",
    "                value_dict['Total Difference']=0\n",
    "                \n",
    "            sum_df = pd.DataFrame(value_dict.values(),index=value_dict.keys(),columns=[column_name]).T.reset_index().rename(columns={'index':\"COLUMN_NAME\"})\n",
    "            sum_df[additional_filter] = value\n",
    "            summary_df = pd.concat([summary_df,sum_df])\n",
    "    else:\n",
    "        value_dict = {\n",
    "            'Total Combined Records':len(temp_df),\n",
    "            'Values Equal':temp_df['VALUES_EQUAL'].sum(),\n",
    "            'Values Not Equal':len(temp_df[temp_df['VALUES_EQUAL']==0]),\n",
    "            'Percent Values Equal': (temp_df['VALUES_EQUAL'].sum()/len(temp_df))*100,\n",
    "            'Null Records DF':temp['NULL_RECORD_DF'].sum(),\n",
    "            'Null Records DF1':temp['NULL_RECORD_DF1'].sum()}\n",
    "        \n",
    "        try:\n",
    "            value_dict['Total Difference']=temp['DIFFERENCE'].sum()\n",
    "        except:\n",
    "            value_dict['Total Difference']=0\n",
    "            \n",
    "        summary_df = pd.DataFrame(value_dict.values(),index=value_dict.keys(),columns=[column_name]).T.reset_index().rename(columns={'index':\"COLUMN_NAME\"})\n",
    "        \n",
    "    output_dict['summary_df'] = summary_df\n",
    "    output_dict['account_df'] = temp_df\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "def DfDqComparison(df,\n",
    "                   primary_key_list,\n",
    "                   additional_filter,\n",
    "                   column_distinction='_',\n",
    "                   bracketing=[-10000,-1000,-100,-1,0,1,100,1000,10000],\n",
    "                   file_name=None):\n",
    "    \n",
    "    '''\n",
    "    Function to Apply ColumnDQComparison against DataFrame.\n",
    "    Assumes you start with a Dataframe with Multiple Columns Different only by Column Distinction.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame)\n",
    "        primary_key_list (list): List of Primary Keys, which are REMOVE from comparison Loop.\n",
    "        additional_filter (str): Filter Used to Create a distinct Dimension. Currently DOES NOT accept List\n",
    "        column_distinction (str): String which is expected to compare Columns. Added as default with MergeIdenticalDF\n",
    "        bracketing (list): Numbers which can be used to Calculate a Bracketed difference Column in COmparison\n",
    "        file_name (str): If Included, it will generate Excel Copies (Excel Used as CSV had issues uploading to DF)\n",
    "        \n",
    "    Return:\n",
    "        DataFrame of Groupby, Account and Summary calculations.\n",
    "        \n",
    "        Account: Listing of All Account Values, with Calculations\n",
    "        Summary: A summary Calculation Speaking to Overall Comparison\n",
    "        Groupby: List of Equivalent Values, to compare Material Record Change/Consistency\n",
    "    \n",
    "    Date Created: August 21, 2025\n",
    "    Date Last Modified: \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Only Need to Test Common Records Can do a Simple Dataframe Analysis on Non Common Records.\n",
    "    \n",
    "    #Iterate Through All Columns in Common to create Final Values.\n",
    "    \n",
    "    account_df = pd.DataFrame()\n",
    "    groupby_df =  pd.DataFrame()\n",
    "    summary_df = pd.DataFrame()\n",
    "\n",
    "    for column_name in [x for x in df.columns if (x not in primary_key_list)&(x[-1]!=column_distinction)]:\n",
    "        column_name1 = f\"{column_name}{column_distinction}\"\n",
    "        try:\n",
    "            temp_dict = IdenticalColumnDQValidation(df=df[['ACCTNBR',additional_filter,column_name,column_name1]],\n",
    "                                                    column_name=column_name,\n",
    "                                                    additional_filter=additional_filter,\n",
    "                                                    bracketing=bracketing)\n",
    "    \n",
    "            account_df = pd.concat([account_df,temp_dict['account_df']])\n",
    "            summary_df = pd.concat([summary_df,temp_dict['summary_df']])\n",
    "            groupby_df = pd.concat([groupby_df,temp_dict['groupby_df']])\n",
    "            \n",
    "        except:\n",
    "            print(f'Could Not Compute: {column_name}') \n",
    "            \n",
    "    if file_name:\n",
    "        account_df.to_csv(f\"{file_name}_ACCOUNT.csv\",index=False)\n",
    "        summary_df.to_csv(f\"{file_name}_SUMMARY.csv\",index=False)\n",
    "        groupby_df.to_csv(f\"{file_name}_GROUPBY.csv\",index=False)\n",
    "\n",
    "    return account_df,summary_df,groupby_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bf11f-0b6b-4afc-9ae0-0ce1a7f532c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_list(x):\n",
    "    import ast\n",
    "    if isinstance(x, str):\n",
    "        return ast.literal_eval(x)\n",
    "    return x\n",
    "\n",
    "df['type'] = df['type'].apply(lambda x:string_to_list(x))\n",
    "df['charged_moves'] = df['charged_moves'].apply(lambda x:string_to_list(x))\n",
    "df['fast_moves'] = df['fast_moves'].apply(lambda x:string_to_list(x))\n",
    "\n",
    "df.head(2)\n",
    "\n",
    "\n",
    "def split_list_column_to_fixed_columns(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    prefix: str = \"Column\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a DataFrame list column into a fixed number of positional columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    column : str\n",
    "        Column containing list values\n",
    "    max_columns : int\n",
    "        Number of output columns to create\n",
    "    prefix : str\n",
    "        Prefix for column names (default 'Column')\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with fixed positional columns\n",
    "    \"\"\"\n",
    "\n",
    "    max_columns = df['type'].apply(lambda x:len(x)).max()\n",
    "    \n",
    "    new_cols = [\n",
    "        f\"{prefix}{i+1}\"\n",
    "        for i in range(max_columns)\n",
    "    ]\n",
    "\n",
    "    expanded = pd.DataFrame(\n",
    "        df[column].apply(\n",
    "            lambda x: x[:max_columns] + [None] * (max_columns - len(x))\n",
    "            if isinstance(x, list)\n",
    "            else [None] * max_columns\n",
    "        ).tolist(),\n",
    "        columns=new_cols,\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    return expanded\n",
    "\n",
    "\n",
    "\n",
    "def expand_list_column_to_columns(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    prefix: str = None,\n",
    "    fill_value: int = 0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expand a DataFrame column containing lists into unique indicator columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    column : str\n",
    "        Column containing list values\n",
    "    prefix : str, optional\n",
    "        Optional prefix for new columns\n",
    "    fill_value : int\n",
    "        Value for absence (default 0)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        New DataFrame with one column per unique item\n",
    "    \"\"\"\n",
    "    # Step 1: Explode into long format\n",
    "    exploded = df[[column]].explode(column)\n",
    "\n",
    "    # Step 2: Create indicator column\n",
    "    exploded[\"_value\"] = 1\n",
    "\n",
    "    # Step 3: Pivot to wide format\n",
    "    wide = (\n",
    "        exploded\n",
    "        .pivot_table(\n",
    "            index=exploded.index,\n",
    "            columns=column,\n",
    "            values=\"_value\",\n",
    "            fill_value=fill_value\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Optional prefix\n",
    "    if prefix:\n",
    "        wide = wide.add_prefix(f\"{prefix}_\")\n",
    "\n",
    "    return wide.reset_index(drop=True)\n",
    "\n",
    "def unique_items_from_list_column(df, column):\n",
    "    \"\"\"\n",
    "    Extract unique items from a DataFrame column containing lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    column : str\n",
    "        Column name containing list values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Sorted list of unique items\n",
    "    \"\"\"\n",
    "    return sorted({item for sublist in df[column] for item in sublist})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
