{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addaeb94-d17b-4053-aa49-1cc38017680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed Multithread Processing from Read Directory. Need to have displine and Principles related to \n",
    "# Creation of Functions, Create Individual functions which do components, opposed to long ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7809e9d-3de0-4544-a3d3-66d032360591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a71a1e8-4815-4396-8c95-1574f6f89e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Learning Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdba2cf6-9cc6-4347-96b7-eef5f9cfd903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Process', 'Categorization', 'Word', 'Definition', 'Notes', 'Link',\n",
       "       'Image', 'Markdown Equation', 'Dataset Size', 'Learning Type',\n",
       "       'Algorithm Class', 'Model', 'Ensemble'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0096a942-e912-45a5-9873-052e34384370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model\n",
       "Linear        4\n",
       "Tree          1\n",
       "Meta-Model    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Model'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef430a2-bcb2-4255-a20e-16856d172f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Algorithm Class\n",
       "Classification              4\n",
       "Regression                  2\n",
       "Density-based clustering    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Algorithm Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f701b6c-b5af-42a1-8ec7-963be2453918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learning Type\n",
       "Supervised    6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Learning Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3306c3e3-e4a0-423c-9a4c-c683d43a5841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categorization\n",
       "Algorithm                    201\n",
       "Procedure                     22\n",
       "Operational Definition        18\n",
       "Pipeline Stage                13\n",
       "Definition                    13\n",
       "Optimizer                      7\n",
       "Semantic Type                  6\n",
       "Function                       4\n",
       "Model Type                     4\n",
       "Concept                        4\n",
       "Feature Selection              4\n",
       "Centroid-based clustering      3\n",
       "Regularization                 3\n",
       "General Principles             3\n",
       "Functional Role                3\n",
       "Clustering                     3\n",
       "Theorm                         2\n",
       "Model Architecture             2\n",
       "Density-based clustering       2\n",
       "Theorem                        1\n",
       "Property                       1\n",
       "Evaluation                     1\n",
       "Probabilistic clustering       1\n",
       "Graph-based clustering         1\n",
       "Insance Based Learning         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Categorization'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d549b6-7355-4d13-8918-4029e983355a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "239d1677-52bf-4a03-9f01-8f42433137eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Categorization</th>\n",
       "      <th>Word</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Link</th>\n",
       "      <th>Image</th>\n",
       "      <th>Markdown Equation</th>\n",
       "      <th>Dataset Size</th>\n",
       "      <th>Learning Type</th>\n",
       "      <th>Algorithm Class</th>\n",
       "      <th>Model</th>\n",
       "      <th>Ensemble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Operational Definition</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>Baseline represents the typical or expected be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Operational Definition</td>\n",
       "      <td>Dimension</td>\n",
       "      <td>A Dimension is a conceptual axis of behavior o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Operational Definition</td>\n",
       "      <td>Implementation</td>\n",
       "      <td>An Implementation is a specific measurable rep...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Operational Definition</td>\n",
       "      <td>Momentum</td>\n",
       "      <td>Momentum represents the direction and rate of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Preparation</td>\n",
       "      <td>Operational Definition</td>\n",
       "      <td>Parameter</td>\n",
       "      <td>A Parameter is a tuning choice that controls h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Organization</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Procedure</td>\n",
       "      <td>Is a detailed, step by step set of instruction...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Organization</td>\n",
       "      <td>Definition</td>\n",
       "      <td>RASCI</td>\n",
       "      <td>RASCI is a responsibility assignment framework...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Organization</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Ownership</td>\n",
       "      <td>State of having end-to-end accountability for ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Organization</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Stewardship</td>\n",
       "      <td>Responsibility to safeguard, maintain, and opt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>ML Model</td>\n",
       "      <td>Definition</td>\n",
       "      <td>Multimodal AI</td>\n",
       "      <td>Artificial intelligence systems that can proce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>323 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Process          Categorization            Word  \\\n",
       "0    Data Preparation  Operational Definition        Baseline   \n",
       "1    Data Preparation  Operational Definition       Dimension   \n",
       "2    Data Preparation  Operational Definition  Implementation   \n",
       "3    Data Preparation  Operational Definition        Momentum   \n",
       "4    Data Preparation  Operational Definition       Parameter   \n",
       "..                ...                     ...             ...   \n",
       "318      Organization              Definition       Procedure   \n",
       "319      Organization              Definition           RASCI   \n",
       "320      Organization              Definition       Ownership   \n",
       "321      Organization              Definition     Stewardship   \n",
       "322          ML Model              Definition   Multimodal AI   \n",
       "\n",
       "                                            Definition Notes  Link Image  \\\n",
       "0    Baseline represents the typical or expected be...   NaN   NaN   NaN   \n",
       "1    A Dimension is a conceptual axis of behavior o...   NaN   NaN   NaN   \n",
       "2    An Implementation is a specific measurable rep...   NaN   NaN   NaN   \n",
       "3    Momentum represents the direction and rate of ...   NaN   NaN   NaN   \n",
       "4    A Parameter is a tuning choice that controls h...   NaN   NaN   NaN   \n",
       "..                                                 ...   ...   ...   ...   \n",
       "318  Is a detailed, step by step set of instruction...   NaN   NaN   NaN   \n",
       "319  RASCI is a responsibility assignment framework...   NaN   NaN   NaN   \n",
       "320  State of having end-to-end accountability for ...   NaN   NaN   NaN   \n",
       "321  Responsibility to safeguard, maintain, and opt...   NaN   NaN   NaN   \n",
       "322  Artificial intelligence systems that can proce...   NaN   NaN   NaN   \n",
       "\n",
       "    Markdown Equation Dataset Size Learning Type Algorithm Class Model  \\\n",
       "0                 NaN          NaN           NaN             NaN   NaN   \n",
       "1                 NaN          NaN           NaN             NaN   NaN   \n",
       "2                 NaN          NaN           NaN             NaN   NaN   \n",
       "3                 NaN          NaN           NaN             NaN   NaN   \n",
       "4                 NaN          NaN           NaN             NaN   NaN   \n",
       "..                ...          ...           ...             ...   ...   \n",
       "318               NaN          NaN           NaN             NaN   NaN   \n",
       "319               NaN          NaN           NaN             NaN   NaN   \n",
       "320               NaN          NaN           NaN             NaN   NaN   \n",
       "321               NaN          NaN           NaN             NaN   NaN   \n",
       "322               NaN          NaN           NaN             NaN   NaN   \n",
       "\n",
       "     Ensemble  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  \n",
       "..        ...  \n",
       "318       NaN  \n",
       "319       NaN  \n",
       "320       NaN  \n",
       "321       NaN  \n",
       "322       NaN  \n",
       "\n",
       "[323 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_definition_csv = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vQq1-3cTas8DCWBa2NKYhVFXpl8kLaFDohg0zMfNTAU_Fiw6aIFLWfA5zRem4eSaGPa7UiQvkz05loW/pub?output=csv'\n",
    "\n",
    "df = pd.read_csv(google_definition_csv)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7e1be-9a49-447d-b4c4-81fba48a2686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038dafbb-1907-4ee2-aa52-48fcad615dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    Definition of Function\n",
      "\n",
      "    Parameters:\n",
      "        List of Parameters\n",
      "\n",
      "    Returns:\n",
      "        Object Type\n",
      "\n",
      "    date_created:30-Dec-25\n",
      "    date_last_modified: 30-Dec-25\n",
      "    classification:TBD\n",
      "    sub_classification:TBD\n",
      "    usage:\n",
      "        Example Function Call\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from data_d_strings import template_doc_string_print\n",
    "\n",
    "template_doc_string_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e4883ce-79ab-4b5f-b7fb-6c55de64cc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['utility_functions.py',\n",
       " '.DS_Store',\n",
       " 'dict_processing.py',\n",
       " 'Archive',\n",
       " 'data_d_lists.py',\n",
       " 'shared_folder.py',\n",
       " 'list_processing.py',\n",
       " 'connections.py',\n",
       " '__init__.py',\n",
       " '__pycache__',\n",
       " 'string_processing.py',\n",
       " 'V2',\n",
       " 'input_functions_ignore.py',\n",
       " 'sql_.py',\n",
       " '.ipynb_checkpoints',\n",
       " 'data_d_strings.py',\n",
       " 'data_d_dicts.py',\n",
       " 'df_processing.py']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shared_folder import read_directory\n",
    "d_py_function =  '/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/'\n",
    "read_directory(d_py_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9e33665-a40f-4bdb-a2d0-52dc7bf6a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import a .TXT or .PY File\n",
    "location = '/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions/shared_folder.py'\n",
    "\n",
    "from shared_folder import text_file_import\n",
    "file = text_file_import(location)\n",
    "\n",
    "from shared_folder import parse_dot_py_file\n",
    "function_list, function_parameters = parse_dot_py_file(file)\n",
    "\n",
    "## Generate a Summary File for a Folder.\n",
    "from shared_folder import parse_dot_py_folder\n",
    "function_list, function_parameters = parse_dot_py_folder()\n",
    "\n",
    "from shared_folder import create_py_table_dict\n",
    "create_py_table_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b910ee8-bd2d-4da0-ac9b-6de5dc29535c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "DownloadFilesFromGit()\n",
    "\n",
    "BackUpGoogleSheets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db4215-d821-4838-88af-6c7a729d7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def TranposeNonTimeSeriesDF(df, index, columns=None):\n",
    "    '''\n",
    "    Transposes a non-time-series DataFrame from wide to long format by melting specified columns.\n",
    "\n",
    "    This is especially useful for flattening columns into a single column to support tools \n",
    "    like Power BI, where long format enables dynamic pivoting and aggregation.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The input pandas DataFrame.\n",
    "        index (list): Columns to retain as identifiers (will remain unchanged).\n",
    "        columns (list): Columns to unpivot into key-value pairs.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A long-format DataFrame with 'variable' and 'value' columns.\n",
    "    '''\n",
    "    if not columns:\n",
    "        columns = [col for col in final_df1.columns if (isinstance(col, pd.Timestamp))|(isinstance(col, datetime.datetime))]\n",
    "    \n",
    "    return df.melt(id_vars=index, value_vars=columns)\n",
    "\n",
    "def CreatePivotTableFromTimeSeries(df,\n",
    "                                   index,\n",
    "                                   columns,\n",
    "                                   values,\n",
    "                                   aggfunc='sum',\n",
    "                                   skipna=True):\n",
    "    \n",
    "    '''\n",
    "    Function to Summaryize a Time Series Dataframe into a Pivot. Creating a number of critical Metrics.\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 1. Pivot\n",
    "    if index==None:\n",
    "        df1 = df.pivot_table(columns=columns,values=values,aggfunc=aggfunc)\n",
    "    else:\n",
    "        df1 = df.pivot_table(index=index, columns=columns, values=values, aggfunc=aggfunc)\n",
    "\n",
    "    # 2. Capture original month columns IMMEDIATELY after pivot\n",
    "    month_cols = df1.columns.tolist()\n",
    " \n",
    "    # 3. Add rolling window stats\n",
    "    if len(month_cols) >= 3:\n",
    "        df1['AVG_3M'] = df1[month_cols[-3:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_3M'] = df1[month_cols[-1]]-df1[month_cols[-3]]\n",
    "        try:\n",
    "            df1['PERC_CHG_3M'] = df1['CHG_3M']/df1[month_cols[-3]]\n",
    "        except:\n",
    "            df1['PERC_CHG_3M'] = 0\n",
    "    \n",
    "    if len(month_cols) >= 6:\n",
    "        df1['AVG_6M'] = df1[month_cols[-6:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_6M'] = df1[month_cols[-1]]-df1[month_cols[-6]]\n",
    "        try:\n",
    "            df1['PERC_CHG_6M'] = df1['CHG_6M']/df1[month_cols[-6]]\n",
    "        except:\n",
    "            df1['PERC_CHG_6M'] = 0\n",
    "            \n",
    "    if len(month_cols) >= 12:\n",
    "        df1['AVG_12M'] = df1[month_cols[-12:]].mean(axis=1, skipna=skipna)\n",
    "        df1['CHG_12M'] = df1[month_cols[-1]]-df1[month_cols[-12]]\n",
    "        try:\n",
    "            df1['PERC_CHG_12M'] = df1['CHG_12M']/df1[month_cols[-12]]\n",
    "        except:\n",
    "            df1['PERC_CHG_12M'] = 0\n",
    "\n",
    "    df1['CHG_DF']  = df1[month_cols[-1]]-df1[month_cols[0]]\n",
    "    df1['AVG_DF'] = df1[month_cols[-1:]].mean(axis=1, skipna=skipna)\n",
    "    df1['PERC_CHG_DF'] = df1['AVG_DF']/df1[month_cols[-1]]\n",
    "\n",
    "    \n",
    "    # 4. Now calculate global stats **only using the original month columns**\n",
    "    stats = pd.DataFrame({\n",
    "        'MEAN': df1[month_cols].mean(axis=1, skipna=skipna),\n",
    "        'STD': df1[month_cols].std(axis=1, skipna=skipna),\n",
    "        'MAX': df1[month_cols].max(axis=1, skipna=skipna),\n",
    "        'MIN': df1[month_cols].min(axis=1, skipna=skipna),\n",
    "        'COUNT': df1[month_cols].count(axis=1)\n",
    "    })\n",
    "\n",
    "    # 5. Merge the stats\n",
    "    df1 = pd.concat([df1, stats], axis=1)\n",
    "    \n",
    "    return df1.fillna(0)\n",
    "\n",
    "\n",
    "def CreateMultiplePivotTableFromTimeSeries(df,\n",
    "                                           index_list,\n",
    "                                           metric_list,\n",
    "                                           column):\n",
    "    '''\n",
    "    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through all Possible Metrics Selected.\n",
    "    \n",
    "    for metric in metric_list:\n",
    "        print(f'Attempting to Process:{metric}')\n",
    "        try:\n",
    "            all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') \n",
    "            cols = list(all_df.columns)\n",
    "            all_df = all_df.reset_index(drop=True)\n",
    "            all_df['METRIC'] = metric\n",
    "            cols.insert(0,'METRIC')\n",
    "\n",
    "            for key in index_list:\n",
    "                cols.insert(0,key)\n",
    "                all_df[key] = 'All'\n",
    "\n",
    "            final_df = pd.concat([final_df,all_df[cols]])\n",
    "            # Iterate through all Index Items Individually\n",
    "            for key in index_list:\n",
    "                temp = CreatePivotTableFromTimeSeries(df,\n",
    "                                                      index=key,\n",
    "                                                      values=metric,\n",
    "                                                      columns=column).reset_index() \n",
    "                for missing in [x for x in index_list if x != key]:\n",
    "                    temp[missing] = 'All'\n",
    "                temp['METRIC'] = metric\n",
    "                final_df = pd.concat([final_df,temp])\n",
    "\n",
    "            # Add Value for Metric with Entire Index Combination\n",
    "            temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()\n",
    "            temp['METRIC'] = metric\n",
    "            final_df = pd.concat([final_df,temp])\n",
    "        except:\n",
    "            print(f'Could Not Process Metric:{metric}.')\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def CreateMultiplePivotTableFromTimeSeries(df,index_list,metric_list,column):\n",
    "    '''\n",
    "    Function to utilize when Attempting to Create Multip[le Times Series. Specifically Multiple Metrics, and Multiple Index's\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate through all Possible Metrics Selected.\n",
    "    for metric in metric_list:\n",
    "        all_df = CreatePivotTableFromTimeSeries(df=df,index=None,columns=column,values=metric,aggfunc='sum') \n",
    "        cols = list(all_df.columns)\n",
    "        all_df = all_df.reset_index(drop=True)\n",
    "        all_df['METRIC'] = metric\n",
    "        cols.insert(0,'METRIC')\n",
    "\n",
    "        for key in index:\n",
    "            cols.insert(0,key)\n",
    "            all_df[key] = 'All'\n",
    "\n",
    "        final_df = pd.concat([final_df,all_df[cols]])\n",
    "\n",
    "        # Iterate through all Index Items Individually\n",
    "        for key in index_list:\n",
    "            temp = CreatePivotTableFromTimeSeries(df,index=key,\n",
    "                                                  values=metric,\n",
    "                                                  columns=column).reset_index() \n",
    "            for missing in [x for x in index if x != key]:\n",
    "                temp[missing] = 'All'\n",
    "            temp['METRIC'] = metric\n",
    "            final_df = pd.concat([final_df,temp])\n",
    "        \n",
    "        # Add Value for Metric with Entire Index Combination\n",
    "        temp = CreatePivotTableFromTimeSeries(df,index=index_list,values=metric,columns=column).reset_index()\n",
    "        temp['METRIC'] = metric\n",
    "        final_df = pd.concat([final_df,temp])\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def SummarizeTimeSeriesDf(df,\n",
    "                          summary_cols,\n",
    "                          primary_key_list):\n",
    "    '''\n",
    "    Function to Summarize a Time Series dataframe based on a finite number of identified Columns.\n",
    "    \n",
    "    Parameters\n",
    "        df (Dataframe): TimeSeries in Nature\n",
    "        summary_cols (List): List of Columns which are to be included in SUmmary\n",
    "        primary_key_list (list): Primary Key of Dataframe\n",
    "    \n",
    "    Returns\n",
    "        temp_df1: Raw Data of SUmmary Cols with a Count of Observations. If include Month Variable Easy to add to Pivot Table\n",
    "        summary: Summary (Excluding Primary Key). including Total Observations, MEan, Max, Min.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    temp_df = df[summary_cols].copy()\n",
    "    temp_df['COUNT'] = 1\n",
    "    \n",
    "    # Unique Occurances by Pivot Criteria. Important to Include Month\n",
    "    temp_df1 = temp_df.groupby(summary_cols).sum().reset_index().rename(columns={'COUNT':'TOTAL_DAYS'})\n",
    "    \n",
    "    pivot_columns1 = [x for x in summary_cols if x not in primary_key_list]\n",
    "    \n",
    "    summary = temp_df1.groupby(pivot_columns1).agg(\n",
    "        TOTAL=('TOTAL_DAYS', 'count'),\n",
    "        AVG_DAYS_OPEN=('TOTAL_DAYS', 'mean'),\n",
    "        MAX_OBS=('TOTAL_DAYS', 'min'),\n",
    "        MIN_OBS=('TOTAL_DAYS', 'max')).reset_index()\n",
    "    \n",
    "    return temp_df1,summary\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def SampleDataFrame(df, \n",
    "                    conf=.95, \n",
    "                    me=0.05,\n",
    "                    mv=0.5,\n",
    "                    print_=0,\n",
    "                    new_column_name=\"\"):\n",
    "    \"\"\"\n",
    "    Returns a random sample from a DataFrame based on confidence level and margin of error.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataset to sample from.\n",
    "        conf(float): Desired Confidence Percentage Level (e.g., 90, 95, 99).\n",
    "        me (float): Margin of Error, (default is 5%).\n",
    "        mv (float): Maximum Variability (Expected Level of Default)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A random sample of the required size.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if not 0 <= mv <= 1:\n",
    "        raise ValueError(\"mv (failure rate) must be between 0 and 1.\")\n",
    "\n",
    "    N = len(df)\n",
    "    if N == 0:\n",
    "        raise ValueError(\"DataFrame is empty\")\n",
    "\n",
    "    # Calculate the Z-score based on the confidence level\n",
    "    z = norm.ppf(1 - (1 - conf) / 2)\n",
    "    \n",
    "\n",
    "    # Calculate the initial sample size (without finite population correction)\n",
    "    n0 = (z**2 * mv * (1 - mv)) / (me**2)\n",
    "    \n",
    "    # Apply finite population correction if the population is smaller than 100,000\n",
    "    if N >= 10000:  # For large populations, skip the correction\n",
    "        n = int(n0)\n",
    "    else:\n",
    "        n = int((n0 * N) / (n0 + N - 1))\n",
    "\n",
    "    if print_==1:\n",
    "        print(f\"Z-score: {z}\")  # Debug Z-score\n",
    "        print(f\"Initial sample size (n0): {n0}\")  # Debug n0\n",
    "        print(f\"Sample size with FPC: {n}\")  # Debug final sample size\n",
    "    \n",
    "    sample = df.sample(n=n, random_state=42)\n",
    "    \n",
    "    if len(new_column_name)==0:\n",
    "        return sample \n",
    "\n",
    "    else:\n",
    "        sample_index = sample.index\n",
    "        df[new_column_name] = 0\n",
    "        df.loc[sample_index, new_column_name] = 1\n",
    "        return df\n",
    "\n",
    "def ReviewEntireDataframe(df,file_name=None):\n",
    "    \n",
    "    final_df = pd.DataFrame()\n",
    "    \n",
    "    for column in df.columns:\n",
    "        start_time = timeit.default_timer()\n",
    "        temp_df = ColumnStatisticalReview(df,column)\n",
    "        print(f'Elapsed time to process {column}:{timeit.default_timer() - start_time:,.2f}')\n",
    "        final_df = pd.concat([final_df,temp_df],axis=1)\n",
    "    if file_name:\n",
    "        final_df.to_csv(f\"{file_name}.csv\")\n",
    "        \n",
    "    return final_df\n",
    "\n",
    "def ColumnStatisticalReview(df,\n",
    "                            column_name,\n",
    "                            partitions=10,\n",
    "                            top_x_records=10,\n",
    "                            exclude_blanks_from_segments=1,\n",
    "                            exclude_zeroes_from_segments=1):\n",
    "\n",
    "    '''\n",
    "    Function to Conduct a Simple Statistical Review of a Column, Including Understanding the positional distribution\n",
    "    of values. \n",
    "\n",
    "    Args:\n",
    "        column_name (str): Name of Column\n",
    "\n",
    "        partitions (int): Number of partitions to include (Decile 10)\n",
    "\n",
    "        exclude_blanks_from_segments (int): Binary Flag, whether to exclude Blank Values from Segment determination.\n",
    "        If blank values are excluded it gives a better representation for the members of the set, however it might \n",
    "        provide a misleading representation of the population.\n",
    "\n",
    "        exclude_zeroes_from_segments (int): As above, with respect to 0 values. Is processed after exclude_blanks, as\n",
    "        such it can include both blanks and true 0 values. \n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    temp_dict = {}\n",
    "    \n",
    "    is_numeric = pd.api.types.is_numeric_dtype(df[column_name])\n",
    "    \n",
    "    if is_numeric:\n",
    "        temp_dict['SUM'] = df[column_name].sum()\n",
    "        temp_dict['MEAN'] = df[column_name].mean()\n",
    "        temp_dict['STD_DEV'] =  df[column_name].std()\n",
    "        temp_dict['MEDIAN'] = df[column_name].median()\n",
    "        temp_dict['MAX'] = df[column_name].max()\n",
    "        temp_dict['MIN'] = df[column_name].min()\n",
    "        \n",
    "    temp_dict['TOTAL_RECORDS'] = len(df)\n",
    "    temp_dict['UNIQUE_RECORDS'] = len(df.drop_duplicates(column_name))\n",
    "    temp_dict['NA_RECORDS'] = len(df[df[column_name].isna()])\n",
    "    temp_dict['NULL_RECORDS'] = len(df[df[column_name].isnull()])\n",
    "    \n",
    "    if is_numeric:\n",
    "        temp_dict['ZERO_RECORDS'] = len(df[df[column_name]==0])\n",
    "        temp_dict['NON_ZERO_RECORDS'] = len(df[df[column_name]!=0])    \n",
    "\n",
    "    temp_df = pd.DataFrame(temp_dict.values(),index=temp_dict.keys(),columns=[column_name])\n",
    "    \n",
    "    if temp_dict['TOTAL_RECORDS']==len(df[df[column_name].isnull()]):\n",
    "        return temp_df\n",
    "\n",
    "    # Add top X records Based on Frequency\n",
    "    if top_x_records>0:\n",
    "        top_instances = pd.DataFrame(df[column_name].value_counts(dropna=False).head(top_x_records)).reset_index().rename(columns={column_name:'count','index':column_name})\n",
    "        if len(top_instances)>0:\n",
    "            top_instances[column_name] = top_instances.apply(lambda row: f\"Value: {row[column_name]}, Frequency: {row['count']}\", axis=1)\n",
    "            top_instances['index'] = [f\"Top {x+1}\" for x in range(len(top_instances[column_name]))]\n",
    "            top_instances = top_instances.drop('count',axis=1).set_index('index')\n",
    "            temp_df = pd.concat([temp_df,top_instances])\n",
    "        \n",
    "    if (partitions>0)&(pd.api.types.is_numeric_dtype(df[column_name]))&(temp_dict['UNIQUE_RECORDS']>1):\n",
    "        segment_df = ColumnPartitioner(df=df,\n",
    "                                       column_name=column_name,\n",
    "                                       partitions=partitions,\n",
    "                                       exclude_blanks=exclude_blanks_from_segments,\n",
    "                                       exclude_zeros=exclude_zeroes_from_segments,\n",
    "                                       return_value='')\n",
    "        \n",
    "        seg_val_df = ColumnPartitioner(df=df,\n",
    "                                           column_name=column_name,\n",
    "                                           partitions=partitions,\n",
    "                                           exclude_blanks=exclude_blanks_from_segments,\n",
    "                                           exclude_zeros=exclude_zeroes_from_segments,\n",
    "                                           return_value='agg_value').rename(columns={'VALUE':column_name})\n",
    "\n",
    "        return pd.concat([temp_df,segment_df.T,seg_val_df])\n",
    "    return temp_df\n",
    "\n",
    "def CompareFunction(func1,func2,additional_records=20):\n",
    "    \n",
    "    '''\n",
    "    Function which Compares 2 Functions and determines if they are different. Specifically, it can help to easily\n",
    "    Manage Version control of Functions outside of a More robust environment such as GIT.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    list1 = FunctionToSTR(func1)\n",
    "    list2 = FunctionToSTR(func2)\n",
    "    \n",
    "    length = max(len(list1),len(list2))\n",
    "    \n",
    "    for record in range(0,length):\n",
    "        if list1[record]==list2[record]:\n",
    "            if record == (length-1):\n",
    "                print(\"All Records Reconcile\")\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "                print(list1[record:record+additional_records])\n",
    "                print(list2[record:record+additional_records])\n",
    "            except:\n",
    "                print(list1[record:record:])\n",
    "                print(list2[record:record:])\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7f03f7-6c88-43bd-85d0-05dfb969eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import webbrowser\n",
    "import datetime\n",
    "import requests\n",
    "import os\n",
    "\n",
    "\n",
    "def DownloadFilesFromGit(user='derek-dewald',\n",
    "                        repo='Python_Tools',\n",
    "                        folder='d_py_functions',\n",
    "                        output_folder=\"\"):\n",
    "    '''\n",
    "    Function to Download Files from Github to a dedicated folder. Specifically used when i DO NOT want to formally link to Github.\n",
    "    \n",
    "    Parameters:\n",
    "        User:\n",
    "        Repo:\n",
    "        folder:\n",
    "        output_folder:\n",
    "        \n",
    "    Returns:\n",
    "        Saves files to Output Folder.\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    if len(output_folder) == 0:\n",
    "        output_folder = os.getcwd()\n",
    "    \n",
    "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents/{folder}\"\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        files = response.json()\n",
    "        py_files = [file for file in files if file['name'].endswith('.py')]\n",
    "\n",
    "        for file in py_files:\n",
    "            file_url = file['download_url']\n",
    "            file_name = file['name']\n",
    "            file_response = requests.get(file_url)\n",
    "\n",
    "            if file_response.status_code == 200:\n",
    "                with open(os.path.join(output_folder, file_name), \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(file_response.text)\n",
    "                    \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def ParamterMapping(Definition=\"\"):\n",
    "    \n",
    "    '''\n",
    "    Function to Google Mapping Sheet, which is used to store Mappings, Links, etc.\n",
    "    For both simplicity and Organization\n",
    "    \n",
    "    Args:\n",
    "        Definition (Str): Key word used to Access individual elements\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe, unless Definition is defined, in which case it might be Str.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSwDznLz-GKgWFT1uN0XZYm3bsos899I9MS-pSvEoDC-Cjqo9CWeEuSdjitxjqzF3O39LmjJB0_Fg-B/pub?output=csv')\n",
    "    \n",
    "    # If user has not included a definition, the return entire DF\n",
    "    if len(Definition)==0:\n",
    "        return df\n",
    "    else:\n",
    "        try:\n",
    "            df1 = df[df['Definition']==Definition]\n",
    "            if len(df1)==1:\n",
    "                if df1['TYPE'].item()=='csv':\n",
    "                    return pd.read_csv(df1['VALUE'].item())\n",
    "                else:\n",
    "                    return df1['VALUE'].item()\n",
    "        except:\n",
    "            return df[df['Definition']==Definition] \n",
    "\n",
    "\n",
    "def BackUpGoogleSheets(location='/Users/derekdewald/Documents/Python/Github_Repo/CSV Backup Files/'):\n",
    "    '''\n",
    "    Function to Create a Backup of Information Stored in Google Sheets.\n",
    "    \n",
    "    Parameters:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        CSV Files \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = ParamterMapping()\n",
    "    \n",
    "    for row in range(len(df)):\n",
    "        try:\n",
    "            file_name = df['Definition'][row]\n",
    "            file_location = df['CSV'][row]\n",
    "            month = datetime.datetime.now().strftime('%b-%y')\n",
    "            \n",
    "            temp_df = pd.read_csv(file_location)\n",
    "            temp_df.to_csv(f'{location}{file_name}_{month}.csv',index=False)\n",
    "            print(f'Back Up Saved, {location}{file_name}')\n",
    "        except:\n",
    "            print(f'Counld Not Print Record {row}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def NavigateUsingDMap():\n",
    "     \n",
    "    '''\n",
    "    Function to Google Mapping Sheet, Navigate to Specific Sites.\n",
    "    Provides Options, Enable Selection based on inputs.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "        \n",
    "    Returns:\n",
    "        \n",
    "    \n",
    "    '''\n",
    "\n",
    "    df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSwDznLz-GKgWFT1uN0XZYm3bsos899I9MS-pSvEoDC-Cjqo9CWeEuSdjitxjqzF3O39LmjJB0_Fg-B/pub?output=csv')\n",
    "    \n",
    "    display(df)\n",
    "    \n",
    "    p = input('Which Process would You like to review?')\n",
    "    v = input('What would you like to return?')\n",
    "    \n",
    "    df1 = df[df['Definition']==p]\n",
    "    \n",
    "    if v.lower() =='link':\n",
    "        webbrowser.open(df1['Link'].item())\n",
    "    elif v.lower() == 'csv':\n",
    "        return pd.read_csv(df1['CSV'].item())\n",
    "    elif v.lower()=='streamlit':\n",
    "        webbrowser.open(df1['Streamlit'].item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
