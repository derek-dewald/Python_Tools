Date,Temp_Score,Historical_Score,Word,Process,Categorization,Definition,Notes,Link,Image,Markdown Equation,Dataset Size,Learning Type,Algorithm Class,Model,Ensemble
2026-01-07,0,-3,SGDClassifier,ML Model,Algorithm,"SGDClassifier is a linear classification algorithm that trains models using stochastic gradient descent, updating parameters incrementally with each batch or observation. It is important because it scales efficiently to very large datasets and supports multiple loss functions (e.g., logistic regression, hinge loss). SGDClassifier is commonly used when data is high-dimensional, streaming, or too large for batch optimization methods. It trades some stability for speed and scalability.",,,,,Large,Supervised,Classification,Linear,
2026-01-07,0,-3,VotingClassifier,ML Model,Algorithm,"VotingClassifier is an ensemble method that combines predictions from multiple different models into a single prediction, using either majority voting (hard voting) or averaged probabilities (soft voting). It is important because combining diverse models often improves robustness and generalization compared to any single model. Voting classifiers are typically used when several reasonably strong models exist and their errors are not perfectly correlated. They are especially useful in production systems where stability is valued over interpretability.",,,,,"Medium, Large",Supervised,Classification,Meta-Model,1.0
2026-01-07,0,-3,Quantile Random Forest,ML Model,Algorithm,"Quantile Random Forest is an extension of random forests that estimates conditional quantiles of the target variable instead of only the mean. It is important because it provides uncertainty information and prediction intervals, not just point estimates. This method is commonly used in risk modeling, forecasting, and decision-making scenarios where understanding the distribution of outcomes matters. It is particularly useful when residuals are heteroskedastic or non-Gaussian.",,,,,,Supervised,Regression,Tree,
2026-01-07,0,-3,RandomTreesEmbedding,ML Model,Algorithm,,,,,,Large,,,,
2026-01-07,0,-3,Area Under the Curve,Model Evaluation,Feature Selection,"AUC (Area Under the Curve) measures a modelâ€™s ability to distinguish between classes by summarizing the performance of the ROC (Receiver Operating Characteristic) curve. It represents the probability that the model ranks a randomly chosen positive example higher than a randomly chosen negative one. AUC ranges from 0.5 (no better than chance) to 1.0 (perfect separation), with higher values indicating better classification performance.
AUC is primarily a binary classification metric, so when using against a multivariate challenge must determine how to score, OVR, OVO. One Vs Rest, One vs One. Using MNist as an example, OVR evaluates 10 0 vs (1,2,3,4,5,6,7,8,9), 1 vs () ... etc, Where OVO measures 45 0 Vs 1, 0 vs 2, etc..",,,,,,,,,
2026-01-08,0,-3,Perceptron,ML Model,Algorithm,"The Perceptron is one of the earliest linear classification algorithms, learning a decision boundary by iteratively adjusting weights based on misclassified examples. It is important historically as the foundation of modern neural networks and gradient-based learning. In practice, it is used mainly for educational purposes or as a fast baseline on linearly separable data. Its simplicity limits its performance on noisy or non-linear problems.",,,,,Large,Supervised,Classification,Linear,
2026-01-08,0,-3,RidgeCV,ML Model,Algorithm,"RidgeCV is a regression model that combines ridge regression (L2 regularization) with cross-validation to automatically select the optimal regularization strength. It is important because it reduces overfitting while maintaining stability in the presence of multicollinearity. RidgeCV is commonly used when predictors are highly correlated and predictive performance is prioritized over feature sparsity. It provides a principled, automated way to tune regularization.",,,,,"Medium, Large",Supervised,Regression,Linear,
2026-01-08,0,-3,Regularization,ML Model,Definition,"Technique used to prevent overfitting by adding a penalty term to the loss function. It discourages the model from fitting too closely to the training data, ensuring better generalization to unseen data. Traditionnaly results in bias, at benefit of materially lowering Variance. Regularization is a Constraint.",,,,,,,,,
2026-01-08,0,-3,SelectFpr,ML Model,Algorithm,SelectFpr is a feature selection method that selects features based on statistical tests while controlling the false positive rate. It is important because it helps remove irrelevant features while limiting the probability of including noise variables. SelectFpr is typically used in high-dimensional settings such as genomics or text data. It is appropriate when interpretability and statistical control are priorities.,,,,,Large,,,,
2026-01-08,0,-3,AdaBoostClassifier,ML Model,Algorithm,"AdaBoostClassifier is an ensemble boosting algorithm that builds a strong classifier by sequentially combining many weak learners, typically decision stumps. It is important because it focuses learning on previously misclassified examples, often achieving high accuracy with simple base models. AdaBoost is commonly used when data is moderately sized and relatively clean. It can be sensitive to noise and outliers due to its reweighting mechanism.",,,,,"Small, Medium",Supervised,Classification,Linear,1.0
2026-01-13,0,-3,Adagrad,Training,Optimizer,"Variation of Gradient Descent, which adapts the learning rate for each parameter by scaling it inversely proportional to the sum of past squared gradients. 
",,,,"$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{G_t + \epsilon}} \nabla J(\theta_t)
$$

where:
- $( G_t = \sum_{i=1}^{t} \nabla J(\theta_i) \odot \nabla J(\theta_i) )$ (element-wise sum of squared gradients),
- $( \epsilon )$ is a small constant to prevent division by zero.",,,,,
2026-01-13,0,-2,NMF,ML Model,Algorithm,,,,,,"Medium, Large",,,,
2026-01-13,0,-3,Cache,Data Engineering,Definition,"Cache takes load away from critical systems. Databases. Things that are looked up every second, minute. Can Cache at every layer.  Memory Buffers exist everywhere. Authentication is terrible caching. Don't Cache purchases and refunds. Search Queries, too broad. Can't cache transaction.",,,,,,,,,
2026-01-13,0,-3,Diagnostic ,Data Preparation,Semantic Type ,"Representing metrics of directionality, momentum or variability, including periodic changes, standard deviations, volatility, or growth percentages. ",,,,,,,,,
2026-01-13,0,-2,Grapth Theory,Graph Theory,Definition,,,,,,,,,,
