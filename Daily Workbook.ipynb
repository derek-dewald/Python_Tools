{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6641b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2de5031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Test   Test1\n",
      "0    None    None\n",
      "1  Value1  Value1\n",
      "2    None  Value1\n",
      "3  Value2  Value2\n",
      "4    None  Value2\n",
      "5    None  Value2\n",
      "6  Value3  Value3\n",
      "7    None  Value3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4m/j9kyjvln7ps4dkntd048fklh0000gn/T/ipykernel_2575/4125152650.py:7: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df['Test1'] = df['Test'].fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({'Test': [None, 'Value1', None, 'Value2', None, None, 'Value3', None]})\n",
    "\n",
    "# Fill blank values with the previous value\n",
    "df['Test1'] = df['Test'].fillna(method='ffill')\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0caa5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8bd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(8,activation='relu'))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(512, activation='tanh'))\n",
    "model.add(layers.Dense(1024, activation='tanh'))\n",
    "model.add(layers.Dense(2028, activation='tanh'))\n",
    "model.add(layers.Dense(512, activation='tanh'))\n",
    "model.add(layers.Dense(512, activation='tanh'))\n",
    "model.add(layers.Dense(128, activation='tanh'))\n",
    "model.add(layers.Dense(32, activation='tanh'))\n",
    "model.add(layers.Dense(1,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_crossentropy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train.fillna(0), y_train, epochs=50, batch_size=1000, validation_split=0.1, verbose=1)\n",
    "\n",
    "\n",
    "word_dictionary = {}\n",
    "\n",
    "for i in column_data['CLEAN']:\n",
    "    word = i.split()\n",
    "    for i in word:\n",
    "        try:\n",
    "            word_dictionary[i] +=1\n",
    "        except:\n",
    "            word_dictionary[i] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71228ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_string(string,remove_chars=['+','-',\"(\",\")\",'/','*']):\n",
    "    \n",
    "    '''\n",
    "    Function to clean some of the punctuation which appears in Column Headers\n",
    "    \n",
    "    '''\n",
    "    new_string = \"\"\n",
    "    for char in string:\n",
    "        if char not in remove_chars:\n",
    "            new_string +=char\n",
    "    return new_string\n",
    "\n",
    "def create_heatmap(df,column_name='',corr_value=.1,figsize=(20,15)):\n",
    "    \n",
    "    sns.set(style='white')\n",
    "    \n",
    "    # View column with Abbreviated title or full. Abbreviated displays nicer.\n",
    "    corr = df.corr()\n",
    "    \n",
    "    if len(column_name)!=0:\n",
    "        corr = corr[[column_name]]\n",
    "        corr = corr[abs(corr[column_name])>corr_value]\n",
    "    \n",
    "    mask= np.zeros_like(corr,dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)]=True\n",
    "    f,ax = plt.subplots(figsize=figsize)\n",
    "    cmap = sns.diverging_palette(220,10,as_cmap=True)\n",
    "    sns.heatmap(corr,mask=mask,cmap=cmap,vmax=.3,center=0,square=True,linewidths=.5)\n",
    "    \n",
    "    plt.title('Heat Map of Correlation')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def create_column_inclusion_review(df,\n",
    "                               columns,\n",
    "                               column_type_df='',\n",
    "                               decile_review_df=''):\n",
    "    \n",
    "    df = df[columns].copy()\n",
    "    \n",
    "    temp_df = review_dataset_dict(df)\n",
    "    \n",
    "    if len(column_type_df)!=0:\n",
    "        temp_df = temp_df.merge(column_type_df,on='Financial Ratio',how='left')\n",
    "    if len(decile_review_df)!=0:\n",
    "        temp_df = temp_df.merge(decile_review_df,on='Financial Ratio',how='left')\n",
    "    \n",
    "    return temp_df\n",
    "\n",
    "\n",
    "def variable_review(df,\n",
    "                    column_name,\n",
    "                    og_column,\n",
    "                    column_inclusion_review_df,\n",
    "                    corr_weight=.15):\n",
    "        \n",
    "    print(column_inclusion_review_df[column_inclusion_review_df['Financial Ratio']==column_name].T)\n",
    "    print(f\"\\n\")\n",
    "    \n",
    "    create_heatmap(df[og_columns],column_name)\n",
    "    create_decile(df,column_name)\n",
    "    \n",
    "    print('Top 20 Records')\n",
    "    print(df.sort_values(column_name)[[column_name,'BANKRUPTCY_FLAG']].tail(20))\n",
    "    \n",
    "    print('Bottom 20 Records')\n",
    "    print(df.sort_values(column_name)[[column_name,'BANKRUPTCY_FLAG']].head(20))\n",
    "    \n",
    "    return df[df[column_name].isnull()].T\n",
    "\n",
    "# Process for reviewing Non Tier 1 Elements.\n",
    "\n",
    "def review_single_variable_manully(df,\n",
    "                                   column_name,\n",
    "                                   baseline_columns,\n",
    "                                   column_inclusion_review_df,\n",
    "                                   og_columns,\n",
    "                                   export_to_excel=0):\n",
    "        \n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    # Currently Included Columns for Simple Reference\n",
    "    print(\"Columns Currently In Scope:\")\n",
    "    for included in baseline_columns:\n",
    "        print(f\"{included}\\n\")\n",
    "    \n",
    "    # EDA\n",
    "    variable_review(df,column_name,og_columns,column_inclusion_review_df)    \n",
    "    print(f\"Number of Null Records with Bankrupcy Flag Yes: {df[df[column_name].isnull()]['BANKRUPTCY_FLAG'].sum()}\")\n",
    "    \n",
    "    # Review Questions\n",
    "    blank_or_remove = input('Remove Null Records/ Zero Null Records/ Exit Loop (remove/zero/exit)')\n",
    "    include_in_model = input('Subjective Belief as to whether variable should be included in model (include/exclude)')\n",
    "    negative_value = input('Remove, Zero or Leave Negative Values (remove/zero/ignore)')\n",
    "    decision_logic = input('Please Provide Comment on Decision for Archival Reference')\n",
    "        \n",
    "    record_df =  pd.DataFrame([blank_or_remove.lower(),include_in_model.lower(),negative_value.lower(),decision_logic.lower(),now],index=['Null Record Approach','Baseline V2 Model Inclusion',\"Negative Valuation\",'Archival Decisioning','Extract Time'],columns=[column_name]).T.reset_index().rename(columns={'index':'Financial Ratio'})\n",
    "    \n",
    "    if export_to_excel==1:\n",
    "        clean_column_name = clean_string(column_name,remove_chars=['+','-',\"(\",\")\",'/','*']).replace(\" \",\"_\")\n",
    "        record_df.to_excel(f\"manual_review/manual_review_{clean_column_name}_{now.strftime('%d%m%y%h%m%s')}.xlsx\",index=False)\n",
    "            \n",
    "    return record_df\n",
    "\n",
    "def read_files_in_folder(folder_location,file_type='*',import_df=0):\n",
    "    \n",
    "    files_ = os.listdir(folder_location)\n",
    "    \n",
    "    if file_type =='*':\n",
    "        files_desired = files_.copy()\n",
    "    else:\n",
    "        files_desired = [x for x in files_ if x.find('xlsx')>-1]\n",
    "        \n",
    "    if import_df ==1:\n",
    "        final_df = pd.DataFrame()\n",
    "        if file_type =='xlsx':\n",
    "            pd_read = pd.read_excel\n",
    "        elif file_type =='csv':\n",
    "            pd_read = pd.read_csv      \n",
    "        else:\n",
    "            print('Update Function')              \n",
    "        for file in files_desired:\n",
    "            final_df = pd.concat([final_df,pd_read(f\"{folder_location}/{file}\")])\n",
    "            \n",
    "        return final_df\n",
    "         \n",
    "    return files_desired\n",
    "\n",
    "\n",
    "\n",
    "def build_binary_classification_model(input_dim, \n",
    "                                      hidden_layer_sizes,\n",
    "                                      activation, \n",
    "                                      optimizer,\n",
    "                                      learning_rate,\n",
    "                                      metrics):\n",
    "\n",
    "    \"\"\"Build a binary classification model using Keras.\n",
    "\n",
    "      Args:\n",
    "        input_dim: Number of features in the input data.\n",
    "        hidden_layer_sizes: A list with the number of units in each hidden layer.\n",
    "        activation: The activation function to use for the hidden layers.\n",
    "        optimizer: The optimizer\n",
    "        learning_rate: The desired learning rate for the optimizer.\n",
    "\n",
    "      Returns:\n",
    "        model: A tf.keras model.\n",
    "    \"\"\"\n",
    "    # Instantiate Model\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # Add Input Layer\n",
    "    model.add(layers.InputLayer(input_shape=(input_dim,)))\n",
    "\n",
    "    # Add Hidden Layers\n",
    "    for nodes in hidden_layer_sizes:\n",
    "        model.add(layers.Dense(units=nodes, activation=activation))\n",
    "\n",
    "    # Add Output Layer\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Configure optimizer and compile the model\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(X,\n",
    "                y,\n",
    "                input_dim,\n",
    "                metrics,\n",
    "                hidden_layer_sizes,\n",
    "                activation, \n",
    "                optimizer,\n",
    "                learning_rate,\n",
    "                batch_size,\n",
    "                num_epochs,\n",
    "                validation_split,\n",
    "                verbose=0):\n",
    "                       \n",
    "\n",
    "    # Build the model.\n",
    "    model = build_binary_classification_model(input_dim=input_dim,\n",
    "                                              hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                              activation=activation, \n",
    "                                              optimizer=optimizer,\n",
    "                                              learning_rate=learning_rate,\n",
    "                                              metrics=metrics)\n",
    "    \n",
    "    print(model.summary())     \n",
    "                        \n",
    "    # Train the model.\n",
    "    history = model.fit(x=X,\n",
    "                        y=y,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=num_epochs,\n",
    "                        validation_split=validation_split,\n",
    "                        verbose=verbose)\n",
    "\n",
    "    # Retrieve the training metrics (after each train epoch) and the final test\n",
    "    # accuracy.\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(train_accuracy, label='train_accuracy')\n",
    "    plt.plot(val_accuracy, label='validation accuracy')\n",
    "    plt.xticks(range(num_epochs))\n",
    "    plt.xlabel('Train epochs')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    " \n",
    "    return history,model\n",
    "\n",
    "def create_balanced_dataset(X,y,observations=0,column_name='BANKRUPTCY_FLAG'):\n",
    "    \n",
    "    '''\n",
    "    Function to take observations and labels, combine them together and then select a even numher of random examples\n",
    "    \n",
    "    X - X_Test or X_Training\n",
    "    y - y_test or y_training\n",
    "    observation - Number of records from both Binary On and Binary Off Column\n",
    "    column_name - Name of Binary Column to filter\n",
    "    \n",
    "    \n",
    "    Used Default Value for purposes of reducing typing in code and given function created for Project Exclusively.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # If length of observations is not defined, create a even 50/ 50 dataset\n",
    "    if observations == 0:\n",
    "        observations = y[column_name].sum()\n",
    "        \n",
    "    if observations>y[column_name].sum():\n",
    "        observations = y[column_name].sum()\n",
    "        \n",
    "    temp_df = pd.concat([X,y],axis=1).copy()\n",
    "    \n",
    "    df1 = temp_df[temp_df[column_name]==1].sample(observations).copy()\n",
    "    df2 = temp_df[temp_df[column_name]==0].sample(observations).copy()\n",
    "    \n",
    "    final_df = pd.concat([df1,df2])\n",
    "    final_df = final_df.sample(frac=1)\n",
    "    \n",
    "    X = final_df.drop(column_name,axis=1)\n",
    "    y = final_df[[column_name]]\n",
    "    \n",
    "    \n",
    "    def train_neural_network(X,\n",
    "                y,\n",
    "                input_dim,\n",
    "                metrics,\n",
    "                hidden_layer_sizes,\n",
    "                activation, \n",
    "                optimizer,\n",
    "                learning_rate,\n",
    "                batch_size,\n",
    "                num_epochs,\n",
    "                validation_split,\n",
    "                verbose=0):\n",
    "                       \n",
    "\n",
    "    # Build the model.\n",
    "    model = build_binary_classification_model(input_dim=input_dim,\n",
    "                                              hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                              activation=activation, \n",
    "                                              optimizer=optimizer,\n",
    "                                              learning_rate=learning_rate,\n",
    "                                              metrics=metrics)\n",
    "    \n",
    "    print(model.summary())     \n",
    "                        \n",
    "    # Train the model.\n",
    "    history = model.fit(x=X,\n",
    "                        y=y,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=num_epochs,\n",
    "                        validation_split=validation_split,\n",
    "                        verbose=verbose)\n",
    "\n",
    "    # Retrieve the training metrics (after each train epoch) and the final test\n",
    "    # accuracy.\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(train_accuracy, label='train_accuracy')\n",
    "    plt.plot(val_accuracy, label='validation accuracy')\n",
    "    plt.xticks(range(num_epochs))\n",
    "    plt.xlabel('Train epochs')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    " \n",
    "    return history,model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def knn_model_creation(X_df,\n",
    "                       y_df,\n",
    "                       neighbors=3):    \n",
    "    model = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "    model.fit(X_df,y_df)    \n",
    "    return model\n",
    "\n",
    "def logistic_regression_creation(X_df,\n",
    "                                 y_df,\n",
    "                                 solver='liblinear'):\n",
    "    '''\n",
    "    ['liblinear','newton-cg','lbfgs','sag','saga']\n",
    "    \n",
    "    for i in [10,20,30]:\n",
    "        temp_df = (y_probabilities >= i/100).astype(int)\n",
    "        knn_df = pd.concat([knn_df,pd.DataFrame(temp_df,columns=[f'LR_{str(i)}p'])],axis=1)\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_df,y_df)\n",
    "    return model\n",
    "\n",
    "\n",
    "def random_forest_classifier(X_df,\n",
    "                             y_df):\n",
    "    '''\n",
    "    important_features_rf = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False).head(10)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_df,y_df)\n",
    "    return model\n",
    "\n",
    "def neural_network(X_df,\n",
    "                   y_df,\n",
    "                   input_dim, \n",
    "                   hidden_layer_sizes,\n",
    "                   activation,\n",
    "                   optimizer,\n",
    "                   learning_rate,\n",
    "                   metrics,\n",
    "                   verbose=0):\n",
    "\n",
    "    \"\"\"Build a binary classification model using Keras.\n",
    "\n",
    "      Args:\n",
    "        input_dim: Number of features in the input data.\n",
    "        hidden_layer_sizes: A list with the number of units in each hidden layer.\n",
    "        activation: The activation function to use for the hidden layers.\n",
    "        optimizer: The optimizer\n",
    "        learning_rate: The desired learning rate for the optimizer.\n",
    "\n",
    "      Returns:\n",
    "        model: A tf.keras model.\n",
    "    \"\"\"\n",
    "    # Instantiate Model\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # Add Input Layer\n",
    "    model.add(layers.InputLayer(input_shape=(input_dim,)))\n",
    "\n",
    "    # Add Hidden Layers\n",
    "    for nodes in hidden_layer_sizes:\n",
    "        model.add(layers.Dense(units=nodes, activation=activation))\n",
    "\n",
    "    # Add Output Layer\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Configure optimizer and compile the model\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "    return model\n",
    "34/22:\n",
    "    # Where am I going to take out the Validation Data.\n",
    "    # Where am I going to do the Standardization of Data?\n",
    "    # Where am I going to take out the testing data.\n",
    "\n",
    "\n",
    "\n",
    "    Takes a Dataframe, which includes Training Data (which includes Label), \n",
    "    \n",
    "\n",
    "    # Step 1 Scale Data\n",
    "if scaling_method.lower() == 'standard_scalar':\n",
    "    X = apply_standard_scaler(X_df,X_df.columns.values)\n",
    "elif scaling_method.lower()=='min_max':\n",
    "    X = apply_min_max_scaler(X_df,X_df.columns.values)\n",
    "\n",
    "    if bankrupcy_observations == 'entire_dataset':\n",
    "a        pass\n",
    "    elif bankrupcy_observations == 'all_bankrupcies':\n",
    "        X,y = create_balanced_dataset(X,y)\n",
    "    else:\n",
    "        X,y = create_balanced_dataset(X,y,bankrupcy_observations)\n",
    "34/23:\n",
    "def create_model(X_df,\n",
    "                 y_df,model,\n",
    "                 scaling_method = 'standard_scalar',\n",
    "                 learning_rate=.05,\n",
    "                 model_parameters={}):\n",
    "    '''\n",
    "    \n",
    "    Function to generate ML Model, based on model type as defined in Parameters.\n",
    "    Below values for illustration purposes, can utilize dictionary to update as desired.\n",
    "\n",
    "    if model is neural_network, \n",
    "    model_parameters = {'hidden_layer_sizes':[8,16,32,64],\n",
    "                                          'activation':'relu',\n",
    "                                          'optimizer':'adam',\n",
    "                                          'learning_rate':.05,\n",
    "                                          'metrics':['accuracy', keras.metrics.Precision(),keras.metrics.Recall()]}\n",
    "    if model Logistic Regression,\n",
    "    model_parameters = {'solver':'liblinear'}\n",
    "\n",
    "    if model knn,\n",
    "    model_parameters = {'n_neighbors':3}\n",
    "\n",
    "    if model random_forsest\n",
    "    model_parameters = {}\n",
    "\n",
    "    '''\n",
    "\n",
    "    if model == 'neural_network':\n",
    "        model = neural_network(X_df,\n",
    "                               y_df,\n",
    "                               input_dim=len(X.columns.values), \n",
    "                               hidden_layer_sizes= model_parameters['hidden_layer_sizes'],\n",
    "                               activation=model_parameters['activation'], \n",
    "                               optimizer=model_parameters['optimizer'],\n",
    "                               learning_rate=model_parameters['learning_rate'],\n",
    "                               metrics=model_parameters['metrics'])\n",
    "\n",
    "    elif model == 'logistic_regression':\n",
    "        model = logistic_regression_creation(X_df,\n",
    "                                             y_df,\n",
    "                                             solver=model_parameters['solver'])\n",
    "    elif model == 'random_forest':\n",
    "        model = random_forest_classifier(X_df,\n",
    "                                         y_df)\n",
    "    elif model == 'knn':\n",
    "        model = knn_model_creation(X_df,\n",
    "                                   y_df,\n",
    "                                   neighbors=model_parameters['n_neighbors'])\n",
    "    return model\n",
    "\n",
    "def generate_predicition(X_test_df,\n",
    "                         y_test_df,\n",
    "                         model,\n",
    "                         py_model,\n",
    "                         model_parameters):\n",
    "\n",
    "    if model == 'neural_network':\n",
    "        history = py_model.fit(x=X_test_df,\n",
    "                            y=y_test_df,\n",
    "                            batch_size=model_parameters['batch_size'],\n",
    "                            epochs=model_parameters['num_epochs'])\n",
    "\n",
    "        acc = pd.DataFrame(history.history['accuracy'],columns=[\"Training Accuracy\"])\n",
    "        loss = pd.DataFrame(history.history['loss'],columns=[\"Training Loss\"])\n",
    "        prec_str = columns=[x for x in history.history.keys() if x.find('pre')!=-1][0]\n",
    "        recall_str = columns=[x for x in history.history.keys() if x.find('reca')!=-1][0]\n",
    "        precision = pd.DataFrame(history.history[prec_str],columns=[prec_str]).rename(columns={prec_str:'Precision'})\n",
    "        recall = pd.DataFrame(history.history[recall_str],columns=[recall_str]).rename(columns={recall_str:'Recall'})\n",
    "\n",
    "        model_results_df = pd.concat([model_results_df,pd.DataFrame([x for x in range(0,len(model_results_df))],columns=['Epoch Number'])],axis=1)\n",
    "        model_results_df['Dataset'] = 'To Be Determined'\n",
    "        model_results_df['Model'] = model\n",
    "        model_results_df['activation'] = model_parameters['activation']\n",
    "        model_results_df['optimizer'] = model_parameters['optimizer']\n",
    "        model_results_df['learning_rate'] = model_parameters['learning_rate']\n",
    "        model_results_df['batch_size'] = model_parameters['batch_size']\n",
    "        model_results_df['epochs'] = model_parameters['num_epochs']\n",
    "        model_results_df['hidden_layer_sizes'] = text_manipulation(model_parameters['hidden_layer_sizes'])\n",
    "        #model_results_df['bankrupt_observations'] = bankrupcy_observations\n",
    "        #model_results_df['total_observations_read'] = len(X)\n",
    "        return model_results_df\n",
    "    \n",
    "    elif model == 'knn':\n",
    "        pred = py_model.predict(X_test_df)\n",
    "        pred_df = pd.DataFrame(pred,columns=['Predicitions'])\n",
    "        final_df = pd.concat([y_test_df.reset_index(drop=True),pred_df],axis=1)\n",
    "        return final_df\n",
    "    \n",
    "    else:\n",
    "        pred = py_model.predict(X_test_df,y_test_df)\n",
    "        pred_df = pd.DataFrame(pred,columns=['Predicitions'])\n",
    "        final_df = pd.concat([y_test_df.reset_index(drop=True),pred_df],axis=1)\n",
    "        return final_df\n",
    "    \n",
    "\n",
    "    # else:\n",
    "    #     predicition = py_model.predict(X_test)\n",
    "    #     temp_x = pd.DataFrame(predicition,columns=[column_name])\n",
    "    #     if df == \"\":\n",
    "    #         return pd.concat([y_test_df,temp_df],axis=1)\n",
    "    #     else:\n",
    "    #         return pd.concat([df,temp_df],axis=1)\n",
    "34/24:\n",
    "X = dataset_dictionary['All Ratios Cleaned']\n",
    "y = dataset_dictionary['labels']\n",
    "\n",
    "model='logistic_regression'\n",
    "model_parameters = {'solver':'liblinear'}\n",
    "\n",
    "lr_  = create_model(X,\n",
    "                      y,\n",
    "                      model=model,\n",
    "                      model_parameters=model_parameters)\n",
    "\n",
    "model = 'knn'\n",
    "model_parameters = {'n_neighbors':3}\n",
    "\n",
    "knn_ = create_model(X,\n",
    "                      y,\n",
    "                      model=model,\n",
    "                      model_parameters=model_parameters)\n",
    "34/25: knn_.predict(X)\n",
    "34/26: X\n",
    "34/27: X_train, X_test, y_train, y_test  = train_test_split(X,y,test_size=0.3, random_state=15)\n",
    "34/28:\n",
    "X = dataset_dictionary['All Ratios Cleaned']\n",
    "y = dataset_dictionary['labels']\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X,y,test_size=0.3, random_state=15)\n",
    "\n",
    "\n",
    "model='logistic_regression'\n",
    "model_parameters = {'solver':'liblinear'}\n",
    "\n",
    "lr_  = create_model(X_train,\n",
    "                      y_train,\n",
    "                      model=model,\n",
    "                      model_parameters=model_parameters)\n",
    "\n",
    "model = 'knn'\n",
    "model_parameters = {'n_neighbors':3}\n",
    "\n",
    "knn_ = create_model(X_train,\n",
    "                      y_train,\n",
    "                      model=model,\n",
    "                      model_parameters=model_parameters)\n",
    "34/29: lr_.n_iter_\n",
    "34/30:\n",
    "X = dataset_dictionary['All Ratios Cleaned']\n",
    "y = dataset_dictionary['labels']\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X,y,test_size=0.3, random_state=15)\n",
    "\n",
    "\n",
    "model='logistic_regression'\n",
    "model_parameters = {'solver':'lbfgs'}\n",
    "\n",
    "lr_  = create_model(X_train,\n",
    "                      y_train,\n",
    "                      model=model,\n",
    "                      model_parameters=model_parameters)\n",
    "\n",
    "model = 'knn'\n",
    "model_parameters = {'n_neighbors':3}\n",
    "\n",
    "knn_ = create_model(X_train,\n",
    "                      y_train,\n",
    "                      model=model,\n",
    "                      model_parameters=model_parameters)\n",
    "34/31:\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def knn_model_creation(X_df,\n",
    "                       y_df,\n",
    "                       neighbors=3):    \n",
    "    model = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "    model.fit(X_df,y_df)    \n",
    "    return model\n",
    "\n",
    "def logistic_regression_creation(X_df,\n",
    "                                 y_df,\n",
    "                                 max_iter=100,\n",
    "                                 solver='liblinear'):\n",
    "    '''\n",
    "    ['liblinear','newton-cg','lbfgs','sag','saga']\n",
    "    \n",
    "    for i in [10,20,30]:\n",
    "        temp_df = (y_probabilities >= i/100).astype(int)\n",
    "        knn_df = pd.concat([knn_df,pd.DataFrame(temp_df,columns=[f'LR_{str(i)}p'])],axis=1)\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    model = LogisticRegression(solver='liblinear',max_iter=max_iter)\n",
    "    model.fit(X_df,y_df)\n",
    "    return model\n",
    "\n",
    "\n",
    "def random_forest_classifier(X_df,\n",
    "                             y_df):\n",
    "    '''\n",
    "    important_features_rf = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False).head(10)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_df,y_df)\n",
    "    return model\n",
    "\n",
    "def neural_network(X_df,\n",
    "                   y_df,\n",
    "                   input_dim, \n",
    "                   hidden_layer_sizes,\n",
    "                   activation,\n",
    "                   optimizer,\n",
    "                   learning_rate,\n",
    "                   metrics,\n",
    "                   verbose=0):\n",
    "\n",
    "    \"\"\"Build a binary classification model using Keras.\n",
    "\n",
    "      Args:\n",
    "        input_dim: Number of features in the input data.\n",
    "        hidden_layer_sizes: A list with the number of units in each hidden layer.\n",
    "        activation: The activation function to use for the hidden layers.\n",
    "        optimizer: The optimizer\n",
    "        learning_rate: The desired learning rate for the optimizer.\n",
    "\n",
    "      Returns:\n",
    "        model: A tf.keras model.\n",
    "    \"\"\"\n",
    "    # Instantiate Model\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # Add Input Layer\n",
    "    model.add(layers.InputLayer(input_shape=(input_dim,)))\n",
    "\n",
    "    # Add Hidden Layers\n",
    "    for nodes in hidden_layer_sizes:\n",
    "        model.add(layers.Dense(units=nodes, activation=activation))\n",
    "\n",
    "    # Add Output Layer\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Configure optimizer and compile the model\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "    return model\n",
    "34/32:\n",
    "def create_model(X_df,\n",
    "                 y_df,model,\n",
    "                 scaling_method = 'standard_scalar',\n",
    "                 learning_rate=.05,\n",
    "                 model_parameters={}):\n",
    "    '''\n",
    "    \n",
    "    Function to generate ML Model, based on model type as defined in Parameters.\n",
    "    Below values for illustration purposes, can utilize dictionary to update as desired.\n",
    "\n",
    "    if model is neural_network, \n",
    "    model_parameters = {'hidden_layer_sizes':[8,16,32,64],\n",
    "                                          'activation':'relu',\n",
    "                                          'optimizer':'adam',\n",
    "                                          'learning_rate':.05,\n",
    "                                          'metrics':['accuracy', keras.metrics.Precision(),keras.metrics.Recall()]}\n",
    "    if model Logistic Regression,\n",
    "    model_parameters = {'solver':'liblinear','max_iter':200}\n",
    "\n",
    "    if model knn,\n",
    "    model_parameters = {'n_neighbors':3}\n",
    "\n",
    "    if model random_forsest\n",
    "    model_parameters = {}\n",
    "\n",
    "    '''\n",
    "\n",
    "    if model == 'neural_network':\n",
    "        model = neural_network(X_df,\n",
    "                               y_df,\n",
    "                               input_dim=len(X.columns.values), \n",
    "                               hidden_layer_sizes= model_parameters['hidden_layer_sizes'],\n",
    "                               activation=model_parameters['activation'], \n",
    "                               optimizer=model_parameters['optimizer'],\n",
    "                               learning_rate=model_parameters['learning_rate'],\n",
    "                               metrics=model_parameters['metrics'])\n",
    "\n",
    "    elif model == 'logistic_regression':\n",
    "        model = logistic_regression_creation(X_df,\n",
    "                                             y_df,\n",
    "                                             max_iter=model_parameters['max_iter'],\n",
    "                                             solver=model_parameters['solver'])\n",
    "    elif model == 'random_forest':\n",
    "        model = random_forest_classifier(X_df,\n",
    "                                         y_df)\n",
    "    elif model == 'knn':\n",
    "        model = knn_model_creation(X_df,\n",
    "                                   y_df,\n",
    "                                   neighbors=model_parameters['n_neighbors'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://en-wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory'\n",
    "pd.read_html(url)\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Correct Wikipedia URL\n",
    "url = 'https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory'\n",
    "\n",
    "# Set User-Agent to mimic a real browser\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"}\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML tables\n",
    "    tables = pd.read_html(response.text)\n",
    "\n",
    "    # Print the number of tables found\n",
    "    print(f\"Total tables found: {len(tables)}\")\n",
    "\n",
    "    # Show the first table\n",
    "    print(tables[0].head())\n",
    "else:\n",
    "    print(f\"Failed to fetch page, status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dab77b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g -f history.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(['We Design Everything','We Design, You Watch','We Design, You Advise','We Design, You Help','You Design, We Help','You Design, We Advise','You Design, We Watch','You Design Everything'],[500,800,1000,1500,2000,3500,5000,8000],columns=['Price List']).reset_index().rename(columns={'Price List':\"Service Offered\",'index':\"Price\"})[['Service Offered','Price']]\n",
    "\n",
    "![Alt Text](https://raw.githubusercontent.com/derek-dewald/d_functions/main/images/Consulting_Fees.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee67883-ac06-4e18-a2fb-ef2b2e22c816",
   "metadata": {},
   "source": [
    "| Dataset Type   | Fits in RAM? | Fits on Local Disk? |\n",
    "|---------------|-------------|---------------------|\n",
    "| Small dataset | Yes         | Yes                 |\n",
    "| Medium dataset | No         | Yes                 |\n",
    "| Big dataset   | No         | No                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22d0cf-203c-4472-a847-b37b48a77c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d0b63-c850-4b76-b83f-d07e217dd31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807639fa-38bc-460d-8bd1-88c86371f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8781f41-5d0f-4301-88c2-039cfc2fe6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "df = pd.read_csv('DKnowledgeSheet.csv')\n",
    "\n",
    "def column_reminder(df,columns=[]):\n",
    "    if len(columns)==0:\n",
    "        columns = df.columns\n",
    "        \n",
    "    for column in columns:\n",
    "        print(df[column].value_counts())\n",
    "\n",
    "def UpdateDictionary(definition,\n",
    "                     overwrite_all=0,\n",
    "                     description=\"\",\n",
    "                     command_code=\"\",\n",
    "                     classification=\"\",\n",
    "                     category=\"\",\n",
    "                     sub_category=\"\"):\n",
    "    \n",
    "    df = pd.read_csv('DKnowledgeSheet.csv')\n",
    "    \n",
    "    df['Definition'] = df['Definition'].fillna(\"\").apply(lambda x:x.lower())\n",
    "    \n",
    "    temp_df = df[df['Definition'].fillna(\"\").str.contains(definition.lower())]\n",
    "\n",
    "    if overwrite_all==1:\n",
    "\n",
    "        if len(temp_df)>1:\n",
    "            confirm = input('Are you Certain you wish to proceed? You will be deleting multiple records.')\n",
    "            if confirm == 'Yes':\n",
    "                pass\n",
    "            else:\n",
    "                return print(\"Manually Ended\")\n",
    "            \n",
    "        df = df[~df['Definition'].fillna(\"\").str.contains(definition.lower())]\n",
    "        new_record = {'Definition': [definition],\n",
    "                      'Description':[description],\n",
    "                      'Command/ Code':[command_code],\n",
    "                      'Classification':[classification],\n",
    "                      'Category':[category],\n",
    "                      'Sub Category':[sub_category]}\n",
    "    \n",
    "        updated_df = pd.concat([df,pd.DataFrame(new_record)]).reset_index(drop=True)\n",
    "        df.to_csv(f\"archive//archive_{datetime.datetime.now().strftime('%d-%b-%y')}.csv\",index=False)\n",
    "        updated_df.to_csv('DKnowledgeSheet.csv',index=False)\n",
    "        print(\"Updated Dictionary\")\n",
    "    \n",
    "        return updated_df\n",
    "        \n",
    "    elif len(temp_df)>1:\n",
    "        return temp_df\n",
    "    \n",
    "    elif (len(temp_df)==1)&((len(description)>0)|(len(command_code)>0)|(len(classification)>0)|(len(category)>0)):\n",
    "        value = temp_df['Definition'].item()    \n",
    "        if len(description)>0:\n",
    "            df.loc[df['Definition']==value,'Description']=description\n",
    "        if len(command_code)>0:\n",
    "            df.loc[df['Definition']==value,'Command/Code']=command_code        \n",
    "        if len(classification)>0:\n",
    "            df.loc[df['Definition']==value,'Classification']=classification\n",
    "        if len(category)>0:\n",
    "            df.loc[df['Definition']==value,'Category']=category\n",
    "        if len(sub_category)>0:\n",
    "            df.loc[df['Definition']==value,'Sub Category']=sub_category\n",
    "            \n",
    "        return df\n",
    "    elif len(temp_df)==1:\n",
    "        return temp_df\n",
    "    elif (len(temp_df)==0)&(len(description)==0)&(len(command_code)==0)&(len(classification)==0)&(len(category)==0):\n",
    "        print('No Records Observed, Nothing to Update')\n",
    "                                                                            \n",
    "    else:\n",
    "        new_record = {'Definition': [definition],\n",
    "                      'Description':[description],\n",
    "                      'Command/ Code':[command_code],\n",
    "                      'Classification':[classification],\n",
    "                      'Category':[category],\n",
    "                      'Sub Category':[sub_category]}\n",
    "                     \n",
    "        updated_df = pd.concat([df,pd.DataFrame(new_record)]).reset_index(drop=True)\n",
    "        df.to_csv(f\"archive//archive_{datetime.datetime.now().strftime('%d-%b-%y')}.csv\",index=False)\n",
    "        updated_df.to_csv('DKnowledgeSheet.csv',index=False)\n",
    "        print(\"Updated Dictionary\")\n",
    "        \n",
    "        return updated_df\n",
    "\n",
    "\n",
    "\n",
    "UpdateDictionary('Data Cleaning',\n",
    "                 description='',\n",
    "                 command_code='',\n",
    "                 classification='Theory',\n",
    "                 category='ML',\n",
    "                 sub_category='')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0880e-9c39-47cc-a871-77a2a90a4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Operational data is usually transactional data that is generated and stored by applications, often in a relational or non-relational database.\n",
    "Analytical data is data that has been optimized for analysis and reporting, often in a data warehouse.\n",
    "Data pipelines are used to orchestrate activities that transfer and transform data. Pipelines are the primary way in which data engineers implement repeatable extract, transform, and load (ETL) solutions that can be triggered based on a schedule or in response to events.\n",
    "A data lake is a storage repository that holds large amounts of data in native, raw formats. Data lake stores are optimized for scaling to massive volumes (terabytes or petabytes) of data. The data typically comes from multiple heterogeneous sources, and may be structured, semi-structured, or unstructured\n",
    "Azure Data Lake Storage Gen2 builds on blob storage and optimizes I/O of high-volume data by using a hierarchical namespace that organizes blob data into directories, and stores metadata about each directory and the files within it. This structure allows operations, such as directory renames and deletes, to be performed in a single atomic operation. Flat namespaces, by contrast, require several operations proportionate to the number of objects in the structure. Hierarchical namespaces keep the data organized, which yields better storage and retrieval performance for an analytical use case and lowers the cost of analysis.\n",
    "In Azure Blob storage, you can store large amounts of unstructured (\"object\") data in a flat namespace within a blob container. Blob names can include \"/\" characters to organize blobs into virtual \"folders\", but in terms of blob manageability the blobs are stored as a single-level hierarchy in a flat namespace.\n",
    "\n",
    "Descriptive analytics, which answers the question “What is happening in my business?”. The data to answer this question is typically answered through the creation of a data warehouse in which historical data is persisted in relational tables for multidimensional modeling and reporting.\n",
    "Diagnostic analytics, which deals with answering the question “Why is it happening?”. This may involve exploring information that already exists in a data warehouse, but typically involves a wider search of your data estate to find more data to support this type of analysis.\n",
    "Predictive analytics, which enables you to answer the question “What is likely to happen in the future based on previous trends and patterns?”\n",
    "Prescriptive analytics, which enables autonomous decision making based on real-time or near real-time analysis of data, using predictive analytics.\n",
    "\n",
    "Data virtualization allows you to interact with data without the need to understand how the data is formatted, structured, or what is its data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a2d95-26be-4123-86f5-6414248a8435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b80e3-2581-4fd8-8f8d-145e9a5aef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD - Build Together, Own it, Lead with Agility, Driven by Curiosity and Always Welcoming\n",
    "Strategic Priorities - Integration, Business Transformation, Diversify Revenue, ESG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2d02a-01fd-46ec-971b-e5000d3af2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RE has issue with Python escape clauses \\, utilize r to make it string literal\n",
    "r\"\\n\"\n",
    "\n",
    "* \\b: Word boundary ensures that we match whole words.\n",
    "* \\w+: Matches one or more word characters (letters, digits, or underscores).\n",
    "* \\b: Another word boundary to ensure the end of the word.\n",
    "* [^\\w\\s]: Matches any character that is not a word character (letters, digits, or underscores) or a whitespace character.\n",
    "* |: OR operator.\n",
    "* [\\d]: Matches any digit.\n",
    "* [^\\w\\s]: Matches any character that is not a word character (letters, digits, or underscores) or a whitespace character.\n",
    "* |: OR operator.\n",
    "* [\\d]: Matches any digit.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Linux\n",
    "Operating Systems are Layered.\n",
    "Kernel is the inter most layer\n",
    "Outermost layer is a GUI\n",
    "Linux can peel off layers, helps towards simplicity\n",
    "GUI can take  ~50% of processing power\n",
    "\n",
    "Linux Distributions\n",
    "GNU (GNU not Unix)\n",
    "- Red Hat (CentOS)\n",
    "- Fedora\n",
    "Debian\n",
    "- Ubuntu\n",
    "- Raspberry Pi\n",
    "- Various IoT\n",
    "- \n",
    "\n",
    "Resources: \n",
    "- Command Line Help: www.explainshell.com\n",
    "\n",
    "\n",
    "Linux Commands\n",
    "man - Manual\n",
    "History - history\n",
    "Clear - clears history\n",
    "Q - quits from current query\n",
    "pwd- current directory\n",
    "~ - short hand for home\n",
    ". Current Directory\n",
    ".. Parent Directory\n",
    "Ls -l adds 6 additional fields \n",
    "ls -lh - adds file sizes\n",
    "First letter - D: Directory, -: File, l - Link, Permission, Number of Hard links, Users who owns it. Size in bytes, date opened or last maintained  \n",
    "Ls -la - shows hidden files\n",
    "sudo = \"super user (root) do\"\n",
    "mkdir - makes new directory\n",
    "rmdir - removes directory\n",
    "rm -r temp_1_3 - recursively removes directories\n",
    "chmod 765 temp_1_1 - change permissions to directory\n",
    "\n",
    "\n",
    "* r = read = 2^2 = 4\n",
    "* w = write = 2^1 = 2\n",
    "* x = execute = 2^0 = 1\n",
    "* 1 = --x\n",
    "* 2 = -w-\n",
    "* 3 = -wx\n",
    "* 4 = r--\n",
    "* 5 = r-x\n",
    "* 6 = rw-\n",
    "* 7 = rwx\n",
    "\n",
    "* u = user\n",
    "* g = group\n",
    "* o = other (warning: NOT owner!)\n",
    "\n",
    "chmod u=rwx,g=rx,o=r *\n",
    "\n",
    "ls -lhR - recursively list out directory tree\n",
    "cp filename - copy\n",
    "rm filename - remove\n",
    "mv filename new_filename- rename\n",
    "\n",
    "head returns the first few lines of a file; optionally pass the number of lines to return\n",
    "tail returns the last few lines of a file; optionally pass the number of lines to return\n",
    "grep matches a pattern in a file and returns lines that match the pattern\n",
    "grep -v returns the opposite, the rows that do NOT match\n",
    "| is a pipeline; Linux spawn a process for each command; Linux creates an \"anonymous pipe\" aka \"un-named pipe\" between the processes; Linux routes standard output to pipes and routes standard input from pipes for intermediate commands\n",
    "diff finds the differences between two files\n",
    "* vi filename\n",
    "* Movement mode: arrow keys, page up, page down, 0 beginning of line, $ end of line, x delete character, dd delete line\n",
    "* Movement mode to insert mode: i for insert, a for append, note that INSERT appears lower left\n",
    "* Insert mode to movement mode: ESC key\n",
    "* Movement mode to command mode: : note that hitting the enter key will run the command and return to movement mode\n",
    "* Save changes: :w\n",
    "* Save a file and exit: :wq\n",
    "* Quit: :q\n",
    "* Quit with unsaved changes: :q!\n",
    "\n",
    "\n",
    "How to Change to Bash\n",
    "chsh -s /bin/bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bd0eff-9d3a-430f-8678-647452eb2112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d_py_functions.Definitions import dictionary_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1952cdbe-2528-45c7-99c6-aa85dfe7d011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Derek'.find('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec25e174-2a5d-458b-a17a-b281fbe1a708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Derek.csv']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from d_py_functions.SharedFolder import ReadDirectory\n",
    "\n",
    "reference = ReadDirectory('//Users//derekdewald//Documents//Python//Github_Repo//Definitions')\n",
    "reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eaf9c202-b626-476d-be28-01e083d8ea2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/derekdewald/Documents/Python/Github_Repo'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59a20b38-d3b3-4f6f-823d-fc788613ea41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Definition</th>\n",
       "      <th>Category</th>\n",
       "      <th>SubCategory</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Links</th>\n",
       "      <th>SKlearn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Derek1</td>\n",
       "      <td>Test</td>\n",
       "      <td>Test</td>\n",
       "      <td>Test</td>\n",
       "      <td>test</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Definition Category SubCategory Notes Links SKlearn\n",
       "0     Derek1     Test        Test  Test  test    Test"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def D_Dictionary(word,\n",
    "                 folder='//Users//derekdewald//Documents//Python//Github_Repo//Definitions//',\n",
    "                 dictionary_definition=dictionary_definition):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(f\"{folder}{word}.csv\")\n",
    "    except:\n",
    "        temp_dict = {}\n",
    "        for key in dictionary_definition.keys():\n",
    "            if key == 'Definition':\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    print(dictionary_definition[key])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                temp_dict[key] = input(f\"Please Select a {key}:\")\n",
    "\n",
    "        final_dict = {word:temp_dict}\n",
    "        df = pd.DataFrame(final_dict).T.reset_index().rename(columns={'index':'Definition'})[list(dictionary_definition.keys())]\n",
    "        df.to_csv(f\"Definitions//{word}.csv\",index=False)\n",
    "        \n",
    "    return df\n",
    "\n",
    "a = D_Dictionary('Derek1')\n",
    "a    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "30e513a9-4bc2-49df-93c0-5bd11604eb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Word</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub Categorization</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>Model</td>\n",
       "      <td>Emsemble</td>\n",
       "      <td>AdaBoost (Adaptive Boosting) is a machine lear...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>ADASYN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Adaptive Synthetic Sampling: generates synthet...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Addition Rule</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P ( AuB) = P(A) + P(B) - P(A intersection B).</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Adjacency Matrix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Occam's Razor</td>\n",
       "      <td>Principle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Philosophical principle that suggests when fac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Normal Distance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Euclidean Distance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Dot Product</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>NaN</td>\n",
       "      <td>AutoRegressive Integrated Moving Average</td>\n",
       "      <td>Model</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Popular statistical method used for time serie...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                      Word   Category  \\\n",
       "0           1.0                                  Accuracy        NaN   \n",
       "1           1.0                                  AdaBoost      Model   \n",
       "2           1.0                                    ADASYN        NaN   \n",
       "3           1.0                             Addition Rule        NaN   \n",
       "4           1.0                          Adjacency Matrix        NaN   \n",
       "..          ...                                       ...        ...   \n",
       "170         NaN                             Occam's Razor  Principle   \n",
       "171         NaN                           Normal Distance        NaN   \n",
       "172         NaN                        Euclidean Distance        NaN   \n",
       "173         NaN                               Dot Product        NaN   \n",
       "174         NaN  AutoRegressive Integrated Moving Average      Model   \n",
       "\n",
       "    Sub Categorization                                         Definition  \\\n",
       "0                  NaN                                                NaN   \n",
       "1             Emsemble  AdaBoost (Adaptive Boosting) is a machine lear...   \n",
       "2                  NaN  Adaptive Synthetic Sampling: generates synthet...   \n",
       "3                  NaN      P ( AuB) = P(A) + P(B) - P(A intersection B).   \n",
       "4                  NaN                                                NaN   \n",
       "..                 ...                                                ...   \n",
       "170                NaN  Philosophical principle that suggests when fac...   \n",
       "171                NaN                                                NaN   \n",
       "172                NaN                                                NaN   \n",
       "173                NaN                                                NaN   \n",
       "174                NaN  Popular statistical method used for time serie...   \n",
       "\n",
       "     Notes  Link  \n",
       "0      NaN   NaN  \n",
       "1      NaN   NaN  \n",
       "2      NaN   NaN  \n",
       "3      NaN   NaN  \n",
       "4      NaN   NaN  \n",
       "..     ...   ...  \n",
       "170    NaN   NaN  \n",
       "171    NaN   NaN  \n",
       "172    NaN   NaN  \n",
       "173    NaN   NaN  \n",
       "174    NaN   NaN  \n",
       "\n",
       "[175 rows x 7 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('test.xlsx',sheet_name='Terminology')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "60248d68-dd32-4f81-b8d7-dc276361068c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Definition</th>\n",
       "      <th>Category</th>\n",
       "      <th>SubCategory</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Links</th>\n",
       "      <th>SKlearn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Derek1</td>\n",
       "      <td>Test</td>\n",
       "      <td>Test</td>\n",
       "      <td>Test</td>\n",
       "      <td>test</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Definition Category SubCategory Notes Links SKlearn\n",
       "0     Derek1     Test        Test  Test  test    Test"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('Definitions/Derek1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c49f22-74f1-4a61-b159-cef932f09452",
   "metadata": {},
   "source": [
    "# Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd371203-0960-47f4-b034-14b9393c172f",
   "metadata": {},
   "source": [
    "1) Streamlit\n",
    "2) Hugging Face\n",
    "3) Cloud Engine Certification\n",
    "4) Automated Function to Download all google sheets somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5ad71-106d-4626-819b-8de45733c293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d86384d7-9db4-49a8-a414-e8f7d6fd9ec0",
   "metadata": {},
   "source": [
    "## Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e82e2-c3d7-4993-9b2f-fa369ad7272e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a32197-73ec-4118-8caf-ea42c322f755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fdff417a-e1f1-4ed5-87ce-742441902a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_definitions['scikit_learn'] = {'Category':\"\",\n",
    "                                    'SubCategory':\"\",\n",
    "                                    'ModelType':\"\",\n",
    "                                    'Definition':\"\",\n",
    "                                    'Images':['scikit_learn_models.png']}\n",
    "\n",
    "data_definitions['Boosting'] = {'Category':\"\",\n",
    "                                'SubCategory':\"\",\n",
    "                                'ModelType':\"\",\n",
    "                                'Definition':\"\",\n",
    "                                'Images':['Boosting_.png']}\n",
    "\n",
    "data_definitions['Bagging'] = {'Category':\"\",\n",
    "                                'SubCategory':\"\",\n",
    "                                'ModelType':\"\",\n",
    "                                'Definition':\"\",\n",
    "                                'Images':['Bagging.png']}\n",
    "\n",
    "data_definitions['Cosine Similiarity'] = {'Category':\"\",\n",
    "                                          'SubCategory':\"\",\n",
    "                                          'ModelType':\"\",\n",
    "                                          'Definition':\"\",\n",
    "                                          'Images':['cosine_similiarity.png']}\n",
    "\n",
    "data_definitions['Kubernetes'] = {'Category':\"\",\n",
    "                                  'SubCategory':\"\",\n",
    "                                  'ModelType':\"\",\n",
    "                                  'Definition':\"\",\n",
    "                                  'Images':['Kubernetes.png']}\n",
    "\n",
    "data_definitions['Anscombe Quartet'] = {'Category':\"\",\n",
    "                                  'SubCategory':\"\",\n",
    "                                  'ModelType':\"\",\n",
    "                                  'Definition':\"\",\n",
    "                                  'Images':['Anscombe_Quartet.png']}\n",
    "\n",
    "data_definitions['Spark'] = {'Category':\"\",\n",
    "                                  'SubCategory':\"\",\n",
    "                                  'ModelType':\"\",\n",
    "                                  'Definition':\"\",\n",
    "                                  'Images':['spark.png']}\n",
    "\n",
    "\n",
    "data_definitions['Harmonic Mean'] = {'Category':\"\",\n",
    "                                  'SubCategory':\"\",\n",
    "                                  'ModelType':\"\",\n",
    "                                  'Definition':\"\",\n",
    "                                  'Images':['harmonic_mean.png']}\n",
    "\n",
    "\n",
    "data_definitions['X Or'] = {'Category':\"\",\n",
    "                                  'SubCategory':\"\",\n",
    "                                  'ModelType':\"\",\n",
    "                                  'Definition':\"\",\n",
    "                                  'Images':['X_OR.png']}\n",
    "\n",
    "data_definitions['Sigmoid'] = {'Category':\"\",\n",
    "                                  'SubCategory':\"\",\n",
    "                                  'ModelType':\"\",\n",
    "                                  'Definition':\"\",\n",
    "                                  'Images':['Sigmoid.png']}\n",
    "\n",
    "data_definitions['Loss Function'] = {'Category':\"\",\n",
    "                                  'SubCategory':\"\",\n",
    "                                  'ModelType':\"\",\n",
    "                                  'Definition':\"Need a systematic method to compare the effictivenes of multiple models, utilize a user defined function which measures for objective analysis\",\n",
    "                                  'Images':['Loss_Functions.png']}\n",
    "\n",
    "data_definitions['Occams Razor'] = {'Category':\"\",\n",
    "                        'SubCategory':\"\",\n",
    "                        'ModelType':\"\",\n",
    "                        'Definition':\"All else equal, a simpler explanation is better.\",\n",
    "                        'Image':[]}\n",
    "\n",
    "\n",
    "data_definitions['Variance Bias Trade Off'] = {'Category':\"\",\n",
    "                                               'SubCategory':\"\",\n",
    "                                               'ModelType':\"\",\n",
    "                                               'Definition':\"\",\n",
    "                                               'Image':['Variamce_Not_Equal.png','bias.png','bias1.png']}\n",
    "\n",
    "\n",
    "data_definitions['Machine Learning'] = {'Category':\"\",\n",
    "                                               'SubCategory':\"\",\n",
    "                                               'ModelType':\"\",\n",
    "                                               'Definition':\"\",\n",
    "                                               'Image':['ML_AI_Significance_Chart.png','AI_ML_DL_Visual.png']}\n",
    "\n",
    "\n",
    "data_definitions['Biparte Graph'] = {'Category':\"\",\n",
    "                                               'SubCategory':\"\",\n",
    "                                               'ModelType':\"\",\n",
    "                                               'Definition':\"\",\n",
    "                                               'Image':['Biparte_Graph.png']}\n",
    "\n",
    "data_definitions['Logistic Regression'] = {'Category':\"\",\n",
    "                                               'SubCategory':\"\",\n",
    "                                               'ModelType':\"\",\n",
    "                                               'Definition':\"\",\n",
    "                                               'Image':['MultiClass_LogisticReg.png']}\n",
    "\n",
    "data_definitions['Linear Regression'] = {'Category':\"\",\n",
    "                                               'SubCategory':\"\",\n",
    "                                               'ModelType':\"\",\n",
    "                                               'Definition':\"\",\n",
    "                                               'Image':['ImpactOutliers.png']}\n",
    "\n",
    "data_definitions['MNist'] = {'Category':\"\",\n",
    "                             'SubCategory':\"\",\n",
    "                             'ModelType':\"\",\n",
    "                             'Definition':\"\",\n",
    "                             'Image':['ImpactOutliers.png','MNist_Results.png']}\n",
    "\n",
    "data_definitions['Optimizers'] = {'Category':\"\",\n",
    "                             'SubCategory':\"\",\n",
    "                             'ModelType':\"\",\n",
    "                             'Definition':\"\",\n",
    "                             'Image':['Optimizers.png']}\n",
    "\n",
    "data_definitions['Correlation'] = {'Category':\"\",\n",
    "                                   'SubCategory':\"\",\n",
    "                                   'ModelType':\"\",\n",
    "                                   'Definition':\"\",\n",
    "                                   'Image':['Correlation_Visual.png']}\n",
    "\n",
    "\n",
    "data_definitions['Data Visualization'] = {'Category':\"\",\n",
    "                                          'SubCategory':\"\",\n",
    "                                          'ModelType':\"\",\n",
    "                                          'Definition':\"Allows people to offload cognition and processing from prefrontal cortex to visual receptors organized by spatial location. Serves as a high definition bandwidth to the brain. When creating a visualization, must consider what your intuitive goal and expectation is. Satify vs Optimize. Set of possiblilities: Known, Proposed, Solution. What data the user sees, why the user intends to use a vis tool, and how the visual encoding and interaction idioms are constructed on terms of design choice. Distinct Actions, Analyze, Search, Query\",\n",
    "                                          'Image':[]}\n",
    "\n",
    "\n",
    "data_definitions['Hyperparameter Tuning'] = {'Category':\"\",\n",
    "                                          'SubCategory':\"\",\n",
    "                                          'ModelType':\"\",\n",
    "                                          'Definition':\"\",\n",
    "                                          'Image':[]}\n",
    "\n",
    "data_definitions['Grid Search'] = {'Category':\"Hyperparameter Tuning\",\n",
    "                                          'SubCategory':\"\",\n",
    "                                          'ModelType':\"\",\n",
    "                                          'Definition':\"\",\n",
    "                                          'Image':[]}\n",
    "\n",
    "data_definitions['Random Search'] = {'Category':\"Hyperparameter Tuning\",\n",
    "                                          'SubCategory':\"\",\n",
    "                                          'ModelType':\"\",\n",
    "                                          'Definition':\"\",\n",
    "                                          'Image':[]}\n",
    "\n",
    "data_definitions['Bayesian'] = {'Category':\"Hyperparameter Tuning\",\n",
    "                                          'SubCategory':\"\",\n",
    "                                          'ModelType':\"\",\n",
    "                                          'Definition':\"\",\n",
    "                                          'Image':[]}\n",
    "\n",
    "data_definitions['Tree Structured Parzen Estimator Search'] = {'Category':\"Hyperparameter Tuning\",\n",
    "                                                               'SubCategory':\"\",\n",
    "                                                               'ModelType':\"\",\n",
    "                                                               'Definition':\"\",\n",
    "                                                               'Image':[]}\n",
    "\n",
    "data_definitions['Activation Functions'] = {'Category':\"\",\n",
    "                                                               'SubCategory':\"\",\n",
    "                                                               'ModelType':\"\",\n",
    "                                                               'Definition':\"\",\n",
    "                                                               'Image':[]}\n",
    "\n",
    "data_definitions['Parametric'] = {'Category':\"\",\n",
    "                                  'SubCategory':\"\",\n",
    "                                  'ModelType':\"\",\n",
    "                                  'Definition':\"\",\n",
    "                                  'Image':[]}\n",
    "\n",
    "data_definitions['Non Parametric'] = {'Category':\"\",\n",
    "                                  'SubCategory':\"\",\n",
    "                                  'ModelType':\"\",\n",
    "                                  'Definition':\"\",\n",
    "                                  'Image':[]}\n",
    "\n",
    "data_definitions['Neural Networks'] = {'Category':\"\",\n",
    "                                       'SubCategory':\"\",\n",
    "                                       'ModelType':\"\",\n",
    "                                       'Definition':\"First Appeared in 2011\",\n",
    "                                       'Image':[]}\n",
    "\n",
    "data_definitions['Softmax'] = {'Category':\"\",\n",
    "                                       'SubCategory':\"\",\n",
    "                                       'ModelType':\"\",\n",
    "                                       'Definition':\"\",\n",
    "                                       'Image':[]}\n",
    "\n",
    "data_definitions['Entopy'] = {'Category':\"\",\n",
    "                                       'SubCategory':\"\",\n",
    "                                       'ModelType':\"\",\n",
    "                                       'Definition':\"\",\n",
    "                                       'Image':[]}\n",
    "\n",
    "\n",
    "data_definitions['Ngram'] = {'Category':\"\",\n",
    "                                       'SubCategory':\"\",\n",
    "                                       'ModelType':\"\",\n",
    "                                       'Definition':\"\",\n",
    "                                       'Image':[]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d17222-0e9b-4330-a84b-0bdbf93723ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd14011-d9d7-4947-a944-10b0015c221c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fad07-2200-4216-8aa1-774ef5005213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5013b91-375a-470b-8a81-90290eeaf185",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b9d8f-6282-4d8d-b5f1-af51265dcdfa",
   "metadata": {},
   "source": [
    "\n",
    "### Model\n",
    "$f(x) = w_0 + w_1X$\n",
    "\n",
    "### Parameters \n",
    "\n",
    "$[w_0, w_1]$\n",
    "\n",
    "### Loss\n",
    "$J(w_0,w_1) = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - (w_0 + w_1 x_i) \\right)^2$\n",
    "\n",
    "### Objective \n",
    "Minimize $J(w_0,w_1)$\n",
    "\n",
    "### Mean Square Error\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f3f82-e081-43c4-bcf0-e5edcfb617dd",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fd8c5-2c59-4b91-9f3b-5631318b9f71",
   "metadata": {},
   "source": [
    "Linear Regression : $\\hat{y} = xw^T + b$ <br>\n",
    "Logistic Regression: $\\hat{y} = \\frac{1}{1 + e^({-xw^T + b})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ce69776-b4c7-4706-8628-73dbe1dd65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2308de-9437-4c0f-9292-72e98ee383db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14156be-49b7-4853-9f06-c0f389ff4b59",
   "metadata": {},
   "source": [
    "## Gradient Descent \n",
    "\n",
    "Predicitions: $\\sigma(XW^T)$\n",
    "\n",
    "Differences: $\\sigma(XW^T) - Y$\n",
    "\n",
    "Gradient: $\\frac{1}{m} (\\sigma(XW^T) -Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afc58d8-dad3-4813-8d86-5d72ecd3cba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9368f-34f7-4e15-b47a-6cda3ea23052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63112174-349b-4a6b-8648-a28fd09b29d2",
   "metadata": {},
   "source": [
    "$\\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f27cee-cf11-4cd8-acf2-848d970c2793",
   "metadata": {},
   "source": [
    "$\\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^k y_{ij} \\log(\\hat{y}_{ij})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4bfc285-7988-491a-9edf-dcf11fb72f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Softmax Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb27f73-0960-42b2-bd53-132ba474c193",
   "metadata": {},
   "source": [
    "$\\sigma(z)_i = \\frac{\\sigma(xW^T_j)}{\\sum_{j=1}^k \\sigma(xW^T_j)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "578c7716-f489-4d25-abec-1d1f93ce316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771233e3-b440-49cc-b895-a529dde00e9a",
   "metadata": {},
   "source": [
    "$h(x) = f(g(x))$\n",
    "\n",
    "\n",
    "$h'(x) = f'(g(x)) \\cdot g'(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d11698-c4e1-4ae0-9632-c475d9f887da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d3a47-7565-4d2b-8a34-e4a1b4cf8d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f75f2b-f398-42e9-9297-17e241f4a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boosting. Train Additive Models in Series Where Each Predicts the Residual from the Previous Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c0e1e-43ee-4613-a1d0-289767ab021c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af72fb-e284-4c4e-9408-bed6c0f1bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bagging. Trains Models in Parallel Via Bootstrap Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c9d7d-9dc2-4b2d-af80-051e671ec9aa",
   "metadata": {},
   "source": [
    "## Distance Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39068a-b22a-4dfa-8336-8fc6327d5016",
   "metadata": {},
   "source": [
    "$ ||A|| = \\sqrt{\\sum \\left (A_i \\right)^2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a75dcf-0790-4036-9e3b-95fbdde9aaa3",
   "metadata": {},
   "source": [
    "$ L_2(A,B) = \\sqrt{\\sum \\left( A_i - B_i \\right)^2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab53a3f-3bdd-4812-8f26-b28f3061d4d6",
   "metadata": {},
   "source": [
    "$\\ A \\cdot B = \\sum_{i=1}^n A_i B_i = ||A|| ||B|| cos(\\theta)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a144d203-f90d-4beb-a390-bb5e8deb4bdc",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acdf390-b24f-4309-9e6b-b164c8844568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90908c98-49e5-4332-a7cc-aec512fbb66e",
   "metadata": {},
   "source": [
    "## Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e5a0d-8ab1-4064-b2f6-eceb0bc2a210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
