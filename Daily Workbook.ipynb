{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  List all Python Functions.\n",
    "## Get Chat GPT to talk about Object Oriented approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d5f72e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d_py_functions.StaticValues import ParamterMapping\n",
    "\n",
    "df = ParamterMapping('ProcessSheet')\n",
    "df1 = df[df['Process']=='Machine Learning Project'].drop('Process',axis=1)\n",
    "\n",
    "\n",
    "from d_py_functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "20da5c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<h4>1. Problem Definition</h4>\\n<ul>\\n  <li>Clearly Defined Problem Statement</li>\\n    <ul><li>Problem statement should include a clear, specific, well considered, and relate to a meaningful solvable, problem. Additionally, the problem statement should highlight why the issue is relavant, and how the problem is known to exist (Facts, Stats, Magnitude).</li></ul>\\n  <li>Current State</li>\\n    <ul><li>It is crirically important that problems are substantiated, clear, and comprehensively understood, at both executive and L6 levels. It is important to have 360 perspective of what the problem is, not simply 1 persons, or a small groups perspective (regardless of how influential that individual is).</li></ul>\\n  <li>Historical Context</li>\\n    <ul><li>You can not create a best in class solution, without knowing what has come before, why those solutoins did not fully satisfy, and where the risks lie.</li></ul>\\n</ul>\\n<h4>2. Problem Desired End State</h4>\\n<ul>\\n  <li>Specific Description of Requirements, Desires and Expectations.</li>\\n    <ul><li>Needs to be very clear, such that a proposed solution can be measured as to whether it meets the criteria, while all KPIs are not necessarily defined yet several primary related to needs and expectations should be defined, and considered for plausibility. </li></ul>\\n</ul>\\n<h4>3. Model Conceptualization</h4>\\n<ul>\\n  <li>on problem statement and desired end state, conceptualize the types of solutions and models which could be developed and implemente</li>\\n  <li>Identify types of Models and Architectures which might be relevant in solving problem and delievering fundamental components of desired end state</li>\\n  <li>Estimate baseline development cost for each model architecture</li>\\n  <li>Identify and define model evaluation criteria and MVP performance requirements</li>\\n  <li>Ensure to include a technical description of why the particular model type is appropriate or not in the context of desired solution</li>\\n  <li>If any core components of desired end state are missing from solutions, validate with Stakeholders to clarify before proceeding forward</li>\\n  <li>Where possible, try to retain as much flexibility and optionality as possible, explicilty noting development time required for baseline solution</li>\\n</ul>\\n<h4>4. Data Collection</h4>\\n<ul>\\n  <li>Identify Data Elements which are required, which are available and which need to be sourced</li>\\n  <li>List of All Required Data</li>\\n  <li>Completed Dictionary for all data elements included</li>\\n  <li>For elements which are not available, ensure individuals are assigned as Data Stewards and a delivery dates (including refresh requirements are identified)</li>\\n  <li>For new data, ensure Data Validaion, Quality and refresh frequency are explicitly dicussed and ownership for each assigned</li>\\n</ul>\\n<h4>5. EDA and Data Preprocessing</h4>\\n<ul>\\n  <li>Explore dataset which are to be used for model creation once creat</li>\\n  <li>Complete EDA Checklist</li>\\n  <li>Insights report</li>\\n</ul>\\n<h4>6. Baseline Model Selection & Validation</h4>\\n<ul>\\n  <li>Run initial ML pipeline, review outputs, validate model, features and preliminary results before tuning and finalization</li>\\n  <li>Identify which models have best performance, rationalize performance</li>\\n  <li>Identify Feature Importance, and determine if model structure is appropriate or changes are appropriate</li>\\n  <li>Test alternative feature structure, explore feature engineering and explore for any unknown, or unexpected results</li>\\n</ul>\\n<h4>7. Tune Model</h4>\\n<ul>\\n  <li>Hyperparameter tune models which meet MVP requirements, appear to be best in class and viable for implementation</li>\\n</ul>\\n'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "CreateMarkdownfromProcess('Machine Learning Project','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d30639d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>1. Check Data for Completeness</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>2. Review Scatter Plot</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>3. Review Correlation</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>4. Review for Multicollinearity</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>5. Review Histogram</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>6. Check Skewness, Kurtosis</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>7. Check Independence</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>8. Determine if Scaling Necessary</h4>\n",
       "<ul>\n",
       "</ul>\n",
       "<h4>9. Review Non Linear Combinations?</h4>\n",
       "<ul>\n",
       "</ul>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CreateMarkdownfromProcess('EDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "031a8e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>1. Problem Definition</h4>\n",
       "<ul>\n",
       "  <li>Clearly Defined Problem Statement</li>\n",
       "    <ul><li>Problem statement should include a clear, specific, well considered, and relate to a meaningful solvable, problem. Additionally, the problem statement should highlight why the issue is relavant, and how the problem is known to exist (Facts, Stats, Magnitude).</li></ul>\n",
       "  <li>Current State</li>\n",
       "    <ul><li>It is crirically important that problems are substantiated, clear, and comprehensively understood, at both executive and L6 levels. It is important to have 360 perspective of what the problem is, not simply 1 persons, or a small groups perspective (regardless of how influential that individual is).</li></ul>\n",
       "  <li>Historical Context</li>\n",
       "    <ul><li>You can not create a best in class solution, without knowing what has come before, why those solutoins did not fully satisfy, and where the risks lie.</li></ul>\n",
       "</ul>\n",
       "<h4>2. Problem Desired End State</h4>\n",
       "<ul>\n",
       "  <li>Specific Description of Requirements, Desires and Expectations.</li>\n",
       "    <ul><li>Needs to be very clear, such that a proposed solution can be measured as to whether it meets the criteria, while all KPIs are not necessarily defined yet several primary related to needs and expectations should be defined, and considered for plausibility. </li></ul>\n",
       "</ul>\n",
       "<h4>3. Model Conceptualization</h4>\n",
       "<ul>\n",
       "  <li>on problem statement and desired end state, conceptualize the types of solutions and models which could be developed and implemente</li>\n",
       "  <li>Identify types of Models and Architectures which might be relevant in solving problem and delievering fundamental components of desired end state</li>\n",
       "  <li>Estimate baseline development cost for each model architecture</li>\n",
       "  <li>Identify and define model evaluation criteria and MVP performance requirements</li>\n",
       "  <li>Ensure to include a technical description of why the particular model type is appropriate or not in the context of desired solution</li>\n",
       "  <li>If any core components of desired end state are missing from solutions, validate with Stakeholders to clarify before proceeding forward</li>\n",
       "  <li>Where possible, try to retain as much flexibility and optionality as possible, explicilty noting development time required for baseline solution</li>\n",
       "</ul>\n",
       "<h4>4. Data Collection</h4>\n",
       "<ul>\n",
       "  <li>Identify Data Elements which are required, which are available and which need to be sourced</li>\n",
       "  <li>List of All Required Data</li>\n",
       "  <li>Completed Dictionary for all data elements included</li>\n",
       "  <li>For elements which are not available, ensure individuals are assigned as Data Stewards and a delivery dates (including refresh requirements are identified)</li>\n",
       "  <li>For new data, ensure Data Validaion, Quality and refresh frequency are explicitly dicussed and ownership for each assigned</li>\n",
       "</ul>\n",
       "<h4>5. EDA and Data Preprocessing</h4>\n",
       "<ul>\n",
       "  <li>Explore dataset which are to be used for model creation once creat</li>\n",
       "  <li>Complete EDA Checklist</li>\n",
       "  <li>Insights report</li>\n",
       "</ul>\n",
       "<h4>6. Baseline Model Selection & Validation</h4>\n",
       "<ul>\n",
       "  <li>Run initial ML pipeline, review outputs, validate model, features and preliminary results before tuning and finalization</li>\n",
       "  <li>Identify which models have best performance, rationalize performance</li>\n",
       "  <li>Identify Feature Importance, and determine if model structure is appropriate or changes are appropriate</li>\n",
       "  <li>Test alternative feature structure, explore feature engineering and explore for any unknown, or unexpected results</li>\n",
       "</ul>\n",
       "<h4>7. Tune Model</h4>\n",
       "<ul>\n",
       "  <li>Hyperparameter tune models which meet MVP requirements, appear to be best in class and viable for implementation</li>\n",
       "</ul>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CreateMarkdown(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8dfc111e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "</ul>\n",
       "<h4>1. Problem Definition</h4>\n",
       "<ul>\n",
       "<li>Clearly Defined Problem Statement</li>\n",
       "<ul>\n",
       "<li>Problem statement should include a clear, specific, well considered, and relate to a meaningful solvable, problem. Additionally, the problem statement should highlight why the issue is relavant, and how the problem is known to exist (Facts, Stats, Magnitude).</li>\n",
       "</ul>\n",
       "<li>Current State</li>\n",
       "<ul>\n",
       "<li>It is crirically important that problems are substantiated, clear, and comprehensively understood, at both executive and L6 levels. It is important to have 360 perspective of what the problem is, not simply 1 persons, or a small groups perspective (regardless of how influential that individual is).</li></ul><li>Historical Context</li>\n",
       "<ul>\n",
       "<li>You can not create a best in class solution, without knowing what has come before, why those solutoins did not fully satisfy, and where the risks lie.</li></ul></ul>\n",
       "<h4>2. Problem Desired End State</h4>\n",
       "<ul>\n",
       "<li>Specific Description of Requirements, Desires and Expectations.</li>\n",
       "<ul>\n",
       "<li>Needs to be very clear, such that a proposed solution can be measured as to whether it meets the criteria, while all KPIs are not necessarily defined yet several primary related to needs and expectations should be defined, and considered for plausibility. </li>\n",
       "</ul>\n",
       "</ul>\n",
       "<h4>3. Model Conceptualization</h4>\n",
       "<ul>\n",
       "<li>on problem statement and desired end state, conceptualize the types of solutions and models which could be developed and implemente</li>\n",
       "<ul>\n",
       "<li>Identify types of Models and Architectures which might be relevant in solving problem and delievering fundamental components of desired end state</li>\n",
       "<li>Estimate baseline development cost for each model architecture</li>\n",
       "<li>Identify and define model evaluation criteria and MVP performance requirements</li>\n",
       "<li>Ensure to include a technical description of why the particular model type is appropriate or not in the context of desired solution</li>\n",
       "<li>If any core components of desired end state are missing from solutions, validate with Stakeholders to clarify before proceeding forward</li>\n",
       "<li>Where possible, try to retain as much flexibility and optionality as possible, explicilty noting development time required for baseline solution</li>\n",
       "</ul>\n",
       "<h4>4. Data Collection</h4>\n",
       "<ul>\n",
       "<li>Identify Data Elements which are required, which are available and which need to be sourced</li>\n",
       "<ul>\n",
       "<li>List of All Required Data</li>\n",
       "<li>Completed Dictionary for all data elements included</li>\n",
       "<li>For elements which are not available, ensure individuals are assigned as Data Stewards and a delivery dates (including refresh requirements are identified)</li>\n",
       "<li>For new data, ensure Data Validaion, Quality and refresh frequency are explicitly dicussed and ownership for each assigned</li>\n",
       "</ul>\n",
       "<h4>5. EDA and Data Preprocessing</h4>\n",
       "<ul>\n",
       "<li>Explore dataset which are to be used for model creation once creat</li>\n",
       "<ul>\n",
       "<li>Complete EDA Checklist</li>\n",
       "<li>Insights report</li>\n",
       "</ul>\n",
       "<h4>6. Baseline Model Selection & Validation</h4>\n",
       "<ul>\n",
       "<li>Run initial ML pipeline, review outputs, validate model, features and preliminary results before tuning and finalization</li>\n",
       "<ul>\n",
       "<li>Identify which models have best performance, rationalize performance</li>\n",
       "<li>Identify Feature Importance, and determine if model structure is appropriate or changes are appropriate</li>\n",
       "<li>Test alternative feature structure, explore feature engineering and explore for any unknown, or unexpected results</li>\n",
       "</ul>\n",
       "<h4>7. Tune Model</h4>\n",
       "<ul>\n",
       "<li>Hyperparameter tune models which meet MVP requirements, appear to be best in class and viable for implementation</li>\n",
       "<ul>\n",
       "</ul>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "title = \"\"\n",
    "step_number = 1\n",
    "text = \"\"\n",
    "\n",
    "l2_bullet = '-'  # Level 2 Bullet\n",
    "l3_bullet = '*'  # Level 3 Bullet\n",
    "\n",
    "for index, row in df1.iterrows():\n",
    "    # Need to identify if we have a New Title, if Yes, then we need to create a Title\n",
    "    if (title == \"\") | (title != row.iloc[0]):\n",
    "        if title != row.iloc[0]: \n",
    "            text += \"</ul>\\n\"\n",
    "            \n",
    "        text += f\"<h4>{step_number}. {row.iloc[0]}</h4>\\n<ul>\\n\"  # Open L2 list\n",
    "        step_number += 1  # Increment the step number for the next objective\n",
    "        title = row.iloc[0]  # Define title to be last recorded value\n",
    "        \n",
    "        if len(row.iloc[1]) != 0:\n",
    "            text += f\"<li>{row.iloc[1]}</li>\\n<ul>\\n\"  # Nested list for L3\n",
    "        if len(row.iloc[2]) != 0:\n",
    "            text += f\"<li>{row.iloc[2]}</li>\\n</ul>\\n\"  # Properly indented under L2\n",
    "        \n",
    "    else:  # Continue under existing title\n",
    "        if len(row.iloc[1]) != 0:\n",
    "            text += f\"<li>{row.iloc[1]}</li>\\n\"  # Nested list for L3\n",
    "            if len(row.iloc[2]) != 0:\n",
    "                text += \"<ul>\\n\"\n",
    "                text += f\"<li>{row.iloc[2]}</li></ul>\"  # Properly indented under L2\n",
    "\n",
    "text += \"</ul>\\n\"  # Close the final unordered list\n",
    "\n",
    "# Display the formatted HTML output in Jupyter Notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f835bb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<h2>1. Problem Definition</h2><br>- Clearly Defined Problem Statement<br>Problem statement should include a clear, specific, well\\n    considered, and relate to a meaningful solvable,\\n    problem. Additionally, the problem statement should\\n    highlight why the issue is relavant, and how the problem\\n    is known to exist (Facts, Stats, Magnitude).<br>  - Current State<br>It is crirically important that problems are substantiated,\\n    clear, and comprehensively understood, at both executive\\n    and L6 levels. It is important to have 360 perspective\\n    of what the problem is, not simply 1 persons, or a small\\n    groups perspective (regardless of how influential that\\n    individual is).<br>  - Historical Context<br>You can not create a best in class solution, without knowing\\n    what has come before, why those solutoins did not fully\\n    satisfy, and where the risks lie.<br><h2>2. Problem Desired End State</h2><br>- Specific Description of Requirements, Desires and Expectations.<br>Needs to be very clear, such that a proposed solution can be\\n    measured as to whether it meets the criteria, while all\\n    KPIs are not necessarily defined yet several primary\\n    related to needs and expectations should be defined, and\\n    considered for plausibility.<br><h2>3. Model Conceptualization</h2><br>- on problem statement and desired end state, conceptualize the types of solutions and models which could be developed and implemente<br>  - Identify types of Models and Architectures which might be relevant in solving problem and delievering fundamental components of desired end state<br>  - Estimate baseline development cost for each model architecture<br>  - Identify and define model evaluation criteria and MVP performance requirements<br>  - Ensure to include a technical description of why the particular model type is appropriate or not in the context of desired solution<br>  - If any core components of desired end state are missing from solutions, validate with Stakeholders to clarify before proceeding forward<br>  - Where possible, try to retain as much flexibility and optionality as possible, explicilty noting development time required for baseline solution<br><h2>4. Data Collection</h2><br>- Identify Data Elements which are required, which are available and which need to be sourced<br>  - List of All Required Data<br>  - Completed Dictionary for all data elements included<br>  - For elements which are not available, ensure individuals are assigned as Data Stewards and a delivery dates (including refresh requirements are identified)<br>  - For new data, ensure Data Validaion, Quality and refresh frequency are explicitly dicussed and ownership for each assigned<br><h2>5. EDA and Data Preprocessing</h2><br>- Explore dataset which are to be used for model creation once creat<br>  - Complete EDA Checklist<br>  - Insights report<br><h2>6. Baseline Model Selection & Validation</h2><br>- Run initial ML pipeline, review outputs, validate model, features and preliminary results before tuning and finalization<br>  - Identify which models have best performance, rationalize performance<br>  - Identify Feature Importance, and determine if model structure is appropriate or changes are appropriate<br>  - Test alternative feature structure, explore feature engineering and explore for any unknown, or unexpected results<br><h2>7. Tune Model</h2><br>- Hyperparameter tune models which meet MVP requirements, appear to be best in class and viable for implementation<br>'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "020f3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54834a28",
   "metadata": {},
   "source": [
    "<h2>1. Problem Definition</h2><br>- Clearly Defined Problem Statement<br>Problem statement should include a clear, specific, well\\n    considered, and relate to a meaningful solvable,\\n    problem. Additionally, the problem statement should\\n    highlight why the issue is relavant, and how the problem\\n    is known to exist (Facts, Stats, Magnitude).<br>  - Current State<br>It is crirically important that problems are substantiated,\\n    clear, and comprehensively understood, at both executive\\n    and L6 levels. It is important to have 360 perspective\\n    of what the problem is, not simply 1 persons, or a small\\n    groups perspective (regardless of how influential that\\n    individual is).<br>  - Historical Context<br>You can not create a best in class solution, without knowing\\n    what has come before, why those solutoins did not fully\\n    satisfy, and where the risks lie.<br><h2>2. Problem Desired End State</h2><br>- Specific Description of Requirements, Desires and Expectations.<br>Needs to be very clear, such that a proposed solution can be\\n    measured as to whether it meets the criteria, while all\\n    KPIs are not necessarily defined yet several primary\\n    related to needs and expectations should be defined, and\\n    considered for plausibility.<br><h2>3. Model Conceptualization</h2><br>- on problem statement and desired end state, conceptualize the types of solutions and models which could be developed and implemente<br>  - Identify types of Models and Architectures which might be relevant in solving problem and delievering fundamental components of desired end state<br>  - Estimate baseline development cost for each model architecture<br>  - Identify and define model evaluation criteria and MVP performance requirements<br>  - Ensure to include a technical description of why the particular model type is appropriate or not in the context of desired solution<br>  - If any core components of desired end state are missing from solutions, validate with Stakeholders to clarify before proceeding forward<br>  - Where possible, try to retain as much flexibility and optionality as possible, explicilty noting development time required for baseline solution<br><h2>4. Data Collection</h2><br>- Identify Data Elements which are required, which are available and which need to be sourced<br>  - List of All Required Data<br>  - Completed Dictionary for all data elements included<br>  - For elements which are not available, ensure individuals are assigned as Data Stewards and a delivery dates (including refresh requirements are identified)<br>  - For new data, ensure Data Validaion, Quality and refresh frequency are explicitly dicussed and ownership for each assigned<br><h2>5. EDA and Data Preprocessing</h2><br>- Explore dataset which are to be used for model creation once creat<br>  - Complete EDA Checklist<br>  - Insights report<br><h2>6. Baseline Model Selection & Validation</h2><br>- Run initial ML pipeline, review outputs, validate model, features and preliminary results before tuning and finalization<br>  - Identify which models have best performance, rationalize performance<br>  - Identify Feature Importance, and determine if model structure is appropriate or changes are appropriate<br>  - Test alternative feature structure, explore feature engineering and explore for any unknown, or unexpected results<br><h2>7. Tune Model</h2><br>- Hyperparameter tune models which meet MVP requirements, appear to be best in class and viable for implementation<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e8dc90",
   "metadata": {},
   "source": [
    "title = \"\"\n",
    "step_number = 1\n",
    "text = \"\"\n",
    "\n",
    "for index,row in df1.iterrows():\n",
    "    # Need to identify if we have a New Title, if Yes, then we need to create a Title\n",
    "    if (title==\"\"):\n",
    "        text += f\"<h4>{step_number}. {row[0]}</h4>\"\n",
    "        step_number += 1  # Increment the step number for the next objective\n",
    "    elif (title != row[0]): # New Title\n",
    "        text += f\"<h4>{step_number}. {row[0]}</h4>\"\n",
    "        step_number += 1  # Increment the step number for the next objective\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b950475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EmployeeID     Name Department  Salary\n",
      "0           2      Bob         IT   80000\n",
      "1           3  Charlie    Finance   75000\n",
      "2           5      Eve         IT   85000\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create an in-memory SQLite database\n",
    "conn = sqlite3.connect(\":memory:\")  # Temporary database stored in RAM\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Step 2: Create a sample table\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE Employees (\n",
    "        EmployeeID INTEGER PRIMARY KEY,\n",
    "        Name TEXT,\n",
    "        Department TEXT,\n",
    "        Salary INTEGER\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Step 3: Insert sample data\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\", 60000),\n",
    "    (2, \"Bob\", \"IT\", 80000),\n",
    "    (3, \"Charlie\", \"Finance\", 75000),\n",
    "    (4, \"David\", \"HR\", 62000),\n",
    "    (5, \"Eve\", \"IT\", 85000)\n",
    "]\n",
    "\n",
    "cursor.executemany(\"INSERT INTO Employees VALUES (?, ?, ?, ?)\", data)\n",
    "conn.commit()\n",
    "\n",
    "# Step 4: Run SQL queries\n",
    "query = \"SELECT * FROM Employees WHERE Salary > 70000;\"\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Step 5: Display results\n",
    "print(df)\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "913a0791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Department</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>IT</td>\n",
       "      <td>80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>Finance</td>\n",
       "      <td>75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Eve</td>\n",
       "      <td>IT</td>\n",
       "      <td>85000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeID     Name Department  Salary\n",
       "0           2      Bob         IT   80000\n",
       "1           3  Charlie    Finance   75000\n",
       "2           5      Eve         IT   85000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b86345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c7049",
   "metadata": {},
   "outputs": [],
   "source": [
    "- EDA\n",
    "- Feature Enginnering\n",
    "- Classification, Regresion, Clustering, Anomly Detection\n",
    "- Bayesian Statistics, Probability, A/B Testing\n",
    "\n",
    "- Scikit-Learn, PyTorch, TensorFlow\n",
    "- Efficient, clean, Scalable code\n",
    "- SQL\n",
    "- ETL Pipeline\n",
    "- PySpark\n",
    "- Cloud\n",
    "- Deployment & MLops (CI/DD)\n",
    "- Graph Theory\n",
    "- Time Series\n",
    "- Deep Learning\n",
    "\n",
    "\n",
    "- Curiosity & Problem Solving\n",
    "- Communicaiton & Story Telling\n",
    "- Resilience & Adaptability\n",
    "- Ethical Awareness\n",
    "\n",
    "- Automate Boring THings\n",
    "- Learn Continously\n",
    "- Think End to End\n",
    "- Reusable Pipeline\n",
    "\n",
    "\n",
    "- Real World Projects which demonstrate these skills/ attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0caa5ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "753615ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Process</th>\n",
       "      <th>Header</th>\n",
       "      <th>SubHeader</th>\n",
       "      <th>Bullet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Machine Learning Project</td>\n",
       "      <td>Problem Definition</td>\n",
       "      <td>Objective</td>\n",
       "      <td>Define problem, in a clear and explicit way su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Machine Learning Project</td>\n",
       "      <td>Problem Definition</td>\n",
       "      <td>Deliverable</td>\n",
       "      <td>Define the problem statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Machine Learning Project</td>\n",
       "      <td>Problem Definition</td>\n",
       "      <td>Deliverable</td>\n",
       "      <td>Identify why problem statement matters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Machine Learning Project</td>\n",
       "      <td>Problem Definition</td>\n",
       "      <td>Guidance</td>\n",
       "      <td>Define the desired end state, in such a way th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Machine Learning Project</td>\n",
       "      <td>Problem Desired End State</td>\n",
       "      <td>Objective</td>\n",
       "      <td>Define the desired end state, in such a way th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Machine Learning Project</td>\n",
       "      <td>Problem Desired End State</td>\n",
       "      <td>Deliverable</td>\n",
       "      <td>Define the desired end state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Machine Learning Project</td>\n",
       "      <td>Problem Desired End State</td>\n",
       "      <td>Deliverable</td>\n",
       "      <td>Articulate how the end state differs from the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Machine Learning Project</td>\n",
       "      <td>Problem Desired End State</td>\n",
       "      <td>Deliverable</td>\n",
       "      <td>Establish key performance indicators (KPIs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Machine Learning Project</td>\n",
       "      <td>Model Conceptualization</td>\n",
       "      <td>Objective</td>\n",
       "      <td>on problem statement and desired end state, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Machine Learning Project</td>\n",
       "      <td>Model Conceptualization</td>\n",
       "      <td>Deliverable</td>\n",
       "      <td>Identify types of Models and Architectures whi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Process                     Header    SubHeader  \\\n",
       "0  Machine Learning Project         Problem Definition    Objective   \n",
       "1  Machine Learning Project         Problem Definition  Deliverable   \n",
       "2  Machine Learning Project         Problem Definition  Deliverable   \n",
       "3  Machine Learning Project         Problem Definition     Guidance   \n",
       "4  Machine Learning Project  Problem Desired End State    Objective   \n",
       "5  Machine Learning Project  Problem Desired End State  Deliverable   \n",
       "6  Machine Learning Project  Problem Desired End State  Deliverable   \n",
       "7  Machine Learning Project  Problem Desired End State  Deliverable   \n",
       "8  Machine Learning Project    Model Conceptualization    Objective   \n",
       "9  Machine Learning Project    Model Conceptualization  Deliverable   \n",
       "\n",
       "                                              Bullet  \n",
       "0  Define problem, in a clear and explicit way su...  \n",
       "1                       Define the problem statement  \n",
       "2             Identify why problem statement matters  \n",
       "3  Define the desired end state, in such a way th...  \n",
       "4  Define the desired end state, in such a way th...  \n",
       "5                       Define the desired end state  \n",
       "6  Articulate how the end state differs from the ...  \n",
       "7        Establish key performance indicators (KPIs)  \n",
       "8  on problem statement and desired end state, co...  \n",
       "9  Identify types of Models and Architectures whi...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vSbgjQNDbwl_UjsXd-zN6dCDofE_mdHJli1kPPp5bmv6gagoT8CEGMa38UWdJ4B9GHXd_ULozunfX1h/pub?output=csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc8a70f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markdown presentation saved as ML_Project_Presentation.md\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate Markdown file\n",
    "generate_markdown_presentation(df, \"Machine Learning Project\", \"ML_Project_Presentation.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8bd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(8,activation='relu'))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(512, activation='tanh'))\n",
    "model.add(layers.Dense(1024, activation='tanh'))\n",
    "model.add(layers.Dense(2028, activation='tanh'))\n",
    "model.add(layers.Dense(512, activation='tanh'))\n",
    "model.add(layers.Dense(512, activation='tanh'))\n",
    "model.add(layers.Dense(128, activation='tanh'))\n",
    "model.add(layers.Dense(32, activation='tanh'))\n",
    "model.add(layers.Dense(1,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_crossentropy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train.fillna(0), y_train, epochs=50, batch_size=1000, validation_split=0.1, verbose=1)\n",
    "\n",
    "\n",
    "word_dictionary = {}\n",
    "\n",
    "for i in column_data['CLEAN']:\n",
    "    word = i.split()\n",
    "    for i in word:\n",
    "        try:\n",
    "            word_dictionary[i] +=1\n",
    "        except:\n",
    "            word_dictionary[i] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71228ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_string(string,remove_chars=['+','-',\"(\",\")\",'/','*']):\n",
    "    \n",
    "    '''\n",
    "    Function to clean some of the punctuation which appears in Column Headers\n",
    "    \n",
    "    '''\n",
    "    new_string = \"\"\n",
    "    for char in string:\n",
    "        if char not in remove_chars:\n",
    "            new_string +=char\n",
    "    return new_string\n",
    "\n",
    "def create_heatmap(df,column_name='',corr_value=.1,figsize=(20,15)):\n",
    "    \n",
    "    sns.set(style='white')\n",
    "    \n",
    "    # View column with Abbreviated title or full. Abbreviated displays nicer.\n",
    "    corr = df.corr()\n",
    "    \n",
    "    if len(column_name)!=0:\n",
    "        corr = corr[[column_name]]\n",
    "        corr = corr[abs(corr[column_name])>corr_value]\n",
    "    \n",
    "    mask= np.zeros_like(corr,dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)]=True\n",
    "    f,ax = plt.subplots(figsize=figsize)\n",
    "    cmap = sns.diverging_palette(220,10,as_cmap=True)\n",
    "    sns.heatmap(corr,mask=mask,cmap=cmap,vmax=.3,center=0,square=True,linewidths=.5)\n",
    "    \n",
    "    plt.title('Heat Map of Correlation')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def create_column_inclusion_review(df,\n",
    "                               columns,\n",
    "                               column_type_df='',\n",
    "                               decile_review_df=''):\n",
    "    \n",
    "    df = df[columns].copy()\n",
    "    \n",
    "    temp_df = review_dataset_dict(df)\n",
    "    \n",
    "    if len(column_type_df)!=0:\n",
    "        temp_df = temp_df.merge(column_type_df,on='Financial Ratio',how='left')\n",
    "    if len(decile_review_df)!=0:\n",
    "        temp_df = temp_df.merge(decile_review_df,on='Financial Ratio',how='left')\n",
    "    \n",
    "    return temp_df\n",
    "\n",
    "\n",
    "def variable_review(df,\n",
    "                    column_name,\n",
    "                    og_column,\n",
    "                    column_inclusion_review_df,\n",
    "                    corr_weight=.15):\n",
    "        \n",
    "    print(column_inclusion_review_df[column_inclusion_review_df['Financial Ratio']==column_name].T)\n",
    "    print(f\"\\n\")\n",
    "    \n",
    "    create_heatmap(df[og_columns],column_name)\n",
    "    create_decile(df,column_name)\n",
    "    \n",
    "    print('Top 20 Records')\n",
    "    print(df.sort_values(column_name)[[column_name,'BANKRUPTCY_FLAG']].tail(20))\n",
    "    \n",
    "    print('Bottom 20 Records')\n",
    "    print(df.sort_values(column_name)[[column_name,'BANKRUPTCY_FLAG']].head(20))\n",
    "    \n",
    "    return df[df[column_name].isnull()].T\n",
    "\n",
    "# Process for reviewing Non Tier 1 Elements.\n",
    "\n",
    "def review_single_variable_manully(df,\n",
    "                                   column_name,\n",
    "                                   baseline_columns,\n",
    "                                   column_inclusion_review_df,\n",
    "                                   og_columns,\n",
    "                                   export_to_excel=0):\n",
    "        \n",
    "    import datetime\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    # Currently Included Columns for Simple Reference\n",
    "    print(\"Columns Currently In Scope:\")\n",
    "    for included in baseline_columns:\n",
    "        print(f\"{included}\\n\")\n",
    "    \n",
    "    # EDA\n",
    "    variable_review(df,column_name,og_columns,column_inclusion_review_df)    \n",
    "    print(f\"Number of Null Records with Bankrupcy Flag Yes: {df[df[column_name].isnull()]['BANKRUPTCY_FLAG'].sum()}\")\n",
    "    \n",
    "    # Review Questions\n",
    "    blank_or_remove = input('Remove Null Records/ Zero Null Records/ Exit Loop (remove/zero/exit)')\n",
    "    include_in_model = input('Subjective Belief as to whether variable should be included in model (include/exclude)')\n",
    "    negative_value = input('Remove, Zero or Leave Negative Values (remove/zero/ignore)')\n",
    "    decision_logic = input('Please Provide Comment on Decision for Archival Reference')\n",
    "        \n",
    "    record_df =  pd.DataFrame([blank_or_remove.lower(),include_in_model.lower(),negative_value.lower(),decision_logic.lower(),now],index=['Null Record Approach','Baseline V2 Model Inclusion',\"Negative Valuation\",'Archival Decisioning','Extract Time'],columns=[column_name]).T.reset_index().rename(columns={'index':'Financial Ratio'})\n",
    "    \n",
    "    if export_to_excel==1:\n",
    "        clean_column_name = clean_string(column_name,remove_chars=['+','-',\"(\",\")\",'/','*']).replace(\" \",\"_\")\n",
    "        record_df.to_excel(f\"manual_review/manual_review_{clean_column_name}_{now.strftime('%d%m%y%h%m%s')}.xlsx\",index=False)\n",
    "            \n",
    "    return record_df\n",
    "\n",
    "def read_files_in_folder(folder_location,file_type='*',import_df=0):\n",
    "    \n",
    "    files_ = os.listdir(folder_location)\n",
    "    \n",
    "    if file_type =='*':\n",
    "        files_desired = files_.copy()\n",
    "    else:\n",
    "        files_desired = [x for x in files_ if x.find('xlsx')>-1]\n",
    "        \n",
    "    if import_df ==1:\n",
    "        final_df = pd.DataFrame()\n",
    "        if file_type =='xlsx':\n",
    "            pd_read = pd.read_excel\n",
    "        elif file_type =='csv':\n",
    "            pd_read = pd.read_csv      \n",
    "        else:\n",
    "            print('Update Function')              \n",
    "        for file in files_desired:\n",
    "            final_df = pd.concat([final_df,pd_read(f\"{folder_location}/{file}\")])\n",
    "            \n",
    "        return final_df\n",
    "         \n",
    "    return files_desired\n",
    "\n",
    "\n",
    "\n",
    "def build_binary_classification_model(input_dim, \n",
    "                                      hidden_layer_sizes,\n",
    "                                      activation, \n",
    "                                      optimizer,\n",
    "                                      learning_rate,\n",
    "                                      metrics):\n",
    "\n",
    "    \"\"\"Build a binary classification model using Keras.\n",
    "\n",
    "      Args:\n",
    "        input_dim: Number of features in the input data.\n",
    "        hidden_layer_sizes: A list with the number of units in each hidden layer.\n",
    "        activation: The activation function to use for the hidden layers.\n",
    "        optimizer: The optimizer\n",
    "        learning_rate: The desired learning rate for the optimizer.\n",
    "\n",
    "      Returns:\n",
    "        model: A tf.keras model.\n",
    "    \"\"\"\n",
    "    # Instantiate Model\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # Add Input Layer\n",
    "    model.add(layers.InputLayer(input_shape=(input_dim,)))\n",
    "\n",
    "    # Add Hidden Layers\n",
    "    for nodes in hidden_layer_sizes:\n",
    "        model.add(layers.Dense(units=nodes, activation=activation))\n",
    "\n",
    "    # Add Output Layer\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Configure optimizer and compile the model\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(X,\n",
    "                y,\n",
    "                input_dim,\n",
    "                metrics,\n",
    "                hidden_layer_sizes,\n",
    "                activation, \n",
    "                optimizer,\n",
    "                learning_rate,\n",
    "                batch_size,\n",
    "                num_epochs,\n",
    "                validation_split,\n",
    "                verbose=0):\n",
    "                       \n",
    "\n",
    "    # Build the model.\n",
    "    model = build_binary_classification_model(input_dim=input_dim,\n",
    "                                              hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                              activation=activation, \n",
    "                                              optimizer=optimizer,\n",
    "                                              learning_rate=learning_rate,\n",
    "                                              metrics=metrics)\n",
    "    \n",
    "    print(model.summary())     \n",
    "                        \n",
    "    # Train the model.\n",
    "    history = model.fit(x=X,\n",
    "                        y=y,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=num_epochs,\n",
    "                        validation_split=validation_split,\n",
    "                        verbose=verbose)\n",
    "\n",
    "    # Retrieve the training metrics (after each train epoch) and the final test\n",
    "    # accuracy.\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(train_accuracy, label='train_accuracy')\n",
    "    plt.plot(val_accuracy, label='validation accuracy')\n",
    "    plt.xticks(range(num_epochs))\n",
    "    plt.xlabel('Train epochs')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    " \n",
    "    return history,model\n",
    "\n",
    "def create_balanced_dataset(X,y,observations=0,column_name='BANKRUPTCY_FLAG'):\n",
    "    \n",
    "    '''\n",
    "    Function to take observations and labels, combine them together and then select a even numher of random examples\n",
    "    \n",
    "    X - X_Test or X_Training\n",
    "    y - y_test or y_training\n",
    "    observation - Number of records from both Binary On and Binary Off Column\n",
    "    column_name - Name of Binary Column to filter\n",
    "    \n",
    "    \n",
    "    Used Default Value for purposes of reducing typing in code and given function created for Project Exclusively.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # If length of observations is not defined, create a even 50/ 50 dataset\n",
    "    if observations == 0:\n",
    "        observations = y[column_name].sum()\n",
    "        \n",
    "    if observations>y[column_name].sum():\n",
    "        observations = y[column_name].sum()\n",
    "        \n",
    "    temp_df = pd.concat([X,y],axis=1).copy()\n",
    "    \n",
    "    df1 = temp_df[temp_df[column_name]==1].sample(observations).copy()\n",
    "    df2 = temp_df[temp_df[column_name]==0].sample(observations).copy()\n",
    "    \n",
    "    final_df = pd.concat([df1,df2])\n",
    "    final_df = final_df.sample(frac=1)\n",
    "    \n",
    "    X = final_df.drop(column_name,axis=1)\n",
    "    y = final_df[[column_name]]\n",
    "    \n",
    "    \n",
    "    def train_neural_network(X,\n",
    "                y,\n",
    "                input_dim,\n",
    "                metrics,\n",
    "                hidden_layer_sizes,\n",
    "                activation, \n",
    "                optimizer,\n",
    "                learning_rate,\n",
    "                batch_size,\n",
    "                num_epochs,\n",
    "                validation_split,\n",
    "                verbose=0):\n",
    "                       \n",
    "\n",
    "    # Build the model.\n",
    "    model = build_binary_classification_model(input_dim=input_dim,\n",
    "                                              hidden_layer_sizes=hidden_layer_sizes,\n",
    "                                              activation=activation, \n",
    "                                              optimizer=optimizer,\n",
    "                                              learning_rate=learning_rate,\n",
    "                                              metrics=metrics)\n",
    "    \n",
    "    print(model.summary())     \n",
    "                        \n",
    "    # Train the model.\n",
    "    history = model.fit(x=X,\n",
    "                        y=y,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=num_epochs,\n",
    "                        validation_split=validation_split,\n",
    "                        verbose=verbose)\n",
    "\n",
    "    # Retrieve the training metrics (after each train epoch) and the final test\n",
    "    # accuracy.\n",
    "    train_accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(train_accuracy, label='train_accuracy')\n",
    "    plt.plot(val_accuracy, label='validation accuracy')\n",
    "    plt.xticks(range(num_epochs))\n",
    "    plt.xlabel('Train epochs')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    " \n",
    "    return history,model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def knn_model_creation(X_df,\n",
    "                       y_df,\n",
    "                       neighbors=3):    \n",
    "    model = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "    model.fit(X_df,y_df)    \n",
    "    return model\n",
    "\n",
    "def logistic_regression_creation(X_df,\n",
    "                                 y_df,\n",
    "                                 solver='liblinear'):\n",
    "    '''\n",
    "    ['liblinear','newton-cg','lbfgs','sag','saga']\n",
    "    \n",
    "    for i in [10,20,30]:\n",
    "        temp_df = (y_probabilities >= i/100).astype(int)\n",
    "        knn_df = pd.concat([knn_df,pd.DataFrame(temp_df,columns=[f'LR_{str(i)}p'])],axis=1)\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X_df,y_df)\n",
    "    return model\n",
    "\n",
    "\n",
    "def random_forest_classifier(X_df,\n",
    "                             y_df):\n",
    "    '''\n",
    "    important_features_rf = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False).head(10)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_df,y_df)\n",
    "    return model\n",
    "\n",
    "def neural_network(X_df,\n",
    "                   y_df,\n",
    "                   input_dim, \n",
    "                   hidden_layer_sizes,\n",
    "                   activation,\n",
    "                   optimizer,\n",
    "                   learning_rate,\n",
    "                   metrics,\n",
    "                   verbose=0):\n",
    "\n",
    "    \"\"\"Build a binary classification model using Keras.\n",
    "\n",
    "      Args:\n",
    "        input_dim: Number of features in the input data.\n",
    "        hidden_layer_sizes: A list with the number of units in each hidden layer.\n",
    "        activation: The activation function to use for the hidden layers.\n",
    "        optimizer: The optimizer\n",
    "        learning_rate: The desired learning rate for the optimizer.\n",
    "\n",
    "      Returns:\n",
    "        model: A tf.keras model.\n",
    "    \"\"\"\n",
    "    # Instantiate Model\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # Add Input Layer\n",
    "    model.add(layers.InputLayer(input_shape=(input_dim,)))\n",
    "\n",
    "    # Add Hidden Layers\n",
    "    for nodes in hidden_layer_sizes:\n",
    "        model.add(layers.Dense(units=nodes, activation=activation))\n",
    "\n",
    "    # Add Output Layer\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Configure optimizer and compile the model\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "    return model\n",
    "34/22:\n",
    "    # Where am I going to take out the Validation Data.\n",
    "    # Where am I going to do the Standardization of Data?\n",
    "    # Where am I going to take out the testing data.\n",
    "\n",
    "\n",
    "\n",
    "    Takes a Dataframe, which includes Training Data (which includes Label), \n",
    "    \n",
    "\n",
    "    # Step 1 Scale Data\n",
    "if scaling_method.lower() == 'standard_scalar':\n",
    "    X = apply_standard_scaler(X_df,X_df.columns.values)\n",
    "elif scaling_method.lower()=='min_max':\n",
    "    X = apply_min_max_scaler(X_df,X_df.columns.values)\n",
    "\n",
    "    if bankrupcy_observations == 'entire_dataset':\n",
    "a        pass\n",
    "    elif bankrupcy_observations == 'all_bankrupcies':\n",
    "        X,y = create_balanced_dataset(X,y)\n",
    "    else:\n",
    "        X,y = create_balanced_dataset(X,y,bankrupcy_observations)\n",
    "34/23:\n",
    "def create_model(X_df,\n",
    "                 y_df,model,\n",
    "                 scaling_method = 'standard_scalar',\n",
    "                 learning_rate=.05,\n",
    "                 model_parameters={}):\n",
    "    '''\n",
    "    \n",
    "    Function to generate ML Model, based on model type as defined in Parameters.\n",
    "    Below values for illustration purposes, can utilize dictionary to update as desired.\n",
    "\n",
    "    if model is neural_network, \n",
    "    model_parameters = {'hidden_layer_sizes':[8,16,32,64],\n",
    "                                          'activation':'relu',\n",
    "                                          'optimizer':'adam',\n",
    "                                          'learning_rate':.05,\n",
    "                                          'metrics':['accuracy', keras.metrics.Precision(),keras.metrics.Recall()]}\n",
    "    if model Logistic Regression,\n",
    "    model_parameters = {'solver':'liblinear'}\n",
    "\n",
    "    if model knn,\n",
    "    model_parameters = {'n_neighbors':3}\n",
    "\n",
    "    if model random_forsest\n",
    "    model_parameters = {}\n",
    "\n",
    "    '''\n",
    "\n",
    "    if model == 'neural_network':\n",
    "        model = neural_network(X_df,\n",
    "                               y_df,\n",
    "                               input_dim=len(X.columns.values), \n",
    "                               hidden_layer_sizes= model_parameters['hidden_layer_sizes'],\n",
    "                               activation=model_parameters['activation'], \n",
    "                               optimizer=model_parameters['optimizer'],\n",
    "                               learning_rate=model_parameters['learning_rate'],\n",
    "                               metrics=model_parameters['metrics'])\n",
    "\n",
    "    elif model == 'logistic_regression':\n",
    "        model = logistic_regression_creation(X_df,\n",
    "                                             y_df,\n",
    "                                             solver=model_parameters['solver'])\n",
    "    elif model == 'random_forest':\n",
    "        model = random_forest_classifier(X_df,\n",
    "                                         y_df)\n",
    "    elif model == 'knn':\n",
    "        model = knn_model_creation(X_df,\n",
    "                                   y_df,\n",
    "                                   neighbors=model_parameters['n_neighbors'])\n",
    "    return model\n",
    "\n",
    "def generate_predicition(X_test_df,\n",
    "                         y_test_df,\n",
    "                         model,\n",
    "                         py_model,\n",
    "                         model_parameters):\n",
    "\n",
    "    if model == 'neural_network':\n",
    "        history = py_model.fit(x=X_test_df,\n",
    "                            y=y_test_df,\n",
    "                            batch_size=model_parameters['batch_size'],\n",
    "                            epochs=model_parameters['num_epochs'])\n",
    "\n",
    "        acc = pd.DataFrame(history.history['accuracy'],columns=[\"Training Accuracy\"])\n",
    "        loss = pd.DataFrame(history.history['loss'],columns=[\"Training Loss\"])\n",
    "        prec_str = columns=[x for x in history.history.keys() if x.find('pre')!=-1][0]\n",
    "        recall_str = columns=[x for x in history.history.keys() if x.find('reca')!=-1][0]\n",
    "        precision = pd.DataFrame(history.history[prec_str],columns=[prec_str]).rename(columns={prec_str:'Precision'})\n",
    "        recall = pd.DataFrame(history.history[recall_str],columns=[recall_str]).rename(columns={recall_str:'Recall'})\n",
    "\n",
    "        model_results_df = pd.concat([model_results_df,pd.DataFrame([x for x in range(0,len(model_results_df))],columns=['Epoch Number'])],axis=1)\n",
    "        model_results_df['Dataset'] = 'To Be Determined'\n",
    "        model_results_df['Model'] = model\n",
    "        model_results_df['activation'] = model_parameters['activation']\n",
    "        model_results_df['optimizer'] = model_parameters['optimizer']\n",
    "        model_results_df['learning_rate'] = model_parameters['learning_rate']\n",
    "        model_results_df['batch_size'] = model_parameters['batch_size']\n",
    "        model_results_df['epochs'] = model_parameters['num_epochs']\n",
    "        model_results_df['hidden_layer_sizes'] = text_manipulation(model_parameters['hidden_layer_sizes'])\n",
    "        #model_results_df['bankrupt_observations'] = bankrupcy_observations\n",
    "        #model_results_df['total_observations_read'] = len(X)\n",
    "        return model_results_df\n",
    "    \n",
    "    elif model == 'knn':\n",
    "        pred = py_model.predict(X_test_df)\n",
    "        pred_df = pd.DataFrame(pred,columns=['Predicitions'])\n",
    "        final_df = pd.concat([y_test_df.reset_index(drop=True),pred_df],axis=1)\n",
    "        return final_df\n",
    "    \n",
    "    else:\n",
    "        pred = py_model.predict(X_test_df,y_test_df)\n",
    "        pred_df = pd.DataFrame(pred,columns=['Predicitions'])\n",
    "        final_df = pd.concat([y_test_df.reset_index(drop=True),pred_df],axis=1)\n",
    "        return final_df\n",
    "    \n",
    "\n",
    "    # else:\n",
    "    #     predicition = py_model.predict(X_test)\n",
    "    #     temp_x = pd.DataFrame(predicition,columns=[column_name])\n",
    "    #     if df == \"\":\n",
    "    #         return pd.concat([y_test_df,temp_df],axis=1)\n",
    "    #     else:\n",
    "    #         return pd.concat([df,temp_df],axis=1)\n",
    "34/24:\n",
    "X = dataset_dictionary['All Ratios Cleaned']\n",
    "y = dataset_dictionary['labels']\n",
    "\n",
    "model='logistic_regression'\n",
    "model_parameters = {'solver':'liblinear'}\n",
    "\n",
    "lr_  = create_model(X,\n",
    "                      y,\n",
    "                      model=model,\n",
    "                      model_parameters=model_parameters)\n",
    "\n",
    "model = 'knn'\n",
    "model_parameters = {'n_neighbors':3}\n",
    "\n",
    "knn_ = create_model(X,\n",
    "                      y,\n",
    "                      model=model,\n",
    "                      model_parameters=model_parameters)\n",
    "34/25: knn_.predict(X)\n",
    "34/26: X\n",
    "34/27: X_train, X_test, y_train, y_test  = train_test_split(X,y,test_size=0.3, random_state=15)\n",
    "34/28:\n",
    "X = dataset_dictionary['All Ratios Cleaned']\n",
    "y = dataset_dictionary['labels']\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X,y,test_size=0.3, random_state=15)\n",
    "\n",
    "\n",
    "model='logistic_regression'\n",
    "model_parameters = {'solver':'liblinear'}\n",
    "\n",
    "lr_  = create_model(X_train,\n",
    "                      y_train,\n",
    "                      model=model,\n",
    "                      model_parameters=model_parameters)\n",
    "\n",
    "model = 'knn'\n",
    "model_parameters = {'n_neighbors':3}\n",
    "\n",
    "knn_ = create_model(X_train,\n",
    "                      y_train,\n",
    "                      model=model,\n",
    "                      model_parameters=model_parameters)\n",
    "34/29: lr_.n_iter_\n",
    "34/30:\n",
    "X = dataset_dictionary['All Ratios Cleaned']\n",
    "y = dataset_dictionary['labels']\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X,y,test_size=0.3, random_state=15)\n",
    "\n",
    "\n",
    "model='logistic_regression'\n",
    "model_parameters = {'solver':'lbfgs'}\n",
    "\n",
    "lr_  = create_model(X_train,\n",
    "                      y_train,\n",
    "                      model=model,\n",
    "                      model_parameters=model_parameters)\n",
    "\n",
    "model = 'knn'\n",
    "model_parameters = {'n_neighbors':3}\n",
    "\n",
    "knn_ = create_model(X_train,\n",
    "                      y_train,\n",
    "                      model=model,\n",
    "                      model_parameters=model_parameters)\n",
    "34/31:\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def knn_model_creation(X_df,\n",
    "                       y_df,\n",
    "                       neighbors=3):    \n",
    "    model = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "    model.fit(X_df,y_df)    \n",
    "    return model\n",
    "\n",
    "def logistic_regression_creation(X_df,\n",
    "                                 y_df,\n",
    "                                 max_iter=100,\n",
    "                                 solver='liblinear'):\n",
    "    '''\n",
    "    ['liblinear','newton-cg','lbfgs','sag','saga']\n",
    "    \n",
    "    for i in [10,20,30]:\n",
    "        temp_df = (y_probabilities >= i/100).astype(int)\n",
    "        knn_df = pd.concat([knn_df,pd.DataFrame(temp_df,columns=[f'LR_{str(i)}p'])],axis=1)\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    model = LogisticRegression(solver='liblinear',max_iter=max_iter)\n",
    "    model.fit(X_df,y_df)\n",
    "    return model\n",
    "\n",
    "\n",
    "def random_forest_classifier(X_df,\n",
    "                             y_df):\n",
    "    '''\n",
    "    important_features_rf = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False).head(10)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_df,y_df)\n",
    "    return model\n",
    "\n",
    "def neural_network(X_df,\n",
    "                   y_df,\n",
    "                   input_dim, \n",
    "                   hidden_layer_sizes,\n",
    "                   activation,\n",
    "                   optimizer,\n",
    "                   learning_rate,\n",
    "                   metrics,\n",
    "                   verbose=0):\n",
    "\n",
    "    \"\"\"Build a binary classification model using Keras.\n",
    "\n",
    "      Args:\n",
    "        input_dim: Number of features in the input data.\n",
    "        hidden_layer_sizes: A list with the number of units in each hidden layer.\n",
    "        activation: The activation function to use for the hidden layers.\n",
    "        optimizer: The optimizer\n",
    "        learning_rate: The desired learning rate for the optimizer.\n",
    "\n",
    "      Returns:\n",
    "        model: A tf.keras model.\n",
    "    \"\"\"\n",
    "    # Instantiate Model\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    # Add Input Layer\n",
    "    model.add(layers.InputLayer(input_shape=(input_dim,)))\n",
    "\n",
    "    # Add Hidden Layers\n",
    "    for nodes in hidden_layer_sizes:\n",
    "        model.add(layers.Dense(units=nodes, activation=activation))\n",
    "\n",
    "    # Add Output Layer\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Configure optimizer and compile the model\n",
    "    if optimizer == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "    return model\n",
    "34/32:\n",
    "def create_model(X_df,\n",
    "                 y_df,model,\n",
    "                 scaling_method = 'standard_scalar',\n",
    "                 learning_rate=.05,\n",
    "                 model_parameters={}):\n",
    "    '''\n",
    "    \n",
    "    Function to generate ML Model, based on model type as defined in Parameters.\n",
    "    Below values for illustration purposes, can utilize dictionary to update as desired.\n",
    "\n",
    "    if model is neural_network, \n",
    "    model_parameters = {'hidden_layer_sizes':[8,16,32,64],\n",
    "                                          'activation':'relu',\n",
    "                                          'optimizer':'adam',\n",
    "                                          'learning_rate':.05,\n",
    "                                          'metrics':['accuracy', keras.metrics.Precision(),keras.metrics.Recall()]}\n",
    "    if model Logistic Regression,\n",
    "    model_parameters = {'solver':'liblinear','max_iter':200}\n",
    "\n",
    "    if model knn,\n",
    "    model_parameters = {'n_neighbors':3}\n",
    "\n",
    "    if model random_forsest\n",
    "    model_parameters = {}\n",
    "\n",
    "    '''\n",
    "\n",
    "    if model == 'neural_network':\n",
    "        model = neural_network(X_df,\n",
    "                               y_df,\n",
    "                               input_dim=len(X.columns.values), \n",
    "                               hidden_layer_sizes= model_parameters['hidden_layer_sizes'],\n",
    "                               activation=model_parameters['activation'], \n",
    "                               optimizer=model_parameters['optimizer'],\n",
    "                               learning_rate=model_parameters['learning_rate'],\n",
    "                               metrics=model_parameters['metrics'])\n",
    "\n",
    "    elif model == 'logistic_regression':\n",
    "        model = logistic_regression_creation(X_df,\n",
    "                                             y_df,\n",
    "                                             max_iter=model_parameters['max_iter'],\n",
    "                                             solver=model_parameters['solver'])\n",
    "    elif model == 'random_forest':\n",
    "        model = random_forest_classifier(X_df,\n",
    "                                         y_df)\n",
    "    elif model == 'knn':\n",
    "        model = knn_model_creation(X_df,\n",
    "                                   y_df,\n",
    "                                   neighbors=model_parameters['n_neighbors'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc91ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://en-wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory'\n",
    "pd.read_html(url)\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Correct Wikipedia URL\n",
    "url = 'https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory'\n",
    "\n",
    "# Set User-Agent to mimic a real browser\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"}\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML tables\n",
    "    tables = pd.read_html(response.text)\n",
    "\n",
    "    # Print the number of tables found\n",
    "    print(f\"Total tables found: {len(tables)}\")\n",
    "\n",
    "    # Show the first table\n",
    "    print(tables[0].head())\n",
    "else:\n",
    "    print(f\"Failed to fetch page, status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dab77b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g -f history.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(['We Design Everything','We Design, You Watch','We Design, You Advise','We Design, You Help','You Design, We Help','You Design, We Advise','You Design, We Watch','You Design Everything'],[500,800,1000,1500,2000,3500,5000,8000],columns=['Price List']).reset_index().rename(columns={'Price List':\"Service Offered\",'index':\"Price\"})[['Service Offered','Price']]\n",
    "\n",
    "![Alt Text](https://raw.githubusercontent.com/derek-dewald/d_functions/main/images/Consulting_Fees.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee67883-ac06-4e18-a2fb-ef2b2e22c816",
   "metadata": {},
   "source": [
    "| Dataset Type   | Fits in RAM? | Fits on Local Disk? |\n",
    "|---------------|-------------|---------------------|\n",
    "| Small dataset | Yes         | Yes                 |\n",
    "| Medium dataset | No         | Yes                 |\n",
    "| Big dataset   | No         | No                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8781f41-5d0f-4301-88c2-039cfc2fe6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "df = pd.read_csv('DKnowledgeSheet.csv')\n",
    "\n",
    "def column_reminder(df,columns=[]):\n",
    "    if len(columns)==0:\n",
    "        columns = df.columns\n",
    "        \n",
    "    for column in columns:\n",
    "        print(df[column].value_counts())\n",
    "\n",
    "def UpdateDictionary(definition,\n",
    "                     overwrite_all=0,\n",
    "                     description=\"\",\n",
    "                     command_code=\"\",\n",
    "                     classification=\"\",\n",
    "                     category=\"\",\n",
    "                     sub_category=\"\"):\n",
    "    \n",
    "    df = pd.read_csv('DKnowledgeSheet.csv')\n",
    "    \n",
    "    df['Definition'] = df['Definition'].fillna(\"\").apply(lambda x:x.lower())\n",
    "    \n",
    "    temp_df = df[df['Definition'].fillna(\"\").str.contains(definition.lower())]\n",
    "\n",
    "    if overwrite_all==1:\n",
    "\n",
    "        if len(temp_df)>1:\n",
    "            confirm = input('Are you Certain you wish to proceed? You will be deleting multiple records.')\n",
    "            if confirm == 'Yes':\n",
    "                pass\n",
    "            else:\n",
    "                return print(\"Manually Ended\")\n",
    "            \n",
    "        df = df[~df['Definition'].fillna(\"\").str.contains(definition.lower())]\n",
    "        new_record = {'Definition': [definition],\n",
    "                      'Description':[description],\n",
    "                      'Command/ Code':[command_code],\n",
    "                      'Classification':[classification],\n",
    "                      'Category':[category],\n",
    "                      'Sub Category':[sub_category]}\n",
    "    \n",
    "        updated_df = pd.concat([df,pd.DataFrame(new_record)]).reset_index(drop=True)\n",
    "        df.to_csv(f\"archive//archive_{datetime.datetime.now().strftime('%d-%b-%y')}.csv\",index=False)\n",
    "        updated_df.to_csv('DKnowledgeSheet.csv',index=False)\n",
    "        print(\"Updated Dictionary\")\n",
    "    \n",
    "        return updated_df\n",
    "        \n",
    "    elif len(temp_df)>1:\n",
    "        return temp_df\n",
    "    \n",
    "    elif (len(temp_df)==1)&((len(description)>0)|(len(command_code)>0)|(len(classification)>0)|(len(category)>0)):\n",
    "        value = temp_df['Definition'].item()    \n",
    "        if len(description)>0:\n",
    "            df.loc[df['Definition']==value,'Description']=description\n",
    "        if len(command_code)>0:\n",
    "            df.loc[df['Definition']==value,'Command/Code']=command_code        \n",
    "        if len(classification)>0:\n",
    "            df.loc[df['Definition']==value,'Classification']=classification\n",
    "        if len(category)>0:\n",
    "            df.loc[df['Definition']==value,'Category']=category\n",
    "        if len(sub_category)>0:\n",
    "            df.loc[df['Definition']==value,'Sub Category']=sub_category\n",
    "            \n",
    "        return df\n",
    "    elif len(temp_df)==1:\n",
    "        return temp_df\n",
    "    elif (len(temp_df)==0)&(len(description)==0)&(len(command_code)==0)&(len(classification)==0)&(len(category)==0):\n",
    "        print('No Records Observed, Nothing to Update')\n",
    "                                                                            \n",
    "    else:\n",
    "        new_record = {'Definition': [definition],\n",
    "                      'Description':[description],\n",
    "                      'Command/ Code':[command_code],\n",
    "                      'Classification':[classification],\n",
    "                      'Category':[category],\n",
    "                      'Sub Category':[sub_category]}\n",
    "                     \n",
    "        updated_df = pd.concat([df,pd.DataFrame(new_record)]).reset_index(drop=True)\n",
    "        df.to_csv(f\"archive//archive_{datetime.datetime.now().strftime('%d-%b-%y')}.csv\",index=False)\n",
    "        updated_df.to_csv('DKnowledgeSheet.csv',index=False)\n",
    "        print(\"Updated Dictionary\")\n",
    "        \n",
    "        return updated_df\n",
    "\n",
    "\n",
    "\n",
    "UpdateDictionary('Data Cleaning',\n",
    "                 description='',\n",
    "                 command_code='',\n",
    "                 classification='Theory',\n",
    "                 category='ML',\n",
    "                 sub_category='')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0880e-9c39-47cc-a871-77a2a90a4ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Operational data is usually transactional data that is generated and stored by applications, often in a relational or non-relational database.\n",
    "Analytical data is data that has been optimized for analysis and reporting, often in a data warehouse.\n",
    "Data pipelines are used to orchestrate activities that transfer and transform data. Pipelines are the primary way in which data engineers implement repeatable extract, transform, and load (ETL) solutions that can be triggered based on a schedule or in response to events.\n",
    "A data lake is a storage repository that holds large amounts of data in native, raw formats. Data lake stores are optimized for scaling to massive volumes (terabytes or petabytes) of data. The data typically comes from multiple heterogeneous sources, and may be structured, semi-structured, or unstructured\n",
    "Azure Data Lake Storage Gen2 builds on blob storage and optimizes I/O of high-volume data by using a hierarchical namespace that organizes blob data into directories, and stores metadata about each directory and the files within it. This structure allows operations, such as directory renames and deletes, to be performed in a single atomic operation. Flat namespaces, by contrast, require several operations proportionate to the number of objects in the structure. Hierarchical namespaces keep the data organized, which yields better storage and retrieval performance for an analytical use case and lowers the cost of analysis.\n",
    "In Azure Blob storage, you can store large amounts of unstructured (\"object\") data in a flat namespace within a blob container. Blob names can include \"/\" characters to organize blobs into virtual \"folders\", but in terms of blob manageability the blobs are stored as a single-level hierarchy in a flat namespace.\n",
    "\n",
    "Descriptive analytics, which answers the question What is happening in my business?. The data to answer this question is typically answered through the creation of a data warehouse in which historical data is persisted in relational tables for multidimensional modeling and reporting.\n",
    "Diagnostic analytics, which deals with answering the question Why is it happening?. This may involve exploring information that already exists in a data warehouse, but typically involves a wider search of your data estate to find more data to support this type of analysis.\n",
    "Predictive analytics, which enables you to answer the question What is likely to happen in the future based on previous trends and patterns?\n",
    "Prescriptive analytics, which enables autonomous decision making based on real-time or near real-time analysis of data, using predictive analytics.\n",
    "\n",
    "Data virtualization allows you to interact with data without the need to understand how the data is formatted, structured, or what is its data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a2d95-26be-4123-86f5-6414248a8435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b80e3-2581-4fd8-8f8d-145e9a5aef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOLD - Build Together, Own it, Lead with Agility, Driven by Curiosity and Always Welcoming\n",
    "Strategic Priorities - Integration, Business Transformation, Diversify Revenue, ESG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2d02a-01fd-46ec-971b-e5000d3af2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RE has issue with Python escape clauses \\, utilize r to make it string literal\n",
    "r\"\\n\"\n",
    "\n",
    "* \\b: Word boundary ensures that we match whole words.\n",
    "* \\w+: Matches one or more word characters (letters, digits, or underscores).\n",
    "* \\b: Another word boundary to ensure the end of the word.\n",
    "* [^\\w\\s]: Matches any character that is not a word character (letters, digits, or underscores) or a whitespace character.\n",
    "* |: OR operator.\n",
    "* [\\d]: Matches any digit.\n",
    "* [^\\w\\s]: Matches any character that is not a word character (letters, digits, or underscores) or a whitespace character.\n",
    "* |: OR operator.\n",
    "* [\\d]: Matches any digit.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Linux\n",
    "Operating Systems are Layered.\n",
    "Kernel is the inter most layer\n",
    "Outermost layer is a GUI\n",
    "Linux can peel off layers, helps towards simplicity\n",
    "GUI can take  ~50% of processing power\n",
    "\n",
    "Linux Distributions\n",
    "GNU (GNU not Unix)\n",
    "- Red Hat (CentOS)\n",
    "- Fedora\n",
    "Debian\n",
    "- Ubuntu\n",
    "- Raspberry Pi\n",
    "- Various IoT\n",
    "- \n",
    "\n",
    "Resources: \n",
    "- Command Line Help: www.explainshell.com\n",
    "\n",
    "\n",
    "Linux Commands\n",
    "man - Manual\n",
    "History - history\n",
    "Clear - clears history\n",
    "Q - quits from current query\n",
    "pwd- current directory\n",
    "~ - short hand for home\n",
    ". Current Directory\n",
    ".. Parent Directory\n",
    "Ls -l adds 6 additional fields \n",
    "ls -lh - adds file sizes\n",
    "First letter - D: Directory, -: File, l - Link, Permission, Number of Hard links, Users who owns it. Size in bytes, date opened or last maintained  \n",
    "Ls -la - shows hidden files\n",
    "sudo = \"super user (root) do\"\n",
    "mkdir - makes new directory\n",
    "rmdir - removes directory\n",
    "rm -r temp_1_3 - recursively removes directories\n",
    "chmod 765 temp_1_1 - change permissions to directory\n",
    "\n",
    "\n",
    "* r = read = 2^2 = 4\n",
    "* w = write = 2^1 = 2\n",
    "* x = execute = 2^0 = 1\n",
    "* 1 = --x\n",
    "* 2 = -w-\n",
    "* 3 = -wx\n",
    "* 4 = r--\n",
    "* 5 = r-x\n",
    "* 6 = rw-\n",
    "* 7 = rwx\n",
    "\n",
    "* u = user\n",
    "* g = group\n",
    "* o = other (warning: NOT owner!)\n",
    "\n",
    "chmod u=rwx,g=rx,o=r *\n",
    "\n",
    "ls -lhR - recursively list out directory tree\n",
    "cp filename - copy\n",
    "rm filename - remove\n",
    "mv filename new_filename- rename\n",
    "\n",
    "head returns the first few lines of a file; optionally pass the number of lines to return\n",
    "tail returns the last few lines of a file; optionally pass the number of lines to return\n",
    "grep matches a pattern in a file and returns lines that match the pattern\n",
    "grep -v returns the opposite, the rows that do NOT match\n",
    "| is a pipeline; Linux spawn a process for each command; Linux creates an \"anonymous pipe\" aka \"un-named pipe\" between the processes; Linux routes standard output to pipes and routes standard input from pipes for intermediate commands\n",
    "diff finds the differences between two files\n",
    "* vi filename\n",
    "* Movement mode: arrow keys, page up, page down, 0 beginning of line, $ end of line, x delete character, dd delete line\n",
    "* Movement mode to insert mode: i for insert, a for append, note that INSERT appears lower left\n",
    "* Insert mode to movement mode: ESC key\n",
    "* Movement mode to command mode: : note that hitting the enter key will run the command and return to movement mode\n",
    "* Save changes: :w\n",
    "* Save a file and exit: :wq\n",
    "* Quit: :q\n",
    "* Quit with unsaved changes: :q!\n",
    "\n",
    "\n",
    "How to Change to Bash\n",
    "chsh -s /bin/bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5013b91-375a-470b-8a81-90290eeaf185",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b9d8f-6282-4d8d-b5f1-af51265dcdfa",
   "metadata": {},
   "source": [
    "\n",
    "### Model\n",
    "$f(x) = w_0 + w_1X$\n",
    "\n",
    "### Parameters \n",
    "\n",
    "$[w_0, w_1]$\n",
    "\n",
    "### Loss\n",
    "$J(w_0,w_1) = \\frac{1}{n} \\sum_{i=1}^n \\left( y_i - (w_0 + w_1 x_i) \\right)^2$\n",
    "\n",
    "### Objective \n",
    "Minimize $J(w_0,w_1)$\n",
    "\n",
    "### Mean Square Error\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4f3f82-e081-43c4-bcf0-e5edcfb617dd",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fd8c5-2c59-4b91-9f3b-5631318b9f71",
   "metadata": {},
   "source": [
    "Linear Regression : $\\hat{y} = xw^T + b$ <br>\n",
    "Logistic Regression: $\\hat{y} = \\frac{1}{1 + e^({-xw^T + b})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ce69776-b4c7-4706-8628-73dbe1dd65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2308de-9437-4c0f-9292-72e98ee383db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63112174-349b-4a6b-8648-a28fd09b29d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16f27cee-cf11-4cd8-acf2-848d970c2793",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "578c7716-f489-4d25-abec-1d1f93ce316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771233e3-b440-49cc-b895-a529dde00e9a",
   "metadata": {},
   "source": [
    "$h(x) = f(g(x))$\n",
    "\n",
    "\n",
    "$h'(x) = f'(g(x)) \\cdot g'(x)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d11698-c4e1-4ae0-9632-c475d9f887da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d3a47-7565-4d2b-8a34-e4a1b4cf8d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f75f2b-f398-42e9-9297-17e241f4a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boosting. Train Additive Models in Series Where Each Predicts the Residual from the Previous Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c0e1e-43ee-4613-a1d0-289767ab021c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af72fb-e284-4c4e-9408-bed6c0f1bf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bagging. Trains Models in Parallel Via Bootstrap Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c9d7d-9dc2-4b2d-af80-051e671ec9aa",
   "metadata": {},
   "source": [
    "## Distance Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39068a-b22a-4dfa-8336-8fc6327d5016",
   "metadata": {},
   "source": [
    "$ ||A|| = \\sqrt{\\sum \\left (A_i \\right)^2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a75dcf-0790-4036-9e3b-95fbdde9aaa3",
   "metadata": {},
   "source": [
    "$ L_2(A,B) = \\sqrt{\\sum \\left( A_i - B_i \\right)^2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab53a3f-3bdd-4812-8f26-b28f3061d4d6",
   "metadata": {},
   "source": [
    "$\\ A \\cdot B = \\sum_{i=1}^n A_i B_i = ||A|| ||B|| cos(\\theta)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e5a0d-8ab1-4064-b2f6-eceb0bc2a210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
