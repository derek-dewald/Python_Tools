{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9545858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def GenerateSKLearnModelList():\n",
    "    \n",
    "    df_estimators = pd.DataFrame(all_estimators(type_filter=None), columns=['Model Name', 'Estimator Class'])\n",
    "    split_columns = df_estimators['Estimator Class'].astype(str).str.split('.', expand=True)\n",
    "\n",
    "    # Rename columns dynamically based on the number of splits\n",
    "    split_columns.columns = [f'Part_{i+1}' for i in range(split_columns.shape[1])]\n",
    "    # Concatenate the original DataFrame with the split columns\n",
    "    df_estimators = pd.concat([df_estimators, split_columns], axis=1).drop('Part_1',axis=1)\n",
    "    df_estimators.to_csv('SKLearnModels.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93b8ddce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp  ...        s4        s5        s6  Target\n",
       "0  0.038076  0.050680  0.061696  0.021872  ... -0.002592  0.019907 -0.017646   151.0\n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328  ... -0.039493 -0.068332 -0.092204    75.0\n",
       "2  0.085299  0.050680  0.044451 -0.005670  ... -0.002592  0.002861 -0.025930   141.0\n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  ...  0.034309  0.022688 -0.009362   206.0\n",
       "4  0.005383 -0.044642 -0.036385  0.021872  ... -0.002592 -0.031988 -0.046641   135.0\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/derekdewald/Documents/Python/Github_Repo/d_py_functions\")\n",
    "\n",
    "from MLPipeline import apply_scaling\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6875b229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 22:52:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:52:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:52:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:52:56 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ CCA failed: `n_components` upper bound is 1. Got 2 instead. Reduce `n_components`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 22:52:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:52:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:52:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:52:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:03 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:04 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:06 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "/Users/derekdewald/anaconda3/envs/BaseRequirements/lib/python3.12/site-packages/sklearn/linear_model/_huber.py:343: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "2025/02/24 22:53:07 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ IsotonicRegression failed: Isotonic regression input X should be a 1d array or 2d array with 1 feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 22:53:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:10 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:15 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:17 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "/Users/derekdewald/anaconda3/envs/BaseRequirements/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "2025/02/24 22:53:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ MultiOutputRegressor failed: MultiOutputRegressor.__init__() missing 1 required positional argument: 'estimator'\n",
      "⚠️ MultiTaskElasticNet failed: For mono-task outputs, use ElasticNet\n",
      "⚠️ MultiTaskElasticNetCV failed: For mono-task outputs, use ElasticNetCVCV\n",
      "⚠️ MultiTaskLasso failed: For mono-task outputs, use ElasticNet\n",
      "⚠️ MultiTaskLassoCV failed: For mono-task outputs, use LassoCVCV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 22:53:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ PLSCanonical failed: `n_components` upper bound is 1. Got 2 instead. Reduce `n_components`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 22:53:21 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:22 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:23 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:25 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ RegressorChain failed: _BaseChain.__init__() missing 1 required positional argument: 'base_estimator'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 22:53:28 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "/Users/derekdewald/anaconda3/envs/BaseRequirements/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:1608: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "2025/02/24 22:53:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ StackingRegressor failed: StackingRegressor.__init__() missing 1 required positional argument: 'estimators'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 22:53:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/02/24 22:53:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ VotingRegressor failed: VotingRegressor.__init__() missing 1 required positional argument: 'estimators'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Metric</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARDRegression</td>\n",
       "      <td>53.138882</td>\n",
       "      <td>1.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdaBoostRegressor</td>\n",
       "      <td>54.118285</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BaggingRegressor</td>\n",
       "      <td>56.147850</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BayesianRidge</td>\n",
       "      <td>53.588824</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>69.507739</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DummyRegressor</td>\n",
       "      <td>73.222493</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>72.878068</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ElasticNetCV</td>\n",
       "      <td>54.838504</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ExtraTreeRegressor</td>\n",
       "      <td>73.855538</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>53.615142</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GammaRegressor</td>\n",
       "      <td>72.953366</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GaussianProcessRegressor</td>\n",
       "      <td>226.313894</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>54.025060</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HistGradientBoostingRegressor</td>\n",
       "      <td>57.535497</td>\n",
       "      <td>1.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HuberRegressor</td>\n",
       "      <td>54.013315</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>KNeighborsRegressor</td>\n",
       "      <td>54.946115</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>KernelRidge</td>\n",
       "      <td>162.528864</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Lars</td>\n",
       "      <td>111.347832</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LarsCV</td>\n",
       "      <td>52.865246</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>58.340172</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LassoCV</td>\n",
       "      <td>52.917509</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LassoLars</td>\n",
       "      <td>58.340136</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LassoLarsCV</td>\n",
       "      <td>52.920431</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LassoLarsIC</td>\n",
       "      <td>53.699074</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>53.853446</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LinearSVR</td>\n",
       "      <td>82.311383</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MLPRegressor</td>\n",
       "      <td>148.818401</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NuSVR</td>\n",
       "      <td>66.380471</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>OrthogonalMatchingPursuit</td>\n",
       "      <td>63.732456</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>OrthogonalMatchingPursuitCV</td>\n",
       "      <td>53.022496</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>PLSRegression</td>\n",
       "      <td>54.404862</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>PassiveAggressiveRegressor</td>\n",
       "      <td>54.477402</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>PoissonRegressor</td>\n",
       "      <td>58.790013</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>QuantileRegressor</td>\n",
       "      <td>72.886244</td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>RANSACRegressor</td>\n",
       "      <td>75.062263</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RadiusNeighborsRegressor</td>\n",
       "      <td>73.222493</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>54.864217</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>55.474462</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>RidgeCV</td>\n",
       "      <td>53.446112</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>SGDRegressor</td>\n",
       "      <td>55.784002</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>SVR</td>\n",
       "      <td>65.827699</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>TheilSenRegressor</td>\n",
       "      <td>53.530973</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>TransformedTargetRegressor</td>\n",
       "      <td>53.853446</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>TweedieRegressor</td>\n",
       "      <td>72.966869</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model      Metric  Time (s)\n",
       "0                   ARDRegression   53.138882      1.32\n",
       "1               AdaBoostRegressor   54.118285      1.00\n",
       "2                BaggingRegressor   56.147850      0.97\n",
       "3                   BayesianRidge   53.588824      0.90\n",
       "4           DecisionTreeRegressor   69.507739      0.93\n",
       "5                  DummyRegressor   73.222493      0.82\n",
       "6                      ElasticNet   72.878068      0.89\n",
       "7                    ElasticNetCV   54.838504      0.89\n",
       "8              ExtraTreeRegressor   73.855538      0.92\n",
       "9             ExtraTreesRegressor   53.615142      1.00\n",
       "10                 GammaRegressor   72.953366      0.91\n",
       "11       GaussianProcessRegressor  226.313894      0.86\n",
       "12      GradientBoostingRegressor   54.025060      0.97\n",
       "13  HistGradientBoostingRegressor   57.535497      1.81\n",
       "14                 HuberRegressor   54.013315      0.95\n",
       "15            KNeighborsRegressor   54.946115      0.95\n",
       "16                    KernelRidge  162.528864      0.91\n",
       "17                           Lars  111.347832      0.93\n",
       "18                         LarsCV   52.865246      0.93\n",
       "19                          Lasso   58.340172      0.90\n",
       "20                        LassoCV   52.917509      0.89\n",
       "21                      LassoLars   58.340136      0.88\n",
       "22                    LassoLarsCV   52.920431      0.91\n",
       "23                    LassoLarsIC   53.699074      0.98\n",
       "24               LinearRegression   53.853446      0.89\n",
       "25                      LinearSVR   82.311383      0.89\n",
       "26                   MLPRegressor  148.818401      0.92\n",
       "27                          NuSVR   66.380471      0.90\n",
       "28      OrthogonalMatchingPursuit   63.732456      0.92\n",
       "29    OrthogonalMatchingPursuitCV   53.022496      0.89\n",
       "30                  PLSRegression   54.404862      0.84\n",
       "31     PassiveAggressiveRegressor   54.477402      0.90\n",
       "32               PoissonRegressor   58.790013      0.90\n",
       "33              QuantileRegressor   72.886244      1.09\n",
       "34                RANSACRegressor   75.062263      0.95\n",
       "35       RadiusNeighborsRegressor   73.222493      0.91\n",
       "36          RandomForestRegressor   54.864217      1.06\n",
       "37                          Ridge   55.474462      0.92\n",
       "38                        RidgeCV   53.446112      0.94\n",
       "39                   SGDRegressor   55.784002      0.94\n",
       "40                            SVR   65.827699      0.97\n",
       "41              TheilSenRegressor   53.530973      1.18\n",
       "42     TransformedTargetRegressor   53.853446      0.96\n",
       "43               TweedieRegressor   72.966869      0.91"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import time\n",
    "\n",
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "df = pd.DataFrame(diabetes['data'],columns=diabetes['feature_names'])\n",
    "df = pd.concat([df,pd.DataFrame(diabetes['target'],columns=['Target'])],axis=1)\n",
    "df.head()\n",
    "\n",
    "def MLPipeline(df, \n",
    "               project_name,\n",
    "               scaler,\n",
    "               ml_model_type='regressor',\n",
    "               target_column='Target',\n",
    "               test_size=0.2):\n",
    "    \"\"\"\n",
    "    Runs multiple ML algorithms, tracks results with MLflow, and saves models.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe)\n",
    "        project_name (str):\n",
    "        scaler (str): None, normal,standard\n",
    "        ml_model_type (str): Option to pronpt all_estimators as to what model type requested.\n",
    "        classifier, regressor, cluster, transformer\n",
    "\n",
    "    Returns:\n",
    "        None (Results are logged in MLflow)\n",
    "    \"\"\"\n",
    "    # Set up MLflow experiment\n",
    "    mlflow.set_experiment(project_name)\n",
    "\n",
    "    # Prepare data\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Apply Scaler as Necessary\n",
    "    X_train, X_test = apply_scaling(X_train, X_test, scaler=scaler)\n",
    "    \n",
    "    # Get all available models\n",
    "    model_list = all_estimators(type_filter=ml_model_type)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, model_class in model_list:\n",
    "        try:\n",
    "            model = model_class()\n",
    "            start_time = time.time()\n",
    "\n",
    "            with mlflow.start_run(run_name=name):  # Start MLflow run\n",
    "                model.fit(X_train, y_train)  # Train model\n",
    "                y_pred = model.predict(X_test)  # Predict\n",
    "\n",
    "                # Evaluate performance\n",
    "                if ml_model_type == \"classifier\":\n",
    "                    metric = accuracy_score(y_test, y_pred)\n",
    "                    mlflow.log_metric(\"Accuracy\", metric)\n",
    "                else:\n",
    "                    metric = mean_squared_error(y_test, y_pred) ** 0.5  # RMSE manually computed\n",
    "                    mlflow.log_metric(\"RMSE\", metric)\n",
    "\n",
    "                # Log model\n",
    "                mlflow.sklearn.log_model(model, name)\n",
    "\n",
    "                # Log metadata\n",
    "                mlflow.log_param(\"Model\", name)\n",
    "                mlflow.log_param(\"Training Time\", round(time.time() - start_time, 2))\n",
    "\n",
    "                # Append results\n",
    "                results.append({\n",
    "                    \"Model\": name,\n",
    "                    \"Metric\": metric,\n",
    "                    \"Time (s)\": round(time.time() - start_time, 2)\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ {name} failed: {str(e)}\")  # Handle errors but continue\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df#.sort_values(by=\"Metric\", ascending=(task_type == \"regression\"))\n",
    "\n",
    "    return results_df\n",
    "    \n",
    "MLPipeline(df,'Test',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23040b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c62685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import all_estimators\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc332bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a56a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn.utils import all_estimators\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cat\n",
    "\n",
    "# Function to run ML pipeline with MLflow tracking\n",
    "\n",
    "\n",
    "from \n",
    "\n",
    "\n",
    "\n",
    "    # Additional models\n",
    "    extra_models = [\n",
    "        (\"XGBoost\", xgb.XGBClassifier() if task_type == \"classification\" else xgb.XGBRegressor()),\n",
    "        (\"LightGBM\", lgb.LGBMClassifier() if task_type == \"classification\" else lgb.LGBMRegressor()),\n",
    "        (\"CatBoost\", cat.CatBoostClassifier(silent=True) if task_type == \"classification\" else cat.CatBoostRegressor(silent=True))\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "   \n",
    "    # Display results in a pandas table\n",
    "    import ace_tools as tools\n",
    "    tools.display_dataframe_to_user(name=\"ML Model Performance with MLflow\", dataframe=results_df)\n",
    "\n",
    "# Example Usage\n",
    "df = pd.read_csv(\"path_to_your_dataset.csv\")  # Load your dataset\n",
    "run_ml_pipeline_with_mlflow(df, target_column=\"target_column_name\", task_type=\"classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c135ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLManualPipeline(df,\n",
    "                     X_Cols,\n",
    "                     y_Col,\n",
    "                     scaler='MinMaxScaler',\n",
    "                     model_list=['Linear Regression'],\n",
    "                     test_size=.3,\n",
    "                     random_state=42):\n",
    "\n",
    "    if len(X_Cols) == 0:\n",
    "        X = np.array(df.drop(y_Col,axis=1).copy())\n",
    "    else:\n",
    "        X = np.array(df[X_Cols])\n",
    "    \n",
    "    y = df[y_Col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    if scaler =='MinMaxScaler':\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    if len(model_list)==0:\n",
    "        return X_train,X_test,y_train,y_test\n",
    "    \n",
    "    else:\n",
    "        for model in model_list:\n",
    "            if model == 'Linear Regression':\n",
    "                lr = LinearRegression()\n",
    "                lr.fit(X_train, y_train)\n",
    "                y_pred_lr = lr.predict(X_test)\n",
    "                \n",
    "            elif model =='Logistic Regression':\n",
    "                logreg=LogisticRegression()\n",
    "                logreg.fit(X_train, y_train)\n",
    "                y_pred = logreg.predict(X_test)\n",
    "                print(f\"Logisitic Regression Model:\\n{confusion_matrix(y_test, y_pred)}\\n{classification_report(y_test, y_pred)})\")\n",
    "\n",
    "            elif model =='Random Forest':\n",
    "                \n",
    "                ############################################ ESTIMATORS\n",
    "                rf = RandomForestClassifier(random_state=random_state, n_estimators=25)\n",
    "                rf.fit(X_train, y_train)\n",
    "                y_pred_rf = rf.predict(X_test)\n",
    "                print(f\"Random Forest with 25 Nodes?>?>?>:\\n{confusion_matrix(y_test, y_pred_rf)}\\n{classification_report(y_test, y_pred_rf)})\")\n",
    "\n",
    "MLManualPipeline(df=df3.drop(['MEMBERNBR','ATTRITION_FLAG_1M'],axis=1),\n",
    "                 X_Cols=\"\",\n",
    "                 y_Col='ATTRITION_FLAG_2M',\n",
    "                model_list=['Logistic Regression','Random Forest'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a305a7",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39d952",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False),\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a2a68",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f8b4e-f9af-49f4-86c4-6311641a67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(X.shape[1], X.shape[2]), return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict and display results\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "print(\"Predictions:\", y_pred_classes.flatten())\n",
    "print(\"Actual:\", y_test)\n",
    "\n",
    "# Plot accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c75725c",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e84177",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(8,activation='relu'))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(512, activation='tanh'))\n",
    "model.add(layers.Dense(1024, activation='tanh'))\n",
    "model.add(layers.Dense(2028, activation='tanh'))\n",
    "model.add(layers.Dense(512, activation='tanh'))\n",
    "model.add(layers.Dense(512, activation='tanh'))\n",
    "model.add(layers.Dense(128, activation='tanh'))\n",
    "model.add(layers.Dense(32, activation='tanh'))\n",
    "model.add(layers.Dense(1,activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_crossentropy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train.fillna(0), y_train, epochs=50, batch_size=1000, validation_split=0.1, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9601c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature crosses - add non linear relationships to linear model\n",
    "Build up intuition gradually by slowly building model up.\n",
    "Analyze Errors, does it make the mistakes that it’s making.\n",
    "Accuracy and Error are dependent on a specific chosen threshold and are not differentiable. \n",
    "Convexity Good - No matter where you are, you will come to global minimum with following the right direction. \n",
    "Multinominal probability distribution - set of numbers equal to 1. \n",
    "Most Bias Model - Mean. Simple, Generalizes Well. Undercuts.\n",
    "Models have noise, if you overfit the noise, it overfits. Variance.\n",
    "Can perfectly fit n points with Polynomial of N-1. Pure Noise.\n",
    "\n",
    "- What is your baseline\n",
    "\n",
    "Output of a prediction, vs output of a bounding window, vs output which highlights area of interest.\n",
    "\n",
    "Training Population and Usage Population\n",
    "- NA Testing Group vs Asian Testing Group. Men vs Women. Etc..\n",
    "\n",
    "Features - Need to generalize. Name of a car vs Engine, Age, Etc.\n",
    "Turn Learning into a numerical optimization task. Computers really good at this. Opposed to Expert Logic.\n",
    "\n",
    "\n",
    "Loss Function. \n",
    "Model is a function of the inputs. \n",
    "Loss is a function of the parameters of the model.\n",
    "For MSE why a Parabolic Shape? - Squared Function.\n",
    "Unlikely to generate a single Linear Function that perfectly fits all data points. Need to generate some super complex function\n",
    "Positive Gradient - Reduce\n",
    "Negative Gradient - Increase\n",
    "Convergence / divergence\n",
    "Greater the slope, the more we want to change\n",
    "\n",
    "Global vs Local Minimum. No great solution.\n",
    "Linear regression is convex, always get to global minima\n",
    "\n",
    "When predicting a car, you need to find common features which generalize well.\n",
    "\n",
    "Linear Model limitations\n",
    "- Sensitive to size, color, orientation\n",
    "- No concept of edges\n",
    "- No concept of structure relationship between pixels\n",
    "\n",
    "Edge detection\n",
    "- Sharp change pixel intensity\n",
    "- Spatial gradient\n",
    "- Convolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
